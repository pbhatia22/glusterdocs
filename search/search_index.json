{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"GlusterFS Documentation GlusterFS is a scalable network filesystem suitable for data-intensive tasks such as cloud storage and media streaming. GlusterFS is free and open source software and can utilize common off-the-shelf hardware. To learn more, please see the Gluster project home page . Get Started: Quick Start/Installation Guide Since Gluster can be used in different ways and for different tasks, it would be difficult to explain everything at once. We recommend that you follow the Quick Start Guide first. By utilizing a number of virtual machines, you will create a functional test setup to learn the basic concepts. You will then be much better equipped to read the more detailed Install Guide. Quick Start Guide - Start here if you are new to Gluster! Installation Guides describes the prerequisites and provides step-by-instructions to install GlusterFS on various operating systems. Presentations related to Gluster from Conferences and summits. More Documentation Administration Guide - describes the configuration and management of GlusterFS. GlusterFS Developer Guide - describes how you can contribute to this open source project; built through the efforts of its dedicated, passionate community. Upgrade Guide - if you need to upgrade from an older version of GlusterFS. Release Notes - Glusterfs Release Notes provides high-level insight into the improvements and additions that have been implemented in various Glusterfs releases. GlusterFS Tools - Guides for GlusterFS tools. Troubleshooting Guide - Guide for troubleshooting. How to Contribute? The Gluster documentation has its home on GitHub, and the easiest way to contribute is to use the \"Edit on GitHub\" link on the top right corner of each page. If you already have a GitHub account, you can simply edit the document in your browser, use the preview tab, and submit your changes for review in a pull request. If you want to help more with Gluster documentation, please subscribe to the Gluster Users and Gluster Developers mailing lists, and share your ideas with the Gluster developer community.","title":"Home"},{"location":"#glusterfs-documentation","text":"GlusterFS is a scalable network filesystem suitable for data-intensive tasks such as cloud storage and media streaming. GlusterFS is free and open source software and can utilize common off-the-shelf hardware. To learn more, please see the Gluster project home page . Get Started: Quick Start/Installation Guide Since Gluster can be used in different ways and for different tasks, it would be difficult to explain everything at once. We recommend that you follow the Quick Start Guide first. By utilizing a number of virtual machines, you will create a functional test setup to learn the basic concepts. You will then be much better equipped to read the more detailed Install Guide. Quick Start Guide - Start here if you are new to Gluster! Installation Guides describes the prerequisites and provides step-by-instructions to install GlusterFS on various operating systems. Presentations related to Gluster from Conferences and summits. More Documentation Administration Guide - describes the configuration and management of GlusterFS. GlusterFS Developer Guide - describes how you can contribute to this open source project; built through the efforts of its dedicated, passionate community. Upgrade Guide - if you need to upgrade from an older version of GlusterFS. Release Notes - Glusterfs Release Notes provides high-level insight into the improvements and additions that have been implemented in various Glusterfs releases. GlusterFS Tools - Guides for GlusterFS tools. Troubleshooting Guide - Guide for troubleshooting. How to Contribute? The Gluster documentation has its home on GitHub, and the easiest way to contribute is to use the \"Edit on GitHub\" link on the top right corner of each page. If you already have a GitHub account, you can simply edit the document in your browser, use the preview tab, and submit your changes for review in a pull request. If you want to help more with Gluster documentation, please subscribe to the Gluster Users and Gluster Developers mailing lists, and share your ideas with the Gluster developer community.","title":"GlusterFS Documentation"},{"location":"glossary/","text":"Glossary Access Control Lists : Access Control Lists (ACLs) allow you to assign different permissions for different users or groups even though they do not correspond to the original owner or the owning group. Block Storage : Block special files, or block devices, correspond to devices through which the system moves data in the form of blocks. These device nodes often represent addressable devices such as hard disks, CD-ROM drives, or memory regions. GlusterFS requires a filesystem (like XFS) that supports extended attributes. Brick : A Brick is the basic unit of storage in GlusterFS, represented by an export directory on a server in the trusted storage pool. A brick is expressed by combining a server with an export directory in the following format: `SERVER:EXPORT` For example: `myhostname:/exports/myexportdir/` Client : Any machine that mounts a GlusterFS volume. Any applications that use libgfapi access mechanism can also be treated as clients in GlusterFS context. Cluster : A trusted pool of linked computers working together, resembling a single computing resource. In GlusterFS, a cluster is also referred to as a trusted storage pool. Distributed File System : A file system that allows multiple clients to concurrently access data which is spread across servers/bricks in a trusted storage pool. Data sharing among multiple locations is fundamental to all distributed file systems. Extended Attributes : Extended file attributes (abbreviated xattr) is a filesystem feature that enables users/programs to associate files/dirs with metadata. Gluster stores metadata in xattrs. Filesystem : A method of storing and organizing computer files and their data. Essentially, it organizes these files into a database for the storage, organization, manipulation, and retrieval by the computer's operating system. Source Wikipedia FUSE : Filesystem in Userspace (FUSE) is a loadable kernel module for Unix-like computer operating systems that lets non-privileged users create their own file systems without editing kernel code. This is achieved by running file system code in user space while the FUSE module provides only a \"bridge\" to the actual kernel interfaces. Source: Wikipedia GFID : Each file/directory on a GlusterFS volume has a unique 128-bit number associated with it called the GFID. This is analogous to inode in a regular filesystem. glusterd : The Gluster daemon/service that manages volumes and cluster membership. It is required to run on all the servers in the trusted storage pool. Geo-Replication : Geo-replication provides a continuous, asynchronous, and incremental replication service from site to another over Local Area Networks (LANs), Wide Area Network (WANs), and across the Internet. Infiniband InfiniBand is a switched fabric computer network communications link used in high-performance computing and enterprise data centers. Metadata : Metadata is defined as data providing information about one or more other pieces of data. There is no special metadata storage concept in GlusterFS. The metadata is stored with the file data itself usually in the form of extended attributes Namespace : A namespace is an abstract container or environment created to hold a logical grouping of unique identifiers or symbols. Each Gluster volume exposes a single namespace as a POSIX mount point that contains every file in the cluster. Node : A server or computer that hosts one or more bricks. N-way Replication : Local synchronous data replication which is typically deployed across campus or Amazon Web Services Availability Zones. Petabyte : A petabyte (derived from the SI prefix peta- ) is a unit of information equal to one quadrillion (short scale) bytes, or 1000 terabytes. The unit symbol for the petabyte is PB. The prefix peta- (P) indicates a power of 1000: 1 PB = 1,000,000,000,000,000 B = 10005 B = 1015 B. The term \"pebibyte\" (PiB), using a binary prefix, is used for the corresponding power of 1024. Source: Wikipedia POSIX : Portable Operating System Interface (for Unix) is the name of a family of related standards specified by the IEEE to define the application programming interface (API), along with shell and utilities interfaces for software compatible with variants of the Unix operating system Gluster exports a POSIX compatible file system. Quorum : The configuration of quorum in a trusted storage pool determines the number of server failures that the trusted storage pool can sustain. If an additional failure occurs, the trusted storage pool becomes unavailable. Quota : Quota allows you to set limits on usage of disk space by directories or by volumes. RAID : Redundant Array of Inexpensive Disks (RAID) is a technology that provides increased storage reliability through redundancy, combining multiple low-cost, less-reliable disk drives components into a logical unit where all drives in the array are interdependent. RDMA : Remote direct memory access (RDMA) is a direct memory access from the memory of one computer into that of another without involving either one's operating system. This permits high-throughput, low-latency networking, which is especially useful in massively parallel computer clusters Rebalance : The process of redistributing data in a distributed volume when a brick is added or removed. RRDNS : Round Robin Domain Name Service (RRDNS) is a method to distribute load across application servers. It is implemented by creating multiple A records with the same name and different IP addresses in the zone file of a DNS server. Samba : Samba allows file and print sharing between computers running Windows and computers running Linux. It is an implementation of several services and protocols including SMB and CIFS. Scale-Up Storage : Increases the capacity of the storage device in a single dimension. For example, adding additional disk capacity to an existing trusted storage pool. Scale-Out Storage : Scale out systems are designed to scale on both capacity and performance. It increases the capability of a storage device in single dimension. For example, adding more systems of the same size, or adding servers to a trusted storage pool that increases CPU, disk capacity, and throughput for the trusted storage pool. Self-Heal : The self-heal daemon that runs in the background, identifies inconsistencies in files/dirs in a replicated or erasure coded volume and then resolves or heals them. This healing process is usually required when one or more bricks of a volume goes down and then comes up later. Server : The machine (virtual or bare metal) that hosts the bricks in which data is stored. Split-brain : A situation where data on two or more bricks in a replicated volume start to diverge in terms of content or metadata. In this state, one cannot determine programmatically which set of data is \"right\" and which is \"wrong\". Subvolume : A brick after being processed by at least one translator. Translator : Translators (also called xlators) are stackable modules where each module has a very specific purpose. Translators are stacked in a hierarchical structure called as graph. A translator receives data from its parent translator, performs necessary operations and then passes the data down to its child translator in hierarchy. Trusted Storage Pool : A storage pool is a trusted network of storage servers. When you start the first server, the storage pool consists of that server alone. Userspace : Applications running in user space don\u2019t directly interact with hardware, instead using the kernel to moderate access. Userspace applications are generally more portable than applications in kernel space. Gluster is a user space application. Virtual File System (VFS) : VFS is a kernel software layer which handles all system calls related to the standard Linux file system. It provides a common interface to several kinds of file systems. Volume : A volume is a logical collection of bricks. Vol file : Vol files or volume (.vol) files are configuration files that determine the behavior of the Gluster trusted storage pool. It is a textual representation of a collection of modules (also known as translators) that together implement the various functions required.","title":"Glossary"},{"location":"glossary/#glossary","text":"Access Control Lists : Access Control Lists (ACLs) allow you to assign different permissions for different users or groups even though they do not correspond to the original owner or the owning group. Block Storage : Block special files, or block devices, correspond to devices through which the system moves data in the form of blocks. These device nodes often represent addressable devices such as hard disks, CD-ROM drives, or memory regions. GlusterFS requires a filesystem (like XFS) that supports extended attributes. Brick : A Brick is the basic unit of storage in GlusterFS, represented by an export directory on a server in the trusted storage pool. A brick is expressed by combining a server with an export directory in the following format: `SERVER:EXPORT` For example: `myhostname:/exports/myexportdir/` Client : Any machine that mounts a GlusterFS volume. Any applications that use libgfapi access mechanism can also be treated as clients in GlusterFS context. Cluster : A trusted pool of linked computers working together, resembling a single computing resource. In GlusterFS, a cluster is also referred to as a trusted storage pool. Distributed File System : A file system that allows multiple clients to concurrently access data which is spread across servers/bricks in a trusted storage pool. Data sharing among multiple locations is fundamental to all distributed file systems. Extended Attributes : Extended file attributes (abbreviated xattr) is a filesystem feature that enables users/programs to associate files/dirs with metadata. Gluster stores metadata in xattrs. Filesystem : A method of storing and organizing computer files and their data. Essentially, it organizes these files into a database for the storage, organization, manipulation, and retrieval by the computer's operating system. Source Wikipedia FUSE : Filesystem in Userspace (FUSE) is a loadable kernel module for Unix-like computer operating systems that lets non-privileged users create their own file systems without editing kernel code. This is achieved by running file system code in user space while the FUSE module provides only a \"bridge\" to the actual kernel interfaces. Source: Wikipedia GFID : Each file/directory on a GlusterFS volume has a unique 128-bit number associated with it called the GFID. This is analogous to inode in a regular filesystem. glusterd : The Gluster daemon/service that manages volumes and cluster membership. It is required to run on all the servers in the trusted storage pool. Geo-Replication : Geo-replication provides a continuous, asynchronous, and incremental replication service from site to another over Local Area Networks (LANs), Wide Area Network (WANs), and across the Internet. Infiniband InfiniBand is a switched fabric computer network communications link used in high-performance computing and enterprise data centers. Metadata : Metadata is defined as data providing information about one or more other pieces of data. There is no special metadata storage concept in GlusterFS. The metadata is stored with the file data itself usually in the form of extended attributes Namespace : A namespace is an abstract container or environment created to hold a logical grouping of unique identifiers or symbols. Each Gluster volume exposes a single namespace as a POSIX mount point that contains every file in the cluster. Node : A server or computer that hosts one or more bricks. N-way Replication : Local synchronous data replication which is typically deployed across campus or Amazon Web Services Availability Zones. Petabyte : A petabyte (derived from the SI prefix peta- ) is a unit of information equal to one quadrillion (short scale) bytes, or 1000 terabytes. The unit symbol for the petabyte is PB. The prefix peta- (P) indicates a power of 1000: 1 PB = 1,000,000,000,000,000 B = 10005 B = 1015 B. The term \"pebibyte\" (PiB), using a binary prefix, is used for the corresponding power of 1024. Source: Wikipedia POSIX : Portable Operating System Interface (for Unix) is the name of a family of related standards specified by the IEEE to define the application programming interface (API), along with shell and utilities interfaces for software compatible with variants of the Unix operating system Gluster exports a POSIX compatible file system. Quorum : The configuration of quorum in a trusted storage pool determines the number of server failures that the trusted storage pool can sustain. If an additional failure occurs, the trusted storage pool becomes unavailable. Quota : Quota allows you to set limits on usage of disk space by directories or by volumes. RAID : Redundant Array of Inexpensive Disks (RAID) is a technology that provides increased storage reliability through redundancy, combining multiple low-cost, less-reliable disk drives components into a logical unit where all drives in the array are interdependent. RDMA : Remote direct memory access (RDMA) is a direct memory access from the memory of one computer into that of another without involving either one's operating system. This permits high-throughput, low-latency networking, which is especially useful in massively parallel computer clusters Rebalance : The process of redistributing data in a distributed volume when a brick is added or removed. RRDNS : Round Robin Domain Name Service (RRDNS) is a method to distribute load across application servers. It is implemented by creating multiple A records with the same name and different IP addresses in the zone file of a DNS server. Samba : Samba allows file and print sharing between computers running Windows and computers running Linux. It is an implementation of several services and protocols including SMB and CIFS. Scale-Up Storage : Increases the capacity of the storage device in a single dimension. For example, adding additional disk capacity to an existing trusted storage pool. Scale-Out Storage : Scale out systems are designed to scale on both capacity and performance. It increases the capability of a storage device in single dimension. For example, adding more systems of the same size, or adding servers to a trusted storage pool that increases CPU, disk capacity, and throughput for the trusted storage pool. Self-Heal : The self-heal daemon that runs in the background, identifies inconsistencies in files/dirs in a replicated or erasure coded volume and then resolves or heals them. This healing process is usually required when one or more bricks of a volume goes down and then comes up later. Server : The machine (virtual or bare metal) that hosts the bricks in which data is stored. Split-brain : A situation where data on two or more bricks in a replicated volume start to diverge in terms of content or metadata. In this state, one cannot determine programmatically which set of data is \"right\" and which is \"wrong\". Subvolume : A brick after being processed by at least one translator. Translator : Translators (also called xlators) are stackable modules where each module has a very specific purpose. Translators are stacked in a hierarchical structure called as graph. A translator receives data from its parent translator, performs necessary operations and then passes the data down to its child translator in hierarchy. Trusted Storage Pool : A storage pool is a trusted network of storage servers. When you start the first server, the storage pool consists of that server alone. Userspace : Applications running in user space don\u2019t directly interact with hardware, instead using the kernel to moderate access. Userspace applications are generally more portable than applications in kernel space. Gluster is a user space application. Virtual File System (VFS) : VFS is a kernel software layer which handles all system calls related to the standard Linux file system. It provides a common interface to several kinds of file systems. Volume : A volume is a logical collection of bricks. Vol file : Vol files or volume (.vol) files are configuration files that determine the behavior of the Gluster trusted storage pool. It is a textual representation of a collection of modules (also known as translators) that together implement the various functions required.","title":"Glossary"},{"location":"security/","text":"This document is to be considered a \"work in progress\" until this message is removed. Reporting security issues Please report any security issues you find in Gluster projects to: security at gluster.org Anyone can post to this list. The subscribers are only trusted individuals who will handle the resolution of any reported security issues in confidence. In your report, please note how you would like to be credited for discovering the issue and the details of any embargo you would like to impose. [need to check if this holds] Currently, the security response teams for the following distributions are subscribed to this list and will respond to your report: Fedora Red Hat Handling security issues If you represent a Gluster project or a distribution which packages Gluster projects, you are welcome to subscribe to the security at gluster.org mailing list. Your subscription will only be approved if you can demonstrate that you will handle issues in confidence and properly credit reporters for discovering issues. A second mailing list exists for discussion of embargoed security issues: security-private at gluster.org You will be invited to subscribe to this list if you are subscribed to security at gluster.org. Security advisories The security advisories page lists all security vulnerabilities fixed in Gluster. [need to check if this holds]","title":"Security"},{"location":"Administrator-Guide/","text":"Administration Guide Managing a Cluster Managing the Gluster Service Managing Trusted Storage Pools Setting Up Storage Brick Naming Conventions Formatting and Mounting Bricks POSIX Access Control Lists Setting Up Clients Handling of users that belong to many groups Volumes Setting Up Volumes Managing Volumes Modifying .vol files with a filter Configuring NFS-Ganesha Features Replication Geo Replication Quotas Snapshots Trash io_uring Data Access With Other Interfaces Managing Object Store Accessing GlusterFS using Cinder Hosts GlusterFS with Keystone Install Gluster on Top of ZFS Configuring Bareos to store backups on Gluster GlusterFS Service Logs and Locations Monitoring Workload Securing GlusterFS Communication using SSL Puppet Gluster RDMA Transport GlusterFS iSCSI Linux Kernel Tuning Export and Netgroup Authentication Thin Arbiter volumes Trash for GlusterFS Split brain and ways to deal with it Arbiter volumes and quorum options Mandatory Locks GlusterFS coreutilities Events APIs Building QEMU With gfapi For Debian Based Systems Appendices Network Configuration Techniques Performance Testing Tuning Volume Options","title":"Index"},{"location":"Administrator-Guide/#administration-guide","text":"Managing a Cluster Managing the Gluster Service Managing Trusted Storage Pools Setting Up Storage Brick Naming Conventions Formatting and Mounting Bricks POSIX Access Control Lists Setting Up Clients Handling of users that belong to many groups Volumes Setting Up Volumes Managing Volumes Modifying .vol files with a filter Configuring NFS-Ganesha Features Replication Geo Replication Quotas Snapshots Trash io_uring Data Access With Other Interfaces Managing Object Store Accessing GlusterFS using Cinder Hosts GlusterFS with Keystone Install Gluster on Top of ZFS Configuring Bareos to store backups on Gluster GlusterFS Service Logs and Locations Monitoring Workload Securing GlusterFS Communication using SSL Puppet Gluster RDMA Transport GlusterFS iSCSI Linux Kernel Tuning Export and Netgroup Authentication Thin Arbiter volumes Trash for GlusterFS Split brain and ways to deal with it Arbiter volumes and quorum options Mandatory Locks GlusterFS coreutilities Events APIs Building QEMU With gfapi For Debian Based Systems Appendices Network Configuration Techniques Performance Testing Tuning Volume Options","title":"Administration Guide"},{"location":"Administrator-Guide/Access-Control-Lists/","text":"POSIX Access Control Lists POSIX Access Control Lists (ACLs) allows you to assign different permissions for different users or groups even though they do not correspond to the original owner or the owning group. For example: User john creates a file but does not want to allow anyone to do anything with this file, except another user, antony (even though there are other users that belong to the group john). This means, in addition to the file owner, the file group, and others, additional users and groups can be granted or denied access by using POSIX ACLs. Activating POSIX ACLs Support To use POSIX ACLs for a file or directory, the partition of the file or directory must be mounted with POSIX ACLs support. Activating POSIX ACLs Support on Server To mount the backend export directories for POSIX ACLs support, use the following command: # mount -o acl For example: # mount -o acl /dev/sda1 /export1 Alternatively, if the partition is listed in the /etc/fstab file, add the following entry for the partition to include the POSIX ACLs option: LABEL=/work /export1 ext3 rw, acl 14 Activating POSIX ACLs Support on Client To mount the glusterfs volumes for POSIX ACLs support, use the following command: # mount \u2013t glusterfs -o acl For example: # mount -t glusterfs -o acl 198.192.198.234:glustervolume /mnt/gluster Setting POSIX ACLs You can set two types of POSIX ACLs, that is, access ACLs and default ACLs. You can use access ACLs to grant permission for a specific file or directory. You can use default ACLs only on a directory but if a file inside that directory does not have an ACLs, it inherits the permissions of the default ACLs of the directory. You can set ACLs for per user, per group, for users not in the user group for the file, and via the effective right mask. Setting Access ACLs You can apply access ACLs to grant permission for both files and directories. To set or modify Access ACLs You can set or modify access ACLs use the following command: # setfacl \u2013m file The ACL entry types are the POSIX ACLs representations of owner, group, and other. Permissions must be a combination of the characters r (read), w (write), and x (execute). You must specify the ACL entry in the following format and can specify multiple entry types separated by commas. ACL Entry Description u:uid:\\<permission> Sets the access ACLs for a user. You can specify user name or UID g:gid:\\<permission> Sets the access ACLs for a group. You can specify group name or GID. m:\\<permission> Sets the effective rights mask. The mask is the combination of all access permissions of the owning group and all of the user and group entries. o:\\<permission> Sets the access ACLs for users other than the ones in the group for the file. If a file or directory already has a POSIX ACLs, and the setfacl command is used, the additional permissions are added to the existing POSIX ACLs or the existing rule is modified. For example, to give read and write permissions to user antony: # setfacl -m u:antony:rw /mnt/gluster/data/testfile Setting Default ACLs You can apply default ACLs only to directories. They determine the permissions of a file system objects that inherits from its parent directory when it is created. To set default ACLs You can set default ACLs for files and directories using the following command: # setfacl \u2013m \u2013-set Permissions must be a combination of the characters r (read), w (write), and x (execute). Specify the ACL entry_type as described below, separating multiple entry types with commas. u: user_name:permissions Sets the access ACLs for a user. Specify the user name, or the UID. g: group_name:permissions Sets the access ACLs for a group. Specify the group name, or the GID. m: permission Sets the effective rights mask. The mask is the combination of all access permissions of the owning group, and all user and group entries. o: permissions Sets the access ACLs for users other than the ones in the group for the file. For example, to set the default ACLs for the /data directory to read for users not in the user group: # setfacl \u2013m --set o::r /mnt/gluster/data Note An access ACLs set for an individual file can override the default ACLs permissions. Effects of a Default ACLs The following are the ways in which the permissions of a directory's default ACLs are passed to the files and subdirectories in it: A subdirectory inherits the default ACLs of the parent directory both as its default ACLs and as an access ACLs. A file inherits the default ACLs as its access ACLs. Retrieving POSIX ACLs You can view the existing POSIX ACLs for a file or directory. To view existing POSIX ACLs View the existing access ACLs of a file using the following command: # getfacl For example, to view the existing POSIX ACLs for sample.jpg # getfacl /mnt/gluster/data/test/sample.jpg # owner: antony # group: antony user::rw- group::rw- other::r-- View the default ACLs of a directory using the following command: # getfacl For example, to view the existing ACLs for /data/doc # getfacl /mnt/gluster/data/doc # owner: antony # group: antony user::rw- user:john:r-- group::r-- mask::r-- other::r-- default:user::rwx default:user:antony:rwx default:group::r-x default:mask::rwx default:other::r-x Removing POSIX ACLs To remove all the permissions for a user, groups, or others, use the following command: # setfacl -x setfaclentry_type Options The ACL entry_type translates to the POSIX ACL representations of owner, group, and other. Permissions must be a combination of the characters r (read), w (write), and x (execute). Specify the ACL entry_type as described below, separating multiple entry types with commas. u: user_name Sets the access ACLs for a user. Specify the user name, or the UID. g: group_name Sets the access ACLs for a group. Specify the group name, or the GID. m: permission Sets the effective rights mask. The mask is the combination of all access permissions of the owning group, and all user and group entries. o: permissions Sets the access ACLs for users other than the ones in the group for the file. For example, to remove all permissions from the user antony: # setfacl -x u:antony /mnt/gluster/data/test-file Samba and ACLs If you are using Samba to access GlusterFS FUSE mount, then POSIX ACLs are enabled by default. Samba has been compiled with the --with-acl-support option, so no special flags are required when accessing or mounting a Samba share. NFS and ACLs Currently GlusterFS supports POSIX ACL configuration through NFS mount, i.e. setfacl and getfacl commands work through NFS mount.","title":"Access Control Lists"},{"location":"Administrator-Guide/Access-Control-Lists/#posix-access-control-lists","text":"POSIX Access Control Lists (ACLs) allows you to assign different permissions for different users or groups even though they do not correspond to the original owner or the owning group. For example: User john creates a file but does not want to allow anyone to do anything with this file, except another user, antony (even though there are other users that belong to the group john). This means, in addition to the file owner, the file group, and others, additional users and groups can be granted or denied access by using POSIX ACLs.","title":"POSIX Access Control Lists"},{"location":"Administrator-Guide/Access-Control-Lists/#activating-posix-acls-support","text":"To use POSIX ACLs for a file or directory, the partition of the file or directory must be mounted with POSIX ACLs support.","title":"Activating POSIX ACLs Support"},{"location":"Administrator-Guide/Access-Control-Lists/#activating-posix-acls-support-on-server","text":"To mount the backend export directories for POSIX ACLs support, use the following command: # mount -o acl For example: # mount -o acl /dev/sda1 /export1 Alternatively, if the partition is listed in the /etc/fstab file, add the following entry for the partition to include the POSIX ACLs option: LABEL=/work /export1 ext3 rw, acl 14","title":"Activating POSIX ACLs Support on Server"},{"location":"Administrator-Guide/Access-Control-Lists/#activating-posix-acls-support-on-client","text":"To mount the glusterfs volumes for POSIX ACLs support, use the following command: # mount \u2013t glusterfs -o acl For example: # mount -t glusterfs -o acl 198.192.198.234:glustervolume /mnt/gluster","title":"Activating POSIX ACLs Support on Client"},{"location":"Administrator-Guide/Access-Control-Lists/#setting-posix-acls","text":"You can set two types of POSIX ACLs, that is, access ACLs and default ACLs. You can use access ACLs to grant permission for a specific file or directory. You can use default ACLs only on a directory but if a file inside that directory does not have an ACLs, it inherits the permissions of the default ACLs of the directory. You can set ACLs for per user, per group, for users not in the user group for the file, and via the effective right mask.","title":"Setting POSIX ACLs"},{"location":"Administrator-Guide/Access-Control-Lists/#setting-access-acls","text":"You can apply access ACLs to grant permission for both files and directories. To set or modify Access ACLs You can set or modify access ACLs use the following command: # setfacl \u2013m file The ACL entry types are the POSIX ACLs representations of owner, group, and other. Permissions must be a combination of the characters r (read), w (write), and x (execute). You must specify the ACL entry in the following format and can specify multiple entry types separated by commas. ACL Entry Description u:uid:\\<permission> Sets the access ACLs for a user. You can specify user name or UID g:gid:\\<permission> Sets the access ACLs for a group. You can specify group name or GID. m:\\<permission> Sets the effective rights mask. The mask is the combination of all access permissions of the owning group and all of the user and group entries. o:\\<permission> Sets the access ACLs for users other than the ones in the group for the file. If a file or directory already has a POSIX ACLs, and the setfacl command is used, the additional permissions are added to the existing POSIX ACLs or the existing rule is modified. For example, to give read and write permissions to user antony: # setfacl -m u:antony:rw /mnt/gluster/data/testfile","title":"Setting Access ACLs"},{"location":"Administrator-Guide/Access-Control-Lists/#setting-default-acls","text":"You can apply default ACLs only to directories. They determine the permissions of a file system objects that inherits from its parent directory when it is created. To set default ACLs You can set default ACLs for files and directories using the following command: # setfacl \u2013m \u2013-set Permissions must be a combination of the characters r (read), w (write), and x (execute). Specify the ACL entry_type as described below, separating multiple entry types with commas. u: user_name:permissions Sets the access ACLs for a user. Specify the user name, or the UID. g: group_name:permissions Sets the access ACLs for a group. Specify the group name, or the GID. m: permission Sets the effective rights mask. The mask is the combination of all access permissions of the owning group, and all user and group entries. o: permissions Sets the access ACLs for users other than the ones in the group for the file. For example, to set the default ACLs for the /data directory to read for users not in the user group: # setfacl \u2013m --set o::r /mnt/gluster/data Note An access ACLs set for an individual file can override the default ACLs permissions. Effects of a Default ACLs The following are the ways in which the permissions of a directory's default ACLs are passed to the files and subdirectories in it: A subdirectory inherits the default ACLs of the parent directory both as its default ACLs and as an access ACLs. A file inherits the default ACLs as its access ACLs.","title":"Setting Default ACLs"},{"location":"Administrator-Guide/Access-Control-Lists/#retrieving-posix-acls","text":"You can view the existing POSIX ACLs for a file or directory. To view existing POSIX ACLs View the existing access ACLs of a file using the following command: # getfacl For example, to view the existing POSIX ACLs for sample.jpg # getfacl /mnt/gluster/data/test/sample.jpg # owner: antony # group: antony user::rw- group::rw- other::r-- View the default ACLs of a directory using the following command: # getfacl For example, to view the existing ACLs for /data/doc # getfacl /mnt/gluster/data/doc # owner: antony # group: antony user::rw- user:john:r-- group::r-- mask::r-- other::r-- default:user::rwx default:user:antony:rwx default:group::r-x default:mask::rwx default:other::r-x","title":"Retrieving POSIX ACLs"},{"location":"Administrator-Guide/Access-Control-Lists/#removing-posix-acls","text":"To remove all the permissions for a user, groups, or others, use the following command: # setfacl -x","title":"Removing POSIX ACLs"},{"location":"Administrator-Guide/Access-Control-Lists/#setfaclentry_type-options","text":"The ACL entry_type translates to the POSIX ACL representations of owner, group, and other. Permissions must be a combination of the characters r (read), w (write), and x (execute). Specify the ACL entry_type as described below, separating multiple entry types with commas. u: user_name Sets the access ACLs for a user. Specify the user name, or the UID. g: group_name Sets the access ACLs for a group. Specify the group name, or the GID. m: permission Sets the effective rights mask. The mask is the combination of all access permissions of the owning group, and all user and group entries. o: permissions Sets the access ACLs for users other than the ones in the group for the file. For example, to remove all permissions from the user antony: # setfacl -x u:antony /mnt/gluster/data/test-file","title":"setfaclentry_type Options"},{"location":"Administrator-Guide/Access-Control-Lists/#samba-and-acls","text":"If you are using Samba to access GlusterFS FUSE mount, then POSIX ACLs are enabled by default. Samba has been compiled with the --with-acl-support option, so no special flags are required when accessing or mounting a Samba share.","title":"Samba and ACLs"},{"location":"Administrator-Guide/Access-Control-Lists/#nfs-and-acls","text":"Currently GlusterFS supports POSIX ACL configuration through NFS mount, i.e. setfacl and getfacl commands work through NFS mount.","title":"NFS and ACLs"},{"location":"Administrator-Guide/Accessing-Gluster-from-Windows/","text":"Accessing Gluster volume via SMB Protocol Layered product Samba is used to export the Gluster volume and ctdb for providing the high availability Samba. Here are the steps to configure Highly Available Samba cluster to export Gluster volume. Note : These configuration steps are applicable to Samba version = 4.1. and Gluster Version >= 3.7. ctdb >= 2.5 Step 1: Choose the servers that will export the Gluster volume. The servers may/may not be part of the trusted storage pool. Preferable number of servers is <=4. Install Samba and ctdb packages on these servers. Step 2: Enable/Disable the auto export of Gluster volume via SMB # gluster volume set VOLNAME user.smb disable/enable Step 3: Setup the CTDB Cluster: Create a ctdb meta volume with replica N, N being the number of the servers that are used as Samba servers. This volume will host only a zero byte lock file, hence choose the minimal sized bricks. To create the n replica volume run the following command: # gluster volume create <volname> replica n <ipaddr/host name>:/<brick_patch>.... N times In the following files, replace \"all\" in the statement \"META=all\" to the newly created volume name. /var/lib/glusterd/hooks/1/start/post/S29CTDBsetup.sh /var/lib/glusterd/hooks/1/stop/pre/S29CTDB-teardown.sh Start the ctdb volume # gluster vol start <volname> Verify the following: If the following lines are added in smb.conf file in all the nodes running samba/ctdb: clustering = yes idmap backend = tdb2 If the ctdb volume is mounted at /gluster/lock on all the nodes that runs ctdb/samba If the mount entry for ctdb volume is added in /etc/fstab If file /etc/sysconfig/ctdb exists on all the nodes that runs ctdb/samba Create /etc/ctdb/nodes files on all the nodes that runs ctdb/samba, and add the IPs of all these nodes in the file. For example, # cat /etc/ctdb/nodes 10.16.157.0 10.16.157.3 10.16.157.6 10.16.157.8 The IPs listed here are the private IPs of Samba/ctdb servers, which should be a private non-routable subnet and are only used for internal cluster traffic. For more details refer to the ctdb man page. Create /etc/ctdb/public_addresses files on all the nodes that runs ctdb/samba, and add the virtual IPs in the following format: <virtual IP><routing prefix> <node interface> Eg: # cat /etc/ctdb/public_addresses 192.168.1.20/24 eth0 192.168.1.21/24 eth0 Either uncomment CTDB_SAMBA_SKIP_SHARE_CHECK=yes or add CTDB_SAMBA_SKIP_SHARE_CHECK=yes in its absence inside /etc/ctdb/script.options to disable checking of the shares by ctdb. If SELinux is enabled and enforcing, try the following command if ctdb fails. # setsebool -P use_fusefs_home_dirs 1 # setsebool -P samba_load_libgfapi 1 Step 4: Performance tunings before exporting the volume To ensure lock and IO coherency: # gluster volume set VOLNAME storage.batch-fsync-delay-usec 0 If using Samba 4.X version add the following line in smb.conf in the global section kernel share modes = no kernel oplocks = no map archive = no map hidden = no map read only = no map system = no store dos attributes = yes Note: Setting 'store dos attributes = no' is recommended if archive/hidden/read-only dos attributes are not used. This can give better performance. If you are using gluster5 or higher execute the following to improve performance: # gluster volume set VOLNAME group samba On older version, please execute the following: # gluster volume set VOLNAME features.cache-invalidation on # gluster volume set VOLNAME features.cache-invalidation-timeout 600 # gluster volume set VOLNAME performance.cache-samba-metadata on # gluster volume set VOLNAME performance.stat-prefetch on # gluster volume set VOLNAME performance.cache-invalidation on # gluster volume set VOLNAME performance.md-cache-timeout 600 # gluster volume set VOLNAME network.inode-lru-limit 200000 # gluster volume set VOLNAME performance.nl-cache on # gluster volume set VOLNAME performance.nl-cache-timeout 600 # gluster volume set VOLNAME performance.readdir-ahead on # gluster volume set VOLNAME performance.parallel-readdir on Tune the number of threads in gluster for better performance: # gluster volume set VOLNAME client.event-threads 4 # gluster volume set VOLNAME server.event-threads 4 # Increasing to a very high value will reduce the performance Step 5: Mount the volume using SMB If no Active directory setup add the user on all the samba server and set the password # adduser USERNAME # smbpasswd -a USERNAME Start the ctdb, smb and other related services: # systemctl re/start ctdb # ctdb status # ctdb ip # ctdb ping -n all To verify if the volume exported by samba can be accessed by a user: # smbclient //<hostname>/gluster-<volname> -U <username>%<password> To mount on a linux system: # mount -t cifs -o user=<username>,pass=<password> //<Virtual IP>/gluster-<volname> /<mountpoint> To mount on Windows system: >net use <device:> \\\\<Virtual IP>\\gluster-<volname> OR \\\\<Virtual IP>\\gluster-<volname> from windows explorer.","title":"Accessing Gluster volume via SMB Protocol"},{"location":"Administrator-Guide/Accessing-Gluster-from-Windows/#accessing-gluster-volume-via-smb-protocol","text":"Layered product Samba is used to export the Gluster volume and ctdb for providing the high availability Samba. Here are the steps to configure Highly Available Samba cluster to export Gluster volume. Note : These configuration steps are applicable to Samba version = 4.1. and Gluster Version >= 3.7. ctdb >= 2.5","title":"Accessing Gluster volume via SMB Protocol"},{"location":"Administrator-Guide/Accessing-Gluster-from-Windows/#step-1-choose-the-servers-that-will-export-the-gluster-volume","text":"The servers may/may not be part of the trusted storage pool. Preferable number of servers is <=4. Install Samba and ctdb packages on these servers.","title":"Step 1: Choose the servers that will export the Gluster volume."},{"location":"Administrator-Guide/Accessing-Gluster-from-Windows/#step-2-enabledisable-the-auto-export-of-gluster-volume-via-smb","text":"# gluster volume set VOLNAME user.smb disable/enable","title":"Step 2: Enable/Disable the auto export of Gluster volume via SMB"},{"location":"Administrator-Guide/Accessing-Gluster-from-Windows/#step-3-setup-the-ctdb-cluster","text":"Create a ctdb meta volume with replica N, N being the number of the servers that are used as Samba servers. This volume will host only a zero byte lock file, hence choose the minimal sized bricks. To create the n replica volume run the following command: # gluster volume create <volname> replica n <ipaddr/host name>:/<brick_patch>.... N times In the following files, replace \"all\" in the statement \"META=all\" to the newly created volume name. /var/lib/glusterd/hooks/1/start/post/S29CTDBsetup.sh /var/lib/glusterd/hooks/1/stop/pre/S29CTDB-teardown.sh Start the ctdb volume # gluster vol start <volname> Verify the following: If the following lines are added in smb.conf file in all the nodes running samba/ctdb: clustering = yes idmap backend = tdb2 If the ctdb volume is mounted at /gluster/lock on all the nodes that runs ctdb/samba If the mount entry for ctdb volume is added in /etc/fstab If file /etc/sysconfig/ctdb exists on all the nodes that runs ctdb/samba Create /etc/ctdb/nodes files on all the nodes that runs ctdb/samba, and add the IPs of all these nodes in the file. For example, # cat /etc/ctdb/nodes 10.16.157.0 10.16.157.3 10.16.157.6 10.16.157.8 The IPs listed here are the private IPs of Samba/ctdb servers, which should be a private non-routable subnet and are only used for internal cluster traffic. For more details refer to the ctdb man page. Create /etc/ctdb/public_addresses files on all the nodes that runs ctdb/samba, and add the virtual IPs in the following format: <virtual IP><routing prefix> <node interface> Eg: # cat /etc/ctdb/public_addresses 192.168.1.20/24 eth0 192.168.1.21/24 eth0 Either uncomment CTDB_SAMBA_SKIP_SHARE_CHECK=yes or add CTDB_SAMBA_SKIP_SHARE_CHECK=yes in its absence inside /etc/ctdb/script.options to disable checking of the shares by ctdb. If SELinux is enabled and enforcing, try the following command if ctdb fails. # setsebool -P use_fusefs_home_dirs 1 # setsebool -P samba_load_libgfapi 1","title":"Step 3: Setup the CTDB Cluster:"},{"location":"Administrator-Guide/Accessing-Gluster-from-Windows/#step-4-performance-tunings-before-exporting-the-volume","text":"To ensure lock and IO coherency: # gluster volume set VOLNAME storage.batch-fsync-delay-usec 0 If using Samba 4.X version add the following line in smb.conf in the global section kernel share modes = no kernel oplocks = no map archive = no map hidden = no map read only = no map system = no store dos attributes = yes Note: Setting 'store dos attributes = no' is recommended if archive/hidden/read-only dos attributes are not used. This can give better performance. If you are using gluster5 or higher execute the following to improve performance: # gluster volume set VOLNAME group samba On older version, please execute the following: # gluster volume set VOLNAME features.cache-invalidation on # gluster volume set VOLNAME features.cache-invalidation-timeout 600 # gluster volume set VOLNAME performance.cache-samba-metadata on # gluster volume set VOLNAME performance.stat-prefetch on # gluster volume set VOLNAME performance.cache-invalidation on # gluster volume set VOLNAME performance.md-cache-timeout 600 # gluster volume set VOLNAME network.inode-lru-limit 200000 # gluster volume set VOLNAME performance.nl-cache on # gluster volume set VOLNAME performance.nl-cache-timeout 600 # gluster volume set VOLNAME performance.readdir-ahead on # gluster volume set VOLNAME performance.parallel-readdir on Tune the number of threads in gluster for better performance: # gluster volume set VOLNAME client.event-threads 4 # gluster volume set VOLNAME server.event-threads 4 # Increasing to a very high value will reduce the performance","title":"Step 4: Performance tunings before exporting the volume"},{"location":"Administrator-Guide/Accessing-Gluster-from-Windows/#step-5-mount-the-volume-using-smb","text":"If no Active directory setup add the user on all the samba server and set the password # adduser USERNAME # smbpasswd -a USERNAME Start the ctdb, smb and other related services: # systemctl re/start ctdb # ctdb status # ctdb ip # ctdb ping -n all To verify if the volume exported by samba can be accessed by a user: # smbclient //<hostname>/gluster-<volname> -U <username>%<password> To mount on a linux system: # mount -t cifs -o user=<username>,pass=<password> //<Virtual IP>/gluster-<volname> /<mountpoint> To mount on Windows system: >net use <device:> \\\\<Virtual IP>\\gluster-<volname> OR \\\\<Virtual IP>\\gluster-<volname> from windows explorer.","title":"Step 5: Mount the volume using SMB"},{"location":"Administrator-Guide/Automatic-File-Replication/","text":"This doc contains information about the synchronous replication module in gluster and has two sections -Replication logic and Self-heal logic. 1. Replication logic AFR is the module (translator) in glusterfs that provides all the features that you would expect of any synchronous replication system: Simultaneous updating of all copies of data on the replica bricks when a client modifies it. Providing continued data availability to clients when say one brick of the replica set goes down. Automatic self-healing of any data that was modified when the brick that was down, once it comes back up, ensuring consistency of data on all the bricks of the replica. 1 and 2 are in the I/O path while 3 is done either in the I/O path (in the background) or via the self-heal daemon. Each gluster translator implements what are known as File Operations (FOPs) which are mapped to the I/O syscalls which the application makes. For example, AFR has afr_writev that gets invoked when application does a write(2) . As is obvious, all FOPs fall into one of 2 types: i) Read based FOPs which only get informtion from and don\u2019t modify the file in any way. viz: afr_readdir, afr_access, afr_stat, afr_fstat, afr_readlink, afr_getxattr, afr_fgetxattr, afr_readv,afr_seek ii) Write based FOPs which change the file or its attributes. viz: afr_create, afr_mknod,afr_mkdir,afr_link, afr_symlink, afr_rename, afr_unlink, afr_rmdir, afr_do_writev, afr_truncate, afr_ftruncate, afr_setattr, afr_fsetattr, afr_setxattr, afr_fsetxattr, afr_removexattr, afr_fremovexattr, afr_fallocate, afr_discard, afr_zerofill, afr_xattrop, afr_fxattrop, afr_fsync. AFR follows a transaction model for both types of FOPs. Read transactions: For every file in the replica, AFR has an in-memory notion/array called \u2018readables\u2019 which indicate whether each brick of the replica is a good copy or a bad one (i.e. in need of a heal). In a healthy state, all bricks are readable and a read FOP will be served from any one of the readable bricks. The read-hash-mode volume option decides which brick is the chosen one. #[root@tuxpad glusterfs]# gluster volume set help|grep read-hash-mode -A7 Option: cluster.read-hash-mode Default Value: 1 Description: inode-read fops happen only on one of the bricks in replicate. AFR will prefer the one computed using the method specified using this option. 0 = first readable child of AFR, starting from 1st child. 1 = hash by GFID of file (all clients use same subvolume). 2 = hash by GFID of file and client PID. 3 = brick having the least outstanding read requests. If the brick is bad for a given file (i.e. it is pending heal), then it won\u2019t be marked readable to begin with. The readables array is populated based on the on-disk AFR xattrs for the file during lookup. These xattrs indicate which bricks are good and which ones are bad. We will see more about these xattrs in the write transactions section below. If the FOP fails on the chosen readable brick, AFR attempts it on the next readable one, until all are exhausted. If the FOP doesn\u2019t succeed on any of the readables, then the application receives an error. Write transactions: Every write based FOP employs a write transaction model which consists of 5 phases: 1) The lock phase Take locks on the file being modified on all bricks so that AFRs of other clients are blocked if they try to modify the same file simultaneously. 2) The pre-op phase Increment the \u2018dirty\u2019 xattr (trusted.afr.dirty) by 1 on all participating bricks as an indication of an impending FOP (in the next phase) 3) The FOP phase Perform the actual FOP (say a setfattr) on all bricks. 4) The post-op phase Decrement the dirty xattr by 1 on bricks where the FOP was successful. In addition, also increment the \u2018pending\u2019 xattr (trusted.afr.$VOLNAME-client-x) xattr on the success bricks to \u2018blame\u2019 the bricks where the FOP failed. 5) The unlock phase Release the locks that were taken in phase 1. Any competing client can now go ahead with its own write transaction. Note : There are certain optimizations done at the code level which reduce the no. of lock/unlock phases done for a transaction by piggybacking on the previous transaction\u2019s locks. These optimizations (eager-locking, piggybacking and delayed post-op) beyond the scope of this post. AFR returns sucess for these FOPs only if they meet quorum. For replica 2, this means it needs to suceed on any one brick. For replica 3, it is two out of theree and so on. More on the AFR xattrs: We saw that AFR modifies the dirty and pending xattrs in the pre-op and post-op phases. To be more precise, only parts of the xattr are modified in a given transaction. Which bytes are modified depends on the type of write transaction which the FOP belongs to. Transaction Type FOPs that belong to it AFR_DATA_TRANSACTION afr_writev, afr_truncate, afr_ftruncate, afr_fsync, afr_fallocate, afr_discard, afr_zerofill AFR_METADATA_TRANSACTION afr_setattr, afr_fsetattr, afr_setxattr, afr_fsetxattr, afr_removexattr, afr_fremovexattr, afr_xattrop, afr_fxattrop AFR_ENTRY_TRANSACTION afr_create, afr_mknod, afr_mkdir, afr_link, afr_symlink, afr_rename, afr_unlink, afr_rmdir Stop here and convince yourself that given a write based FOP, you can say which one of the 3 transaction types it belongs to. Note: In the code, there is also a AFR_ENTRY_RENAME_TRANSACTION (used by afr_rename) but it is safe to assume that it is identical to AFR_ENTRY_TRANSACTION as far as interpreting the xattrs are concerned. Consider the xttr: trusted.afr.dirty=0x000000000000000000000000 The first 4 bytes of the xattr are used for data transactions, the next 4 bytes for metadata transactions and the last 4 for entry transactions. Let us see some examples of how the xattr would look like for various types of FOPs during a transaction: | FOP | Value after pre-op phase | Value after post-op phase | |-------------|------------------------------------------------|------------------------------------------------| | afr_writev | trusted.afr.dirty=0x00000001 00000000 00000000 | trusted.afr.dirty=0x00000000 00000000 00000000 | | afr_setattr | trusted.afr.dirty=0x00000000 00000001 00000000 | trusted.afr.dirty=0x00000000 00000000 00000000 | | afr_create | trusted.afr.dirty=0x00000000 00000000 00000001 | trusted.afr.dirty=0x00000000 00000000 00000000 | Thus depending on the type of FOP (i.e. data/ metadata/ entry transaction), different set of bytes of the dirty xattr get incremented/ decremented. Modification of the pending xattr also follows the same pattern, execept it is incremented only in the post-op phase if the FOP fails on some bricks. Example: Let us say a write was performed on a file, say FILE1, on replica 3 volume called \u2018testvol\u2019. Suppose the lock and pre-op phase succeeded on all bricks. After that the 3rd brick went down, and the transaction completed successfully on the first 2 bricks. What will be the state of the afr xattrs on all bricks? [root@tuxpad ravi]# getfattr -d -m . -e hex /bricks/brick1/FILE1|grep afr getfattr: Removing leading '/' from absolute path names trusted.afr.dirty=0x000000000000000000000000 trusted.afr.testvol-client-2=0x000000010000000000000000 [root@tuxpad ravi]# getfattr -d -m . -e hex /bricks/brick2/FILE1|grep afr getfattr: Removing leading '/' from absolute path names trusted.afr.dirty=0x000000000000000000000000 trusted.afr.testvol-client-2=0x000000010000000000000000 [root@tuxpad ravi]# getfattr -d -m . -e hex /bricks/brick3/FILE1|grep afr getfattr: Removing leading '/' from absolute path names trusted.afr.dirty=0x000000010000000000000000 So Brick3 will still have the dirty xattr set because it went down before the post-op had a chance to decrement it. Bricks 1 and 2 will have a zero dirty xattr and in addition, a non-zero pending xattr set. The client-2 in trusted.afr.testvol-client-2 indicates that the 3rd brick is bad and has some pending data operations. 2. Self-heal logic. We already know that AFR increments and/or decrements the dirty (i.e. trusted.afr.dirty ) and pending (i.e. trusted.afr.$VOLNAME-client-x ) xattrs during the different phases of the transaction. For a given file (or directory), an all zero value of these xattrs or the total absence of these xattrs on all bricks of the replica mean the file is healthy and does not need heal. If any of these xattrs are non-zero even on one of the bricks, then the file is a candidate for heal- it as simple as that. When we say these xattrs are non-zero, it is in the context of no on-going I/O going from client(s) on the file. Otherwise the non-zero values that one observes might be transient as the write transaction is progressing through its five phases. Of course, as an admin, you wouldn\u2019t need to figure out all of this. Just running the heal info set of commands should give you the list of files that need heal. So if self-heal observes a file with non-zero xattrs, it does the following steps: Fetch the afr xattrs, examine which set of 8 bytes are non-zero and determine the corresponding heals that are needed on the file \u2013 i.e. data heal/ metadata heal/ entry heal . Determine which bricks are good (a.k.a. \u2018sources\u2019) and which ones are bad (a.k.a. \u2018sinks\u2019) for each of those heals by interpretting the xattr values. Pick one source brick and heal the file on to all the sink bricks. If the heal is successful, reset the afr xattrs to zero. This is a rather simplified description and I have omitted details about various locks that each of these steps need to take because self-heal and client I/O can happen in parallel on the file. Or even multiple self-heal daemons (described later) can attempt to heal the same file. Data heal : Happens only for files. The contents of the file are copied from the source to the sink bricks. Entry heal : Happens only for directories. Entries (i.e. files and subdirs) under a given directory are deleted from the sinks if they are not present in the source. Likewise, entries are created on the sinks if they are not present in the source. Metadata heal: Happens for both files and directories. File ownership, file permissions and extended attributes are copied from the source to the sink bricks. It can be possible that for a given file, one set of bricks can be the source for data heal while another set could be the source for metadata heals. It all depends on which FOPs failed on what bricks and therefore what set of bytes are non-zero for the afr xattrs. When do self-heals happen? There are two places from which the steps described above for healing can be carried out: i) From the client side. Client-side heals are triggered when the file is accessed from the client (mount). AFR uses a monotonically increasing generation number to keep track of disconnect/connect of its children (i.e. the client translators) to the bricks. When this \u2018event generation\u2019 number changes, the file\u2019s inode is marked as a candidate for refresh. When the next FOP comes on such an inode, a refresh is triggered to update the readables during which a heal is launched (if the AFR xattrs indicate that a heal is needed, that is). This heal happens in the background, meaning it does not block the actual FOP which will continue as usual post the refresh. Specific client-side heals can be turned off by disabling the 3 corresponding volume options: cluster.metadata-self-heal cluster.data-self-heal cluster.entry-self-heal The number of client-side heals that happen in the background can be tuned via the following volume options: background-self-heal-count heal-wait-queue-length See the gluster volume set help for more information on all the above options. Name heal : Name heal is just healing of the file/directory name when it is accessed. For example, say a file is created and written to when a brick is down and all the 3 client side heals are disabled. When the brick comes up and the next I/O comes on it, the file name is created on it as a part of lookup. Its contents/metadata are not healed though. Name heal cannot be disabled. It is there to ensure that the namespace is consistent on all bricks as soon as the file is accessed. ii) By the self-heal daemon. There is a self-heal daemon process (glutershd) that runs on every node of the trusted storage pool. It is a light weight client process consisting mainly of AFR ant the protocol/client translators. It can talk to all bricks of all the replicate volume(s) of the pool. It periodically crawls (every 10 minutes by default; tunable via the heal-timeout volume option) the list of files that need heal and does their healing. As you can see, client side heal is done upon file access but glustershd processes the heal backlog pro-actively. Index heal: But how does glustershd know which files it needs to heal? Where does it get the list from? So in part-1, while we saw the five phases of the AFR write transaction, we left out one detail: In the pre-op phase, in addition to marking the dirty xattr, each brick also stores the gfid string of the file inside its .glusterfs/indices/dirty directory. Likewise, in the post-op phase, it removes the gfid string from its .glusterfs/indices/dirty If addition, if the write failed on some brick, the good bricks will stores the gfid string inside the .glusterfs/indices/xattrop directory. Thus when no I/O is happening on a file and you still find its gfid inside .glusterfs/indices/dirty of a particular brick, it means the brick went down before the post-op phase. If you find the gfid inside .glusterfs/indices/xattrop , it means the write failed on some other brick and this brick has captured it. The glustershd simply reads the list of entries inside .glusterfs/indices/* and triggers heal on them. This is referred to as index heal . While this happens automcatically every heal-timeout seconds, we can also manaully trigger it via the CLI using gluster volume heal $VOLNAME . Full heal: A full heal, triggered from the CLI with gluster volume heal $VOLNAME full , does just what the name implies. It does not process a particular list of entries like index heal, but crawls the whole gluster filesystem beginning with root, examines if files have non zero afr xattrs and triggers heal on them. Of missing xattrs and split-brains: You might now realise how AFR pretty much relies on its xattr values of a given file- from using it to find the good copies to serve a read to finding out the source and sink bricks to heal the file. But what if there is inconsistency in data/metadata of a file and (a) there are zero/ no AFR xattrs (or) (b) if the xattrs all blame each other (i.e. no good copy=>split-brain)? For (a), AFR uses heuristics like picking a local (to that specfic glustershd process) brick, picking the bigger file, picking the file with latest ctime etc. and then does the heal. For (b) you need to resort to using the gluster split-brain resolution CLI or setting the favorite-child-policy volume option to choose a good copy and trigger the heal.","title":"Replication"},{"location":"Administrator-Guide/Automatic-File-Replication/#1-replication-logic","text":"AFR is the module (translator) in glusterfs that provides all the features that you would expect of any synchronous replication system: Simultaneous updating of all copies of data on the replica bricks when a client modifies it. Providing continued data availability to clients when say one brick of the replica set goes down. Automatic self-healing of any data that was modified when the brick that was down, once it comes back up, ensuring consistency of data on all the bricks of the replica. 1 and 2 are in the I/O path while 3 is done either in the I/O path (in the background) or via the self-heal daemon. Each gluster translator implements what are known as File Operations (FOPs) which are mapped to the I/O syscalls which the application makes. For example, AFR has afr_writev that gets invoked when application does a write(2) . As is obvious, all FOPs fall into one of 2 types: i) Read based FOPs which only get informtion from and don\u2019t modify the file in any way. viz: afr_readdir, afr_access, afr_stat, afr_fstat, afr_readlink, afr_getxattr, afr_fgetxattr, afr_readv,afr_seek ii) Write based FOPs which change the file or its attributes. viz: afr_create, afr_mknod,afr_mkdir,afr_link, afr_symlink, afr_rename, afr_unlink, afr_rmdir, afr_do_writev, afr_truncate, afr_ftruncate, afr_setattr, afr_fsetattr, afr_setxattr, afr_fsetxattr, afr_removexattr, afr_fremovexattr, afr_fallocate, afr_discard, afr_zerofill, afr_xattrop, afr_fxattrop, afr_fsync. AFR follows a transaction model for both types of FOPs.","title":"1. Replication logic"},{"location":"Administrator-Guide/Automatic-File-Replication/#read-transactions","text":"For every file in the replica, AFR has an in-memory notion/array called \u2018readables\u2019 which indicate whether each brick of the replica is a good copy or a bad one (i.e. in need of a heal). In a healthy state, all bricks are readable and a read FOP will be served from any one of the readable bricks. The read-hash-mode volume option decides which brick is the chosen one. #[root@tuxpad glusterfs]# gluster volume set help|grep read-hash-mode -A7 Option: cluster.read-hash-mode Default Value: 1 Description: inode-read fops happen only on one of the bricks in replicate. AFR will prefer the one computed using the method specified using this option. 0 = first readable child of AFR, starting from 1st child. 1 = hash by GFID of file (all clients use same subvolume). 2 = hash by GFID of file and client PID. 3 = brick having the least outstanding read requests. If the brick is bad for a given file (i.e. it is pending heal), then it won\u2019t be marked readable to begin with. The readables array is populated based on the on-disk AFR xattrs for the file during lookup. These xattrs indicate which bricks are good and which ones are bad. We will see more about these xattrs in the write transactions section below. If the FOP fails on the chosen readable brick, AFR attempts it on the next readable one, until all are exhausted. If the FOP doesn\u2019t succeed on any of the readables, then the application receives an error.","title":"Read transactions:"},{"location":"Administrator-Guide/Automatic-File-Replication/#write-transactions","text":"Every write based FOP employs a write transaction model which consists of 5 phases: 1) The lock phase Take locks on the file being modified on all bricks so that AFRs of other clients are blocked if they try to modify the same file simultaneously. 2) The pre-op phase Increment the \u2018dirty\u2019 xattr (trusted.afr.dirty) by 1 on all participating bricks as an indication of an impending FOP (in the next phase) 3) The FOP phase Perform the actual FOP (say a setfattr) on all bricks. 4) The post-op phase Decrement the dirty xattr by 1 on bricks where the FOP was successful. In addition, also increment the \u2018pending\u2019 xattr (trusted.afr.$VOLNAME-client-x) xattr on the success bricks to \u2018blame\u2019 the bricks where the FOP failed. 5) The unlock phase Release the locks that were taken in phase 1. Any competing client can now go ahead with its own write transaction. Note : There are certain optimizations done at the code level which reduce the no. of lock/unlock phases done for a transaction by piggybacking on the previous transaction\u2019s locks. These optimizations (eager-locking, piggybacking and delayed post-op) beyond the scope of this post. AFR returns sucess for these FOPs only if they meet quorum. For replica 2, this means it needs to suceed on any one brick. For replica 3, it is two out of theree and so on.","title":"Write transactions:"},{"location":"Administrator-Guide/Automatic-File-Replication/#more-on-the-afr-xattrs","text":"We saw that AFR modifies the dirty and pending xattrs in the pre-op and post-op phases. To be more precise, only parts of the xattr are modified in a given transaction. Which bytes are modified depends on the type of write transaction which the FOP belongs to. Transaction Type FOPs that belong to it AFR_DATA_TRANSACTION afr_writev, afr_truncate, afr_ftruncate, afr_fsync, afr_fallocate, afr_discard, afr_zerofill AFR_METADATA_TRANSACTION afr_setattr, afr_fsetattr, afr_setxattr, afr_fsetxattr, afr_removexattr, afr_fremovexattr, afr_xattrop, afr_fxattrop AFR_ENTRY_TRANSACTION afr_create, afr_mknod, afr_mkdir, afr_link, afr_symlink, afr_rename, afr_unlink, afr_rmdir Stop here and convince yourself that given a write based FOP, you can say which one of the 3 transaction types it belongs to. Note: In the code, there is also a AFR_ENTRY_RENAME_TRANSACTION (used by afr_rename) but it is safe to assume that it is identical to AFR_ENTRY_TRANSACTION as far as interpreting the xattrs are concerned. Consider the xttr: trusted.afr.dirty=0x000000000000000000000000 The first 4 bytes of the xattr are used for data transactions, the next 4 bytes for metadata transactions and the last 4 for entry transactions. Let us see some examples of how the xattr would look like for various types of FOPs during a transaction: | FOP | Value after pre-op phase | Value after post-op phase | |-------------|------------------------------------------------|------------------------------------------------| | afr_writev | trusted.afr.dirty=0x00000001 00000000 00000000 | trusted.afr.dirty=0x00000000 00000000 00000000 | | afr_setattr | trusted.afr.dirty=0x00000000 00000001 00000000 | trusted.afr.dirty=0x00000000 00000000 00000000 | | afr_create | trusted.afr.dirty=0x00000000 00000000 00000001 | trusted.afr.dirty=0x00000000 00000000 00000000 | Thus depending on the type of FOP (i.e. data/ metadata/ entry transaction), different set of bytes of the dirty xattr get incremented/ decremented. Modification of the pending xattr also follows the same pattern, execept it is incremented only in the post-op phase if the FOP fails on some bricks. Example: Let us say a write was performed on a file, say FILE1, on replica 3 volume called \u2018testvol\u2019. Suppose the lock and pre-op phase succeeded on all bricks. After that the 3rd brick went down, and the transaction completed successfully on the first 2 bricks. What will be the state of the afr xattrs on all bricks? [root@tuxpad ravi]# getfattr -d -m . -e hex /bricks/brick1/FILE1|grep afr getfattr: Removing leading '/' from absolute path names trusted.afr.dirty=0x000000000000000000000000 trusted.afr.testvol-client-2=0x000000010000000000000000 [root@tuxpad ravi]# getfattr -d -m . -e hex /bricks/brick2/FILE1|grep afr getfattr: Removing leading '/' from absolute path names trusted.afr.dirty=0x000000000000000000000000 trusted.afr.testvol-client-2=0x000000010000000000000000 [root@tuxpad ravi]# getfattr -d -m . -e hex /bricks/brick3/FILE1|grep afr getfattr: Removing leading '/' from absolute path names trusted.afr.dirty=0x000000010000000000000000 So Brick3 will still have the dirty xattr set because it went down before the post-op had a chance to decrement it. Bricks 1 and 2 will have a zero dirty xattr and in addition, a non-zero pending xattr set. The client-2 in trusted.afr.testvol-client-2 indicates that the 3rd brick is bad and has some pending data operations.","title":"More on the AFR xattrs:"},{"location":"Administrator-Guide/Automatic-File-Replication/#2-self-heal-logic","text":"We already know that AFR increments and/or decrements the dirty (i.e. trusted.afr.dirty ) and pending (i.e. trusted.afr.$VOLNAME-client-x ) xattrs during the different phases of the transaction. For a given file (or directory), an all zero value of these xattrs or the total absence of these xattrs on all bricks of the replica mean the file is healthy and does not need heal. If any of these xattrs are non-zero even on one of the bricks, then the file is a candidate for heal- it as simple as that. When we say these xattrs are non-zero, it is in the context of no on-going I/O going from client(s) on the file. Otherwise the non-zero values that one observes might be transient as the write transaction is progressing through its five phases. Of course, as an admin, you wouldn\u2019t need to figure out all of this. Just running the heal info set of commands should give you the list of files that need heal. So if self-heal observes a file with non-zero xattrs, it does the following steps: Fetch the afr xattrs, examine which set of 8 bytes are non-zero and determine the corresponding heals that are needed on the file \u2013 i.e. data heal/ metadata heal/ entry heal . Determine which bricks are good (a.k.a. \u2018sources\u2019) and which ones are bad (a.k.a. \u2018sinks\u2019) for each of those heals by interpretting the xattr values. Pick one source brick and heal the file on to all the sink bricks. If the heal is successful, reset the afr xattrs to zero. This is a rather simplified description and I have omitted details about various locks that each of these steps need to take because self-heal and client I/O can happen in parallel on the file. Or even multiple self-heal daemons (described later) can attempt to heal the same file. Data heal : Happens only for files. The contents of the file are copied from the source to the sink bricks. Entry heal : Happens only for directories. Entries (i.e. files and subdirs) under a given directory are deleted from the sinks if they are not present in the source. Likewise, entries are created on the sinks if they are not present in the source. Metadata heal: Happens for both files and directories. File ownership, file permissions and extended attributes are copied from the source to the sink bricks. It can be possible that for a given file, one set of bricks can be the source for data heal while another set could be the source for metadata heals. It all depends on which FOPs failed on what bricks and therefore what set of bytes are non-zero for the afr xattrs.","title":"2. Self-heal logic."},{"location":"Administrator-Guide/Automatic-File-Replication/#when-do-self-heals-happen","text":"There are two places from which the steps described above for healing can be carried out:","title":"When do self-heals happen?"},{"location":"Administrator-Guide/Automatic-File-Replication/#i-from-the-client-side","text":"Client-side heals are triggered when the file is accessed from the client (mount). AFR uses a monotonically increasing generation number to keep track of disconnect/connect of its children (i.e. the client translators) to the bricks. When this \u2018event generation\u2019 number changes, the file\u2019s inode is marked as a candidate for refresh. When the next FOP comes on such an inode, a refresh is triggered to update the readables during which a heal is launched (if the AFR xattrs indicate that a heal is needed, that is). This heal happens in the background, meaning it does not block the actual FOP which will continue as usual post the refresh. Specific client-side heals can be turned off by disabling the 3 corresponding volume options: cluster.metadata-self-heal cluster.data-self-heal cluster.entry-self-heal The number of client-side heals that happen in the background can be tuned via the following volume options: background-self-heal-count heal-wait-queue-length See the gluster volume set help for more information on all the above options. Name heal : Name heal is just healing of the file/directory name when it is accessed. For example, say a file is created and written to when a brick is down and all the 3 client side heals are disabled. When the brick comes up and the next I/O comes on it, the file name is created on it as a part of lookup. Its contents/metadata are not healed though. Name heal cannot be disabled. It is there to ensure that the namespace is consistent on all bricks as soon as the file is accessed.","title":"i) From the client side."},{"location":"Administrator-Guide/Automatic-File-Replication/#ii-by-the-self-heal-daemon","text":"There is a self-heal daemon process (glutershd) that runs on every node of the trusted storage pool. It is a light weight client process consisting mainly of AFR ant the protocol/client translators. It can talk to all bricks of all the replicate volume(s) of the pool. It periodically crawls (every 10 minutes by default; tunable via the heal-timeout volume option) the list of files that need heal and does their healing. As you can see, client side heal is done upon file access but glustershd processes the heal backlog pro-actively.","title":"ii) By the self-heal daemon."},{"location":"Administrator-Guide/Automatic-File-Replication/#index-heal","text":"But how does glustershd know which files it needs to heal? Where does it get the list from? So in part-1, while we saw the five phases of the AFR write transaction, we left out one detail: In the pre-op phase, in addition to marking the dirty xattr, each brick also stores the gfid string of the file inside its .glusterfs/indices/dirty directory. Likewise, in the post-op phase, it removes the gfid string from its .glusterfs/indices/dirty If addition, if the write failed on some brick, the good bricks will stores the gfid string inside the .glusterfs/indices/xattrop directory. Thus when no I/O is happening on a file and you still find its gfid inside .glusterfs/indices/dirty of a particular brick, it means the brick went down before the post-op phase. If you find the gfid inside .glusterfs/indices/xattrop , it means the write failed on some other brick and this brick has captured it. The glustershd simply reads the list of entries inside .glusterfs/indices/* and triggers heal on them. This is referred to as index heal . While this happens automcatically every heal-timeout seconds, we can also manaully trigger it via the CLI using gluster volume heal $VOLNAME .","title":"Index heal:"},{"location":"Administrator-Guide/Automatic-File-Replication/#full-heal","text":"A full heal, triggered from the CLI with gluster volume heal $VOLNAME full , does just what the name implies. It does not process a particular list of entries like index heal, but crawls the whole gluster filesystem beginning with root, examines if files have non zero afr xattrs and triggers heal on them.","title":"Full heal:"},{"location":"Administrator-Guide/Automatic-File-Replication/#of-missing-xattrs-and-split-brains","text":"You might now realise how AFR pretty much relies on its xattr values of a given file- from using it to find the good copies to serve a read to finding out the source and sink bricks to heal the file. But what if there is inconsistency in data/metadata of a file and (a) there are zero/ no AFR xattrs (or) (b) if the xattrs all blame each other (i.e. no good copy=>split-brain)? For (a), AFR uses heuristics like picking a local (to that specfic glustershd process) brick, picking the bigger file, picking the file with latest ctime etc. and then does the heal. For (b) you need to resort to using the gluster split-brain resolution CLI or setting the favorite-child-policy volume option to choose a good copy and trigger the heal.","title":"Of missing xattrs and split-brains:"},{"location":"Administrator-Guide/Bareos/","text":"Configuring Bareos to store backups on Gluster This description assumes that you already have a Gluster environment ready and configured. The examples use storage.example.org as a Round Robin DNS name that can be used to contact any of the available GlusterD processes. The Gluster Volume that is used, is called backups . Client systems would be able to access the volume by mounting it with FUSE like this: # mount -t glusterfs storage.example.org:/backups /mnt Bareos contains a plugin for the Storage Daemon that uses libgfapi . This makes it possible for Bareos to access the Gluster Volumes without the need to have a FUSE mount available. Here we will use one server that is dedicated for doing backups. This system is called backup.example.org . The Bareos Director is running on this host, together with the Bareos Storage Daemon. In the example, there is a File Daemon running on the same server. This makes it possible to backup the Bareos Director, which is useful as a backup of the Bareos database and configuration is kept that way. Bareos Installation An absolute minimal Bareos installation needs a Bareos Director and a Storage Daemon. In order to backup a filesystem, a File Daemon needs to be available too. For the description in this document, CentOS-7 was used, with the following packages and versions: glusterfs-3.7.4 bareos-14.2 with bareos-storage-glusterfs The Gluster Storage Servers do not need to have any Bareos packages installed. It is often better to keep applications (Bareos) and storage servers on different systems. So, when the Bareos repository has been configured, install the packages on the backup.example.org server: # yum install bareos-director bareos-database-sqlite3 \\ bareos-storage-glusterfs bareos-filedaemon \\ bareos-bconsole To keep things as simple as possible, SQlite it used. For production deployments either MySQL or PostgrSQL is advised. It is needed to create the initial database: # sqlite3 /var/lib/bareos/bareos.db < /usr/lib/bareos/scripts/ddl/creates/sqlite3.sql # chown bareos:bareos /var/lib/bareos/bareos.db # chmod 0660 /var/lib/bareos/bareos.db The bareos-bconsole package is optional. bconsole is a terminal application that can be used to initiate backups, check the status of different Bareos components and the like. Testing the configuration with bconsole is relatively simple. Once the packages are installed, you will need to start and enable the daemons: # systemctl start bareos\u00adsd # systemctl start bareos\u00adfd # systemctl start bareos\u00addir # systemctl enable bareos\u00adsd # systemctl enable bareos\u00adfd # systemctl enable bareos\u00addir Gluster Volume preparation There are a few steps needed to allow Bareos to access the Gluster Volume. By default Gluster does not allow clients to connect from an unprivileged port. Because the Bareos Storage Daemon does not run as root, permissions to connect need to be opened up. There are two processes involved when a client accesses a Gluster Volume. For the initial phase, GlusterD is contacted, when the client received the layout of the volume, the client will connect to the bricks directly. The changes to allow unprivileged processes to connect, are therefore twofold: In /etc/glusterfs/glusterd.vol the option rpc-auth-allow-insecure on needs to be added on all storage servers. After the modification of the configuration file, the GlusterD process needs to be restarted with systemctl restart glusterd . The brick processes for the volume are configured through a volume option. By executing gluster volume set backups server.allow-insecure on the needed option gets set. Some versions of Gluster require a volume stop/start before the option is taken into account, for these versions you will need to execute gluster volume stop backups and gluster volume start backups . Except for the network permissions, the Bareos Storage Daemon needs to be allowed to write to the filesystem provided by the Gluster Volume. This is achieved by setting normal UNIX permissions/ownership so that the right user/group can write to the volume: # mount -t glusterfs storage.example.org:/backups /mnt # mkdir /mnt/bareos # chown bareos:bareos /mnt/bareos # chmod ug=rwx /mnt/bareos # umount /mnt Depending on how users/groups are maintained in the environment, the bareos user and group may not be available on the storage servers. If that is the case, the chown command above can be adapted to use the uid and gid of the bareos user and group from backup.example.org . On the Bareos server, the output would look similar to: # id bareos uid=998(bareos) gid=997(bareos) groups=997(bareos),6(disk),30(tape) And that makes the chown command look like this: # chown 998:997 /mnt/bareos Bareos Configuration When bareos-storage-glusterfs got installed, an example configuration file has been added too. The /etc/bareos/bareos-sd.d/device-gluster.conf contains the Archive Device directive, which is a URL for the Gluster Volume and path where the backups should get stored. In our example, the entry should get set to: Device { Name = GlusterStorage Archive Device = gluster://storage.example.org/backups/bareos Device Type = gfapi Media Type = GlusterFile ... } The default configuration of the Bareos provided jobs is to write backups to /var/lib/bareos/storage . In order to write all the backups to the Gluster Volume instead, the configuration for the Bareos Director needs to be modified. In the /etc/bareos/bareos-dir.conf configuration, the defaults for all jobs can be changed to use the GlusterFile storage: JobDefs { Name = \"DefaultJob\" ... # Storage = File Storage = GlusterFile ... } After changing the configuration files, the Bareos daemons need to apply them. The easiest to inform the processes of the changed configuration files is by instructing them to reload their configuration: # bconsole Connecting to Director backup:9101 1000 OK: backup-dir Version: 14.2.2 (12 December 2014) Enter a period to cancel a command. *reload With bconsole it is also possible to check if the configuration has been applied. The status command can be used to show the URL of the storage that is configured. When all is setup correctly, the result looks like this: *status storage=GlusterFile Connecting to Storage daemon GlusterFile at backup:9103 ... Device \"GlusterStorage\" (gluster://storage.example.org/backups/bareos) is not open. ... Create your first backup There are several default jobs configured in the Bareos Director. One of them is the DefaultJob which was modified in an earlier step. This job uses the SelfTest FileSet, which backups /usr/sbin . Running this job will verify if the configuration is working correctly. Additional jobs, other FileSets and more File Daemons (clients that get backed up) can be added later. *run A job name must be specified. The defined Job resources are: 1: BackupClient1 2: BackupCatalog 3: RestoreFiles Select Job resource (1-3): 1 Run Backup job JobName: BackupClient1 Level: Incremental Client: backup-fd ... OK to run? (yes/mod/no): yes Job queued. JobId=1 The job will need a few seconds to complete, the status command can be used to show the progress. Once done, the messages command will display the result: *messages ... JobId: 1 Job: BackupClient1.2015-09-30_21.17.56_12 ... Termination: Backup OK The archive that contains the backup will be located on the Gluster Volume. To check if the file is available, mount the volume on a storage server: # mount -t glusterfs storage.example.org:/backups /mnt # ls /mnt/bareos Further Reading This document intends to provide a quick start of configuring Bareos to use Gluster as a storage backend. Bareos can be configured to create backups of different clients (which run a File Daemon), run jobs at scheduled time and intervals and much more. The excellent Bareos documentation can be consulted to find out how to create backups in a much more useful way than can get expressed on this page.","title":"Configuring Bareos to store backups on Gluster"},{"location":"Administrator-Guide/Bareos/#configuring-bareos-to-store-backups-on-gluster","text":"This description assumes that you already have a Gluster environment ready and configured. The examples use storage.example.org as a Round Robin DNS name that can be used to contact any of the available GlusterD processes. The Gluster Volume that is used, is called backups . Client systems would be able to access the volume by mounting it with FUSE like this: # mount -t glusterfs storage.example.org:/backups /mnt Bareos contains a plugin for the Storage Daemon that uses libgfapi . This makes it possible for Bareos to access the Gluster Volumes without the need to have a FUSE mount available. Here we will use one server that is dedicated for doing backups. This system is called backup.example.org . The Bareos Director is running on this host, together with the Bareos Storage Daemon. In the example, there is a File Daemon running on the same server. This makes it possible to backup the Bareos Director, which is useful as a backup of the Bareos database and configuration is kept that way.","title":"Configuring Bareos to store backups on Gluster"},{"location":"Administrator-Guide/Bareos/#bareos-installation","text":"An absolute minimal Bareos installation needs a Bareos Director and a Storage Daemon. In order to backup a filesystem, a File Daemon needs to be available too. For the description in this document, CentOS-7 was used, with the following packages and versions: glusterfs-3.7.4 bareos-14.2 with bareos-storage-glusterfs The Gluster Storage Servers do not need to have any Bareos packages installed. It is often better to keep applications (Bareos) and storage servers on different systems. So, when the Bareos repository has been configured, install the packages on the backup.example.org server: # yum install bareos-director bareos-database-sqlite3 \\ bareos-storage-glusterfs bareos-filedaemon \\ bareos-bconsole To keep things as simple as possible, SQlite it used. For production deployments either MySQL or PostgrSQL is advised. It is needed to create the initial database: # sqlite3 /var/lib/bareos/bareos.db < /usr/lib/bareos/scripts/ddl/creates/sqlite3.sql # chown bareos:bareos /var/lib/bareos/bareos.db # chmod 0660 /var/lib/bareos/bareos.db The bareos-bconsole package is optional. bconsole is a terminal application that can be used to initiate backups, check the status of different Bareos components and the like. Testing the configuration with bconsole is relatively simple. Once the packages are installed, you will need to start and enable the daemons: # systemctl start bareos\u00adsd # systemctl start bareos\u00adfd # systemctl start bareos\u00addir # systemctl enable bareos\u00adsd # systemctl enable bareos\u00adfd # systemctl enable bareos\u00addir","title":"Bareos Installation"},{"location":"Administrator-Guide/Bareos/#gluster-volume-preparation","text":"There are a few steps needed to allow Bareos to access the Gluster Volume. By default Gluster does not allow clients to connect from an unprivileged port. Because the Bareos Storage Daemon does not run as root, permissions to connect need to be opened up. There are two processes involved when a client accesses a Gluster Volume. For the initial phase, GlusterD is contacted, when the client received the layout of the volume, the client will connect to the bricks directly. The changes to allow unprivileged processes to connect, are therefore twofold: In /etc/glusterfs/glusterd.vol the option rpc-auth-allow-insecure on needs to be added on all storage servers. After the modification of the configuration file, the GlusterD process needs to be restarted with systemctl restart glusterd . The brick processes for the volume are configured through a volume option. By executing gluster volume set backups server.allow-insecure on the needed option gets set. Some versions of Gluster require a volume stop/start before the option is taken into account, for these versions you will need to execute gluster volume stop backups and gluster volume start backups . Except for the network permissions, the Bareos Storage Daemon needs to be allowed to write to the filesystem provided by the Gluster Volume. This is achieved by setting normal UNIX permissions/ownership so that the right user/group can write to the volume: # mount -t glusterfs storage.example.org:/backups /mnt # mkdir /mnt/bareos # chown bareos:bareos /mnt/bareos # chmod ug=rwx /mnt/bareos # umount /mnt Depending on how users/groups are maintained in the environment, the bareos user and group may not be available on the storage servers. If that is the case, the chown command above can be adapted to use the uid and gid of the bareos user and group from backup.example.org . On the Bareos server, the output would look similar to: # id bareos uid=998(bareos) gid=997(bareos) groups=997(bareos),6(disk),30(tape) And that makes the chown command look like this: # chown 998:997 /mnt/bareos","title":"Gluster Volume preparation"},{"location":"Administrator-Guide/Bareos/#bareos-configuration","text":"When bareos-storage-glusterfs got installed, an example configuration file has been added too. The /etc/bareos/bareos-sd.d/device-gluster.conf contains the Archive Device directive, which is a URL for the Gluster Volume and path where the backups should get stored. In our example, the entry should get set to: Device { Name = GlusterStorage Archive Device = gluster://storage.example.org/backups/bareos Device Type = gfapi Media Type = GlusterFile ... } The default configuration of the Bareos provided jobs is to write backups to /var/lib/bareos/storage . In order to write all the backups to the Gluster Volume instead, the configuration for the Bareos Director needs to be modified. In the /etc/bareos/bareos-dir.conf configuration, the defaults for all jobs can be changed to use the GlusterFile storage: JobDefs { Name = \"DefaultJob\" ... # Storage = File Storage = GlusterFile ... } After changing the configuration files, the Bareos daemons need to apply them. The easiest to inform the processes of the changed configuration files is by instructing them to reload their configuration: # bconsole Connecting to Director backup:9101 1000 OK: backup-dir Version: 14.2.2 (12 December 2014) Enter a period to cancel a command. *reload With bconsole it is also possible to check if the configuration has been applied. The status command can be used to show the URL of the storage that is configured. When all is setup correctly, the result looks like this: *status storage=GlusterFile Connecting to Storage daemon GlusterFile at backup:9103 ... Device \"GlusterStorage\" (gluster://storage.example.org/backups/bareos) is not open. ...","title":"Bareos Configuration"},{"location":"Administrator-Guide/Bareos/#create-your-first-backup","text":"There are several default jobs configured in the Bareos Director. One of them is the DefaultJob which was modified in an earlier step. This job uses the SelfTest FileSet, which backups /usr/sbin . Running this job will verify if the configuration is working correctly. Additional jobs, other FileSets and more File Daemons (clients that get backed up) can be added later. *run A job name must be specified. The defined Job resources are: 1: BackupClient1 2: BackupCatalog 3: RestoreFiles Select Job resource (1-3): 1 Run Backup job JobName: BackupClient1 Level: Incremental Client: backup-fd ... OK to run? (yes/mod/no): yes Job queued. JobId=1 The job will need a few seconds to complete, the status command can be used to show the progress. Once done, the messages command will display the result: *messages ... JobId: 1 Job: BackupClient1.2015-09-30_21.17.56_12 ... Termination: Backup OK The archive that contains the backup will be located on the Gluster Volume. To check if the file is available, mount the volume on a storage server: # mount -t glusterfs storage.example.org:/backups /mnt # ls /mnt/bareos","title":"Create your first backup"},{"location":"Administrator-Guide/Bareos/#further-reading","text":"This document intends to provide a quick start of configuring Bareos to use Gluster as a storage backend. Bareos can be configured to create backups of different clients (which run a File Daemon), run jobs at scheduled time and intervals and much more. The excellent Bareos documentation can be consulted to find out how to create backups in a much more useful way than can get expressed on this page.","title":"Further Reading"},{"location":"Administrator-Guide/Brick-Naming-Conventions/","text":"Brick Naming Conventions FHS-2.3 isn't entirely clear on where data shared by the server should reside. It does state that \" /srv contains site-specific data which is served by this system \", but is GlusterFS data site-specific? The consensus seems to lean toward using /data . A good hierarchical method for placing bricks is: /data/glusterfs/<volume>/<brick>/brick In this example, <brick> is the filesystem that is mounted. Example: One Brick Per Server A physical disk /dev/sdb is going to be used as brick storage for a volume you're about to create named myvol1 . You've partitioned and formatted /dev/sdb1 with XFS on each of 4 servers. On all 4 servers: mkdir -p /data/glusterfs/myvol1/brick1 mount /dev/sdb1 /data/glusterfs/myvol1/brick1 We're going to define the actual brick in the brick directory on that filesystem. This helps by causing the brick to fail to start if the XFS filesystem isn't mounted. On just one server: gluster volume create myvol1 replica 2 server{1..4}:/data/glusterfs/myvol1/brick1/brick This will create the volume myvol1 which uses the directory /data/glusterfs/myvol1/brick1/brick on all 4 servers. Example: Two Bricks Per Server Two physical disks /dev/sdb and /dev/sdc are going to be used as brick storage for a volume you're about to create named myvol2 . You've partitioned and formatted /dev/sdb1 and /dev/sdc1 with XFS on each of 4 servers. On all 4 servers: mkdir -p /data/glusterfs/myvol2/brick{1,2} mount /dev/sdb1 /data/glusterfs/myvol2/brick1 mount /dev/sdc1 /data/glusterfs/myvol2/brick2 Again we're going to define the actual brick in the brick directory on these filesystems. On just one server: gluster volume create myvol2 replica 2 \\ server{1..4}:/data/glusterfs/myvol2/brick1/brick \\ server{1..4}:/data/glusterfs/myvol2/brick2/brick Note: It might be tempting to try gluster volume create myvol2 replica 2 server{1..4}:/data/glusterfs/myvol2/brick{1,2}/brick but Bash would expand the last {} first, so you would end up replicating between the two bricks on each servers, instead of across servers.","title":"Brick Naming Conventions"},{"location":"Administrator-Guide/Brick-Naming-Conventions/#brick-naming-conventions","text":"FHS-2.3 isn't entirely clear on where data shared by the server should reside. It does state that \" /srv contains site-specific data which is served by this system \", but is GlusterFS data site-specific? The consensus seems to lean toward using /data . A good hierarchical method for placing bricks is: /data/glusterfs/<volume>/<brick>/brick In this example, <brick> is the filesystem that is mounted.","title":"Brick Naming Conventions"},{"location":"Administrator-Guide/Brick-Naming-Conventions/#example-one-brick-per-server","text":"A physical disk /dev/sdb is going to be used as brick storage for a volume you're about to create named myvol1 . You've partitioned and formatted /dev/sdb1 with XFS on each of 4 servers. On all 4 servers: mkdir -p /data/glusterfs/myvol1/brick1 mount /dev/sdb1 /data/glusterfs/myvol1/brick1 We're going to define the actual brick in the brick directory on that filesystem. This helps by causing the brick to fail to start if the XFS filesystem isn't mounted. On just one server: gluster volume create myvol1 replica 2 server{1..4}:/data/glusterfs/myvol1/brick1/brick This will create the volume myvol1 which uses the directory /data/glusterfs/myvol1/brick1/brick on all 4 servers.","title":"Example: One Brick Per Server"},{"location":"Administrator-Guide/Brick-Naming-Conventions/#example-two-bricks-per-server","text":"Two physical disks /dev/sdb and /dev/sdc are going to be used as brick storage for a volume you're about to create named myvol2 . You've partitioned and formatted /dev/sdb1 and /dev/sdc1 with XFS on each of 4 servers. On all 4 servers: mkdir -p /data/glusterfs/myvol2/brick{1,2} mount /dev/sdb1 /data/glusterfs/myvol2/brick1 mount /dev/sdc1 /data/glusterfs/myvol2/brick2 Again we're going to define the actual brick in the brick directory on these filesystems. On just one server: gluster volume create myvol2 replica 2 \\ server{1..4}:/data/glusterfs/myvol2/brick1/brick \\ server{1..4}:/data/glusterfs/myvol2/brick2/brick Note: It might be tempting to try gluster volume create myvol2 replica 2 server{1..4}:/data/glusterfs/myvol2/brick{1,2}/brick but Bash would expand the last {} first, so you would end up replicating between the two bricks on each servers, instead of across servers.","title":"Example: Two Bricks Per Server"},{"location":"Administrator-Guide/Building-QEMU-With-gfapi-For-Debian-Based-Systems/","text":"Building QEMU With gfapi For Debian Based Systems This how-to has been tested on Ubuntu 13.10 in a clean, up to date environment. Older Ubuntu distros required some hacks if I remembered rightly. Other Debian based distros should be able to follow this adjusting for dependencies. Please update this if you get it working on another distro. Satisfying dependencies Make the first stab at getting qemu dependencies apt-get build-dep qemu This next command grabs all the dependencies specified in the debian control file as asked for from upstream Debian sid You can look into the options specified there and adjust to taste. # get almost all the rest and the tools to work up the Debian magic apt-get install devscripts quilt libiscsi-dev libusbredirparser-dev libssh2-1-dev libvdeplug-dev libjpeg-dev glusterfs* we need a newer version of libseccomp for Ubuntu 13.10 mkdir libseccomp cd libseccomp # grab it from upstream sid wget http://ftp.de.debian.org/debian/pool/main/libs/libseccomp/libseccomp_2.1.0+dfsg.orig.tar.gz wget http://ftp.de.debian.org/debian/pool/main/libs/libseccomp/libseccomp_2.1.0+dfsg-1.debian.tar.gz # get it ready tar xf libseccomp_2.1.0+dfsg.orig.tar.gz cd libseccomp-2.1.0+dfsg/ # install the debian magic tar xf ../libseccomp_2.1.0+dfsg-1.debian.tar.gz # apply series files if any while quilt push; do quilt refresh; done # build debs, they'll appear one directory up debuild -i -us -uc -b cd .. # install it dpkg -i *.deb Building QEMU This next part is straightforward if your dependencies are met. For the advanced reader look around debian/control once it is extracted before you install as you may want to change what options QEMU is built with and what targets are requested. cd .. mkdir qemu cd qemu # download our sources. you'll want to check back frequently on these for changes wget http://ftp.de.debian.org/debian/pool/main/q/qemu/qemu_1.7.0+dfsg.orig.tar.xz wget http://ftp.de.debian.org/debian/pool/main/q/qemu/qemu_1.7.0+dfsg-2.debian.tar.gz wget http://download.gluster.org/pub/gluster/glusterfs/3.4/LATEST/glusterfs-3.4.2.tar.gz tar xf glusterfs-3.4.2.tar.gz tar xf qemu_1.7.0+dfsg.orig.tar.xz cd qemu-1.7.0+dfsg/ # unpack the debian magic tar xf ../qemu_1.7.0+dfsg-2.debian.tar.gz # bring glusterfs in to the buiild cp -r ../glusterfs-3.4.2 glusterfs # the glusterfs check in configure looks around weird. I've never asked why but moving the src stuff up one works and tests fine cd glusterfs/api/ mv src/* . cd ../.. #you'll need to edit debian/control to enable glusterfs replacing - ##--enable-glusterfs todo + # --enable-glusterfs + glusterfs-common (>= 3.4.0), #And finally build. It'll take ages. http://xkcd.com/303/ # apply series if any while quilt push; do quilt refresh; done # build packages debuild -i -us -uc -b cd .. Your debs now available to install. It is up to the reader to determine what targets they want installed.","title":"Building QEMU with gfapi For Debian Based Systems"},{"location":"Administrator-Guide/Building-QEMU-With-gfapi-For-Debian-Based-Systems/#building-qemu-with-gfapi-for-debian-based-systems","text":"This how-to has been tested on Ubuntu 13.10 in a clean, up to date environment. Older Ubuntu distros required some hacks if I remembered rightly. Other Debian based distros should be able to follow this adjusting for dependencies. Please update this if you get it working on another distro.","title":"Building QEMU With gfapi For Debian Based Systems"},{"location":"Administrator-Guide/Building-QEMU-With-gfapi-For-Debian-Based-Systems/#satisfying-dependencies","text":"Make the first stab at getting qemu dependencies apt-get build-dep qemu This next command grabs all the dependencies specified in the debian control file as asked for from upstream Debian sid You can look into the options specified there and adjust to taste. # get almost all the rest and the tools to work up the Debian magic apt-get install devscripts quilt libiscsi-dev libusbredirparser-dev libssh2-1-dev libvdeplug-dev libjpeg-dev glusterfs* we need a newer version of libseccomp for Ubuntu 13.10 mkdir libseccomp cd libseccomp # grab it from upstream sid wget http://ftp.de.debian.org/debian/pool/main/libs/libseccomp/libseccomp_2.1.0+dfsg.orig.tar.gz wget http://ftp.de.debian.org/debian/pool/main/libs/libseccomp/libseccomp_2.1.0+dfsg-1.debian.tar.gz # get it ready tar xf libseccomp_2.1.0+dfsg.orig.tar.gz cd libseccomp-2.1.0+dfsg/ # install the debian magic tar xf ../libseccomp_2.1.0+dfsg-1.debian.tar.gz # apply series files if any while quilt push; do quilt refresh; done # build debs, they'll appear one directory up debuild -i -us -uc -b cd .. # install it dpkg -i *.deb","title":"Satisfying dependencies"},{"location":"Administrator-Guide/Building-QEMU-With-gfapi-For-Debian-Based-Systems/#building-qemu","text":"This next part is straightforward if your dependencies are met. For the advanced reader look around debian/control once it is extracted before you install as you may want to change what options QEMU is built with and what targets are requested. cd .. mkdir qemu cd qemu # download our sources. you'll want to check back frequently on these for changes wget http://ftp.de.debian.org/debian/pool/main/q/qemu/qemu_1.7.0+dfsg.orig.tar.xz wget http://ftp.de.debian.org/debian/pool/main/q/qemu/qemu_1.7.0+dfsg-2.debian.tar.gz wget http://download.gluster.org/pub/gluster/glusterfs/3.4/LATEST/glusterfs-3.4.2.tar.gz tar xf glusterfs-3.4.2.tar.gz tar xf qemu_1.7.0+dfsg.orig.tar.xz cd qemu-1.7.0+dfsg/ # unpack the debian magic tar xf ../qemu_1.7.0+dfsg-2.debian.tar.gz # bring glusterfs in to the buiild cp -r ../glusterfs-3.4.2 glusterfs # the glusterfs check in configure looks around weird. I've never asked why but moving the src stuff up one works and tests fine cd glusterfs/api/ mv src/* . cd ../.. #you'll need to edit debian/control to enable glusterfs replacing - ##--enable-glusterfs todo + # --enable-glusterfs + glusterfs-common (>= 3.4.0), #And finally build. It'll take ages. http://xkcd.com/303/ # apply series if any while quilt push; do quilt refresh; done # build packages debuild -i -us -uc -b cd .. Your debs now available to install. It is up to the reader to determine what targets they want installed.","title":"Building QEMU"},{"location":"Administrator-Guide/Consul/","text":"Consul and GlusterFS integration Consul is used for service discovery and configuration. It consists of consul server and agents connecting to it. Apps can get configuration data from consul via HTTP API or DNS queries. Long story short, instead of using standard hostnames and relying on official DNS servers which we may not control, we can use consul to resolve hosts with services under .consul domain, which turns this classic setup: mount -t glusterfs -o backupvolfile-server=gluster-poc-02 gluster-poc-01:/g0 /mnt/gluster/g0 into more convenient entry: mount -t glusterfs gluster.service.consul:/g0 /mnt/gluster/g0 which is especially useful when using image-based servers without further provisioning, and spreading load across all healthy servers registered in consul. Warning In this document you will get a proof-of-concept basic setup - gluster servers and gluster clients configured, which should be a point to expand. You should read Further steps section to harden it. Tested on: isolated virtual network selinux permissive (yay!) consul server/agents version v0.7.5 gluster servers with glusterfs 3.8.x on CentOS 7.3 + samba 4 with simple auth and vfs gluster module gluster volume set as distributed-replicated + 'features.shard: true' and 'features.shard-block-size: 512MB' gluster agents with glusterfs 3.8.x on Ubuntu 14.04 gluster agents with glusterfs 3.8.x on CentOS 7.3 gluster agents with glusterfs 3.7.x on CentOS 5.9 Windows 2012R2 connected to gluster servers via samba Scenario We want to create shared storage accessible via different operating systems - Linux and Windows. we do not control DNS server so we cannot add/remove entries on gluster server add/remove gluster servers are in the gluster pool and have gluster volume created named g0 gluster servers have consul agent installed, and they will register to consul as gluster service gluster servers have also SMB installed with very simple setup using gluster vfs plugin gluster client have consul agent installed, and they will use gluster.service.consul as entry point. DNS resolution under Linux will be handled via dnsmasq DNS resolution under Windows will be handled via consul itself Known limitations consul health checks introduce delay, also remember that consul can cache DNS entries to increase performance the way Windows share works is that it will connect to one of the samba servers, if this server die then transfers are aborted, and we must retry operation, but watch out for delay. anything other than gluster volume distributed-replicated was not tested - it may not work for Windows. Requirements you should have consul server (or cluster) up and running, and the best, also accessible via default HTTP port. you should have gluster servers already joined in the gluster pool, bricks and volume configured. check you firewall rules for outbound and inbound for DNS, gluster, samba, consul make yourself familiar with consul documentation (or specific branch on github) Linux setup Consul agent on Linux on gluster clients First, install consul agent. The best way is to use for example puppet module In general your Linux boxes should register in the consul server and be visible under Nodes section. To verify if consul agent is working properly, you can query its DNS interface, asking it to list consul servers: [centos@gluster-poc-01]# dig consul.service.consul 127.0.0.1:8600 ; <<>> DiG 9.9.4-RedHat-9.9.4-38.el7_3.3 <<>> consul.service.consul 127.0.0.1:8600 ;; global options: +cmd ;; Got answer: ;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 39354 ;; flags: qr aa rd ra; QUERY: 1, ANSWER: 3, AUTHORITY: 0, ADDITIONAL: 0 ;; QUESTION SECTION: ;consul.service.consul. IN A ;; ANSWER SECTION: consul.service.consul. 0 IN A 172.30.64.198 consul.service.consul. 0 IN A 172.30.82.255 consul.service.consul. 0 IN A 172.30.81.155 ;; Query time: 1 msec ;; SERVER: 127.0.0.1#53(127.0.0.1) ;; WHEN: Sat May 20 08:50:21 UTC 2017 ;; MSG SIZE rcvd: 87 ;; Got answer: ;; ->>HEADER<<- opcode: QUERY, status: NXDOMAIN, id: 22224 ;; flags: qr rd ra ad; QUERY: 1, ANSWER: 0, AUTHORITY: 0, ADDITIONAL: 0 ;; QUESTION SECTION: ;127.0.0.1:8600. IN A ;; Query time: 0 msec ;; SERVER: 127.0.0.1#53(127.0.0.1) ;; WHEN: Sat May 20 08:50:21 UTC 2017 ;; MSG SIZE rcvd: 32 Now, to be able to use it on system level, we want it to work without specifying port. We can achieve it with running consul on port 53 (not advised), or redirecting network traffic from port 53 to 8600 or proxy it via local DNS resolver - for example use locally installed dnsmasq. First, install dnsmasq, and add file /etc/dnsmasq.d/10-consul : server=/consul/127.0.0.1#8600 This will ensure that any *.consul requests will be forwarded to local consul listening on its default DNS port 8600. Make sure that /etc/resolv.conf contains nameserver 127.0.0.1 . Under Debian distros it should be there, under RedHat - not really. You can fix this in two ways, choose on your onw which one to apply: add nameserver 127.0.0.1 to /etc/resolvconf/resolv.conf.d/header or update /etc/dhcp/dhclient.conf and add to it line prepend domain-name-servers 127.0.0.1; . In both cases it ensures that dnsmasq will be a first nameserver, and requires reloading resolver or networking. Eventually you should have nameserver 127.0.0.1 set as first entry in /etc/resolv.conf and have DNS resolving consul entries: [centos@gluster-poc-01]# dig consul.service.consul ; <<>> DiG 9.9.4-RedHat-9.9.4-38.el7_3.3 <<>> consul.service.consul ;; global options: +cmd ;; Got answer: ;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 42571 ;; flags: qr aa rd ra; QUERY: 1, ANSWER: 3, AUTHORITY: 0, ADDITIONAL: 0 ;; QUESTION SECTION: ;consul.service.consul. IN A ;; ANSWER SECTION: consul.service.consul. 0 IN A 172.30.64.198 consul.service.consul. 0 IN A 172.30.82.255 consul.service.consul. 0 IN A 172.30.81.155 ;; Query time: 1 msec ;; SERVER: 127.0.0.1#53(127.0.0.1) ;; WHEN: Sat May 20 09:01:12 UTC 2017 ;; MSG SIZE rcvd: 87 From now on we should be able to use <servicename>.service.consul in places, where we had FQDN entries of the single servers. Next, we must define gluster service consul on the servers. Consul agent on Linux on gluster servers Install consul agent as described in previous section. You can define consul services as gluster to run health checks, to do that we must add consul to sudoers or allow it executing certain sudo commands without password: /etc/sudoers.d/99-consul.conf : consul ALL=(ALL) NOPASSWD: /sbin/gluster pool list First, lets define service in consul, it will be very basic, without volume names. Service name gluster , with default port 24007, and we will tag it as gluster and server . Our service will have service health checks every 10s: check if the gluster service is responding to TCP on 24007 port check if the gluster server is connected to other peers in the pool (to avoid registering as healthy service which is actaully not serving anything) Below is an example of /etc/consul/service_gluster.json : { \"service\": { \"address\": \"\", \"checks\": [ { \"interval\": \"10s\", \"tcp\": \"localhost:24007\", \"timeout\": \"5s\" }, { \"interval\": \"10s\", \"script\": \"/bin/bash -c \\\"sudo -n /sbin/gluster pool list |grep -v UUID|grep -v localhost|grep Connected\\\"\", \"timeout\": \"5s\" } ], \"enableTagOverride\": false, \"id\": \"gluster\", \"name\": \"gluster\", \"port\": 24007, \"tags\": [ \"gluster\", \"server\" ] } } Restart consul service and you should see gluster servers in consul web ui. After a while service should be in healthy stage and be available under nslookup: [centos@gluster-poc-02]# nslookup gluster.service.consul Server: 127.0.0.1 Address: 127.0.0.1#53 Name: gluster.service.consul Address: 172.30.64.144 Name: gluster.service.consul Address: 172.30.65.61 Notice that gluster server can be also gluster client, for example if we want to mount gluster volume on the servers. Mounting gluster volume under Linux As a moutpoint we would usually select one of the gluster servers, and another as backup server, like this: mount -t glusterfs -o backupvolfile-server=gluster-poc-02 gluster-poc-01:/g0 /mnt/gluster/g0 This is a bit inconvenient, for example we have an image with hardcoded hostnames, and old servers are gone due to maintenance. We would have to recreate image, or reconfigure existing nodes if they unmount gluster storage. To mitigate that issue we can now use consul for fetching the server pool: mount -t glusterfs gluster.service.consul:/g0 /mnt/gluster/g0 So we can populate that to /etc/fstab or one of the autofs files. Windows setup Configuring gluster servers as samba shares This is the simplest and not so secure setup, you have been warned. Proper setup suggests using LDAP or CTDB . You can configure it with puppet using module kakwa-samba . First, we want to reconfigure gluster servers so that they serve as samba shares using user/pass credentials, which is separate to Linux credentials. We assume that accessing windows share will be done as user steve with password steve-loves-bacon , make sure you create that user on each gluster server host. sudo adduser steve sudo smbpasswd -a steve Notice that if you do not set user.smb = disable in gluster volume then it may auto-add itself to samba configuration. So better disable this by executing: gluster volume get g0 user.smb disable Now install samba and samba-vfs-glusterfs packages and configure /etc/samba/smb.conf : [global] workgroup = test security = user min protocol = SMB2 netbios name = gluster realm = test vfs objects = acl_xattr map acl inherit = Yes store dos attributes = Yes log level = 1 dedicated keytab file = /etc/krb5.keytab map untrusted to domain = Yes [vfs-g0] guest only = no writable = yes guest ok = no force user = steve create mask = 0666 directory mask = 0777 comment = Gluster via VFS (native gluster) path = / vfs objects = glusterfs glusterfs:volume = g0 kernel share modes = no glusterfs:loglevel = 7 glusterfs:logfile = /var/log/samba/glusterfs-g0.%M.log browsable = yes force group = steve Some notes: when using vfs plugin then path is a relative path via gluster volume. kernel share modes = no may be required to make it work. We can also use classic fuse mount and use it under samba as share path, then configuration is even simpler. For detailed description between those two solutions see gluster vfs blog posts . Remember to add user steve to samba with a password unblock firewall ports for samba test samba config and reload samba Defining new samba service under consul Now we define gluster-samba service on gluster server hosts in a similiar way as we defined it for gluster itself. Below is an example of /etc/consul/service_samba.json : { \"service\": { \"address\": \"\", \"checks\": [ { \"interval\": \"10s\", \"tcp\": \"localhost:139\", \"timeout\": \"5s\" }, { \"interval\": \"10s\", \"tcp\": \"localhost:445\", \"timeout\": \"5s\" } ], \"enableTagOverride\": false, \"id\": \"gluster-samba\", \"name\": \"gluster-samba\", \"port\": 139, \"tags\": [ \"gluster\", \"samba\" ] } } We have two health checks here, just checking if we can connect to samba service. It could be also expanded to see if the network share is actually accessible. Reload consul service and you should after a while see new service registered in the consul. Check if it exists in dns: nslookup gluster-samba.service.consul Server: 127.0.0.1 Address: 127.0.0.1#53 Name: gluster-samba.service.consul Address: 172.30.65.61 Name: gluster-samba.service.consul Address: 172.30.64.144 Install samba-client and check connectivity to samba from gluster server itself. [centos@gluster-poc-01]# smbclient -L //gluster-samba.service.consul/g0 -U steve Enter steve's password: Domain=[test] OS=[Windows 6.1] Server=[Samba 4.4.4] Sharename Type Comment --------- ---- ------- vfs-g0 Disk Gluster via VFS (native gluster) IPC$ IPC IPC Service (Samba 4.4.4) Domain=[test] OS=[Windows 6.1] Server=[Samba 4.4.4] Server Comment --------- ------- Workgroup Master --------- ------- Now check if we can list share directory as steve : smbclient //gluster-samba.service.consul/vfs-g0/ -U steve -c ls Enter steve's password: Domain=[test] OS=[Windows 6.1] Server=[Samba 4.4.4] . D 0 Wed May 17 20:48:06 2017 .. D 0 Wed May 17 20:48:06 2017 .trashcan DH 0 Mon May 15 15:41:37 2017 CentOS-7-x86_64-Everything-1611.iso N 8280604672 Mon Dec 5 13:57:33 2016 hello.world D 0 Fri May 19 08:54:02 2017 ipconfig.all.txt A 2931 Wed May 17 20:18:52 2017 nslookup.txt A 126 Wed May 17 20:19:13 2017 net.view.txt A 315 Wed May 17 20:47:44 2017 463639360 blocks of size 1024. 447352464 blocks available Notice that this might take a few seconds, because when we try to connect to the share, samba vfs connects to the gluster servers as agent. Looks good, time to configure Windows. Installing Consul agent on Windows Log in as administrator and install consul agent on the Windows machine, the easiest way is to use chocolatey. install chocolatey and use preferred installation method, for example via cmd.exe optionally install some tools via chocolatey to edit files: chocolatey install notepadplusplus install consul as agent with specific version and configs to load: chocolatey install consul --version 0.7.5 -params '-config-dir \"%PROGRAMDATA%\\consul\\\"' stop consul service in command prompt: net stop consul edit consul config %PROGRAMDATA%\\consul\\config.json : start notepad++.exe \"%PROGRAMDATA%\\consul\\config\\config.json\" fill it with data (description below): { \"datacenter\": \"virt-gluster\", \"retry_join\": [ \"192.178.1.11\", \"192.178.1.12\", \"192.178.1.13\", ], \"recursors\": [\"8.8.8.8\", \"8.8.4.4\"], \"ports\": { \"dns\": 53 } } Remember to replace datacenter , recursors with preferred local DNS servers and retry_join with list of consul server hosts or for example some generic Route53 entry from private zone (if it exists) which points to real consul servers. In AWS you can also use retry_join_ec2 - his way Windows instance will always search other consul server EC2 instances and join them. Notice that recursors section is required if not using retry_join and just relying on AWS EC2 tags - otherwise consul will fail to resolve anything else, thus not joining to the consul. We use port 53 so that consul will serve as local DNS. start consul service net start consul update DNS settings for network interface in Windows, make it the primary entry netsh interface ipv4 add dnsserver \\\"Ethernet\\\" address=127.0.0.1 index=1 verify that DNS Servers is pointing to localhost: ipconfig /all Windows IP Configuration Host Name . . . . . . . . . . . . : WIN-S8N782O8GG3 ... ... DNS Servers . . . . . . . . . . . : 127.0.0.1 ... ... verify that consul resolves some services: nslookup gluster.service.consul nslookup gluster-samba.service.consul Server: UnKnown Address: 127.0.0.1 Name: gluster-samba.service.consul Addresses: 172.30.65.61 172.30.64.144 Mounting gluster volume under Windows We have running gluster servers with volume and samba share, registered in consul. We have Windows with running consul agent. All hosts are registered in consul and can connect to each other. verify that samba can see network share: net view \\\\gluster-samba.service.consul Shared resources at \\\\gluster-samba.service.consul Samba 4.4.4 Share name Type Used as Comment ------------------------------------------------------------------------------- vfs-g0 Disk Gluster via VFS (native gluster) The command completed successfully. mount network share, providing credentials for gluster samba share: net use s: \\\\gluster-samba.service.consul\\vfs-g0 /user:steve password: steve-loves-bacon /persistent:yes If mounting fails due to error message: System error 1219 has occurred. Multiple connections to a server or shared resource by the same user, using more than one user name, are not allowed.... then you must delete existing connections, for example: net use /delete \\\\gluster-samba.service.consul\\IPC$ And then retry the net use commands again. From now on this windows share should reconnect to the random gluster samba server, if it is healthy. Enjoy. Further steps for improvements Below is a list of things to improve: enable selinux harden samba setup on gluster servers to use domain logons use consul ACL lists to control access to consul data export gluster volumes as key/value in consul, use consul-template to create mountpoints on consul updates - in autofs/ samba mounts/unmounts expand consul health checks with more detailed checks, like: better checking if gluster volume exists etc if samba share is accessible by the client (to avoid situation samba tries to share non-mounted volume)","title":"Consul integration"},{"location":"Administrator-Guide/Consul/#consul-and-glusterfs-integration","text":"Consul is used for service discovery and configuration. It consists of consul server and agents connecting to it. Apps can get configuration data from consul via HTTP API or DNS queries. Long story short, instead of using standard hostnames and relying on official DNS servers which we may not control, we can use consul to resolve hosts with services under .consul domain, which turns this classic setup: mount -t glusterfs -o backupvolfile-server=gluster-poc-02 gluster-poc-01:/g0 /mnt/gluster/g0 into more convenient entry: mount -t glusterfs gluster.service.consul:/g0 /mnt/gluster/g0 which is especially useful when using image-based servers without further provisioning, and spreading load across all healthy servers registered in consul.","title":"Consul and GlusterFS integration"},{"location":"Administrator-Guide/Consul/#warning","text":"In this document you will get a proof-of-concept basic setup - gluster servers and gluster clients configured, which should be a point to expand. You should read Further steps section to harden it. Tested on: isolated virtual network selinux permissive (yay!) consul server/agents version v0.7.5 gluster servers with glusterfs 3.8.x on CentOS 7.3 + samba 4 with simple auth and vfs gluster module gluster volume set as distributed-replicated + 'features.shard: true' and 'features.shard-block-size: 512MB' gluster agents with glusterfs 3.8.x on Ubuntu 14.04 gluster agents with glusterfs 3.8.x on CentOS 7.3 gluster agents with glusterfs 3.7.x on CentOS 5.9 Windows 2012R2 connected to gluster servers via samba","title":"Warning"},{"location":"Administrator-Guide/Consul/#scenario","text":"We want to create shared storage accessible via different operating systems - Linux and Windows. we do not control DNS server so we cannot add/remove entries on gluster server add/remove gluster servers are in the gluster pool and have gluster volume created named g0 gluster servers have consul agent installed, and they will register to consul as gluster service gluster servers have also SMB installed with very simple setup using gluster vfs plugin gluster client have consul agent installed, and they will use gluster.service.consul as entry point. DNS resolution under Linux will be handled via dnsmasq DNS resolution under Windows will be handled via consul itself","title":"Scenario"},{"location":"Administrator-Guide/Consul/#known-limitations","text":"consul health checks introduce delay, also remember that consul can cache DNS entries to increase performance the way Windows share works is that it will connect to one of the samba servers, if this server die then transfers are aborted, and we must retry operation, but watch out for delay. anything other than gluster volume distributed-replicated was not tested - it may not work for Windows.","title":"Known limitations"},{"location":"Administrator-Guide/Consul/#requirements","text":"you should have consul server (or cluster) up and running, and the best, also accessible via default HTTP port. you should have gluster servers already joined in the gluster pool, bricks and volume configured. check you firewall rules for outbound and inbound for DNS, gluster, samba, consul make yourself familiar with consul documentation (or specific branch on github)","title":"Requirements"},{"location":"Administrator-Guide/Consul/#linux-setup","text":"","title":"Linux setup"},{"location":"Administrator-Guide/Consul/#consul-agent-on-linux-on-gluster-clients","text":"First, install consul agent. The best way is to use for example puppet module In general your Linux boxes should register in the consul server and be visible under Nodes section. To verify if consul agent is working properly, you can query its DNS interface, asking it to list consul servers: [centos@gluster-poc-01]# dig consul.service.consul 127.0.0.1:8600 ; <<>> DiG 9.9.4-RedHat-9.9.4-38.el7_3.3 <<>> consul.service.consul 127.0.0.1:8600 ;; global options: +cmd ;; Got answer: ;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 39354 ;; flags: qr aa rd ra; QUERY: 1, ANSWER: 3, AUTHORITY: 0, ADDITIONAL: 0 ;; QUESTION SECTION: ;consul.service.consul. IN A ;; ANSWER SECTION: consul.service.consul. 0 IN A 172.30.64.198 consul.service.consul. 0 IN A 172.30.82.255 consul.service.consul. 0 IN A 172.30.81.155 ;; Query time: 1 msec ;; SERVER: 127.0.0.1#53(127.0.0.1) ;; WHEN: Sat May 20 08:50:21 UTC 2017 ;; MSG SIZE rcvd: 87 ;; Got answer: ;; ->>HEADER<<- opcode: QUERY, status: NXDOMAIN, id: 22224 ;; flags: qr rd ra ad; QUERY: 1, ANSWER: 0, AUTHORITY: 0, ADDITIONAL: 0 ;; QUESTION SECTION: ;127.0.0.1:8600. IN A ;; Query time: 0 msec ;; SERVER: 127.0.0.1#53(127.0.0.1) ;; WHEN: Sat May 20 08:50:21 UTC 2017 ;; MSG SIZE rcvd: 32 Now, to be able to use it on system level, we want it to work without specifying port. We can achieve it with running consul on port 53 (not advised), or redirecting network traffic from port 53 to 8600 or proxy it via local DNS resolver - for example use locally installed dnsmasq. First, install dnsmasq, and add file /etc/dnsmasq.d/10-consul : server=/consul/127.0.0.1#8600 This will ensure that any *.consul requests will be forwarded to local consul listening on its default DNS port 8600. Make sure that /etc/resolv.conf contains nameserver 127.0.0.1 . Under Debian distros it should be there, under RedHat - not really. You can fix this in two ways, choose on your onw which one to apply: add nameserver 127.0.0.1 to /etc/resolvconf/resolv.conf.d/header or update /etc/dhcp/dhclient.conf and add to it line prepend domain-name-servers 127.0.0.1; . In both cases it ensures that dnsmasq will be a first nameserver, and requires reloading resolver or networking. Eventually you should have nameserver 127.0.0.1 set as first entry in /etc/resolv.conf and have DNS resolving consul entries: [centos@gluster-poc-01]# dig consul.service.consul ; <<>> DiG 9.9.4-RedHat-9.9.4-38.el7_3.3 <<>> consul.service.consul ;; global options: +cmd ;; Got answer: ;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 42571 ;; flags: qr aa rd ra; QUERY: 1, ANSWER: 3, AUTHORITY: 0, ADDITIONAL: 0 ;; QUESTION SECTION: ;consul.service.consul. IN A ;; ANSWER SECTION: consul.service.consul. 0 IN A 172.30.64.198 consul.service.consul. 0 IN A 172.30.82.255 consul.service.consul. 0 IN A 172.30.81.155 ;; Query time: 1 msec ;; SERVER: 127.0.0.1#53(127.0.0.1) ;; WHEN: Sat May 20 09:01:12 UTC 2017 ;; MSG SIZE rcvd: 87 From now on we should be able to use <servicename>.service.consul in places, where we had FQDN entries of the single servers. Next, we must define gluster service consul on the servers.","title":"Consul agent on Linux on gluster clients"},{"location":"Administrator-Guide/Consul/#consul-agent-on-linux-on-gluster-servers","text":"Install consul agent as described in previous section. You can define consul services as gluster to run health checks, to do that we must add consul to sudoers or allow it executing certain sudo commands without password: /etc/sudoers.d/99-consul.conf : consul ALL=(ALL) NOPASSWD: /sbin/gluster pool list First, lets define service in consul, it will be very basic, without volume names. Service name gluster , with default port 24007, and we will tag it as gluster and server . Our service will have service health checks every 10s: check if the gluster service is responding to TCP on 24007 port check if the gluster server is connected to other peers in the pool (to avoid registering as healthy service which is actaully not serving anything) Below is an example of /etc/consul/service_gluster.json : { \"service\": { \"address\": \"\", \"checks\": [ { \"interval\": \"10s\", \"tcp\": \"localhost:24007\", \"timeout\": \"5s\" }, { \"interval\": \"10s\", \"script\": \"/bin/bash -c \\\"sudo -n /sbin/gluster pool list |grep -v UUID|grep -v localhost|grep Connected\\\"\", \"timeout\": \"5s\" } ], \"enableTagOverride\": false, \"id\": \"gluster\", \"name\": \"gluster\", \"port\": 24007, \"tags\": [ \"gluster\", \"server\" ] } } Restart consul service and you should see gluster servers in consul web ui. After a while service should be in healthy stage and be available under nslookup: [centos@gluster-poc-02]# nslookup gluster.service.consul Server: 127.0.0.1 Address: 127.0.0.1#53 Name: gluster.service.consul Address: 172.30.64.144 Name: gluster.service.consul Address: 172.30.65.61 Notice that gluster server can be also gluster client, for example if we want to mount gluster volume on the servers.","title":"Consul agent on Linux on gluster servers"},{"location":"Administrator-Guide/Consul/#mounting-gluster-volume-under-linux","text":"As a moutpoint we would usually select one of the gluster servers, and another as backup server, like this: mount -t glusterfs -o backupvolfile-server=gluster-poc-02 gluster-poc-01:/g0 /mnt/gluster/g0 This is a bit inconvenient, for example we have an image with hardcoded hostnames, and old servers are gone due to maintenance. We would have to recreate image, or reconfigure existing nodes if they unmount gluster storage. To mitigate that issue we can now use consul for fetching the server pool: mount -t glusterfs gluster.service.consul:/g0 /mnt/gluster/g0 So we can populate that to /etc/fstab or one of the autofs files.","title":"Mounting gluster volume under Linux"},{"location":"Administrator-Guide/Consul/#windows-setup","text":"","title":"Windows setup"},{"location":"Administrator-Guide/Consul/#configuring-gluster-servers-as-samba-shares","text":"This is the simplest and not so secure setup, you have been warned. Proper setup suggests using LDAP or CTDB . You can configure it with puppet using module kakwa-samba . First, we want to reconfigure gluster servers so that they serve as samba shares using user/pass credentials, which is separate to Linux credentials. We assume that accessing windows share will be done as user steve with password steve-loves-bacon , make sure you create that user on each gluster server host. sudo adduser steve sudo smbpasswd -a steve Notice that if you do not set user.smb = disable in gluster volume then it may auto-add itself to samba configuration. So better disable this by executing: gluster volume get g0 user.smb disable Now install samba and samba-vfs-glusterfs packages and configure /etc/samba/smb.conf : [global] workgroup = test security = user min protocol = SMB2 netbios name = gluster realm = test vfs objects = acl_xattr map acl inherit = Yes store dos attributes = Yes log level = 1 dedicated keytab file = /etc/krb5.keytab map untrusted to domain = Yes [vfs-g0] guest only = no writable = yes guest ok = no force user = steve create mask = 0666 directory mask = 0777 comment = Gluster via VFS (native gluster) path = / vfs objects = glusterfs glusterfs:volume = g0 kernel share modes = no glusterfs:loglevel = 7 glusterfs:logfile = /var/log/samba/glusterfs-g0.%M.log browsable = yes force group = steve Some notes: when using vfs plugin then path is a relative path via gluster volume. kernel share modes = no may be required to make it work. We can also use classic fuse mount and use it under samba as share path, then configuration is even simpler. For detailed description between those two solutions see gluster vfs blog posts . Remember to add user steve to samba with a password unblock firewall ports for samba test samba config and reload samba","title":"Configuring gluster servers as samba shares"},{"location":"Administrator-Guide/Consul/#defining-new-samba-service-under-consul","text":"Now we define gluster-samba service on gluster server hosts in a similiar way as we defined it for gluster itself. Below is an example of /etc/consul/service_samba.json : { \"service\": { \"address\": \"\", \"checks\": [ { \"interval\": \"10s\", \"tcp\": \"localhost:139\", \"timeout\": \"5s\" }, { \"interval\": \"10s\", \"tcp\": \"localhost:445\", \"timeout\": \"5s\" } ], \"enableTagOverride\": false, \"id\": \"gluster-samba\", \"name\": \"gluster-samba\", \"port\": 139, \"tags\": [ \"gluster\", \"samba\" ] } } We have two health checks here, just checking if we can connect to samba service. It could be also expanded to see if the network share is actually accessible. Reload consul service and you should after a while see new service registered in the consul. Check if it exists in dns: nslookup gluster-samba.service.consul Server: 127.0.0.1 Address: 127.0.0.1#53 Name: gluster-samba.service.consul Address: 172.30.65.61 Name: gluster-samba.service.consul Address: 172.30.64.144 Install samba-client and check connectivity to samba from gluster server itself. [centos@gluster-poc-01]# smbclient -L //gluster-samba.service.consul/g0 -U steve Enter steve's password: Domain=[test] OS=[Windows 6.1] Server=[Samba 4.4.4] Sharename Type Comment --------- ---- ------- vfs-g0 Disk Gluster via VFS (native gluster) IPC$ IPC IPC Service (Samba 4.4.4) Domain=[test] OS=[Windows 6.1] Server=[Samba 4.4.4] Server Comment --------- ------- Workgroup Master --------- ------- Now check if we can list share directory as steve : smbclient //gluster-samba.service.consul/vfs-g0/ -U steve -c ls Enter steve's password: Domain=[test] OS=[Windows 6.1] Server=[Samba 4.4.4] . D 0 Wed May 17 20:48:06 2017 .. D 0 Wed May 17 20:48:06 2017 .trashcan DH 0 Mon May 15 15:41:37 2017 CentOS-7-x86_64-Everything-1611.iso N 8280604672 Mon Dec 5 13:57:33 2016 hello.world D 0 Fri May 19 08:54:02 2017 ipconfig.all.txt A 2931 Wed May 17 20:18:52 2017 nslookup.txt A 126 Wed May 17 20:19:13 2017 net.view.txt A 315 Wed May 17 20:47:44 2017 463639360 blocks of size 1024. 447352464 blocks available Notice that this might take a few seconds, because when we try to connect to the share, samba vfs connects to the gluster servers as agent. Looks good, time to configure Windows.","title":"Defining new samba service under consul"},{"location":"Administrator-Guide/Consul/#installing-consul-agent-on-windows","text":"Log in as administrator and install consul agent on the Windows machine, the easiest way is to use chocolatey. install chocolatey and use preferred installation method, for example via cmd.exe optionally install some tools via chocolatey to edit files: chocolatey install notepadplusplus install consul as agent with specific version and configs to load: chocolatey install consul --version 0.7.5 -params '-config-dir \"%PROGRAMDATA%\\consul\\\"' stop consul service in command prompt: net stop consul edit consul config %PROGRAMDATA%\\consul\\config.json : start notepad++.exe \"%PROGRAMDATA%\\consul\\config\\config.json\" fill it with data (description below): { \"datacenter\": \"virt-gluster\", \"retry_join\": [ \"192.178.1.11\", \"192.178.1.12\", \"192.178.1.13\", ], \"recursors\": [\"8.8.8.8\", \"8.8.4.4\"], \"ports\": { \"dns\": 53 } } Remember to replace datacenter , recursors with preferred local DNS servers and retry_join with list of consul server hosts or for example some generic Route53 entry from private zone (if it exists) which points to real consul servers. In AWS you can also use retry_join_ec2 - his way Windows instance will always search other consul server EC2 instances and join them. Notice that recursors section is required if not using retry_join and just relying on AWS EC2 tags - otherwise consul will fail to resolve anything else, thus not joining to the consul. We use port 53 so that consul will serve as local DNS. start consul service net start consul update DNS settings for network interface in Windows, make it the primary entry netsh interface ipv4 add dnsserver \\\"Ethernet\\\" address=127.0.0.1 index=1 verify that DNS Servers is pointing to localhost: ipconfig /all Windows IP Configuration Host Name . . . . . . . . . . . . : WIN-S8N782O8GG3 ... ... DNS Servers . . . . . . . . . . . : 127.0.0.1 ... ... verify that consul resolves some services: nslookup gluster.service.consul nslookup gluster-samba.service.consul Server: UnKnown Address: 127.0.0.1 Name: gluster-samba.service.consul Addresses: 172.30.65.61 172.30.64.144","title":"Installing Consul agent on Windows"},{"location":"Administrator-Guide/Consul/#mounting-gluster-volume-under-windows","text":"We have running gluster servers with volume and samba share, registered in consul. We have Windows with running consul agent. All hosts are registered in consul and can connect to each other. verify that samba can see network share: net view \\\\gluster-samba.service.consul Shared resources at \\\\gluster-samba.service.consul Samba 4.4.4 Share name Type Used as Comment ------------------------------------------------------------------------------- vfs-g0 Disk Gluster via VFS (native gluster) The command completed successfully. mount network share, providing credentials for gluster samba share: net use s: \\\\gluster-samba.service.consul\\vfs-g0 /user:steve password: steve-loves-bacon /persistent:yes If mounting fails due to error message: System error 1219 has occurred. Multiple connections to a server or shared resource by the same user, using more than one user name, are not allowed.... then you must delete existing connections, for example: net use /delete \\\\gluster-samba.service.consul\\IPC$ And then retry the net use commands again. From now on this windows share should reconnect to the random gluster samba server, if it is healthy. Enjoy.","title":"Mounting gluster volume under Windows"},{"location":"Administrator-Guide/Consul/#further-steps-for-improvements","text":"Below is a list of things to improve: enable selinux harden samba setup on gluster servers to use domain logons use consul ACL lists to control access to consul data export gluster volumes as key/value in consul, use consul-template to create mountpoints on consul updates - in autofs/ samba mounts/unmounts expand consul health checks with more detailed checks, like: better checking if gluster volume exists etc if samba share is accessible by the client (to avoid situation samba tries to share non-mounted volume)","title":"Further steps for improvements"},{"location":"Administrator-Guide/Directory-Quota/","text":"Managing Directory Quota Directory quotas in GlusterFS allow you to set limits on the usage of the disk space by directories or volumes. The storage administrators can control the disk space utilization at the directory and/or volume levels in GlusterFS by setting limits to allocatable disk space at any level in the volume and directory hierarchy. This is particularly useful in cloud deployments to facilitate the utility billing model. Note For now, only Hard limits are supported. Here, the limit cannot be exceeded, and attempts to use more disk space or inodes beyond the set limit are denied. System administrators can also monitor the resource utilization to limit the storage for the users depending on their role in the organization. You can set the quota at the following levels: Directory level \u2013 limits the usage at the directory level Volume level \u2013 limits the usage at the volume level Note You can set the quota limit on an empty directory. The quota limit will be automatically enforced when files are added to the directory. Enabling Quota You must enable Quota to set disk limits. To enable quota: Use the following command to enable quota: # gluster volume quota <VOLNAME> enable For example, to enable quota on the test-volume: # gluster volume quota test-volume enable Quota is enabled on /test-volume Disabling Quota You can disable Quota if needed. To disable quota: Use the following command to disable quota: # gluster volume quota <VOLNAME> disable For example, to disable quota translator on the test-volume: # gluster volume quota test-volume disable Quota translator is disabled on /test-volume Setting or Replacing Disk Limit You can create new directories in your storage environment and set the disk limit or set disk limit for the existing directories. The directory name should be relative to the volume with the export directory/mount being treated as \"/\". To set or replace disk limit: Set the disk limit using the following command: # gluster volume quota <VOLNAME> limit-usage <DIR> <HARD_LIMIT> For example, to set a limit on data directory on the test-volume where data is a directory under the export directory: # gluster volume quota test-volume limit-usage /data 10GB Usage limit has been set on /data Note In a multi-level directory hierarchy, the strictest disk limit will be considered for enforcement. Also, whenever the quota limit is set for the first time, an auxiliary mount point will be created under /var/run/gluster/ . This is just like any other mount point with some special permissions and remains until the quota is disabled. This mount point is being used by quota to set and display limits and lists respectively. Displaying Disk Limit Information You can display disk limit information on all the directories on which the limit is set. To display disk limit information: Display disk limit information of all the directories on which limit is set, using the following command: # gluster volume quota <VOLNAME> list For example, to see the set disks limit on the test-volume: # gluster volume quota test-volume list /Test/data 10 GB 6 GB /Test/data1 10 GB 4 GB Display disk limit information on a particular directory on which limit is set, using the following command: # gluster volume quota <VOLNAME> list <DIR> For example, to view the set limit on /data directory of test-volume: # gluster volume quota test-volume list /data /Test/data 10 GB 6 GB Displaying Quota Limit Information Using the df Utility You can create a report of the disk usage using the df utility by considering quota limits. To generate a report, run the following command: # gluster volume set <VOLNAME> quota-deem-statfs on In this case, the total disk space of the directory is taken as the quota hard limit set on the directory of the volume. Note The default value for quota-deem-statfs is on when the quota is enabled and it is recommended to keep quota-deem-statfs on. The following example displays the disk usage when quota-deem-statfs is off: # gluster volume set test-volume features.quota-deem-statfs off volume set: success # gluster volume quota test-volume list Path Hard-limit Soft-limit Used Available --------------------------------------------------------------- / 300.0GB 90% 11.5GB 288.5GB /John/Downloads 77.0GB 75% 11.5GB 65.5GB Disk usage for volume test-volume as seen on client1: # df -hT /home Filesystem Type Size Used Avail Use% Mounted on server1:/test-volume fuse.glusterfs 400G 12G 389G 3% /home The following example displays the disk usage when quota-deem-statfs is on: # gluster volume set test-volume features.quota-deem-statfs on volume set: success # gluster vol quota test-volume list Path Hard-limit Soft-limit Used Available ----------------------------------------------------------- / 300.0GB 90% 11.5GB 288.5GB /John/Downloads 77.0GB 75% 11.5GB 65.5GB Disk usage for volume test-volume as seen on client1: # df -hT /home Filesystem Type Size Used Avail Use% Mounted on server1:/test-volume fuse.glusterfs 300G 12G 289G 4% /home The quota-deem-statfs option when set to on, allows the administrator to make the user view the total disk space available on the directory as the hard limit set on it. Updating Memory Cache Size Setting Timeout For performance reasons, quota caches the directory sizes on the client. You can set a timeout indicating the maximum valid duration of directory sizes in the cache, from the time they are populated. For example: If multiple clients are writing to a single directory, there are chances that some other client might write till the quota limit is exceeded. However, this new file-size may not get reflected in the client till the size entry in the cache has become stale because of timeout. If writes happen on this client during this duration, they are allowed even though they would lead to exceeding of quota-limits, since the size in the cache is not in sync with the actual size. When a timeout happens, the size in the cache is updated from servers and will be in sync and no further writes will be allowed. A timeout of zero will force fetching of directory sizes from the server for every operation that modifies file data and will effectively disable directory size caching on the client-side. To update the memory cache size: Use the following command to update the memory cache size: Soft Timeout: The frequency at which the quota server-side translator checks the volume usage when the usage is below the soft limit. The soft timeout is in effect when the disk usage is less than the soft limit. # gluster volume set <VOLNAME> features.soft-timeout <time> Hard Timeout: The frequency at which the quota server-side translator checks the volume usage when the usage is above the soft limit. The hard timeout is in effect when the disk usage is between the soft limit and the hard limit. # gluster volume set <VOLNAME> features.hard-timeout <time> For example, to update the memory cache size for every 5 seconds on test-volume in case of hard-timeout: # gluster volume set test-volume features.hard-timeout 5 Set volume successful Setting Alert Time Alert time is the frequency at which you want your usage information to be logged after you reach the soft limit. To set the alert time: Use the following command to set the alert time: # gluster volume quota <VOLNAME> alert-time <time> Note The default alert-time is one week. For example, to set the alert time to one day: # gluster volume quota test-volume alert-time 1d volume quota : success Removing Disk Limit You can remove the set disk limit if you do not want a quota anymore. To remove disk limit: Use the following command to remove the disk limit set on a particular directory: # gluster volume quota <VOLNAME> remove <DIR> For example, to remove the disk limit on /data directory of test-volume: # gluster volume quota test-volume remove /data Usage limit set on /data is removed","title":"Quotas"},{"location":"Administrator-Guide/Directory-Quota/#managing-directory-quota","text":"Directory quotas in GlusterFS allow you to set limits on the usage of the disk space by directories or volumes. The storage administrators can control the disk space utilization at the directory and/or volume levels in GlusterFS by setting limits to allocatable disk space at any level in the volume and directory hierarchy. This is particularly useful in cloud deployments to facilitate the utility billing model. Note For now, only Hard limits are supported. Here, the limit cannot be exceeded, and attempts to use more disk space or inodes beyond the set limit are denied. System administrators can also monitor the resource utilization to limit the storage for the users depending on their role in the organization. You can set the quota at the following levels: Directory level \u2013 limits the usage at the directory level Volume level \u2013 limits the usage at the volume level Note You can set the quota limit on an empty directory. The quota limit will be automatically enforced when files are added to the directory.","title":"Managing Directory Quota"},{"location":"Administrator-Guide/Directory-Quota/#enabling-quota","text":"You must enable Quota to set disk limits. To enable quota: Use the following command to enable quota: # gluster volume quota <VOLNAME> enable For example, to enable quota on the test-volume: # gluster volume quota test-volume enable Quota is enabled on /test-volume","title":"Enabling Quota"},{"location":"Administrator-Guide/Directory-Quota/#disabling-quota","text":"You can disable Quota if needed. To disable quota: Use the following command to disable quota: # gluster volume quota <VOLNAME> disable For example, to disable quota translator on the test-volume: # gluster volume quota test-volume disable Quota translator is disabled on /test-volume","title":"Disabling Quota"},{"location":"Administrator-Guide/Directory-Quota/#setting-or-replacing-disk-limit","text":"You can create new directories in your storage environment and set the disk limit or set disk limit for the existing directories. The directory name should be relative to the volume with the export directory/mount being treated as \"/\". To set or replace disk limit: Set the disk limit using the following command: # gluster volume quota <VOLNAME> limit-usage <DIR> <HARD_LIMIT> For example, to set a limit on data directory on the test-volume where data is a directory under the export directory: # gluster volume quota test-volume limit-usage /data 10GB Usage limit has been set on /data Note In a multi-level directory hierarchy, the strictest disk limit will be considered for enforcement. Also, whenever the quota limit is set for the first time, an auxiliary mount point will be created under /var/run/gluster/ . This is just like any other mount point with some special permissions and remains until the quota is disabled. This mount point is being used by quota to set and display limits and lists respectively.","title":"Setting or Replacing Disk Limit"},{"location":"Administrator-Guide/Directory-Quota/#displaying-disk-limit-information","text":"You can display disk limit information on all the directories on which the limit is set. To display disk limit information: Display disk limit information of all the directories on which limit is set, using the following command: # gluster volume quota <VOLNAME> list For example, to see the set disks limit on the test-volume: # gluster volume quota test-volume list /Test/data 10 GB 6 GB /Test/data1 10 GB 4 GB Display disk limit information on a particular directory on which limit is set, using the following command: # gluster volume quota <VOLNAME> list <DIR> For example, to view the set limit on /data directory of test-volume: # gluster volume quota test-volume list /data /Test/data 10 GB 6 GB","title":"Displaying Disk Limit Information"},{"location":"Administrator-Guide/Directory-Quota/#displaying-quota-limit-information-using-the-df-utility","text":"You can create a report of the disk usage using the df utility by considering quota limits. To generate a report, run the following command: # gluster volume set <VOLNAME> quota-deem-statfs on In this case, the total disk space of the directory is taken as the quota hard limit set on the directory of the volume. Note The default value for quota-deem-statfs is on when the quota is enabled and it is recommended to keep quota-deem-statfs on. The following example displays the disk usage when quota-deem-statfs is off: # gluster volume set test-volume features.quota-deem-statfs off volume set: success # gluster volume quota test-volume list Path Hard-limit Soft-limit Used Available --------------------------------------------------------------- / 300.0GB 90% 11.5GB 288.5GB /John/Downloads 77.0GB 75% 11.5GB 65.5GB Disk usage for volume test-volume as seen on client1: # df -hT /home Filesystem Type Size Used Avail Use% Mounted on server1:/test-volume fuse.glusterfs 400G 12G 389G 3% /home The following example displays the disk usage when quota-deem-statfs is on: # gluster volume set test-volume features.quota-deem-statfs on volume set: success # gluster vol quota test-volume list Path Hard-limit Soft-limit Used Available ----------------------------------------------------------- / 300.0GB 90% 11.5GB 288.5GB /John/Downloads 77.0GB 75% 11.5GB 65.5GB Disk usage for volume test-volume as seen on client1: # df -hT /home Filesystem Type Size Used Avail Use% Mounted on server1:/test-volume fuse.glusterfs 300G 12G 289G 4% /home The quota-deem-statfs option when set to on, allows the administrator to make the user view the total disk space available on the directory as the hard limit set on it.","title":"Displaying Quota Limit Information Using the df Utility"},{"location":"Administrator-Guide/Directory-Quota/#updating-memory-cache-size","text":"","title":"Updating Memory Cache Size"},{"location":"Administrator-Guide/Directory-Quota/#setting-timeout","text":"For performance reasons, quota caches the directory sizes on the client. You can set a timeout indicating the maximum valid duration of directory sizes in the cache, from the time they are populated. For example: If multiple clients are writing to a single directory, there are chances that some other client might write till the quota limit is exceeded. However, this new file-size may not get reflected in the client till the size entry in the cache has become stale because of timeout. If writes happen on this client during this duration, they are allowed even though they would lead to exceeding of quota-limits, since the size in the cache is not in sync with the actual size. When a timeout happens, the size in the cache is updated from servers and will be in sync and no further writes will be allowed. A timeout of zero will force fetching of directory sizes from the server for every operation that modifies file data and will effectively disable directory size caching on the client-side. To update the memory cache size: Use the following command to update the memory cache size: Soft Timeout: The frequency at which the quota server-side translator checks the volume usage when the usage is below the soft limit. The soft timeout is in effect when the disk usage is less than the soft limit. # gluster volume set <VOLNAME> features.soft-timeout <time> Hard Timeout: The frequency at which the quota server-side translator checks the volume usage when the usage is above the soft limit. The hard timeout is in effect when the disk usage is between the soft limit and the hard limit. # gluster volume set <VOLNAME> features.hard-timeout <time> For example, to update the memory cache size for every 5 seconds on test-volume in case of hard-timeout: # gluster volume set test-volume features.hard-timeout 5 Set volume successful","title":"Setting Timeout"},{"location":"Administrator-Guide/Directory-Quota/#setting-alert-time","text":"Alert time is the frequency at which you want your usage information to be logged after you reach the soft limit. To set the alert time: Use the following command to set the alert time: # gluster volume quota <VOLNAME> alert-time <time> Note The default alert-time is one week. For example, to set the alert time to one day: # gluster volume quota test-volume alert-time 1d volume quota : success","title":"Setting Alert Time"},{"location":"Administrator-Guide/Directory-Quota/#removing-disk-limit","text":"You can remove the set disk limit if you do not want a quota anymore. To remove disk limit: Use the following command to remove the disk limit set on a particular directory: # gluster volume quota <VOLNAME> remove <DIR> For example, to remove the disk limit on /data directory of test-volume: # gluster volume quota test-volume remove /data Usage limit set on /data is removed","title":"Removing Disk Limit"},{"location":"Administrator-Guide/Events-APIs/","text":"Events APIs New in version 3.9 NOTE : glusterfs-selinux package would have to be installed for events feature to function properly when the selinux is in enforced mode. In addition to that, the default port to be used for eventsd has now been changed to 55555 and it has to lie between the ephemeral port ranges. Set PYTHONPATH(Only in case of Source installation) If Gluster is installed using source install, cliutils will get installed under /usr/local/lib/python.2.7/site-packages Set PYTHONPATH by adding in ~/.bashrc # export PYTHONPATH=/usr/local/lib/python2.7/site-packages:$PYTHONPATH Enable and Start Events APIs Enable and Start glustereventsd in all peer nodes In Systems using Systemd, # systemctl enable glustereventsd # systemctl start glustereventsd FreeBSD or others, add the following in /etc/rc.conf glustereventsd_enable=\"YES\" And start the glustereventsd using, # service glustereventsd start SysVInit(CentOS 6), # chkconfig glustereventsd on # service glustereventsd start Status Status Can be checked using, # gluster-eventsapi status Example output: Webhooks: None +-----------+-------------+-----------------------+ | NODE | NODE STATUS | GLUSTEREVENTSD STATUS | +-----------+-------------+-----------------------+ | localhost | UP | UP | | node2 | UP | UP | +-----------+-------------+-----------------------+ Webhooks Webhooks are similar to callbacks(over HTTP), on event Gluster will call the Webhook URL(via POST) which is configured. Webhook is a web server which listens on a URL, this can be deployed outside of the Cluster. Gluster nodes should be able to access this Webhook server on the configured port. Example Webhook written in python, from flask import Flask, request app = Flask(__name__) @app.route(\"/listen\", methods=[\"POST\"]) def events_listener(): gluster_event = request.json if gluster_event is None: # No event to process, may be test call return \"OK\" # Process gluster_event # { # \"nodeid\": NODEID, # \"ts\": EVENT_TIMESTAMP, # \"event\": EVENT_TYPE, # \"message\": EVENT_DATA # } print (gluster_event) return \"OK\" app.run(host=\"0.0.0.0\", port=9000) Test and Register webhook using following commands, usage: gluster-eventsapi webhook-test [-h] [--bearer_token BEARER_TOKEN] url positional arguments: url URL of Webhook optional arguments: -h, --help show this help message and exit --bearer_token BEARER_TOKEN, -t BEARER_TOKEN Bearer Token Example(Webhook server is running in 192.168.122.188:9000 ), # gluster-eventsapi webhook-test http://192.168.122.188:9000/listen +-----------+-------------+----------------+ | NODE | NODE STATUS | WEBHOOK STATUS | +-----------+-------------+----------------+ | localhost | UP | OK | | node2 | UP | OK | +-----------+-------------+----------------+ If Webhook status is OK from all peer nodes then register the Webhook using, usage: gluster-eventsapi webhook-add [-h] [--bearer_token BEARER_TOKEN] url positional arguments: url URL of Webhook optional arguments: -h, --help show this help message and exit --bearer_token BEARER_TOKEN, -t BEARER_TOKEN Bearer Token Example, # gluster-eventsapi webhook-add http://192.168.122.188:9000/listen +-----------+-------------+-------------+ | NODE | NODE STATUS | SYNC STATUS | +-----------+-------------+-------------+ | localhost | UP | OK | | node2 | UP | OK | +-----------+-------------+-------------+ Note : If Sync status is Not OK for any node, then make sure to run following command from a peer node when that node comes up. # gluster-eventsapi sync To unsubscribe from events, delete the webhook using following command usage: gluster-eventsapi webhook-del [-h] url positional arguments: url URL of Webhook optional arguments: -h, --help show this help message and exit Example, # gluster-eventsapi webhook-del http://192.168.122.188:9000/listen Configuration View all configurations using, usage: gluster-eventsapi config-get [-h] [--name NAME] optional arguments: -h, --help show this help message and exit --name NAME Config Name Example output: +--------------------+-------+ | NAME | VALUE | +--------------------+-------+ | log-level | INFO | | port | 55555 | | disable-events-log | False | +--------------------+-------+ To change any configuration, usage: gluster-eventsapi config-set [-h] name value positional arguments: name Config Name value Config Value optional arguments: -h, --help show this help message and exit Example output, +-----------+-------------+-------------+ | NODE | NODE STATUS | SYNC STATUS | +-----------+-------------+-------------+ | localhost | UP | OK | | node2 | UP | OK | +-----------+-------------+-------------+ To Reset any configuration, usage: gluster-eventsapi config-reset [-h] name positional arguments: name Config Name or all optional arguments: -h, --help show this help message and exit Example output, +-----------+-------------+-------------+ | NODE | NODE STATUS | SYNC STATUS | +-----------+-------------+-------------+ | localhost | UP | OK | | node2 | UP | OK | +-----------+-------------+-------------+ Note : If any node status is not UP or sync status is not OK, make sure to run gluster-eventsapi sync from a peer node. Add node to the Cluster When a new node added to the cluster, Enable and Start Eventsd in the new node using the steps mentioned above Run gluster-eventsapi sync command from a peer node other than the new node. APIs documentation Glustereventsd pushes the Events in JSON format to configured Webhooks. All Events will have following attributes. Attribute Description nodeid Node UUID ts Event Timestamp event Event Type message Event Specific Data Example: { \"nodeid\": \"95cd599c-5d87-43c1-8fba-b12821fd41b6\", \"ts\": 1468303352, \"event\": \"VOLUME_CREATE\", \"message\": { \"name\": \"gv1\" } } \"message\" can have following attributes based on the type of event. Peer Events Event Type Attribute Description PEER_ATTACH host Hostname or IP of added node PEER_DETACH host Hostname or IP of detached node Volume Events Event Type Attribute Description VOLUME_CREATE name Volume Name VOLUME_START force Force option used or not during Start name Volume Name VOLUME_STOP force Force option used or not during Stop name Volume Name VOLUME_DELETE name Volume Name VOLUME_SET name Volume Name options List of Options[(key1, val1), (key2, val2),..] VOLUME_RESET name Volume Name option Option Name Brick Events Event Type Attribute Description BRICK_RESET_START volume Volume Name source-brick Source Brick details BRICK_RESET_COMMIT volume Volume Name destination-brick Destination Brick source-brick Source Brick details BRICK_REPLACE volume Volume Name destination-brick Destination Brick source-brick Source Brick details Georep Events Event Type Attribute Description GEOREP_CREATE force Force option used during session Create secondary Secondary Details(Secondaryhost::SecondaryVolume) no_verify No verify option is used or not push_pem Push pem option is used or Not ssh_port If SSH port is configured during Session Create primary Primary Volume Name GEOREP_START force Force option used during session Start Primary Primary Volume Name secondary Secondary Details(Secondaryhost::SecondaryVolume) GEOREP_STOP force Force option used during session Stop primary Primary Volume Name secondary Secondary Details(Secondaryhost::SecondaryVolume) GEOREP_PAUSE force Force option used during session Pause primary Primary Volume Name secondary Secondary Details(Secondaryhost::SecondaryVolume) GEOREP_RESUME force Force option used during session Resume primary Primary Volume Name secondary Secondary Details(Secondaryhost::SecondaryVolume) GEOREP_DELETE force Force option used during session Delete primary Primary Volume Name secondary Secondary Details(Secondaryhost::SecondaryVolume) GEOREP_CONFIG_SET primary Primary Volume Name secondary Secondary Details(Secondaryhost::SecondaryVolume) option Name of Geo-rep config value Changed Value GEOREP_CONFIG_RESET primary Primary Volume Name secondary Secondary Details(Secondaryhost::SecondaryVolume) option Name of Geo-rep config Bitrot Events Event Type Attribute Description BITROT_ENABLE name Volume Name BITROT_DISABLE name Volume Name BITROT_SCRUB_THROTTLE name Volume Name value Changed Value BITROT_SCRUB_FREQ name Volume Name value Changed Value BITROT_SCRUB_OPTION name Volume Name value Changed Value Quota Events Event Type Attribute Description QUOTA_ENABLE volume Volume Name QUOTA_DISABLE volume Volume Name QUOTA_SET_USAGE_LIMIT volume Volume Name path Path in Volume on which Quota option is set limit Changed Value QUOTA_SET_OBJECTS_LIMIT volume Volume Name path Path in Volume on which Quota option is set limit Changed Value QUOTA_REMOVE_USAGE_LIMIT volume Volume Name path Path in Volume on which Quota option is Reset QUOTA_REMOVE_OBJECTS_LIMIT volume Volume Name path Path in Volume on which Quota option is Reset QUOTA_ALERT_TIME volume Volume Name time Changed Alert Time QUOTA_SOFT_TIMEOUT volume Volume Name soft-timeout Changed Value QUOTA_HARD_TIMEOUT volume Volume Name hard-timeout Changed Value QUOTA_DEFAULT_SOFT_LIMIT volume Volume Name default-soft-limit Changed Value Snapshot Events Event Type Attribute Description SNAPSHOT_CREATED snapshot_name Snapshot Name volume_name Volume Name snapshot_uuid Snapshot UUID SNAPSHOT_CREATE_FAILED snapshot_name Snapshot Name volume_name Volume Name error Failure details SNAPSHOT_ACTIVATED snapshot_name Snapshot Name snapshot_uuid Snapshot UUID SNAPSHOT_ACTIVATE_FAILED snapshot_name Snapshot Name error Failure details SNAPSHOT_DEACTIVATED snapshot_name Snapshot Name snapshot_uuid Snapshot UUID SNAPSHOT_DEACTIVATE_FAILED snapshot_name Snapshot Name error Failure details SNAPSHOT_SOFT_LIMIT_REACHED volume_name Volume Name volume_id Volume ID SNAPSHOT_HARD_LIMIT_REACHED volume_name Volume Name volume_id Volume ID SNAPSHOT_RESTORED snapshot_name Snapshot Name volume_name Volume Name snapshot_uuid Snapshot UUID SNAPSHOT_RESTORE_FAILED snapshot_name Snapshot Name error Failure details SNAPSHOT_DELETED snapshot_name Snapshot Name snapshot_uuid Snapshot UUID SNAPSHOT_DELETE_FAILED snapshot_name Snapshot Name error Failure details SNAPSHOT_CLONED clone_uuid Snapshot Clone UUID snapshot_name Snapshot Name clone_name Snapshot Clone Name SNAPSHOT_CLONE_FAILED snapshot_name Snapshot Name clone_name Snapshot Clone Name error Failure details SNAPSHOT_CONFIG_UPDATED auto-delete Auto delete Value if available config_type Volume Config or System Config hard_limit Hard Limit Value if available soft_limit Soft Limit Value if available snap-activate Snap activate Value if available SNAPSHOT_CONFIG_UPDATE_FAILED error Error details SNAPSHOT_SCHEDULER_INITIALISED status Succss Status SNAPSHOT_SCHEDULER_INIT_FAILED error Error details SNAPSHOT_SCHEDULER_ENABLED status Succss Status SNAPSHOT_SCHEDULER_ENABLE_FAILED error Error details SNAPSHOT_SCHEDULER_DISABLED status Succss Status SNAPSHOT_SCHEDULER_DISABLE_FAILED error Error details SNAPSHOT_SCHEDULER_SCHEDULE_ADDED status Succss Status SNAPSHOT_SCHEDULER_SCHEDULE_ADD_FAILED error Error details SNAPSHOT_SCHEDULER_SCHEDULE_EDITED status Succss Status SNAPSHOT_SCHEDULER_SCHEDULE_EDIT_FAILED error Error details SNAPSHOT_SCHEDULER_SCHEDULE_DELETED status Succss Status SNAPSHOT_SCHEDULER_SCHEDULE_DELETE_FAILED error Error details Svc Events Event Type Attribute Description SVC_MANAGER_FAILED volume Volume Name if available svc_name Service Name SVC_CONNECTED volume Volume Name if available svc_name Service Name SVC_DISCONNECTED svc_name Service Name Peer Events Event Type Attribute Description PEER_STORE_FAILURE peer Hostname or IP PEER_RPC_CREATE_FAILED peer Hostname or IP PEER_REJECT peer Hostname or IP PEER_CONNECT host Hostname or IP uuid Host UUID PEER_DISCONNECT host Hostname or IP uuid Host UUID state Disconnect State PEER_NOT_FOUND peer Hostname or IP uuid Host UUID Unknown Events Event Type Attribute Description UNKNOWN_PEER peer Hostname or IP Brick Events Event Type Attribute Description BRICK_START_FAILED peer Hostname or IP volume Volume Name brick Brick BRICK_STOP_FAILED peer Hostname or IP volume Volume Name brick Brick BRICK_DISCONNECTED peer Hostname or IP volume Volume Name brick Brick BRICK_CONNECTED peer Hostname or IP volume Volume Name brick Brick Bricks Events Event Type Attribute Description BRICKS_START_FAILED volume Volume Name Brickpath Events Event Type Attribute Description BRICKPATH_RESOLVE_FAILED peer Hostname or IP volume Volume Name brick Brick Notify Events Event Type Attribute Description NOTIFY_UNKNOWN_OP op Operation Name Quorum Events Event Type Attribute Description QUORUM_LOST volume Volume Name QUORUM_REGAINED volume Volume Name Rebalance Events Event Type Attribute Description REBALANCE_START_FAILED volume Volume Name REBALANCE_STATUS_UPDATE_FAILED volume Volume Name Import Events Event Type Attribute Description IMPORT_QUOTA_CONF_FAILED volume Volume Name IMPORT_VOLUME_FAILED volume Volume Name IMPORT_BRICK_FAILED peer Hostname or IP brick Brick details Compare Events Event Type Attribute Description COMPARE_FRIEND_VOLUME_FAILED volume Volume Name Ec Events Event Type Attribute Description EC_MIN_BRICKS_NOT_UP subvol Subvolume EC_MIN_BRICKS_UP subvol Subvolume Georep Events Event Type Attribute Description GEOREP_FAULTY primary_node Hostname or IP of Primary Volume brick_path Brick Path secondary_host Secondary Hostname or IP primary_volume Primary Volume Name current_secondary_host Current Secondary Host to which Geo-rep worker was trying to connect to secondary_volume Secondary Volume Name Quota Events Event Type Attribute Description QUOTA_CROSSED_SOFT_LIMIT usage Usage volume Volume Name path Path Bitrot Events Event Type Attribute Description BITROT_BAD_FILE gfid GFID of File path Path if Available brick Brick details Client Events Event Type Attribute Description CLIENT_CONNECT client_identifier Client Identifier client_uid Client UID server_identifier Server Identifier brick_path Path of Brick CLIENT_AUTH_REJECT client_identifier Client Identifier client_uid Client UID server_identifier Server Identifier brick_path Path of Brick CLIENT_DISCONNECT client_identifier Client Identifier client_uid Client UID server_identifier Server Identifier brick_path Path of Brick Posix Events Event Type Attribute Description POSIX_SAME_GFID gfid GFID of File path Path of File newpath New Path of File brick Brick details POSIX_ALREADY_PART_OF_VOLUME volume-id Volume ID brick Brick details POSIX_BRICK_NOT_IN_VOLUME brick Brick details POSIX_BRICK_VERIFICATION_FAILED brick Brick details POSIX_ACL_NOT_SUPPORTED brick Brick details POSIX_HEALTH_CHECK_FAILED path Path brick Brick details op Error Number error Error Afr Events Event Type Attribute Description AFR_QUORUM_MET subvol Sub Volume Name AFR_QUORUM_FAIL subvol Sub Volume Name AFR_SUBVOL_UP subvol Sub Volume Name AFR_SUBVOLS_DOWN subvol Sub Volume Name AFR_SPLIT_BRAIN subvol Sub Volume Name Tier Events Event Type Attribute Description TIER_ATTACH vol Volume Name TIER_ATTACH_FORCE vol Volume Name TIER_DETACH_START vol Volume Name TIER_DETACH_STOP vol Volume Name TIER_DETACH_COMMIT vol Volume Name TIER_DETACH_FORCE vol Volume Name TIER_PAUSE vol Volume Name TIER_RESUME vol Volume Name TIER_WATERMARK_HI vol Volume Name TIER_WATERMARK_DROPPED_TO_MID vol Volume Name TIER_WATERMARK_RAISED_TO_MID vol Volume Name TIER_WATERMARK_DROPPED_TO_LOW vol Volume Name Volume Events Event Type Attribute Description VOLUME_ADD_BRICK volume Volume Name bricks Bricks details separated by Space VOLUME_REMOVE_BRICK_START volume Volume Name bricks Bricks details separated by Space VOLUME_REMOVE_BRICK_COMMIT volume Volume Name bricks Bricks details separated by Space VOLUME_REMOVE_BRICK_STOP volume Volume Name bricks Bricks details separated by Space VOLUME_REMOVE_BRICK_FORCE volume Volume Name bricks Bricks details separated by Space VOLUME_REBALANCE_START volume Volume Name VOLUME_REBALANCE_STOP volume Volume Name VOLUME_REBALANCE_FAILED volume Volume Name VOLUME_REBALANCE_COMPLETE volume Volume Name","title":"Events APIs"},{"location":"Administrator-Guide/Events-APIs/#events-apis","text":"New in version 3.9 NOTE : glusterfs-selinux package would have to be installed for events feature to function properly when the selinux is in enforced mode. In addition to that, the default port to be used for eventsd has now been changed to 55555 and it has to lie between the ephemeral port ranges.","title":"Events APIs"},{"location":"Administrator-Guide/Events-APIs/#set-pythonpathonly-in-case-of-source-installation","text":"If Gluster is installed using source install, cliutils will get installed under /usr/local/lib/python.2.7/site-packages Set PYTHONPATH by adding in ~/.bashrc # export PYTHONPATH=/usr/local/lib/python2.7/site-packages:$PYTHONPATH","title":"Set PYTHONPATH(Only in case of Source installation)"},{"location":"Administrator-Guide/Events-APIs/#enable-and-start-events-apis","text":"Enable and Start glustereventsd in all peer nodes In Systems using Systemd, # systemctl enable glustereventsd # systemctl start glustereventsd FreeBSD or others, add the following in /etc/rc.conf glustereventsd_enable=\"YES\" And start the glustereventsd using, # service glustereventsd start SysVInit(CentOS 6), # chkconfig glustereventsd on # service glustereventsd start","title":"Enable and Start Events APIs"},{"location":"Administrator-Guide/Events-APIs/#status","text":"Status Can be checked using, # gluster-eventsapi status Example output: Webhooks: None +-----------+-------------+-----------------------+ | NODE | NODE STATUS | GLUSTEREVENTSD STATUS | +-----------+-------------+-----------------------+ | localhost | UP | UP | | node2 | UP | UP | +-----------+-------------+-----------------------+","title":"Status"},{"location":"Administrator-Guide/Events-APIs/#webhooks","text":"Webhooks are similar to callbacks(over HTTP), on event Gluster will call the Webhook URL(via POST) which is configured. Webhook is a web server which listens on a URL, this can be deployed outside of the Cluster. Gluster nodes should be able to access this Webhook server on the configured port. Example Webhook written in python, from flask import Flask, request app = Flask(__name__) @app.route(\"/listen\", methods=[\"POST\"]) def events_listener(): gluster_event = request.json if gluster_event is None: # No event to process, may be test call return \"OK\" # Process gluster_event # { # \"nodeid\": NODEID, # \"ts\": EVENT_TIMESTAMP, # \"event\": EVENT_TYPE, # \"message\": EVENT_DATA # } print (gluster_event) return \"OK\" app.run(host=\"0.0.0.0\", port=9000) Test and Register webhook using following commands, usage: gluster-eventsapi webhook-test [-h] [--bearer_token BEARER_TOKEN] url positional arguments: url URL of Webhook optional arguments: -h, --help show this help message and exit --bearer_token BEARER_TOKEN, -t BEARER_TOKEN Bearer Token Example(Webhook server is running in 192.168.122.188:9000 ), # gluster-eventsapi webhook-test http://192.168.122.188:9000/listen +-----------+-------------+----------------+ | NODE | NODE STATUS | WEBHOOK STATUS | +-----------+-------------+----------------+ | localhost | UP | OK | | node2 | UP | OK | +-----------+-------------+----------------+ If Webhook status is OK from all peer nodes then register the Webhook using, usage: gluster-eventsapi webhook-add [-h] [--bearer_token BEARER_TOKEN] url positional arguments: url URL of Webhook optional arguments: -h, --help show this help message and exit --bearer_token BEARER_TOKEN, -t BEARER_TOKEN Bearer Token Example, # gluster-eventsapi webhook-add http://192.168.122.188:9000/listen +-----------+-------------+-------------+ | NODE | NODE STATUS | SYNC STATUS | +-----------+-------------+-------------+ | localhost | UP | OK | | node2 | UP | OK | +-----------+-------------+-------------+ Note : If Sync status is Not OK for any node, then make sure to run following command from a peer node when that node comes up. # gluster-eventsapi sync To unsubscribe from events, delete the webhook using following command usage: gluster-eventsapi webhook-del [-h] url positional arguments: url URL of Webhook optional arguments: -h, --help show this help message and exit Example, # gluster-eventsapi webhook-del http://192.168.122.188:9000/listen","title":"Webhooks"},{"location":"Administrator-Guide/Events-APIs/#configuration","text":"View all configurations using, usage: gluster-eventsapi config-get [-h] [--name NAME] optional arguments: -h, --help show this help message and exit --name NAME Config Name Example output: +--------------------+-------+ | NAME | VALUE | +--------------------+-------+ | log-level | INFO | | port | 55555 | | disable-events-log | False | +--------------------+-------+ To change any configuration, usage: gluster-eventsapi config-set [-h] name value positional arguments: name Config Name value Config Value optional arguments: -h, --help show this help message and exit Example output, +-----------+-------------+-------------+ | NODE | NODE STATUS | SYNC STATUS | +-----------+-------------+-------------+ | localhost | UP | OK | | node2 | UP | OK | +-----------+-------------+-------------+ To Reset any configuration, usage: gluster-eventsapi config-reset [-h] name positional arguments: name Config Name or all optional arguments: -h, --help show this help message and exit Example output, +-----------+-------------+-------------+ | NODE | NODE STATUS | SYNC STATUS | +-----------+-------------+-------------+ | localhost | UP | OK | | node2 | UP | OK | +-----------+-------------+-------------+ Note : If any node status is not UP or sync status is not OK, make sure to run gluster-eventsapi sync from a peer node.","title":"Configuration"},{"location":"Administrator-Guide/Events-APIs/#add-node-to-the-cluster","text":"When a new node added to the cluster, Enable and Start Eventsd in the new node using the steps mentioned above Run gluster-eventsapi sync command from a peer node other than the new node.","title":"Add node to the Cluster"},{"location":"Administrator-Guide/Events-APIs/#apis-documentation","text":"Glustereventsd pushes the Events in JSON format to configured Webhooks. All Events will have following attributes. Attribute Description nodeid Node UUID ts Event Timestamp event Event Type message Event Specific Data Example: { \"nodeid\": \"95cd599c-5d87-43c1-8fba-b12821fd41b6\", \"ts\": 1468303352, \"event\": \"VOLUME_CREATE\", \"message\": { \"name\": \"gv1\" } } \"message\" can have following attributes based on the type of event.","title":"APIs documentation"},{"location":"Administrator-Guide/Events-APIs/#peer-events","text":"Event Type Attribute Description PEER_ATTACH host Hostname or IP of added node PEER_DETACH host Hostname or IP of detached node","title":"Peer Events"},{"location":"Administrator-Guide/Events-APIs/#volume-events","text":"Event Type Attribute Description VOLUME_CREATE name Volume Name VOLUME_START force Force option used or not during Start name Volume Name VOLUME_STOP force Force option used or not during Stop name Volume Name VOLUME_DELETE name Volume Name VOLUME_SET name Volume Name options List of Options[(key1, val1), (key2, val2),..] VOLUME_RESET name Volume Name option Option Name","title":"Volume Events"},{"location":"Administrator-Guide/Events-APIs/#brick-events","text":"Event Type Attribute Description BRICK_RESET_START volume Volume Name source-brick Source Brick details BRICK_RESET_COMMIT volume Volume Name destination-brick Destination Brick source-brick Source Brick details BRICK_REPLACE volume Volume Name destination-brick Destination Brick source-brick Source Brick details","title":"Brick Events"},{"location":"Administrator-Guide/Events-APIs/#georep-events","text":"Event Type Attribute Description GEOREP_CREATE force Force option used during session Create secondary Secondary Details(Secondaryhost::SecondaryVolume) no_verify No verify option is used or not push_pem Push pem option is used or Not ssh_port If SSH port is configured during Session Create primary Primary Volume Name GEOREP_START force Force option used during session Start Primary Primary Volume Name secondary Secondary Details(Secondaryhost::SecondaryVolume) GEOREP_STOP force Force option used during session Stop primary Primary Volume Name secondary Secondary Details(Secondaryhost::SecondaryVolume) GEOREP_PAUSE force Force option used during session Pause primary Primary Volume Name secondary Secondary Details(Secondaryhost::SecondaryVolume) GEOREP_RESUME force Force option used during session Resume primary Primary Volume Name secondary Secondary Details(Secondaryhost::SecondaryVolume) GEOREP_DELETE force Force option used during session Delete primary Primary Volume Name secondary Secondary Details(Secondaryhost::SecondaryVolume) GEOREP_CONFIG_SET primary Primary Volume Name secondary Secondary Details(Secondaryhost::SecondaryVolume) option Name of Geo-rep config value Changed Value GEOREP_CONFIG_RESET primary Primary Volume Name secondary Secondary Details(Secondaryhost::SecondaryVolume) option Name of Geo-rep config","title":"Georep Events"},{"location":"Administrator-Guide/Events-APIs/#bitrot-events","text":"Event Type Attribute Description BITROT_ENABLE name Volume Name BITROT_DISABLE name Volume Name BITROT_SCRUB_THROTTLE name Volume Name value Changed Value BITROT_SCRUB_FREQ name Volume Name value Changed Value BITROT_SCRUB_OPTION name Volume Name value Changed Value","title":"Bitrot Events"},{"location":"Administrator-Guide/Events-APIs/#quota-events","text":"Event Type Attribute Description QUOTA_ENABLE volume Volume Name QUOTA_DISABLE volume Volume Name QUOTA_SET_USAGE_LIMIT volume Volume Name path Path in Volume on which Quota option is set limit Changed Value QUOTA_SET_OBJECTS_LIMIT volume Volume Name path Path in Volume on which Quota option is set limit Changed Value QUOTA_REMOVE_USAGE_LIMIT volume Volume Name path Path in Volume on which Quota option is Reset QUOTA_REMOVE_OBJECTS_LIMIT volume Volume Name path Path in Volume on which Quota option is Reset QUOTA_ALERT_TIME volume Volume Name time Changed Alert Time QUOTA_SOFT_TIMEOUT volume Volume Name soft-timeout Changed Value QUOTA_HARD_TIMEOUT volume Volume Name hard-timeout Changed Value QUOTA_DEFAULT_SOFT_LIMIT volume Volume Name default-soft-limit Changed Value","title":"Quota Events"},{"location":"Administrator-Guide/Events-APIs/#snapshot-events","text":"Event Type Attribute Description SNAPSHOT_CREATED snapshot_name Snapshot Name volume_name Volume Name snapshot_uuid Snapshot UUID SNAPSHOT_CREATE_FAILED snapshot_name Snapshot Name volume_name Volume Name error Failure details SNAPSHOT_ACTIVATED snapshot_name Snapshot Name snapshot_uuid Snapshot UUID SNAPSHOT_ACTIVATE_FAILED snapshot_name Snapshot Name error Failure details SNAPSHOT_DEACTIVATED snapshot_name Snapshot Name snapshot_uuid Snapshot UUID SNAPSHOT_DEACTIVATE_FAILED snapshot_name Snapshot Name error Failure details SNAPSHOT_SOFT_LIMIT_REACHED volume_name Volume Name volume_id Volume ID SNAPSHOT_HARD_LIMIT_REACHED volume_name Volume Name volume_id Volume ID SNAPSHOT_RESTORED snapshot_name Snapshot Name volume_name Volume Name snapshot_uuid Snapshot UUID SNAPSHOT_RESTORE_FAILED snapshot_name Snapshot Name error Failure details SNAPSHOT_DELETED snapshot_name Snapshot Name snapshot_uuid Snapshot UUID SNAPSHOT_DELETE_FAILED snapshot_name Snapshot Name error Failure details SNAPSHOT_CLONED clone_uuid Snapshot Clone UUID snapshot_name Snapshot Name clone_name Snapshot Clone Name SNAPSHOT_CLONE_FAILED snapshot_name Snapshot Name clone_name Snapshot Clone Name error Failure details SNAPSHOT_CONFIG_UPDATED auto-delete Auto delete Value if available config_type Volume Config or System Config hard_limit Hard Limit Value if available soft_limit Soft Limit Value if available snap-activate Snap activate Value if available SNAPSHOT_CONFIG_UPDATE_FAILED error Error details SNAPSHOT_SCHEDULER_INITIALISED status Succss Status SNAPSHOT_SCHEDULER_INIT_FAILED error Error details SNAPSHOT_SCHEDULER_ENABLED status Succss Status SNAPSHOT_SCHEDULER_ENABLE_FAILED error Error details SNAPSHOT_SCHEDULER_DISABLED status Succss Status SNAPSHOT_SCHEDULER_DISABLE_FAILED error Error details SNAPSHOT_SCHEDULER_SCHEDULE_ADDED status Succss Status SNAPSHOT_SCHEDULER_SCHEDULE_ADD_FAILED error Error details SNAPSHOT_SCHEDULER_SCHEDULE_EDITED status Succss Status SNAPSHOT_SCHEDULER_SCHEDULE_EDIT_FAILED error Error details SNAPSHOT_SCHEDULER_SCHEDULE_DELETED status Succss Status SNAPSHOT_SCHEDULER_SCHEDULE_DELETE_FAILED error Error details","title":"Snapshot Events"},{"location":"Administrator-Guide/Events-APIs/#svc-events","text":"Event Type Attribute Description SVC_MANAGER_FAILED volume Volume Name if available svc_name Service Name SVC_CONNECTED volume Volume Name if available svc_name Service Name SVC_DISCONNECTED svc_name Service Name","title":"Svc Events"},{"location":"Administrator-Guide/Events-APIs/#peer-events_1","text":"Event Type Attribute Description PEER_STORE_FAILURE peer Hostname or IP PEER_RPC_CREATE_FAILED peer Hostname or IP PEER_REJECT peer Hostname or IP PEER_CONNECT host Hostname or IP uuid Host UUID PEER_DISCONNECT host Hostname or IP uuid Host UUID state Disconnect State PEER_NOT_FOUND peer Hostname or IP uuid Host UUID","title":"Peer Events"},{"location":"Administrator-Guide/Events-APIs/#unknown-events","text":"Event Type Attribute Description UNKNOWN_PEER peer Hostname or IP","title":"Unknown Events"},{"location":"Administrator-Guide/Events-APIs/#brick-events_1","text":"Event Type Attribute Description BRICK_START_FAILED peer Hostname or IP volume Volume Name brick Brick BRICK_STOP_FAILED peer Hostname or IP volume Volume Name brick Brick BRICK_DISCONNECTED peer Hostname or IP volume Volume Name brick Brick BRICK_CONNECTED peer Hostname or IP volume Volume Name brick Brick","title":"Brick Events"},{"location":"Administrator-Guide/Events-APIs/#bricks-events","text":"Event Type Attribute Description BRICKS_START_FAILED volume Volume Name","title":"Bricks Events"},{"location":"Administrator-Guide/Events-APIs/#brickpath-events","text":"Event Type Attribute Description BRICKPATH_RESOLVE_FAILED peer Hostname or IP volume Volume Name brick Brick","title":"Brickpath Events"},{"location":"Administrator-Guide/Events-APIs/#notify-events","text":"Event Type Attribute Description NOTIFY_UNKNOWN_OP op Operation Name","title":"Notify Events"},{"location":"Administrator-Guide/Events-APIs/#quorum-events","text":"Event Type Attribute Description QUORUM_LOST volume Volume Name QUORUM_REGAINED volume Volume Name","title":"Quorum Events"},{"location":"Administrator-Guide/Events-APIs/#rebalance-events","text":"Event Type Attribute Description REBALANCE_START_FAILED volume Volume Name REBALANCE_STATUS_UPDATE_FAILED volume Volume Name","title":"Rebalance Events"},{"location":"Administrator-Guide/Events-APIs/#import-events","text":"Event Type Attribute Description IMPORT_QUOTA_CONF_FAILED volume Volume Name IMPORT_VOLUME_FAILED volume Volume Name IMPORT_BRICK_FAILED peer Hostname or IP brick Brick details","title":"Import Events"},{"location":"Administrator-Guide/Events-APIs/#compare-events","text":"Event Type Attribute Description COMPARE_FRIEND_VOLUME_FAILED volume Volume Name","title":"Compare Events"},{"location":"Administrator-Guide/Events-APIs/#ec-events","text":"Event Type Attribute Description EC_MIN_BRICKS_NOT_UP subvol Subvolume EC_MIN_BRICKS_UP subvol Subvolume","title":"Ec Events"},{"location":"Administrator-Guide/Events-APIs/#georep-events_1","text":"Event Type Attribute Description GEOREP_FAULTY primary_node Hostname or IP of Primary Volume brick_path Brick Path secondary_host Secondary Hostname or IP primary_volume Primary Volume Name current_secondary_host Current Secondary Host to which Geo-rep worker was trying to connect to secondary_volume Secondary Volume Name","title":"Georep Events"},{"location":"Administrator-Guide/Events-APIs/#quota-events_1","text":"Event Type Attribute Description QUOTA_CROSSED_SOFT_LIMIT usage Usage volume Volume Name path Path","title":"Quota Events"},{"location":"Administrator-Guide/Events-APIs/#bitrot-events_1","text":"Event Type Attribute Description BITROT_BAD_FILE gfid GFID of File path Path if Available brick Brick details","title":"Bitrot Events"},{"location":"Administrator-Guide/Events-APIs/#client-events","text":"Event Type Attribute Description CLIENT_CONNECT client_identifier Client Identifier client_uid Client UID server_identifier Server Identifier brick_path Path of Brick CLIENT_AUTH_REJECT client_identifier Client Identifier client_uid Client UID server_identifier Server Identifier brick_path Path of Brick CLIENT_DISCONNECT client_identifier Client Identifier client_uid Client UID server_identifier Server Identifier brick_path Path of Brick","title":"Client Events"},{"location":"Administrator-Guide/Events-APIs/#posix-events","text":"Event Type Attribute Description POSIX_SAME_GFID gfid GFID of File path Path of File newpath New Path of File brick Brick details POSIX_ALREADY_PART_OF_VOLUME volume-id Volume ID brick Brick details POSIX_BRICK_NOT_IN_VOLUME brick Brick details POSIX_BRICK_VERIFICATION_FAILED brick Brick details POSIX_ACL_NOT_SUPPORTED brick Brick details POSIX_HEALTH_CHECK_FAILED path Path brick Brick details op Error Number error Error","title":"Posix Events"},{"location":"Administrator-Guide/Events-APIs/#afr-events","text":"Event Type Attribute Description AFR_QUORUM_MET subvol Sub Volume Name AFR_QUORUM_FAIL subvol Sub Volume Name AFR_SUBVOL_UP subvol Sub Volume Name AFR_SUBVOLS_DOWN subvol Sub Volume Name AFR_SPLIT_BRAIN subvol Sub Volume Name","title":"Afr Events"},{"location":"Administrator-Guide/Events-APIs/#tier-events","text":"Event Type Attribute Description TIER_ATTACH vol Volume Name TIER_ATTACH_FORCE vol Volume Name TIER_DETACH_START vol Volume Name TIER_DETACH_STOP vol Volume Name TIER_DETACH_COMMIT vol Volume Name TIER_DETACH_FORCE vol Volume Name TIER_PAUSE vol Volume Name TIER_RESUME vol Volume Name TIER_WATERMARK_HI vol Volume Name TIER_WATERMARK_DROPPED_TO_MID vol Volume Name TIER_WATERMARK_RAISED_TO_MID vol Volume Name TIER_WATERMARK_DROPPED_TO_LOW vol Volume Name","title":"Tier Events"},{"location":"Administrator-Guide/Events-APIs/#volume-events_1","text":"Event Type Attribute Description VOLUME_ADD_BRICK volume Volume Name bricks Bricks details separated by Space VOLUME_REMOVE_BRICK_START volume Volume Name bricks Bricks details separated by Space VOLUME_REMOVE_BRICK_COMMIT volume Volume Name bricks Bricks details separated by Space VOLUME_REMOVE_BRICK_STOP volume Volume Name bricks Bricks details separated by Space VOLUME_REMOVE_BRICK_FORCE volume Volume Name bricks Bricks details separated by Space VOLUME_REBALANCE_START volume Volume Name VOLUME_REBALANCE_STOP volume Volume Name VOLUME_REBALANCE_FAILED volume Volume Name VOLUME_REBALANCE_COMPLETE volume Volume Name","title":"Volume Events"},{"location":"Administrator-Guide/Export-And-Netgroup-Authentication/","text":"Exports and Netgroups Authentication for NFS This feature adds Linux-style exports & netgroups authentication to Gluster's NFS server. More specifically, this feature allows users to restrict access specific IPs (exports authentication) or a netgroup (netgroups authentication), or a combination of both for both Gluster volumes and subdirectories within Gluster volumes. Netgroups are used in Unix environments to control access for NFS exports, remote logins and remote shells. Each netgroup has a unique name and defines a set of hosts, users, groups and other netgroups. This information is stored in files and gluster NFS server manage permission for clients based on those file Implications and Usage Currently, gluster can restrict access to volumes through simple IP list. But this feature makes that capability more scalable by allowing large lists of IPs to be managed through a netgroup. Moreover it provides more granular permission handling on volumes like wildcard support, read-only permission to certain client etc. The file /var/lib/glusterd/nfs/export contains the details of machines which can be used as clients for that server.An typical export entry use the following format : /<export path> <host/netgroup> (options),.. Here export name can be gluster volume or subdirectory path inside that volume. Next it contains list of host/netgroup , followed by the options applicable to that entry.A string beginning with an '@' is treated as a netgroup and a string beginning without an @ is a host. The options include mount related parameters , right now options such as \"sec\", \"ro/rw\", \"anonuid\" valid one. If * is mention as host/netgroup field , then any client can mount that export path. The file /var/lib/glusterd/nfs/netgroup should mention the expansion of each netgroup which mentioned in the export file. An typical netgroup entry will look like : <netgroup name> ng1000\\nng1000 ng999\\nng999 ng1\\nng1 ng2\\nng2 (ip1,ip2,..) The gluster NFS server will check the contents of these file after specific time intervals Volume Options Enabling export/netgroup feature gluster volume set <volname> nfs.exports-auth-enable on Changing the refresh interval for gluster NFS server gluster volume set <volname> nfs.auth-refresh-interval-sec <time in seconds> Changing the cache interval for an export entry gluster volume set <volname> nfs.auth-cache-ttl-sec <time in seconds> Testing the export/netgroup file An user should have the ability to check the validity of the files before applying the configuration. The \"glusterfsd\" command now has the following additional arguments that can be used to check the configuration: - --print-netgroups: Validate the netgroups file and print it out. For example, - glusterfsd --print-netgroups <name of the file> --print-exports: Validate the exports file and print it out. For example, glusterfsd --print-export <name of the file> Points to be noted. This feature does not currently support all the options in the man page of exports, but we can easily add them. The files /var/lib/glusterd/nfs/export and /var/lib/glusterd/nfs/netgroup should be created before setting the nfs.exports-auth-enable option in every node in Trusted Storage Pool. These files are handled manually by the users. So that, their contents can be different among the gluster nfs servers across Trusted Storage Pool . i.e it is possible to have different authenticate mechanism for the gluster NFS servers in the same cluster. Do not mixup this feature and authentication using nfs.rpc-auth-allow , nfs.export-dir which may result in inconsistency. Troubleshooting After changing the contents of the file, if it is not reflected properly in the authentication mechanism , just restart the server using volume stop and start, So that gluster NFS server will forcefully read the contents of those files again.","title":"Export and Netgroup Authentication"},{"location":"Administrator-Guide/Export-And-Netgroup-Authentication/#exports-and-netgroups-authentication-for-nfs","text":"This feature adds Linux-style exports & netgroups authentication to Gluster's NFS server. More specifically, this feature allows users to restrict access specific IPs (exports authentication) or a netgroup (netgroups authentication), or a combination of both for both Gluster volumes and subdirectories within Gluster volumes. Netgroups are used in Unix environments to control access for NFS exports, remote logins and remote shells. Each netgroup has a unique name and defines a set of hosts, users, groups and other netgroups. This information is stored in files and gluster NFS server manage permission for clients based on those file","title":"Exports and Netgroups Authentication for NFS"},{"location":"Administrator-Guide/Export-And-Netgroup-Authentication/#implications-and-usage","text":"Currently, gluster can restrict access to volumes through simple IP list. But this feature makes that capability more scalable by allowing large lists of IPs to be managed through a netgroup. Moreover it provides more granular permission handling on volumes like wildcard support, read-only permission to certain client etc. The file /var/lib/glusterd/nfs/export contains the details of machines which can be used as clients for that server.An typical export entry use the following format : /<export path> <host/netgroup> (options),.. Here export name can be gluster volume or subdirectory path inside that volume. Next it contains list of host/netgroup , followed by the options applicable to that entry.A string beginning with an '@' is treated as a netgroup and a string beginning without an @ is a host. The options include mount related parameters , right now options such as \"sec\", \"ro/rw\", \"anonuid\" valid one. If * is mention as host/netgroup field , then any client can mount that export path. The file /var/lib/glusterd/nfs/netgroup should mention the expansion of each netgroup which mentioned in the export file. An typical netgroup entry will look like : <netgroup name> ng1000\\nng1000 ng999\\nng999 ng1\\nng1 ng2\\nng2 (ip1,ip2,..) The gluster NFS server will check the contents of these file after specific time intervals","title":"Implications and Usage"},{"location":"Administrator-Guide/Export-And-Netgroup-Authentication/#volume-options","text":"Enabling export/netgroup feature gluster volume set <volname> nfs.exports-auth-enable on Changing the refresh interval for gluster NFS server gluster volume set <volname> nfs.auth-refresh-interval-sec <time in seconds> Changing the cache interval for an export entry gluster volume set <volname> nfs.auth-cache-ttl-sec <time in seconds>","title":"Volume Options"},{"location":"Administrator-Guide/Export-And-Netgroup-Authentication/#testing-the-exportnetgroup-file","text":"An user should have the ability to check the validity of the files before applying the configuration. The \"glusterfsd\" command now has the following additional arguments that can be used to check the configuration: - --print-netgroups: Validate the netgroups file and print it out. For example, - glusterfsd --print-netgroups <name of the file> --print-exports: Validate the exports file and print it out. For example, glusterfsd --print-export <name of the file>","title":"Testing the export/netgroup file"},{"location":"Administrator-Guide/Export-And-Netgroup-Authentication/#points-to-be-noted","text":"This feature does not currently support all the options in the man page of exports, but we can easily add them. The files /var/lib/glusterd/nfs/export and /var/lib/glusterd/nfs/netgroup should be created before setting the nfs.exports-auth-enable option in every node in Trusted Storage Pool. These files are handled manually by the users. So that, their contents can be different among the gluster nfs servers across Trusted Storage Pool . i.e it is possible to have different authenticate mechanism for the gluster NFS servers in the same cluster. Do not mixup this feature and authentication using nfs.rpc-auth-allow , nfs.export-dir which may result in inconsistency.","title":"Points to be noted."},{"location":"Administrator-Guide/Export-And-Netgroup-Authentication/#troubleshooting","text":"After changing the contents of the file, if it is not reflected properly in the authentication mechanism , just restart the server using volume stop and start, So that gluster NFS server will forcefully read the contents of those files again.","title":"Troubleshooting"},{"location":"Administrator-Guide/Geo-Replication/","text":"Geo-Replication Introduction Geo-replication provides a continuous, asynchronous, and incremental replication service from one site to another over Local Area Networks (LANs), Wide Area Network (WANs), and across the Internet. Prerequisites Primary and Secondary Volumes should be Gluster Volumes. Primary and Secondary clusters should have the same GlusterFS version. Replicated Volumes vs Geo-replication The following table lists the difference between replicated volumes and Geo-replication: Replicated Volumes Geo-replication Mirrors data across clusters Mirrors data across geographically distributed clusters Provides high-availability Ensures backing up of data for disaster recovery Synchronous replication (each and every file operation is sent across all the bricks) Asynchronous replication (checks for the changes in files periodically and syncs them on detecting differences) Exploring Geo-replication Deployment Scenarios Geo-replication provides an incremental replication service over Local Area Networks (LANs), Wide Area Network (WANs), and across the Internet. This section illustrates the most common deployment scenarios for Geo-replication, including the following: Geo-replication over Local Area Network(LAN) Geo-replication over Wide Area Network(WAN) Geo-replication over Internet Mirror data in a cascading fashion across multiple sites(Multi-site cascading Geo-replication) Secondary User setup Setup an unprivileged user in Secondary nodes to secure the SSH connectivity to those nodes. The unprivileged Secondary user uses the mountbroker service of glusterd to set up an auxiliary gluster mount for the user in a special environment, which ensures that the user is only allowed to access with special parameters that provide administrative level access to the particular Volume. In all the Secondary nodes, create a new group as \"geogroup\". # sudo groupadd geogroup In all the Secondary nodes, create an unprivileged account. For example, \"geoaccount\". Add geoaccount as a member of \"geogroup\" group. # useradd -G geogroup geoaccount In any one Secondary node, run the following command to setup the mountbroker root directory and group. gluster-mountbroker setup <MOUNT ROOT> <GROUP> For example, # gluster-mountbroker setup /var/mountbroker-root geogroup In any one of Secondary node, Run the following commands to add Volume and user to mountbroker service. gluster-mountbroker add <VOLUME> <USER> For example, # gluster-mountbroker add gvol-secondary geoaccount ( Note : To remove a user, use gluster-mountbroker remove command) Check the status of setup using, # gluster-mountbroker status Restart glusterd service on all Secondary nodes. Setting Up the Environment for Geo-replication Time Synchronization On bricks of a geo-replication Primary volume, all the servers' time must be uniform. You are recommended to set up NTP (Network Time Protocol) or similar service to keep the bricks sync in time and avoid the out-of-time sync effect. For example: In a Replicated volume where brick1 of the Primary is at 12.20 hrs, and brick 2 of the Primary is at 12.10 hrs with 10 minutes time lag, all the changes in brick2 between this period may go unnoticed during synchronization of files with Secondary. Password-less SSH Password-less login has to be set up between the host machine (where geo-replication Create command will be issued) and one of the Secondary node for the unprivileged account created above. Note : This is required to run Create command. This can be disabled once the session is established.(Required again while running create force) On one of the Primary node where geo-replication Create command will be issued, run the following command to generate the SSH key(Press Enter twice to avoid passphrase). # ssh-keygen Run the following command on the same node to one Secondary node which is identified as the main Secondary node. # ssh-copy-id geoaccount@snode1.example.com Creating secret pem pub file Execute the below command from the node where you setup the password-less ssh to Secondary. This will generate Geo-rep session specific ssh-keys in all Primary peer nodes and collect public keys from all peer nodes to the command initiated node. # gluster-georep-sshkey generate This command adds extra prefix inside common_secret.pem.pub file to each pub keys to prevent running extra commands using this key, to disable that prefix, # gluster-georep-sshkey generate --no-prefix Creating the session Create a geo-rep session between Primary and Secondary volume using the following command. The node in which this command is executed and the <Secondary_host> specified in the command should have password less ssh setup between them. The push-pem option actually uses the secret pem pub file created earlier and establishes geo-rep specific password less ssh between each node in Primary to each node of Secondary. gluster volume geo-replication <primary_volume> \\ <secondary_user>@<secondary_host>::<secondary_volume> \\ create [ssh-port <port>] push-pem|no-verify [force] For example, # gluster volume geo-replication gvol-primary \\ geoaccount@snode1.example.com::gvol-secondary \\ create push-pem If custom SSH port (example: 50022) is configured in Secondary nodes then # gluster volume geo-replication gvol-primary \\ geoaccount@snode1.example.com::gvol-secondary \\ config ssh_port 50022 # gluster volume geo-replication gvol-primary \\ geoaccount@snode1.example.com::gvol-secondary \\ create ssh-port 50022 push-pem If the total available size in Secondary volume is less than the total size of Primary, the command will throw error message. In such cases 'force' option can be used. In use cases where the rsa-keys of nodes in Primary volume is distributed to Secondary nodes through an external agent and following Secondary side verifications are taken care of by the external agent, then if ssh port 22 or custom port is open in Secondary has proper passwordless ssh login setup Secondary volume is created and is empty if Secondary has enough memory Then use following command to create Geo-rep session with no-verify option. gluster volume geo-replication <primary_volume> \\ <secondary_user>@<secondary_host>::<secondary_volume> create no-verify [force] For example, # gluster volume geo-replication gvol-primary \\ geoaccount@snode1.example.com::gvol-secondary \\ create no-verify In this case the Primary node rsa-key distribution to Secondary node does not happen and above mentioned Secondary verification is not performed and these two things has to be taken care externaly. Post Creation steps Run the following command as root in any one of Secondary node. /usr/libexec/glusterfs/set_geo_rep_pem_keys.sh <secondary_user> \\ <primary_volume> <secondary_volume> For example, # /usr/libexec/glusterfs/set_geo_rep_pem_keys.sh geoaccount \\ gvol-primary gvol-secondary Configuration Configuration can be changed anytime after creating the session. After successful configuration change, Geo-rep session will be automatically restarted. To view all configured options of a session, gluster volume geo-replication <primary_volume> \\ <secondary_user>@<secondary_host>::<secondary_volume> config [option] For Example, # gluster volume geo-replication gvol-primary \\ geoaccount@snode1.example.com::gvol-secondary \\ config # gluster volume geo-replication gvol-primary \\ geoaccount@snode1.example.com::gvol-secondary \\ config sync-jobs To configure Gluster Geo-replication, use the following command at the Gluster command line gluster volume geo-replication <primary_volume> \\ <secondary_user>@<secondary_host>::<secondary_volume> config [option] For example: # gluster volume geo-replication gvol-primary \\ geoaccount@snode1.example.com::gvol-secondary \\ config sync-jobs 3 Note : If Geo-rep is in between sync, restart due to configuration change may cause resyncing a few entries which are already synced. Configurable Options Meta Volume In case of Replica bricks, one brick worker will be Active and participate in syncing and others will be waiting as Passive. By default Geo-rep uses node-uuid , if node-uuid of worker present in first up subvolume node ids list then that worker will become Active. With this method, multiple workers of same replica becomes Active if multiple bricks used from same machine. To prevent this, Meta Volume(Extra Gluster Volume) can be used in Geo-rep. With this method, Each worker will try to acquire lock on a file inside meta volume. Lock file name pattern will be different for each sub volume. If a worker acquire lock, then it will become Active else remain as Passive. gluster volume geo-replication <primary_volume> \\ <secondary_user>@<secondary_host>::<secondary_volume> config use-meta-volume true Note : Meta Volume is shared replica 3 Gluster Volume. The name of the meta-volume should be gluster_shared_storage and should be mounted at /var/run/gluster/shared_storage/ . The following table provides an overview of the configurable options for a geo-replication setting: Option Description log-level LOGFILELEVEL The log level for geo-replication. gluster-log-level LOGFILELEVEL The log level for glusterfs processes. changelog-log-level LOGFILELEVEL The log level for Changelog processes. ssh-command COMMAND The SSH command to connect to the remote machine (the default is ssh). If ssh is installed in custom location, that path can be configured. For ex /usr/local/sbin/ssh rsync-command COMMAND The rsync command to use for synchronizing the files (the default is rsync). use-tarssh true The use-tarssh command allows tar over Secure Shell protocol. Use this option to handle workloads of files that have not undergone edits. timeout SECONDS The timeout period in seconds. sync-jobs N The number of simultaneous files/directories that can be synchronized. ignore-deletes If this option is set to 1, a file deleted on the primary will not trigger a delete operation on the secondary. As a result, the secondary will remain as a superset of the primary and can be used to recover the primary in the event of a crash and/or accidental delete. Starting Geo-replication Use the following command to start geo-replication session, gluster volume geo-replication <primary_volume> \\ <secondary_user>@<secondary_host>::<secondary_volume> \\ start [force] For example, # gluster volume geo-replication gvol-primary \\ geoaccount@snode1.example.com::gvol-secondary \\ start Note You may need to configure the session before starting Gluster Geo-replication. Stopping Geo-replication Use the following command to stop geo-replication sesion, gluster volume geo-replication <primary_volume> \\ <secondary_user>@<secondary_host>::<secondary_volume> \\ stop [force] For example, # gluster volume geo-replication gvol-primary \\ geoaccount@snode1.example.com::gvol-secondary \\ stop Status To check the status of all Geo-replication sessions in the Cluster # gluster volume geo-replication status To check the status of one session, gluster volume geo-replication <primary_volume> \\ <secondary_user>@<secondary_host>::<secondary_volume> status [detail] Example, # gluster volume geo-replication gvol-primary \\ geoaccount@snode1::gvol-secondary status # gluster volume geo-replication gvol-primary \\ geoaccount@snode1::gvol-secondary status detail Example Status Output PRIMARY NODE PRIMARY VOL PRIMARY BRICK SECONDARY USER SECONDARY SECONDARY NODE STATUS CRAWL STATUS LAST_SYNCED --------------------------------------------------------------------------------------------------------------------------------------------------------- mnode1 gvol-primary /bricks/b1 root snode1::gvol-secondary snode1 Active Changelog Crawl 2016-10-12 23:07:13 mnode2 gvol-primary /bricks/b2 root snode1::gvol-secondary snode2 Active Changelog Crawl 2016-10-12 23:07:13 Example Status detail Output PRIMARY NODE PRIMARY VOL PRIMARY BRICK SECONDARY USER SECONDARY SECONDARY NODE STATUS CRAWL STATUS LAST_SYNCED ENTRY DATA META FAILURES CHECKPOINT TIME CHECKPOINT COMPLETED CHECKPOINT COMPLETION TIME -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- mnode1 gvol-primary /bricks/b1 root snode1::gvol-secondary snode1 Active Changelog Crawl 2016-10-12 23:07:13 0 0 0 0 N/A N/A N/A mnode2 gvol-primary /bricks/b2 root snode1::gvol-secondary snode2 Active Changelog Crawl 2016-10-12 23:07:13 0 0 0 0 N/A N/A N/A The STATUS of the session could be one of the following, Initializing : This is the initial phase of the Geo-replication session; it remains in this state for a minute in order to make sure no abnormalities are present. Created : The geo-replication session is created, but not started. Active : The gsync daemon in this node is active and syncing the data. (One worker among the replica pairs will be in Active state) Passive : A replica pair of the active node. The data synchronization is handled by active node. Hence, this node does not sync any data. If Active node goes down, Passive worker will become Active Faulty : The geo-replication session has experienced a problem, and the issue needs to be investigated further. Check log files for more details about the Faulty status. Log file path can be found using gluster volume geo-replication <primary_volume> \\ <secondary_user>@<secondary_host>::<secondary_volume> config log-file Stopped : The geo-replication session has stopped, but has not been deleted. The CRAWL STATUS can be one of the following: Hybrid Crawl : The gsyncd daemon is crawling the glusterFS file system and generating pseudo changelog to sync data. This crawl is used during initial sync and if Changelogs are not available. History Crawl : gsyncd daemon syncs data by consuming Historical Changelogs. On every worker restart, Geo-rep uses this Crawl to process backlog Changelogs. Changelog Crawl : The changelog translator has produced the changelog and that is being consumed by gsyncd daemon to sync data. The ENTRY denotes: The number of pending entry operations (create, mkdir, mknod, symlink, link, rename, unlink, rmdir) per session. The DATA denotes: The number of pending Data operations (write, writev, truncate, ftruncate) per session. The META denotes: The number of pending Meta operations (setattr, fsetattr, setxattr, fsetxattr, removexattr, fremovexattr) per session. The FAILURE denotes: The number of failures per session . On encountering failures, one can proceed to look at the log files. Deleting the session Established Geo-replication session can be deleted using the following command, gluster volume geo-replication <primary_volume> \\ <secondary_user>@<secondary_host>::<secondary_volume> delete [force] For example, # gluster volume geo-replication gvol-primary \\ geoaccount@snode1.example.com::gvol-secondary delete Note: If the same session is created again then syncing will resume from where it was stopped before deleting the session. If the session to be deleted permanently then use reset-sync-time option with delete command. For example, gluster volume geo-replication gvol-primary geoaccount@snode1::gvol-secondary delete reset-sync-time Checkpoint Using Checkpoint feature we can find the status of sync with respect to the Checkpoint time. Checkpoint completion status shows \"Yes\" once Geo-rep syncs all the data from that brick which are created or modified before the Checkpoint Time. Set the Checkpoint using, gluster volume geo-replication <primary_volume> \\ <secondary_user>@<secondary_host>::<secondary_volume> config checkpoint now Example, # gluster volume geo-replication gvol-primary \\ geoaccount@snode1.example.com::gvol-secondary \\ config checkpoint now Touch the Primary mount point to make sure Checkpoint completes even though no I/O happening in the Volume # mount -t glusterfs <primaryhost>:<primaryvol> /mnt # touch /mnt Checkpoint status can be checked using Geo-rep status command. Following columns in status output gives more information about Checkpoint CHECKPOINT TIME : Checkpoint Set Time CHECKPOINT COMPLETED : Yes/No/NA, Status of Checkpoint CHECKPOINT COMPLETION TIME : Checkpoint Completion Time if completed, else N/A Log Files Primary Log files are located in /var/log/glusterfs/geo-replication directory in each Primary nodes. Secondary log files are located in /var/log/glusterfs/geo-replication-secondary directory in Secondary nodes. Gluster Snapshots and Geo-replicated Volumes Gluster snapshot of Primary and Secondary should not go out of order on restore. So while taking snapshot take snapshot of both Primary and Secondary Volumes. Pause the Geo-replication session using, gluster volume geo-replication <primary_volume> \\ <secondary_user>@<secondary_host>::<secondary_volume> pause Take Gluster Snapshot of Secondary Volume and Primary Volume(Use same name for snapshots) gluster snapshot create <snapname> <volname> Example, # gluster snapshot create snap1 gvol-secondary # gluster snapshot create snap1 gvol-primary Resume Geo-replication session using, gluster volume geo-replication <primary_volume> \\ <secondary_user>@<secondary_host>::<secondary_volume> resume If we want to continue Geo-rep session after snapshot restore, we need to restore both Primary and Secondary Volume and resume the Geo-replication session using force option gluster snapshot restore <snapname> gluster volume geo-replication <primary_volume> \\ <secondary_user>@<secondary_host>::<secondary_volume> resume force Example, # gluster snapshot restore snap1 # Secondary Snap # gluster snapshot restore snap1 # Primary Snap # gluster volume geo-replication gvol-primary geoaccount@snode1::gvol-secondary \\ resume force","title":"Geo Replication"},{"location":"Administrator-Guide/Geo-Replication/#geo-replication","text":"","title":"Geo-Replication"},{"location":"Administrator-Guide/Geo-Replication/#introduction","text":"Geo-replication provides a continuous, asynchronous, and incremental replication service from one site to another over Local Area Networks (LANs), Wide Area Network (WANs), and across the Internet.","title":"Introduction"},{"location":"Administrator-Guide/Geo-Replication/#prerequisites","text":"Primary and Secondary Volumes should be Gluster Volumes. Primary and Secondary clusters should have the same GlusterFS version.","title":"Prerequisites"},{"location":"Administrator-Guide/Geo-Replication/#replicated-volumes-vs-geo-replication","text":"The following table lists the difference between replicated volumes and Geo-replication: Replicated Volumes Geo-replication Mirrors data across clusters Mirrors data across geographically distributed clusters Provides high-availability Ensures backing up of data for disaster recovery Synchronous replication (each and every file operation is sent across all the bricks) Asynchronous replication (checks for the changes in files periodically and syncs them on detecting differences)","title":"Replicated Volumes vs Geo-replication"},{"location":"Administrator-Guide/Geo-Replication/#exploring-geo-replication-deployment-scenarios","text":"Geo-replication provides an incremental replication service over Local Area Networks (LANs), Wide Area Network (WANs), and across the Internet. This section illustrates the most common deployment scenarios for Geo-replication, including the following:","title":"Exploring Geo-replication Deployment Scenarios"},{"location":"Administrator-Guide/Geo-Replication/#geo-replication-over-local-area-networklan","text":"","title":"Geo-replication over Local Area Network(LAN)"},{"location":"Administrator-Guide/Geo-Replication/#geo-replication-over-wide-area-networkwan","text":"","title":"Geo-replication over Wide Area Network(WAN)"},{"location":"Administrator-Guide/Geo-Replication/#geo-replication-over-internet","text":"","title":"Geo-replication over Internet"},{"location":"Administrator-Guide/Geo-Replication/#mirror-data-in-a-cascading-fashion-across-multiple-sitesmulti-site-cascading-geo-replication","text":"","title":"Mirror data in a cascading fashion across multiple sites(Multi-site cascading Geo-replication)"},{"location":"Administrator-Guide/Geo-Replication/#secondary-user-setup","text":"Setup an unprivileged user in Secondary nodes to secure the SSH connectivity to those nodes. The unprivileged Secondary user uses the mountbroker service of glusterd to set up an auxiliary gluster mount for the user in a special environment, which ensures that the user is only allowed to access with special parameters that provide administrative level access to the particular Volume. In all the Secondary nodes, create a new group as \"geogroup\". # sudo groupadd geogroup In all the Secondary nodes, create an unprivileged account. For example, \"geoaccount\". Add geoaccount as a member of \"geogroup\" group. # useradd -G geogroup geoaccount In any one Secondary node, run the following command to setup the mountbroker root directory and group. gluster-mountbroker setup <MOUNT ROOT> <GROUP> For example, # gluster-mountbroker setup /var/mountbroker-root geogroup In any one of Secondary node, Run the following commands to add Volume and user to mountbroker service. gluster-mountbroker add <VOLUME> <USER> For example, # gluster-mountbroker add gvol-secondary geoaccount ( Note : To remove a user, use gluster-mountbroker remove command) Check the status of setup using, # gluster-mountbroker status Restart glusterd service on all Secondary nodes.","title":"Secondary User setup"},{"location":"Administrator-Guide/Geo-Replication/#setting-up-the-environment-for-geo-replication","text":"","title":"Setting Up the Environment for Geo-replication"},{"location":"Administrator-Guide/Geo-Replication/#time-synchronization","text":"On bricks of a geo-replication Primary volume, all the servers' time must be uniform. You are recommended to set up NTP (Network Time Protocol) or similar service to keep the bricks sync in time and avoid the out-of-time sync effect. For example: In a Replicated volume where brick1 of the Primary is at 12.20 hrs, and brick 2 of the Primary is at 12.10 hrs with 10 minutes time lag, all the changes in brick2 between this period may go unnoticed during synchronization of files with Secondary.","title":"Time Synchronization"},{"location":"Administrator-Guide/Geo-Replication/#password-less-ssh","text":"Password-less login has to be set up between the host machine (where geo-replication Create command will be issued) and one of the Secondary node for the unprivileged account created above. Note : This is required to run Create command. This can be disabled once the session is established.(Required again while running create force) On one of the Primary node where geo-replication Create command will be issued, run the following command to generate the SSH key(Press Enter twice to avoid passphrase). # ssh-keygen Run the following command on the same node to one Secondary node which is identified as the main Secondary node. # ssh-copy-id geoaccount@snode1.example.com","title":"Password-less SSH"},{"location":"Administrator-Guide/Geo-Replication/#creating-secret-pem-pub-file","text":"Execute the below command from the node where you setup the password-less ssh to Secondary. This will generate Geo-rep session specific ssh-keys in all Primary peer nodes and collect public keys from all peer nodes to the command initiated node. # gluster-georep-sshkey generate This command adds extra prefix inside common_secret.pem.pub file to each pub keys to prevent running extra commands using this key, to disable that prefix, # gluster-georep-sshkey generate --no-prefix","title":"Creating secret pem pub file"},{"location":"Administrator-Guide/Geo-Replication/#creating-the-session","text":"Create a geo-rep session between Primary and Secondary volume using the following command. The node in which this command is executed and the <Secondary_host> specified in the command should have password less ssh setup between them. The push-pem option actually uses the secret pem pub file created earlier and establishes geo-rep specific password less ssh between each node in Primary to each node of Secondary. gluster volume geo-replication <primary_volume> \\ <secondary_user>@<secondary_host>::<secondary_volume> \\ create [ssh-port <port>] push-pem|no-verify [force] For example, # gluster volume geo-replication gvol-primary \\ geoaccount@snode1.example.com::gvol-secondary \\ create push-pem If custom SSH port (example: 50022) is configured in Secondary nodes then # gluster volume geo-replication gvol-primary \\ geoaccount@snode1.example.com::gvol-secondary \\ config ssh_port 50022 # gluster volume geo-replication gvol-primary \\ geoaccount@snode1.example.com::gvol-secondary \\ create ssh-port 50022 push-pem If the total available size in Secondary volume is less than the total size of Primary, the command will throw error message. In such cases 'force' option can be used. In use cases where the rsa-keys of nodes in Primary volume is distributed to Secondary nodes through an external agent and following Secondary side verifications are taken care of by the external agent, then if ssh port 22 or custom port is open in Secondary has proper passwordless ssh login setup Secondary volume is created and is empty if Secondary has enough memory Then use following command to create Geo-rep session with no-verify option. gluster volume geo-replication <primary_volume> \\ <secondary_user>@<secondary_host>::<secondary_volume> create no-verify [force] For example, # gluster volume geo-replication gvol-primary \\ geoaccount@snode1.example.com::gvol-secondary \\ create no-verify In this case the Primary node rsa-key distribution to Secondary node does not happen and above mentioned Secondary verification is not performed and these two things has to be taken care externaly.","title":"Creating the session"},{"location":"Administrator-Guide/Geo-Replication/#post-creation-steps","text":"Run the following command as root in any one of Secondary node. /usr/libexec/glusterfs/set_geo_rep_pem_keys.sh <secondary_user> \\ <primary_volume> <secondary_volume> For example, # /usr/libexec/glusterfs/set_geo_rep_pem_keys.sh geoaccount \\ gvol-primary gvol-secondary","title":"Post Creation steps"},{"location":"Administrator-Guide/Geo-Replication/#configuration","text":"Configuration can be changed anytime after creating the session. After successful configuration change, Geo-rep session will be automatically restarted. To view all configured options of a session, gluster volume geo-replication <primary_volume> \\ <secondary_user>@<secondary_host>::<secondary_volume> config [option] For Example, # gluster volume geo-replication gvol-primary \\ geoaccount@snode1.example.com::gvol-secondary \\ config # gluster volume geo-replication gvol-primary \\ geoaccount@snode1.example.com::gvol-secondary \\ config sync-jobs To configure Gluster Geo-replication, use the following command at the Gluster command line gluster volume geo-replication <primary_volume> \\ <secondary_user>@<secondary_host>::<secondary_volume> config [option] For example: # gluster volume geo-replication gvol-primary \\ geoaccount@snode1.example.com::gvol-secondary \\ config sync-jobs 3 Note : If Geo-rep is in between sync, restart due to configuration change may cause resyncing a few entries which are already synced.","title":"Configuration"},{"location":"Administrator-Guide/Geo-Replication/#configurable-options","text":"Meta Volume In case of Replica bricks, one brick worker will be Active and participate in syncing and others will be waiting as Passive. By default Geo-rep uses node-uuid , if node-uuid of worker present in first up subvolume node ids list then that worker will become Active. With this method, multiple workers of same replica becomes Active if multiple bricks used from same machine. To prevent this, Meta Volume(Extra Gluster Volume) can be used in Geo-rep. With this method, Each worker will try to acquire lock on a file inside meta volume. Lock file name pattern will be different for each sub volume. If a worker acquire lock, then it will become Active else remain as Passive. gluster volume geo-replication <primary_volume> \\ <secondary_user>@<secondary_host>::<secondary_volume> config use-meta-volume true Note : Meta Volume is shared replica 3 Gluster Volume. The name of the meta-volume should be gluster_shared_storage and should be mounted at /var/run/gluster/shared_storage/ . The following table provides an overview of the configurable options for a geo-replication setting: Option Description log-level LOGFILELEVEL The log level for geo-replication. gluster-log-level LOGFILELEVEL The log level for glusterfs processes. changelog-log-level LOGFILELEVEL The log level for Changelog processes. ssh-command COMMAND The SSH command to connect to the remote machine (the default is ssh). If ssh is installed in custom location, that path can be configured. For ex /usr/local/sbin/ssh rsync-command COMMAND The rsync command to use for synchronizing the files (the default is rsync). use-tarssh true The use-tarssh command allows tar over Secure Shell protocol. Use this option to handle workloads of files that have not undergone edits. timeout SECONDS The timeout period in seconds. sync-jobs N The number of simultaneous files/directories that can be synchronized. ignore-deletes If this option is set to 1, a file deleted on the primary will not trigger a delete operation on the secondary. As a result, the secondary will remain as a superset of the primary and can be used to recover the primary in the event of a crash and/or accidental delete.","title":"Configurable Options"},{"location":"Administrator-Guide/Geo-Replication/#starting-geo-replication","text":"Use the following command to start geo-replication session, gluster volume geo-replication <primary_volume> \\ <secondary_user>@<secondary_host>::<secondary_volume> \\ start [force] For example, # gluster volume geo-replication gvol-primary \\ geoaccount@snode1.example.com::gvol-secondary \\ start Note You may need to configure the session before starting Gluster Geo-replication.","title":"Starting Geo-replication"},{"location":"Administrator-Guide/Geo-Replication/#stopping-geo-replication","text":"Use the following command to stop geo-replication sesion, gluster volume geo-replication <primary_volume> \\ <secondary_user>@<secondary_host>::<secondary_volume> \\ stop [force] For example, # gluster volume geo-replication gvol-primary \\ geoaccount@snode1.example.com::gvol-secondary \\ stop","title":"Stopping Geo-replication"},{"location":"Administrator-Guide/Geo-Replication/#status","text":"To check the status of all Geo-replication sessions in the Cluster # gluster volume geo-replication status To check the status of one session, gluster volume geo-replication <primary_volume> \\ <secondary_user>@<secondary_host>::<secondary_volume> status [detail] Example, # gluster volume geo-replication gvol-primary \\ geoaccount@snode1::gvol-secondary status # gluster volume geo-replication gvol-primary \\ geoaccount@snode1::gvol-secondary status detail Example Status Output PRIMARY NODE PRIMARY VOL PRIMARY BRICK SECONDARY USER SECONDARY SECONDARY NODE STATUS CRAWL STATUS LAST_SYNCED --------------------------------------------------------------------------------------------------------------------------------------------------------- mnode1 gvol-primary /bricks/b1 root snode1::gvol-secondary snode1 Active Changelog Crawl 2016-10-12 23:07:13 mnode2 gvol-primary /bricks/b2 root snode1::gvol-secondary snode2 Active Changelog Crawl 2016-10-12 23:07:13 Example Status detail Output PRIMARY NODE PRIMARY VOL PRIMARY BRICK SECONDARY USER SECONDARY SECONDARY NODE STATUS CRAWL STATUS LAST_SYNCED ENTRY DATA META FAILURES CHECKPOINT TIME CHECKPOINT COMPLETED CHECKPOINT COMPLETION TIME -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- mnode1 gvol-primary /bricks/b1 root snode1::gvol-secondary snode1 Active Changelog Crawl 2016-10-12 23:07:13 0 0 0 0 N/A N/A N/A mnode2 gvol-primary /bricks/b2 root snode1::gvol-secondary snode2 Active Changelog Crawl 2016-10-12 23:07:13 0 0 0 0 N/A N/A N/A The STATUS of the session could be one of the following, Initializing : This is the initial phase of the Geo-replication session; it remains in this state for a minute in order to make sure no abnormalities are present. Created : The geo-replication session is created, but not started. Active : The gsync daemon in this node is active and syncing the data. (One worker among the replica pairs will be in Active state) Passive : A replica pair of the active node. The data synchronization is handled by active node. Hence, this node does not sync any data. If Active node goes down, Passive worker will become Active Faulty : The geo-replication session has experienced a problem, and the issue needs to be investigated further. Check log files for more details about the Faulty status. Log file path can be found using gluster volume geo-replication <primary_volume> \\ <secondary_user>@<secondary_host>::<secondary_volume> config log-file Stopped : The geo-replication session has stopped, but has not been deleted. The CRAWL STATUS can be one of the following: Hybrid Crawl : The gsyncd daemon is crawling the glusterFS file system and generating pseudo changelog to sync data. This crawl is used during initial sync and if Changelogs are not available. History Crawl : gsyncd daemon syncs data by consuming Historical Changelogs. On every worker restart, Geo-rep uses this Crawl to process backlog Changelogs. Changelog Crawl : The changelog translator has produced the changelog and that is being consumed by gsyncd daemon to sync data. The ENTRY denotes: The number of pending entry operations (create, mkdir, mknod, symlink, link, rename, unlink, rmdir) per session. The DATA denotes: The number of pending Data operations (write, writev, truncate, ftruncate) per session. The META denotes: The number of pending Meta operations (setattr, fsetattr, setxattr, fsetxattr, removexattr, fremovexattr) per session. The FAILURE denotes: The number of failures per session . On encountering failures, one can proceed to look at the log files.","title":"Status"},{"location":"Administrator-Guide/Geo-Replication/#deleting-the-session","text":"Established Geo-replication session can be deleted using the following command, gluster volume geo-replication <primary_volume> \\ <secondary_user>@<secondary_host>::<secondary_volume> delete [force] For example, # gluster volume geo-replication gvol-primary \\ geoaccount@snode1.example.com::gvol-secondary delete Note: If the same session is created again then syncing will resume from where it was stopped before deleting the session. If the session to be deleted permanently then use reset-sync-time option with delete command. For example, gluster volume geo-replication gvol-primary geoaccount@snode1::gvol-secondary delete reset-sync-time","title":"Deleting the session"},{"location":"Administrator-Guide/Geo-Replication/#checkpoint","text":"Using Checkpoint feature we can find the status of sync with respect to the Checkpoint time. Checkpoint completion status shows \"Yes\" once Geo-rep syncs all the data from that brick which are created or modified before the Checkpoint Time. Set the Checkpoint using, gluster volume geo-replication <primary_volume> \\ <secondary_user>@<secondary_host>::<secondary_volume> config checkpoint now Example, # gluster volume geo-replication gvol-primary \\ geoaccount@snode1.example.com::gvol-secondary \\ config checkpoint now Touch the Primary mount point to make sure Checkpoint completes even though no I/O happening in the Volume # mount -t glusterfs <primaryhost>:<primaryvol> /mnt # touch /mnt Checkpoint status can be checked using Geo-rep status command. Following columns in status output gives more information about Checkpoint CHECKPOINT TIME : Checkpoint Set Time CHECKPOINT COMPLETED : Yes/No/NA, Status of Checkpoint CHECKPOINT COMPLETION TIME : Checkpoint Completion Time if completed, else N/A","title":"Checkpoint"},{"location":"Administrator-Guide/Geo-Replication/#log-files","text":"Primary Log files are located in /var/log/glusterfs/geo-replication directory in each Primary nodes. Secondary log files are located in /var/log/glusterfs/geo-replication-secondary directory in Secondary nodes.","title":"Log Files"},{"location":"Administrator-Guide/Geo-Replication/#gluster-snapshots-and-geo-replicated-volumes","text":"Gluster snapshot of Primary and Secondary should not go out of order on restore. So while taking snapshot take snapshot of both Primary and Secondary Volumes. Pause the Geo-replication session using, gluster volume geo-replication <primary_volume> \\ <secondary_user>@<secondary_host>::<secondary_volume> pause Take Gluster Snapshot of Secondary Volume and Primary Volume(Use same name for snapshots) gluster snapshot create <snapname> <volname> Example, # gluster snapshot create snap1 gvol-secondary # gluster snapshot create snap1 gvol-primary Resume Geo-replication session using, gluster volume geo-replication <primary_volume> \\ <secondary_user>@<secondary_host>::<secondary_volume> resume If we want to continue Geo-rep session after snapshot restore, we need to restore both Primary and Secondary Volume and resume the Geo-replication session using force option gluster snapshot restore <snapname> gluster volume geo-replication <primary_volume> \\ <secondary_user>@<secondary_host>::<secondary_volume> resume force Example, # gluster snapshot restore snap1 # Secondary Snap # gluster snapshot restore snap1 # Primary Snap # gluster volume geo-replication gvol-primary geoaccount@snode1::gvol-secondary \\ resume force","title":"Gluster Snapshots and Geo-replicated Volumes"},{"location":"Administrator-Guide/Gluster-On-ZFS/","text":"Gluster On ZFS This is a step-by-step set of instructions to install Gluster on top of ZFS as the backing file store. There are some commands which were specific to my installation, specifically, the ZFS tuning section. Moniti estis. Preparation Install CentOS 6.3 Assumption is that your hostname is gfs01 Run all commands as the root user yum update Disable IP Tables chkconfig iptables off service iptables stop Disable SELinux edit /etc/selinux/config set SELINUX=disabled reboot Install ZFS on Linux For RHEL6 or 7 and derivatives, you can install the ZFSoL repo (and EPEL) and use that to install ZFS RHEL 6: yum localinstall --nogpgcheck https://download.fedoraproject.org/pub/epel/6/x86_64/epel-release-6-8.noarch.rpm yum localinstall --nogpgcheck http://archive.zfsonlinux.org/epel/zfs-release.el6.noarch.rpm yum install kernel-devel zfs RHEL 7: yum localinstall --nogpgcheck https://download.fedoraproject.org/pub/epel/7/x86_64/e/epel-release-7-2.noarch.rpm yum localinstall --nogpgcheck http://archive.zfsonlinux.org/epel/zfs-release.el7.noarch.rpm yum install kernel-devel zfs and skip to Finish ZFS Configuration below. Or you can roll your own if you want specific patches: yum groupinstall \"Development Tools\" Download & unpack latest SPL and ZFS tarballs from zfsonlinux.org Install DKMS We want automatically rebuild the kernel modules when we upgrade the kernel, so you definitely want DKMS with ZFS on Linux. Download latest RPM from http://linux.dell.com/dkms Install DKMS rpm -Uvh dkms*.rpm Build & Install SPL Enter SPL source directory The following commands create two source & three binary RPMs. Remove the static module RPM (we are using DKMS) and install the rest: ./configure make rpm rm spl-modules-0.6.0*.x86_64.rpm rpm -Uvh spl*.x86_64.rpm spl*.noarch.rpm Build & Install ZFS Notice: If you plan to use the xattr=sa filesystem option, make sure you have the ZFS fix for https://github.com/zfsonlinux/zfs/issues/1648 so your symlinks don't get corrupted. (applies to ZFSoL before 0.6.3, xattr=sa is safe to use on 0.6.3 and later) Enter ZFS source directory The following commands create two source & five binary RPMs. Remove the static module RPM and install the rest. Note we have a few preliminary packages to install before we can compile. yum install zlib-devel libuuid-devel libblkid-devel libselinux-devel parted lsscsi ./configure make rpm rm zfs-modules-0.6.0*.x86_64.rpm rpm -Uvh zfs*.x86_64.rpm zfs*.noarch.rpm Finish ZFS Configuration Reboot to allow all changes to take effect, if desired Create ZFS storage pool, in below examples it will be named sp1 . This is a simple example of 4 HDDs in RAID10. NOTE: Check the latest ZFS on Linux FAQ about configuring the /etc/zfs/zdev.conf file. You want to create mirrored devices across controllers to maximize performance. Make sure to run udevadm trigger after creating zdev.conf. zpool create -f sp1 mirror A0 B0 mirror A1 B1 zpool status sp1 df -h You should see the /sp1 mount point Enable ZFS compression to save disk space: zfs set compression=on sp1 you can also use lz4 compression on later versions of ZFS as it can be faster, especially for incompressible workloads. It is safe to change this on the fly, as ZFS will compress new data with the current setting: zfs set compression=lz4 sp1 Set ZFS tunables. This is specific to my environment. Set ARC cache min to 33% and max to 75% of installed RAM. Since this is a dedicated storage node, I can get away with this. In my case my servers have 24G of RAM. More RAM is better with ZFS. We use SATA drives which do not accept command tagged queuing, therefore set the min and max pending requests to 1 Disable read prefetch because it is almost completely useless and does nothing in our environment but work the drives unnecessarily. I see < 10% prefetch cache hits, so it's really not required and actually hurts performance. Set transaction group timeout to 5 seconds to prevent the volume from appearing to freeze due to a large batch of writes. 5 seconds is the default, but safe to force this. Ignore client flush/sync commands; let ZFS handle this with the transaction group timeout flush. NOTE: Requires a UPS backup solution unless you don't mind losing that 5 seconds worth of data. echo \"options zfs zfs_arc_min=8G zfs_arc_max=18G zfs_vdev_min_pending=1 zfs_vdev_max_pending=1 zfs_prefetch_disable=1 zfs_txg_timeout=5\" > /etc/modprobe.d/zfs.conf reboot Setting the acltype property to posixacl indicates Posix ACLs should be used. zfs set acltype=posixacl sp1 Install GlusterFS wget -P /etc/yum.repos.d http://download.gluster.org/pub/gluster/glusterfs/LATEST/EPEL.repo/glusterfs-epel.repo yum install glusterfs{-fuse,-server} service glusterd start service glusterd status chkconfig glusterd on Continue with your GFS peer probe, volume creation, etc. To mount GFS volumes automatically after reboot, add these lines to /etc/rc.local (assuming your gluster volume is called export and your desired mount point is /export : # Mount GFS Volumes mount -t glusterfs gfs01:/export /export Miscellaneous Notes & TODO Daily e-mail status reports Python script source; put your desired e-mail address in the toAddr variable. Add a crontab entry to run this daily. #!/usr/bin/python ''' Send e-mail to given user with zfs status ''' import datetime import socket import smtplib import subprocess def doShellCmd(cmd): '''execute system shell command, return output as string''' subproc = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE) cmdOutput = subproc.communicate()[0] return cmdOutput hostname = socket.gethostname() statusLine = \"Status of \" + hostname + \" at \" + str(datetime.datetime.now()) zpoolList = doShellCmd('zpool list') zpoolStatus = doShellCmd('zpool status') zfsList = doShellCmd('zfs list') report = (statusLine + \"\\n\" + \"-----------------------------------------------------------\\n\" + zfsList + \"-----------------------------------------------------------\\n\" + zpoolList + \"-----------------------------------------------------------\\n\" + zpoolStatus) fromAddr = \"From: root@\" + hostname + \"\\r\\n\" toAddr = \"To: user@your.com\\r\\n\" subject = \"Subject: ZFS Status from \" + hostname + \"\\r\\n\" msg = (subject + report) server = smtplib.SMTP('localhost') server.set_debuglevel(1) server.sendmail(fromAddr, toAddr, msg) server.quit() Restoring files from ZFS Snapshots Show which node a file is on (for restoring files from ZFS snapshots): `getfattr -n trusted.glusterfs.pathinfo <file>` Recurring ZFS Snapshots Since the community site will not let me actually post the script due to some random bug with Akismet spam blocking, I'll just post links instead. Recurring ZFS Snapshots Or use https://github.com/zfsonlinux/zfs-auto-snapshot","title":"Gluster On ZFS"},{"location":"Administrator-Guide/Gluster-On-ZFS/#gluster-on-zfs","text":"This is a step-by-step set of instructions to install Gluster on top of ZFS as the backing file store. There are some commands which were specific to my installation, specifically, the ZFS tuning section. Moniti estis.","title":"Gluster On ZFS"},{"location":"Administrator-Guide/Gluster-On-ZFS/#preparation","text":"Install CentOS 6.3 Assumption is that your hostname is gfs01 Run all commands as the root user yum update Disable IP Tables chkconfig iptables off service iptables stop Disable SELinux edit /etc/selinux/config set SELINUX=disabled reboot","title":"Preparation"},{"location":"Administrator-Guide/Gluster-On-ZFS/#install-zfs-on-linux","text":"For RHEL6 or 7 and derivatives, you can install the ZFSoL repo (and EPEL) and use that to install ZFS RHEL 6: yum localinstall --nogpgcheck https://download.fedoraproject.org/pub/epel/6/x86_64/epel-release-6-8.noarch.rpm yum localinstall --nogpgcheck http://archive.zfsonlinux.org/epel/zfs-release.el6.noarch.rpm yum install kernel-devel zfs RHEL 7: yum localinstall --nogpgcheck https://download.fedoraproject.org/pub/epel/7/x86_64/e/epel-release-7-2.noarch.rpm yum localinstall --nogpgcheck http://archive.zfsonlinux.org/epel/zfs-release.el7.noarch.rpm yum install kernel-devel zfs and skip to Finish ZFS Configuration below. Or you can roll your own if you want specific patches: yum groupinstall \"Development Tools\" Download & unpack latest SPL and ZFS tarballs from zfsonlinux.org","title":"Install ZFS on Linux"},{"location":"Administrator-Guide/Gluster-On-ZFS/#install-dkms","text":"We want automatically rebuild the kernel modules when we upgrade the kernel, so you definitely want DKMS with ZFS on Linux. Download latest RPM from http://linux.dell.com/dkms Install DKMS rpm -Uvh dkms*.rpm","title":"Install DKMS"},{"location":"Administrator-Guide/Gluster-On-ZFS/#build-install-spl","text":"Enter SPL source directory The following commands create two source & three binary RPMs. Remove the static module RPM (we are using DKMS) and install the rest: ./configure make rpm rm spl-modules-0.6.0*.x86_64.rpm rpm -Uvh spl*.x86_64.rpm spl*.noarch.rpm","title":"Build &amp; Install SPL"},{"location":"Administrator-Guide/Gluster-On-ZFS/#build-install-zfs","text":"Notice: If you plan to use the xattr=sa filesystem option, make sure you have the ZFS fix for https://github.com/zfsonlinux/zfs/issues/1648 so your symlinks don't get corrupted. (applies to ZFSoL before 0.6.3, xattr=sa is safe to use on 0.6.3 and later) Enter ZFS source directory The following commands create two source & five binary RPMs. Remove the static module RPM and install the rest. Note we have a few preliminary packages to install before we can compile. yum install zlib-devel libuuid-devel libblkid-devel libselinux-devel parted lsscsi ./configure make rpm rm zfs-modules-0.6.0*.x86_64.rpm rpm -Uvh zfs*.x86_64.rpm zfs*.noarch.rpm","title":"Build &amp; Install ZFS"},{"location":"Administrator-Guide/Gluster-On-ZFS/#finish-zfs-configuration","text":"Reboot to allow all changes to take effect, if desired Create ZFS storage pool, in below examples it will be named sp1 . This is a simple example of 4 HDDs in RAID10. NOTE: Check the latest ZFS on Linux FAQ about configuring the /etc/zfs/zdev.conf file. You want to create mirrored devices across controllers to maximize performance. Make sure to run udevadm trigger after creating zdev.conf. zpool create -f sp1 mirror A0 B0 mirror A1 B1 zpool status sp1 df -h You should see the /sp1 mount point Enable ZFS compression to save disk space: zfs set compression=on sp1 you can also use lz4 compression on later versions of ZFS as it can be faster, especially for incompressible workloads. It is safe to change this on the fly, as ZFS will compress new data with the current setting: zfs set compression=lz4 sp1 Set ZFS tunables. This is specific to my environment. Set ARC cache min to 33% and max to 75% of installed RAM. Since this is a dedicated storage node, I can get away with this. In my case my servers have 24G of RAM. More RAM is better with ZFS. We use SATA drives which do not accept command tagged queuing, therefore set the min and max pending requests to 1 Disable read prefetch because it is almost completely useless and does nothing in our environment but work the drives unnecessarily. I see < 10% prefetch cache hits, so it's really not required and actually hurts performance. Set transaction group timeout to 5 seconds to prevent the volume from appearing to freeze due to a large batch of writes. 5 seconds is the default, but safe to force this. Ignore client flush/sync commands; let ZFS handle this with the transaction group timeout flush. NOTE: Requires a UPS backup solution unless you don't mind losing that 5 seconds worth of data. echo \"options zfs zfs_arc_min=8G zfs_arc_max=18G zfs_vdev_min_pending=1 zfs_vdev_max_pending=1 zfs_prefetch_disable=1 zfs_txg_timeout=5\" > /etc/modprobe.d/zfs.conf reboot Setting the acltype property to posixacl indicates Posix ACLs should be used. zfs set acltype=posixacl sp1","title":"Finish ZFS Configuration"},{"location":"Administrator-Guide/Gluster-On-ZFS/#install-glusterfs","text":"wget -P /etc/yum.repos.d http://download.gluster.org/pub/gluster/glusterfs/LATEST/EPEL.repo/glusterfs-epel.repo yum install glusterfs{-fuse,-server} service glusterd start service glusterd status chkconfig glusterd on Continue with your GFS peer probe, volume creation, etc. To mount GFS volumes automatically after reboot, add these lines to /etc/rc.local (assuming your gluster volume is called export and your desired mount point is /export : # Mount GFS Volumes mount -t glusterfs gfs01:/export /export","title":"Install GlusterFS"},{"location":"Administrator-Guide/Gluster-On-ZFS/#miscellaneous-notes-todo","text":"","title":"Miscellaneous Notes &amp; TODO"},{"location":"Administrator-Guide/Gluster-On-ZFS/#daily-e-mail-status-reports","text":"Python script source; put your desired e-mail address in the toAddr variable. Add a crontab entry to run this daily. #!/usr/bin/python ''' Send e-mail to given user with zfs status ''' import datetime import socket import smtplib import subprocess def doShellCmd(cmd): '''execute system shell command, return output as string''' subproc = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE) cmdOutput = subproc.communicate()[0] return cmdOutput hostname = socket.gethostname() statusLine = \"Status of \" + hostname + \" at \" + str(datetime.datetime.now()) zpoolList = doShellCmd('zpool list') zpoolStatus = doShellCmd('zpool status') zfsList = doShellCmd('zfs list') report = (statusLine + \"\\n\" + \"-----------------------------------------------------------\\n\" + zfsList + \"-----------------------------------------------------------\\n\" + zpoolList + \"-----------------------------------------------------------\\n\" + zpoolStatus) fromAddr = \"From: root@\" + hostname + \"\\r\\n\" toAddr = \"To: user@your.com\\r\\n\" subject = \"Subject: ZFS Status from \" + hostname + \"\\r\\n\" msg = (subject + report) server = smtplib.SMTP('localhost') server.set_debuglevel(1) server.sendmail(fromAddr, toAddr, msg) server.quit()","title":"Daily e-mail status reports"},{"location":"Administrator-Guide/Gluster-On-ZFS/#restoring-files-from-zfs-snapshots","text":"Show which node a file is on (for restoring files from ZFS snapshots): `getfattr -n trusted.glusterfs.pathinfo <file>`","title":"Restoring files from ZFS Snapshots"},{"location":"Administrator-Guide/Gluster-On-ZFS/#recurring-zfs-snapshots","text":"Since the community site will not let me actually post the script due to some random bug with Akismet spam blocking, I'll just post links instead. Recurring ZFS Snapshots Or use https://github.com/zfsonlinux/zfs-auto-snapshot","title":"Recurring ZFS Snapshots"},{"location":"Administrator-Guide/GlusterFS-Cinder/","text":"Accessing GlusterFS using Cinder Hosts Note: GlusterFS driver was removed from Openstack since Ocata. This guide applies only to older Openstack releases. 1. Introduction GlusterFS and Cinder integration provides a system for data storage that enables users to access the same data, both as an object and as a file, thus simplifying management and controlling storage costs. GlusterFS - GlusterFS is an open source, distributed file system capable of scaling to several petabytes and handling thousands of clients. GlusterFS clusters together storage building blocks over Infiniband RDMA or TCP/IP interconnect, aggregating disk and memory resources and managing data in a single global namespace. GlusterFS is based on a stackable user space design and can deliver exceptional performance for diverse workloads. Cinder - Cinder is the OpenStack service which is responsible for handling persistent storage for virtual machines. This is persistent block storage for the instances running in Nova. Snapshots can be taken for backing up and data, either for restoring data, or to be used to create new block storage volumes. With Enterprise Linux 6, configuring OpenStack Grizzly to use GlusterFS for its Cinder (block) storage is fairly simple. These instructions have been tested with both GlusterFS 3.3 and GlusterFS 3.4. Other releases may also work, but have not been tested. 2. Prerequisites GlusterFS For information on prerequisites and instructions for installing GlusterFS, see http://www.gluster.org/community/documentation/index.php . Cinder For information on prerequisites and instructions for installing Cinder, see http://docs.openstack.org/ . Before beginning, you must ensure there are no existing volumes in Cinder. Use \"cinder delete\" to remove any, and \"cinder list\" to verify that they are deleted. If you do not delete the existing cinder volumes, it will cause errors later in the process, breaking your Cinder installation. NOTE - Unlike other software, the \"openstack-config\" and \"cinder\" commands generally require you to run them as a root user. Without prior configuration, running them through sudo generally does not work. (This can be changed, but is beyond the scope of this HOW-TO.) 3 Installing GlusterFS Client on Cinder hosts On each Cinder host, install the GlusterFS client packages: $ sudo yum -y install glusterfs-fuse 4. Configuring Cinder to Add GlusterFS On each Cinder host, run the following commands to add GlusterFS to the Cinder configuration: # openstack-config --set /etc/cinder/cinder.conf DEFAULT volume_driver cinder.volume.drivers.glusterfs.GlusterfsDriver # openstack-config --set /etc/cinder/cinder.conf DEFAULT glusterfs_shares_config /etc/cinder/shares.conf # openstack-config --set /etc/cinder/cinder.conf DEFAULT glusterfs_mount_point_base /var/lib/cinder/volumes 5. Creating GlusterFS Volume List On each of the Cinder nodes, create a simple text file /etc/cinder/shares.conf . This file is a simple list of the GlusterFS volumes to be used, one per line, using the following format: GLUSTERHOST:VOLUME GLUSTERHOST:NEXTVOLUME GLUSTERHOST2:SOMEOTHERVOLUME For example: myglusterbox.example.org:myglustervol 6. Updating Firewall for GlusterFS You must update the firewall rules on each Cinder node to communicate with the GlusterFS nodes. The ports to open are explained in Step 3: https://docs.gluster.org/en/latest/Install-Guide/Install/ If you are using iptables as your firewall, these lines can be added under :OUTPUT ACCEPT in the \"*filter\" section. You should probably adjust them to suit your environment (eg. only accept connections from your GlusterFS servers). -A INPUT -m state --state NEW -m tcp -p tcp --dport 111 -j ACCEPT -A INPUT -m state --state NEW -m tcp -p tcp --dport 24007 -j ACCEPT -A INPUT -m state --state NEW -m tcp -p tcp --dport 24008 -j ACCEPT -A INPUT -m state --state NEW -m tcp -p tcp --dport 24009 -j ACCEPT -A INPUT -m state --state NEW -m tcp -p tcp --dport 24010 -j ACCEPT -A INPUT -m state --state NEW -m tcp -p tcp --dport 24011 -j ACCEPT -A INPUT -m state --state NEW -m tcp -p tcp --dport 38465:38469 -j ACCEPT Restart the firewall service: $ sudo service iptables restart 7. Restarting Cinder Services Configuration is complete and now you must restart the Cinder services to make it active. $ for i in api scheduler volume; do sudo service openstack-cinder-${i} start; done Check the Cinder volume log to make sure that there are no errors: $ sudo tail -50 /var/log/cinder/volume.log 8. Verify GlusterFS Integration with Cinder To verify if the installation and configuration is successful, create a Cinder volume then check using GlusterFS. Create a Cinder volume: # cinder create --display_name myvol 10 Volume creation takes a few seconds. Once created, run the following command: # cinder list The volume should be in \"available\" status. Now, look for a new file in the GlusterFS volume directory: $ sudo ls -lah /var/lib/cinder/volumes/XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX/ (the XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX will be a number specific to your installation) A newly created file should be inside that directory which is the new volume you just created. A new file will appear each time you create a volume. For example: $ sudo ls -lah /var/lib/cinder/volumes/29e55f0f3d56494ef1b1073ab927d425/ total 4.0K drwxr-xr-x. 3 root root 73 Apr 4 15:46 . drwxr-xr-x. 3 cinder cinder 4.0K Apr 3 09:31 .. -rw-rw-rw-. 1 root root 10G Apr 4 15:46 volume-a4b97d2e-0f8e-45b2-9b94-b8fa36bd51b9","title":"GlusterFS Cinder"},{"location":"Administrator-Guide/GlusterFS-Cinder/#accessing-glusterfs-using-cinder-hosts","text":"Note: GlusterFS driver was removed from Openstack since Ocata. This guide applies only to older Openstack releases.","title":"Accessing GlusterFS using Cinder Hosts"},{"location":"Administrator-Guide/GlusterFS-Cinder/#1-introduction","text":"GlusterFS and Cinder integration provides a system for data storage that enables users to access the same data, both as an object and as a file, thus simplifying management and controlling storage costs. GlusterFS - GlusterFS is an open source, distributed file system capable of scaling to several petabytes and handling thousands of clients. GlusterFS clusters together storage building blocks over Infiniband RDMA or TCP/IP interconnect, aggregating disk and memory resources and managing data in a single global namespace. GlusterFS is based on a stackable user space design and can deliver exceptional performance for diverse workloads. Cinder - Cinder is the OpenStack service which is responsible for handling persistent storage for virtual machines. This is persistent block storage for the instances running in Nova. Snapshots can be taken for backing up and data, either for restoring data, or to be used to create new block storage volumes. With Enterprise Linux 6, configuring OpenStack Grizzly to use GlusterFS for its Cinder (block) storage is fairly simple. These instructions have been tested with both GlusterFS 3.3 and GlusterFS 3.4. Other releases may also work, but have not been tested.","title":"1. Introduction"},{"location":"Administrator-Guide/GlusterFS-Cinder/#2-prerequisites","text":"","title":"2. Prerequisites"},{"location":"Administrator-Guide/GlusterFS-Cinder/#glusterfs","text":"For information on prerequisites and instructions for installing GlusterFS, see http://www.gluster.org/community/documentation/index.php .","title":"GlusterFS"},{"location":"Administrator-Guide/GlusterFS-Cinder/#cinder","text":"For information on prerequisites and instructions for installing Cinder, see http://docs.openstack.org/ . Before beginning, you must ensure there are no existing volumes in Cinder. Use \"cinder delete\" to remove any, and \"cinder list\" to verify that they are deleted. If you do not delete the existing cinder volumes, it will cause errors later in the process, breaking your Cinder installation. NOTE - Unlike other software, the \"openstack-config\" and \"cinder\" commands generally require you to run them as a root user. Without prior configuration, running them through sudo generally does not work. (This can be changed, but is beyond the scope of this HOW-TO.)","title":"Cinder"},{"location":"Administrator-Guide/GlusterFS-Cinder/#3-installing-glusterfs-client-on-cinder-hosts","text":"On each Cinder host, install the GlusterFS client packages: $ sudo yum -y install glusterfs-fuse","title":"3 Installing GlusterFS Client on Cinder hosts"},{"location":"Administrator-Guide/GlusterFS-Cinder/#4-configuring-cinder-to-add-glusterfs","text":"On each Cinder host, run the following commands to add GlusterFS to the Cinder configuration: # openstack-config --set /etc/cinder/cinder.conf DEFAULT volume_driver cinder.volume.drivers.glusterfs.GlusterfsDriver # openstack-config --set /etc/cinder/cinder.conf DEFAULT glusterfs_shares_config /etc/cinder/shares.conf # openstack-config --set /etc/cinder/cinder.conf DEFAULT glusterfs_mount_point_base /var/lib/cinder/volumes","title":"4. Configuring Cinder to Add GlusterFS"},{"location":"Administrator-Guide/GlusterFS-Cinder/#5-creating-glusterfs-volume-list","text":"On each of the Cinder nodes, create a simple text file /etc/cinder/shares.conf . This file is a simple list of the GlusterFS volumes to be used, one per line, using the following format: GLUSTERHOST:VOLUME GLUSTERHOST:NEXTVOLUME GLUSTERHOST2:SOMEOTHERVOLUME For example: myglusterbox.example.org:myglustervol","title":"5. Creating GlusterFS Volume List"},{"location":"Administrator-Guide/GlusterFS-Cinder/#6-updating-firewall-for-glusterfs","text":"You must update the firewall rules on each Cinder node to communicate with the GlusterFS nodes. The ports to open are explained in Step 3: https://docs.gluster.org/en/latest/Install-Guide/Install/ If you are using iptables as your firewall, these lines can be added under :OUTPUT ACCEPT in the \"*filter\" section. You should probably adjust them to suit your environment (eg. only accept connections from your GlusterFS servers). -A INPUT -m state --state NEW -m tcp -p tcp --dport 111 -j ACCEPT -A INPUT -m state --state NEW -m tcp -p tcp --dport 24007 -j ACCEPT -A INPUT -m state --state NEW -m tcp -p tcp --dport 24008 -j ACCEPT -A INPUT -m state --state NEW -m tcp -p tcp --dport 24009 -j ACCEPT -A INPUT -m state --state NEW -m tcp -p tcp --dport 24010 -j ACCEPT -A INPUT -m state --state NEW -m tcp -p tcp --dport 24011 -j ACCEPT -A INPUT -m state --state NEW -m tcp -p tcp --dport 38465:38469 -j ACCEPT Restart the firewall service: $ sudo service iptables restart","title":"6. Updating Firewall for GlusterFS"},{"location":"Administrator-Guide/GlusterFS-Cinder/#7-restarting-cinder-services","text":"Configuration is complete and now you must restart the Cinder services to make it active. $ for i in api scheduler volume; do sudo service openstack-cinder-${i} start; done Check the Cinder volume log to make sure that there are no errors: $ sudo tail -50 /var/log/cinder/volume.log","title":"7. Restarting Cinder Services"},{"location":"Administrator-Guide/GlusterFS-Cinder/#8-verify-glusterfs-integration-with-cinder","text":"To verify if the installation and configuration is successful, create a Cinder volume then check using GlusterFS. Create a Cinder volume: # cinder create --display_name myvol 10 Volume creation takes a few seconds. Once created, run the following command: # cinder list The volume should be in \"available\" status. Now, look for a new file in the GlusterFS volume directory: $ sudo ls -lah /var/lib/cinder/volumes/XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX/ (the XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX will be a number specific to your installation) A newly created file should be inside that directory which is the new volume you just created. A new file will appear each time you create a volume. For example: $ sudo ls -lah /var/lib/cinder/volumes/29e55f0f3d56494ef1b1073ab927d425/ total 4.0K drwxr-xr-x. 3 root root 73 Apr 4 15:46 . drwxr-xr-x. 3 cinder cinder 4.0K Apr 3 09:31 .. -rw-rw-rw-. 1 root root 10G Apr 4 15:46 volume-a4b97d2e-0f8e-45b2-9b94-b8fa36bd51b9","title":"8. Verify GlusterFS Integration with Cinder"},{"location":"Administrator-Guide/GlusterFS-Coreutils/","text":"Coreutils for GlusterFS volumes The GlusterFS Coreutils is a suite of utilities that aims to mimic the standard Linux coreutils, with the exception that it utilizes the gluster C API in order to do work. It offers an interface similar to that of the ftp program. Operations include things like getting files from the server to the local machine, putting files from the local machine to the server, retrieving directory information from the server and so on. Installation Install GlusterFS For information on prerequisites, instructions and configuration of GlusterFS, see Installation Guides from http://docs.gluster.org/en/latest/ . Install glusterfs-coreutils For now glusterfs-coreutils will be packaged only as rpm. Other package formats will be supported very soon. For fedora Use dnf/yum to install glusterfs-coreutils: # dnf install glusterfs-coreutils OR # yum install glusterfs-coreutils Usage glusterfs-coreutils provides a set of basic utilities such as cat, cp, flock, ls, mkdir, rm, stat and tail that are implemented specifically using the GlusterFS API commonly known as libgfapi. These utilities can be used either inside a gluster remote shell or as standalone commands with 'gf' prepended to their respective base names. For example, glusterfs cat utility is named as gfcat and so on with an exception to flock core utility for which a standalone gfflock command is not provided as such(see the notes section on why flock is designed in that way). Using coreutils within a remote gluster-shell Invoke a new shell In order to enter into a gluster client-shell, type gfcli and press enter. You will now be presented with a similar prompt as shown below: # gfcli gfcli> See the man page for gfcli for more options. Connect to a gluster volume Now we need to connect as a client to some glusterfs volume which has already started. Use connect command to do so as follows: gfcli> connect glfs://<SERVER-IP or HOSTNAME>/<VOLNAME> For example if you have a volume named vol on a server with hostname localhost the above command will take the following form: gfcli> connect glfs://localhost/vol Make sure that you are successfully attached to a remote gluster volume by verifying the new prompt which should look like: gfcli (<SERVER IP or HOSTNAME/<VOLNAME>) Try out your favorite utilities Please go through the man pages for different utilities and available options for each command. For example, man gfcp will display details on the usage of cp command outside or within a gluster-shell. Run different commands as follows: gfcli (localhost/vol) ls . gfcli (localhost/vol) stat .trashcan Terminate the client connection from the volume Use disconnect command to close the connection: gfcli (localhost/vol) disconnect gfcli> Exit from shell Run quit from shell: gfcli> quit Using standalone glusterfs coreutil commands As mentioned above glusterfs coreutils also provides standalone commands to perform the basic GNU coreutil functionalities. All those commands are prepended by 'gf'. Instead of invoking a gluster client-shell you can directly make use of these to establish and perform the operation in one shot. For example see the following sample usage of gfstat command: gfstat glfs://localhost/vol/foo There is an exemption regarding flock coreutility which is not available as a standalone command for a reason described under 'Notes' section. For more information on each command and corresponding options see associated man pages. Notes Within a particular session of gluster client-shell, history of commands are preserved i.e, you can use up/down arrow keys to search through previously executed commands or the reverse history search technique using Ctrl+R. flock is not available as standalone 'gfflock'. Because locks are always associated with file descriptors. Unlike all other commands flock cannot straight away clean up the file descriptor after acquiring the lock. For flock we need to maintain an active connection as a glusterfs client.","title":"GlusterFS coreutilities"},{"location":"Administrator-Guide/GlusterFS-Coreutils/#coreutils-for-glusterfs-volumes","text":"The GlusterFS Coreutils is a suite of utilities that aims to mimic the standard Linux coreutils, with the exception that it utilizes the gluster C API in order to do work. It offers an interface similar to that of the ftp program. Operations include things like getting files from the server to the local machine, putting files from the local machine to the server, retrieving directory information from the server and so on.","title":"Coreutils for GlusterFS volumes"},{"location":"Administrator-Guide/GlusterFS-Coreutils/#installation","text":"","title":"Installation"},{"location":"Administrator-Guide/GlusterFS-Coreutils/#install-glusterfs","text":"For information on prerequisites, instructions and configuration of GlusterFS, see Installation Guides from http://docs.gluster.org/en/latest/ .","title":"Install GlusterFS"},{"location":"Administrator-Guide/GlusterFS-Coreutils/#install-glusterfs-coreutils","text":"For now glusterfs-coreutils will be packaged only as rpm. Other package formats will be supported very soon.","title":"Install glusterfs-coreutils"},{"location":"Administrator-Guide/GlusterFS-Coreutils/#for-fedora","text":"Use dnf/yum to install glusterfs-coreutils: # dnf install glusterfs-coreutils OR # yum install glusterfs-coreutils","title":"For fedora"},{"location":"Administrator-Guide/GlusterFS-Coreutils/#usage","text":"glusterfs-coreutils provides a set of basic utilities such as cat, cp, flock, ls, mkdir, rm, stat and tail that are implemented specifically using the GlusterFS API commonly known as libgfapi. These utilities can be used either inside a gluster remote shell or as standalone commands with 'gf' prepended to their respective base names. For example, glusterfs cat utility is named as gfcat and so on with an exception to flock core utility for which a standalone gfflock command is not provided as such(see the notes section on why flock is designed in that way).","title":"Usage"},{"location":"Administrator-Guide/GlusterFS-Coreutils/#using-coreutils-within-a-remote-gluster-shell","text":"","title":"Using coreutils within a remote gluster-shell"},{"location":"Administrator-Guide/GlusterFS-Coreutils/#invoke-a-new-shell","text":"In order to enter into a gluster client-shell, type gfcli and press enter. You will now be presented with a similar prompt as shown below: # gfcli gfcli> See the man page for gfcli for more options.","title":"Invoke a new shell"},{"location":"Administrator-Guide/GlusterFS-Coreutils/#connect-to-a-gluster-volume","text":"Now we need to connect as a client to some glusterfs volume which has already started. Use connect command to do so as follows: gfcli> connect glfs://<SERVER-IP or HOSTNAME>/<VOLNAME> For example if you have a volume named vol on a server with hostname localhost the above command will take the following form: gfcli> connect glfs://localhost/vol Make sure that you are successfully attached to a remote gluster volume by verifying the new prompt which should look like: gfcli (<SERVER IP or HOSTNAME/<VOLNAME>)","title":"Connect to a gluster volume"},{"location":"Administrator-Guide/GlusterFS-Coreutils/#try-out-your-favorite-utilities","text":"Please go through the man pages for different utilities and available options for each command. For example, man gfcp will display details on the usage of cp command outside or within a gluster-shell. Run different commands as follows: gfcli (localhost/vol) ls . gfcli (localhost/vol) stat .trashcan","title":"Try out your favorite utilities"},{"location":"Administrator-Guide/GlusterFS-Coreutils/#terminate-the-client-connection-from-the-volume","text":"Use disconnect command to close the connection: gfcli (localhost/vol) disconnect gfcli>","title":"Terminate the client connection from the volume"},{"location":"Administrator-Guide/GlusterFS-Coreutils/#exit-from-shell","text":"Run quit from shell: gfcli> quit","title":"Exit from shell"},{"location":"Administrator-Guide/GlusterFS-Coreutils/#using-standalone-glusterfs-coreutil-commands","text":"As mentioned above glusterfs coreutils also provides standalone commands to perform the basic GNU coreutil functionalities. All those commands are prepended by 'gf'. Instead of invoking a gluster client-shell you can directly make use of these to establish and perform the operation in one shot. For example see the following sample usage of gfstat command: gfstat glfs://localhost/vol/foo There is an exemption regarding flock coreutility which is not available as a standalone command for a reason described under 'Notes' section. For more information on each command and corresponding options see associated man pages.","title":"Using standalone glusterfs coreutil commands"},{"location":"Administrator-Guide/GlusterFS-Coreutils/#notes","text":"Within a particular session of gluster client-shell, history of commands are preserved i.e, you can use up/down arrow keys to search through previously executed commands or the reverse history search technique using Ctrl+R. flock is not available as standalone 'gfflock'. Because locks are always associated with file descriptors. Unlike all other commands flock cannot straight away clean up the file descriptor after acquiring the lock. For flock we need to maintain an active connection as a glusterfs client.","title":"Notes"},{"location":"Administrator-Guide/GlusterFS-Filter/","text":"Modifying .vol files with a filter If you need to make manual changes to a .vol file it is recommended to make these through the client interface ('gluster foo'). Making changes directly to .vol files is discouraged, because it cannot be predicted when a .vol file will be reset on disk, for example with a 'gluster set foo' command. The command line interface was never designed to read the .vol files, but rather to keep state and rebuild them (from '/var/lib/glusterd/vols/\\$vol/info'). There is, however, another way to do this. You can create a shell script in the directory '/usr/lib*/glusterfs/\\$VERSION/filter'. All scripts located there will be executed every time the .vol files are written back to disk. The first and only argument passed to all script located there is the name of the .vol file. So you could create a script there that looks like this: #!/bin/sh`\\ sed -i 'some-sed-magic' \"$1\" Which will run the script, which in turn will run the sed command on the .vol file (passed as \\$1). Importantly, the script needs to be set as executable (eg via chmod), else it won't be run.","title":"GlusterFS Filter"},{"location":"Administrator-Guide/GlusterFS-Filter/#modifying-vol-files-with-a-filter","text":"If you need to make manual changes to a .vol file it is recommended to make these through the client interface ('gluster foo'). Making changes directly to .vol files is discouraged, because it cannot be predicted when a .vol file will be reset on disk, for example with a 'gluster set foo' command. The command line interface was never designed to read the .vol files, but rather to keep state and rebuild them (from '/var/lib/glusterd/vols/\\$vol/info'). There is, however, another way to do this. You can create a shell script in the directory '/usr/lib*/glusterfs/\\$VERSION/filter'. All scripts located there will be executed every time the .vol files are written back to disk. The first and only argument passed to all script located there is the name of the .vol file. So you could create a script there that looks like this: #!/bin/sh`\\ sed -i 'some-sed-magic' \"$1\" Which will run the script, which in turn will run the sed command on the .vol file (passed as \\$1). Importantly, the script needs to be set as executable (eg via chmod), else it won't be run.","title":"Modifying .vol files with a filter"},{"location":"Administrator-Guide/GlusterFS-Introduction/","text":"What is Gluster ? Gluster is a scalable, distributed file system that aggregates disk storage resources from multiple servers into a single global namespace. Advantages Scales to several petabytes Handles thousands of clients POSIX compatible Uses commodity hardware Can use any ondisk filesystem that supports extended attributes Accessible using industry standard protocols like NFS and SMB Provides replication, quotas, geo-replication, snapshots and bitrot detection Allows optimization for different workloads Open Source Enterprises can scale capacity, performance, and availability on demand, with no vendor lock-in, across on-premise, public cloud, and hybrid environments. Gluster is used in production at thousands of organisations spanning media, healthcare, government, education, web 2.0, and financial services. Commercial offerings and support Several companies offer support or consulting . Red Hat Gluster Storage is a commercial storage software product, based on Gluster.","title":"Introduction"},{"location":"Administrator-Guide/GlusterFS-Introduction/#what-is-gluster","text":"Gluster is a scalable, distributed file system that aggregates disk storage resources from multiple servers into a single global namespace.","title":"What is Gluster ?"},{"location":"Administrator-Guide/GlusterFS-Introduction/#advantages","text":"Scales to several petabytes Handles thousands of clients POSIX compatible Uses commodity hardware Can use any ondisk filesystem that supports extended attributes Accessible using industry standard protocols like NFS and SMB Provides replication, quotas, geo-replication, snapshots and bitrot detection Allows optimization for different workloads Open Source Enterprises can scale capacity, performance, and availability on demand, with no vendor lock-in, across on-premise, public cloud, and hybrid environments. Gluster is used in production at thousands of organisations spanning media, healthcare, government, education, web 2.0, and financial services.","title":"Advantages"},{"location":"Administrator-Guide/GlusterFS-Introduction/#commercial-offerings-and-support","text":"Several companies offer support or consulting . Red Hat Gluster Storage is a commercial storage software product, based on Gluster.","title":"Commercial offerings and support"},{"location":"Administrator-Guide/GlusterFS-Keystone-Quickstart/","text":"GlusterFS Keystone Quickstart This is a document in progress, and may contain some errors or missing information. I am currently in the process of building an AWS Image with this installed, however if you can't wait, and want to install this with a script, here are the commands from both articles, with defaults appropriate for an Amazon CentOS/RHEL 6 AMI, such as ami-a6e15bcf This document assumes you already have GlusterFS with UFO installed, 3.3.1-11 or later, and are using the instructions here: http://www.gluster.org/2012/09/howto-using-ufo-swift-a-quick-and-dirty-setup-guide/ These docs are largely derived from: http://fedoraproject.org/wiki/Getting_started_with_OpenStack_on_Fedora_17#Initial_Keystone_setup Add the RDO Openstack Grizzly and Epel repos: $ sudo yum install -y `[`http://dl.fedoraproject.org/pub/epel/6/i386/epel-release-6-8.noarch.rpm`](http://dl.fedoraproject.org/pub/epel/6/i386/epel-release-6-8.noarch.rpm) $ sudo yum install -y `[`http://rdo.fedorapeople.org/openstack/openstack-grizzly/rdo-release-grizzly-1.noarch.rpm`](http://rdo.fedorapeople.org/openstack/openstack-grizzly/rdo-release-grizzly-1.noarch.rpm) Install Openstack-Keystone $ sudo yum install openstack-keystone openstack-utils python-keystoneclient Configure keystone $ cat > keystonerc << _EOF export ADMIN_TOKEN=$(openssl rand -hex 10) export OS_USERNAME=admin export OS_PASSWORD=$(openssl rand -hex 10) export OS_TENANT_NAME=admin export OS_AUTH_URL=`[`https://127.0.0.1:5000/v2.0/`](https://127.0.0.1:5000/v2.0/) export SERVICE_ENDPOINT=`[`https://127.0.0.1:35357/v2.0/`](https://127.0.0.1:35357/v2.0/) export SERVICE_TOKEN=\\$ADMIN_TOKEN _EOF $ . ./keystonerc $ sudo openstack-db --service keystone --init Append the keystone configs to /etc/swift/proxy-server.conf $ sudo -i` # cat >> /etc/swift/proxy-server.conf << _EOM` [filter:keystone]` use = egg:swift#keystoneauth` operator_roles = admin, swiftoperator` [filter:authtoken] paste.filter_factory = keystoneclient.middleware.auth_token:filter_factory auth_port = 35357 auth_host = 127.0.0.1 auth_protocol = https _EOM exit Finish configuring both swift and keystone using the command-line tool: $ sudo openstack-config --set /etc/swift/proxy-server.conf filter:authtoken admin_token $ADMIN_TOKEN $ sudo openstack-config --set /etc/swift/proxy-server.conf filter:authtoken auth_token $ADMIN_TOKEN $ sudo openstack-config --set /etc/swift/proxy-server.conf DEFAULT log_name proxy_server $ sudo openstack-config --set /etc/swift/proxy-server.conf filter:authtoken signing_dir /etc/swift $ sudo openstack-config --set /etc/swift/proxy-server.conf pipeline:main pipeline \"healthcheck cache authtoken keystone proxy-server\" $ sudo openstack-config --set /etc/keystone/keystone.conf DEFAULT admin_token $ADMIN_TOKEN $ sudo openstack-config --set /etc/keystone/keystone.conf ssl enable True $ sudo openstack-config --set /etc/keystone/keystone.conf ssl keyfile /etc/swift/cert.key $ sudo openstack-config --set /etc/keystone/keystone.conf ssl certfile /etc/swift/cert.crt $ sudo openstack-config --set /etc/keystone/keystone.conf signing token_format UUID $ sudo openstack-config --set /etc/keystone/keystone.conf sql connection mysql://keystone:keystone@127.0.0.1/keystone Configure keystone to start at boot and start it up. $ sudo chkconfig openstack-keystone on $ sudo service openstack-keystone start # If you script this, you'll want to wait a few seconds to start using it We are using untrusted certs, so tell keystone not to complain. If you replace with trusted certs, or are not using SSL, set this to \"\". $ INSECURE=\"--insecure\" Create the keystone and swift services in keystone: $ KS_SERVICEID=$(keystone $INSECURE service-create --name=keystone --type=identity --description=\"Keystone Identity Service\" | grep \" id \" | cut -d \"|\" -f 3) $ SW_SERVICEID=$(keystone $INSECURE service-create --name=swift --type=object-store --description=\"Swift Service\" | grep \" id \" | cut -d \"|\" -f 3) $ endpoint=\"`[`https://127.0.0.1:443`](https://127.0.0.1:443)`\" $ keystone $INSECURE endpoint-create --service_id $KS_SERVICEID \\ --publicurl $endpoint'/v2.0' --adminurl `[`https://127.0.0.1:35357/v2.0`](https://127.0.0.1:35357/v2.0)` \\ --internalurl `[`https://127.0.0.1:5000/v2.0`](https://127.0.0.1:5000/v2.0) $ keystone $INSECURE endpoint-create --service_id $SW_SERVICEID \\ --publicurl $endpoint'/v1/AUTH_$(tenant_id)s' \\ --adminurl $endpoint'/v1/AUTH_$(tenant_id)s' \\ --internalurl $endpoint'/v1/AUTH_$(tenant_id)s' Create the admin tenant: $ admin_id=$(keystone $INSECURE tenant-create --name admin --description \"Internal Admin Tenant\" | grep id | awk '{print $4}') Create the admin roles: $ admin_role=$(keystone $INSECURE role-create --name admin | grep id | awk '{print $4}') $ ksadmin_role=$(keystone $INSECURE role-create --name KeystoneServiceAdmin | grep id | awk '{print $4}') $ kadmin_role=$(keystone $INSECURE role-create --name KeystoneAdmin | grep id | awk '{print $4}') $ member_role=$(keystone $INSECURE role-create --name member | grep id | awk '{print $4}') Create the admin user: $ user_id=$(keystone $INSECURE user-create --name admin --tenant-id $admin_id --pass $OS_PASSWORD | grep id | awk '{print $4}') $ keystone $INSECURE user-role-add --user-id $user_id --tenant-id $admin_id \\ --role-id $admin_role $ keystone $INSECURE user-role-add --user-id $user_id --tenant-id $admin_id \\ --role-id $kadmin_role $ keystone $INSECURE user-role-add --user-id $user_id --tenant-id $admin_id \\ --role-id $ksadmin_role If you do not have multi-volume support (broken in 3.3.1-11), then the volume names will not correlate to the tenants, and all tenants will map to the same volume, so just use a normal name. (This will be fixed in 3.4, and should be fixed in 3.4 Beta. The bug report for this is here: https://bugzilla.redhat.com/show_bug.cgi?id=924792 ) $ volname=\"admin\" # or if you have the multi-volume patch $ volname=$admin_id Create and start the admin volume: $ sudo gluster volume create $volname $myhostname:$pathtobrick $ sudo gluster volume start $volname $ sudo service openstack-keystone start Create the ring for the admin tenant. If you have working multi-volume support, then you can specify multiple volume names in the call: $ cd /etc/swift $ sudo /usr/bin/gluster-swift-gen-builders $volname $ sudo swift-init main restart Create a testadmin user associated with the admin tenant with password testadmin and admin role: $ user_id=$(keystone $INSECURE user-create --name testadmin --tenant-id $admin_id --pass testadmin | grep id | awk '{print $4}') $ keystone $INSECURE user-role-add --user-id $user_id --tenant-id $admin_id \\ --role-id $admin_role Test the user: $ curl $INSECURE -d '{\"auth\":{\"tenantName\": \"admin\", \"passwordCredentials\":{\"username\": \"testadmin\", \"password\": \"testadmin\"}}}' -H \"Content-type: application/json\" `[`https://127.0.0.1:5000/v2.0/tokens`](https://127.0.0.1:5000/v2.0/tokens) See here for more examples: http://docs.openstack.org/developer/keystone/api_curl_examples.html","title":"GlusterFS Keystone Quickstart"},{"location":"Administrator-Guide/GlusterFS-Keystone-Quickstart/#glusterfs-keystone-quickstart","text":"This is a document in progress, and may contain some errors or missing information. I am currently in the process of building an AWS Image with this installed, however if you can't wait, and want to install this with a script, here are the commands from both articles, with defaults appropriate for an Amazon CentOS/RHEL 6 AMI, such as ami-a6e15bcf This document assumes you already have GlusterFS with UFO installed, 3.3.1-11 or later, and are using the instructions here: http://www.gluster.org/2012/09/howto-using-ufo-swift-a-quick-and-dirty-setup-guide/ These docs are largely derived from: http://fedoraproject.org/wiki/Getting_started_with_OpenStack_on_Fedora_17#Initial_Keystone_setup Add the RDO Openstack Grizzly and Epel repos: $ sudo yum install -y `[`http://dl.fedoraproject.org/pub/epel/6/i386/epel-release-6-8.noarch.rpm`](http://dl.fedoraproject.org/pub/epel/6/i386/epel-release-6-8.noarch.rpm) $ sudo yum install -y `[`http://rdo.fedorapeople.org/openstack/openstack-grizzly/rdo-release-grizzly-1.noarch.rpm`](http://rdo.fedorapeople.org/openstack/openstack-grizzly/rdo-release-grizzly-1.noarch.rpm) Install Openstack-Keystone $ sudo yum install openstack-keystone openstack-utils python-keystoneclient Configure keystone $ cat > keystonerc << _EOF export ADMIN_TOKEN=$(openssl rand -hex 10) export OS_USERNAME=admin export OS_PASSWORD=$(openssl rand -hex 10) export OS_TENANT_NAME=admin export OS_AUTH_URL=`[`https://127.0.0.1:5000/v2.0/`](https://127.0.0.1:5000/v2.0/) export SERVICE_ENDPOINT=`[`https://127.0.0.1:35357/v2.0/`](https://127.0.0.1:35357/v2.0/) export SERVICE_TOKEN=\\$ADMIN_TOKEN _EOF $ . ./keystonerc $ sudo openstack-db --service keystone --init Append the keystone configs to /etc/swift/proxy-server.conf $ sudo -i` # cat >> /etc/swift/proxy-server.conf << _EOM` [filter:keystone]` use = egg:swift#keystoneauth` operator_roles = admin, swiftoperator` [filter:authtoken] paste.filter_factory = keystoneclient.middleware.auth_token:filter_factory auth_port = 35357 auth_host = 127.0.0.1 auth_protocol = https _EOM exit Finish configuring both swift and keystone using the command-line tool: $ sudo openstack-config --set /etc/swift/proxy-server.conf filter:authtoken admin_token $ADMIN_TOKEN $ sudo openstack-config --set /etc/swift/proxy-server.conf filter:authtoken auth_token $ADMIN_TOKEN $ sudo openstack-config --set /etc/swift/proxy-server.conf DEFAULT log_name proxy_server $ sudo openstack-config --set /etc/swift/proxy-server.conf filter:authtoken signing_dir /etc/swift $ sudo openstack-config --set /etc/swift/proxy-server.conf pipeline:main pipeline \"healthcheck cache authtoken keystone proxy-server\" $ sudo openstack-config --set /etc/keystone/keystone.conf DEFAULT admin_token $ADMIN_TOKEN $ sudo openstack-config --set /etc/keystone/keystone.conf ssl enable True $ sudo openstack-config --set /etc/keystone/keystone.conf ssl keyfile /etc/swift/cert.key $ sudo openstack-config --set /etc/keystone/keystone.conf ssl certfile /etc/swift/cert.crt $ sudo openstack-config --set /etc/keystone/keystone.conf signing token_format UUID $ sudo openstack-config --set /etc/keystone/keystone.conf sql connection mysql://keystone:keystone@127.0.0.1/keystone Configure keystone to start at boot and start it up. $ sudo chkconfig openstack-keystone on $ sudo service openstack-keystone start # If you script this, you'll want to wait a few seconds to start using it We are using untrusted certs, so tell keystone not to complain. If you replace with trusted certs, or are not using SSL, set this to \"\". $ INSECURE=\"--insecure\" Create the keystone and swift services in keystone: $ KS_SERVICEID=$(keystone $INSECURE service-create --name=keystone --type=identity --description=\"Keystone Identity Service\" | grep \" id \" | cut -d \"|\" -f 3) $ SW_SERVICEID=$(keystone $INSECURE service-create --name=swift --type=object-store --description=\"Swift Service\" | grep \" id \" | cut -d \"|\" -f 3) $ endpoint=\"`[`https://127.0.0.1:443`](https://127.0.0.1:443)`\" $ keystone $INSECURE endpoint-create --service_id $KS_SERVICEID \\ --publicurl $endpoint'/v2.0' --adminurl `[`https://127.0.0.1:35357/v2.0`](https://127.0.0.1:35357/v2.0)` \\ --internalurl `[`https://127.0.0.1:5000/v2.0`](https://127.0.0.1:5000/v2.0) $ keystone $INSECURE endpoint-create --service_id $SW_SERVICEID \\ --publicurl $endpoint'/v1/AUTH_$(tenant_id)s' \\ --adminurl $endpoint'/v1/AUTH_$(tenant_id)s' \\ --internalurl $endpoint'/v1/AUTH_$(tenant_id)s' Create the admin tenant: $ admin_id=$(keystone $INSECURE tenant-create --name admin --description \"Internal Admin Tenant\" | grep id | awk '{print $4}') Create the admin roles: $ admin_role=$(keystone $INSECURE role-create --name admin | grep id | awk '{print $4}') $ ksadmin_role=$(keystone $INSECURE role-create --name KeystoneServiceAdmin | grep id | awk '{print $4}') $ kadmin_role=$(keystone $INSECURE role-create --name KeystoneAdmin | grep id | awk '{print $4}') $ member_role=$(keystone $INSECURE role-create --name member | grep id | awk '{print $4}') Create the admin user: $ user_id=$(keystone $INSECURE user-create --name admin --tenant-id $admin_id --pass $OS_PASSWORD | grep id | awk '{print $4}') $ keystone $INSECURE user-role-add --user-id $user_id --tenant-id $admin_id \\ --role-id $admin_role $ keystone $INSECURE user-role-add --user-id $user_id --tenant-id $admin_id \\ --role-id $kadmin_role $ keystone $INSECURE user-role-add --user-id $user_id --tenant-id $admin_id \\ --role-id $ksadmin_role If you do not have multi-volume support (broken in 3.3.1-11), then the volume names will not correlate to the tenants, and all tenants will map to the same volume, so just use a normal name. (This will be fixed in 3.4, and should be fixed in 3.4 Beta. The bug report for this is here: https://bugzilla.redhat.com/show_bug.cgi?id=924792 ) $ volname=\"admin\" # or if you have the multi-volume patch $ volname=$admin_id Create and start the admin volume: $ sudo gluster volume create $volname $myhostname:$pathtobrick $ sudo gluster volume start $volname $ sudo service openstack-keystone start Create the ring for the admin tenant. If you have working multi-volume support, then you can specify multiple volume names in the call: $ cd /etc/swift $ sudo /usr/bin/gluster-swift-gen-builders $volname $ sudo swift-init main restart Create a testadmin user associated with the admin tenant with password testadmin and admin role: $ user_id=$(keystone $INSECURE user-create --name testadmin --tenant-id $admin_id --pass testadmin | grep id | awk '{print $4}') $ keystone $INSECURE user-role-add --user-id $user_id --tenant-id $admin_id \\ --role-id $admin_role Test the user: $ curl $INSECURE -d '{\"auth\":{\"tenantName\": \"admin\", \"passwordCredentials\":{\"username\": \"testadmin\", \"password\": \"testadmin\"}}}' -H \"Content-type: application/json\" `[`https://127.0.0.1:5000/v2.0/tokens`](https://127.0.0.1:5000/v2.0/tokens) See here for more examples: http://docs.openstack.org/developer/keystone/api_curl_examples.html","title":"GlusterFS Keystone Quickstart"},{"location":"Administrator-Guide/GlusterFS-iSCSI/","text":"GlusterFS iSCSI Introduction iSCSI on Gluster can be set up using the Linux Target driver. This is a user space daemon that accepts iSCSI (as well as iSER and FCoE.) It interprets iSCSI CDBs and converts them into some other I/O operation, according to user configuration. In our case, we can convert the CDBs into file operations that run against a gluster file. The file represents the LUN and the offset in the file the LBA. A plug-in for the Linux target driver has been written to use the libgfapi. It is part of the Linux target driver (bs_glfs.c). Using it, the datapath skips FUSE. This document will be updated to describe how to use it. You can see README.glfs in the Linux target driver's documentation subdirectory. LIO is a replacement for the Linux Target Driver that is included in RHEL7. A user-space plug-in mechanism for it is under development. Once that piece of code exists a similar mechanism can be built for gluster as was done for the Linux target driver. Below is a cookbook to set it up using the Linux Target Driver on the server. This has been tested on XEN and KVM instances within RHEL6, RHEL7, and Fedora 19 instances. In this setup a single path leads to gluster, which represents a performance bottleneck and single point of failure. For HA and load balancing, it is possible to setup two or more paths to different gluster servers using mpio; if the target name is equivalent over each path, mpio will coalless both paths into a single device. For more information on iSCSI and the Linux target driver, see [1] and [2]. Setup Mount gluster locally on your gluster server. Note you can also run it on the gluster client. There are pros and cons to these configurations, described below . # mount -t glusterfs 127.0.0.1:gserver /mnt Create a large file representing your block device within the gluster fs. In this case, the lun is 2G. ( You could also create a gluster \"block device\" for this purpose, which would skip the file system ). # dd if=/dev/zero of=disk3 bs=2G count=25 Create a target using the file as the backend storage. If necessary, download the Linux SCSI target. Then start the service. # yum install scsi-target-utils # service tgtd start You must give an iSCSI Qualified name (IQN), in the format : iqn.yyyy-mm.reversed.domain.name:OptionalIdentifierText where: yyyy-mm represents the 4-digit year and 2-digit month the device was started (for example: 2011-07) # tgtadm --lld iscsi --op new --mode target --tid 1 -T iqn.20013-10.com.redhat You can look at the target: # tgtadm --lld iscsi --op show --mode conn --tid 1 Session: 11 Connection: 0 Initiator iqn.1994-05.com.redhat:cf75c8d4274d Next, add a logical unit to the target # tgtadm --lld iscsi --op new --mode logicalunit --tid 1 --lun 1 -b /mnt/disk3 Allow any initiator to access the target. # tgtadm --lld iscsi --op bind --mode target --tid 1 -I ALL Now it\u2019s time to set up your client. Discover your targets. Note in this example's case, the target IP address is 192.168.1.2 # iscsiadm --mode discovery --type sendtargets --portal 192.168.1.2 Login to your target session. # iscsiadm --mode node --targetname iqn.2001-04.com.example:storage.disk1.amiens.sys1.xyz --portal 192.168.1.2:3260 --login You should have a new SCSI disk. You will see it created in /var/log/messages. You will see it in lsblk. You can send I/O to it: # dd if=/dev/zero of=/dev/sda bs=4K count=100 To tear down your iSCSI connection: # iscsiadm -m node -T iqn.2001-04.com.redhat -p 172.17.40.21 -u Running the iSCSI target on the gluster client You can run the Linux target daemon on the gluster client. The advantages to this setup is the client could run gluster and enjoy all of gluster's benefits. For example, gluster could \"fan out\" I/O to different gluster servers. The downside would be that the client would need to load and configure gluster. It is better to run gluster on the client if it is possible. References [1] http://www.linuxjournal.com/content/creating-software-backed-iscsi-targets-red-hat-enterprise-linux-6 [2] http://www.cyberciti.biz/tips/howto-setup-linux-iscsi-target-sanwith-tgt.html","title":"GlusterFS iSCSI"},{"location":"Administrator-Guide/GlusterFS-iSCSI/#glusterfs-iscsi","text":"","title":"GlusterFS iSCSI"},{"location":"Administrator-Guide/GlusterFS-iSCSI/#introduction","text":"iSCSI on Gluster can be set up using the Linux Target driver. This is a user space daemon that accepts iSCSI (as well as iSER and FCoE.) It interprets iSCSI CDBs and converts them into some other I/O operation, according to user configuration. In our case, we can convert the CDBs into file operations that run against a gluster file. The file represents the LUN and the offset in the file the LBA. A plug-in for the Linux target driver has been written to use the libgfapi. It is part of the Linux target driver (bs_glfs.c). Using it, the datapath skips FUSE. This document will be updated to describe how to use it. You can see README.glfs in the Linux target driver's documentation subdirectory. LIO is a replacement for the Linux Target Driver that is included in RHEL7. A user-space plug-in mechanism for it is under development. Once that piece of code exists a similar mechanism can be built for gluster as was done for the Linux target driver. Below is a cookbook to set it up using the Linux Target Driver on the server. This has been tested on XEN and KVM instances within RHEL6, RHEL7, and Fedora 19 instances. In this setup a single path leads to gluster, which represents a performance bottleneck and single point of failure. For HA and load balancing, it is possible to setup two or more paths to different gluster servers using mpio; if the target name is equivalent over each path, mpio will coalless both paths into a single device. For more information on iSCSI and the Linux target driver, see [1] and [2].","title":"Introduction"},{"location":"Administrator-Guide/GlusterFS-iSCSI/#setup","text":"Mount gluster locally on your gluster server. Note you can also run it on the gluster client. There are pros and cons to these configurations, described below . # mount -t glusterfs 127.0.0.1:gserver /mnt Create a large file representing your block device within the gluster fs. In this case, the lun is 2G. ( You could also create a gluster \"block device\" for this purpose, which would skip the file system ). # dd if=/dev/zero of=disk3 bs=2G count=25 Create a target using the file as the backend storage. If necessary, download the Linux SCSI target. Then start the service. # yum install scsi-target-utils # service tgtd start You must give an iSCSI Qualified name (IQN), in the format : iqn.yyyy-mm.reversed.domain.name:OptionalIdentifierText where: yyyy-mm represents the 4-digit year and 2-digit month the device was started (for example: 2011-07) # tgtadm --lld iscsi --op new --mode target --tid 1 -T iqn.20013-10.com.redhat You can look at the target: # tgtadm --lld iscsi --op show --mode conn --tid 1 Session: 11 Connection: 0 Initiator iqn.1994-05.com.redhat:cf75c8d4274d Next, add a logical unit to the target # tgtadm --lld iscsi --op new --mode logicalunit --tid 1 --lun 1 -b /mnt/disk3 Allow any initiator to access the target. # tgtadm --lld iscsi --op bind --mode target --tid 1 -I ALL Now it\u2019s time to set up your client. Discover your targets. Note in this example's case, the target IP address is 192.168.1.2 # iscsiadm --mode discovery --type sendtargets --portal 192.168.1.2 Login to your target session. # iscsiadm --mode node --targetname iqn.2001-04.com.example:storage.disk1.amiens.sys1.xyz --portal 192.168.1.2:3260 --login You should have a new SCSI disk. You will see it created in /var/log/messages. You will see it in lsblk. You can send I/O to it: # dd if=/dev/zero of=/dev/sda bs=4K count=100 To tear down your iSCSI connection: # iscsiadm -m node -T iqn.2001-04.com.redhat -p 172.17.40.21 -u","title":"Setup"},{"location":"Administrator-Guide/GlusterFS-iSCSI/#running-the-iscsi-target-on-the-gluster-client","text":"You can run the Linux target daemon on the gluster client. The advantages to this setup is the client could run gluster and enjoy all of gluster's benefits. For example, gluster could \"fan out\" I/O to different gluster servers. The downside would be that the client would need to load and configure gluster. It is better to run gluster on the client if it is possible.","title":"Running the iSCSI target on the gluster client"},{"location":"Administrator-Guide/GlusterFS-iSCSI/#references","text":"[1] http://www.linuxjournal.com/content/creating-software-backed-iscsi-targets-red-hat-enterprise-linux-6 [2] http://www.cyberciti.biz/tips/howto-setup-linux-iscsi-target-sanwith-tgt.html","title":"References"},{"location":"Administrator-Guide/Handling-of-users-with-many-groups/","text":"Handling of users that belong to many groups Users can belong to many different (UNIX) groups. These groups are generally used to allow or deny permissions for executing commands or access to files and directories. The number of groups a user can belong to depends on the operating system, but there are also components that support fewer groups. In Gluster, there are different restrictions on different levels in the stack. The explanations in this document should clarify which restrictions exist, and how these can be handled. tl;dr if users belong to more than 90 groups, the brick processes need to resolve the secondary/auxiliary groups with the server.manage-gids volume option the linux kernels /proc filesystem provides up to 32 groups of a running process, if this is not sufficient the mount option resolve-gids can be used Gluster/NFS needs nfs.server-aux-gids when users accessing a Gluster volume over NFS belong to more than 16 groups For all of the above options counts that the system doing the group resolving must be configured ( nsswitch , sssd , ..) to be able to get all groups when only a UID is known. Limit in the GlusterFS protocol When a Gluster client does some action on a Gluster volume, the operation is sent in an RPC packet. This RPC packet contains an header with the credentials of the user. The server-side receives the RPC packet and uses the credentials from the RPC header to perform ownership operations and allow/deny checks. The RPC header used by the GlusterFS protocols can contain at most ~93 groups. In order to pass this limit, the server process (brick) receiving the RPC procedure can do the resolving of the groups locally, and ignore the (too few) groups from the RPC header. This requires that the service process can resolve all of the users groups by the UID of the client process. Most environments that have many groups, already use a configuration where users and groups are maintained in a central location. for enterprises it is common to manage users and their groups in LDAP, Active Directory, NIS or similar. To have the groups of a user resolved on the server-side (brick), the volume option server.manage-gids needs to be set. Once this option is enabled, the brick processes will not use the groups that the Gluster clients send, but will use the POSIX getgrouplist() function to fetch them. Because this is a protocol limitation, all clients, including FUSE mounts, Gluster/NFS server and libgfapi applications are affected by this. Group limit with FUSE The FUSE client gets the groups of the process that does the I/O by reading the information from /proc/$pid/status . This file only contains up to 32 groups. If client-side xlators rely on all groups of a process/user (like posix-acl), these 32 groups could limit functionality. For that reason a mount option has been added. With the resolve-gids mount option, the FUSE client calls the POSIX getgrouplist() function instead of reading /proc/$pid/status . Group limit for NFS The NFS protocol (actually the AUTH_SYS/AUTH_UNIX RPC header) allows up to 16 groups. These are the groups that the NFS-server receives from NFS-clients. Similar to the way the brick processes can resolve the groups on the server-side, the NFS-server can take the UID passed by the NFS-client and use that to resolve the groups. the volume option for that is nfs.server-aux-gids . Other NFS-servers offer options like this too. The Linux kernel nfsd server uses rpc.mountd --manage-gids . NFS-Ganesha has the configuration option Manage_Gids . Implications of these solutions All of the mentioned options are disabled by default. one of the reasons is that resolving groups is an expensive operation. in many cases there is no need for supporting many groups and there could be a performance hit. When groups are resolved, the list is cached. the validity of the cache is configurable. the Gluster processes are not the only ones that cache these groups. nscd or sssd also keep a cache when they handle the getgroupslist() requests. When there are many requests, and querying the groups from a centralized management system takes long, caches might benefit from a longer validity. An other, less obvious difference might be noticed too. Many processes that are written with security in mind reduce the groups that the process can effectively use. This is normally done with the setegids() function. When storage processes do not honour the fewer groups that are effective, and the processes use the UID to resolve all groups of a process, the groups that got dropped with setegids() are added back again. this could lead to permissions that the process should not have.","title":"Handling of users that belong to many groups"},{"location":"Administrator-Guide/Handling-of-users-with-many-groups/#handling-of-users-that-belong-to-many-groups","text":"Users can belong to many different (UNIX) groups. These groups are generally used to allow or deny permissions for executing commands or access to files and directories. The number of groups a user can belong to depends on the operating system, but there are also components that support fewer groups. In Gluster, there are different restrictions on different levels in the stack. The explanations in this document should clarify which restrictions exist, and how these can be handled.","title":"Handling of users that belong to many groups"},{"location":"Administrator-Guide/Handling-of-users-with-many-groups/#tldr","text":"if users belong to more than 90 groups, the brick processes need to resolve the secondary/auxiliary groups with the server.manage-gids volume option the linux kernels /proc filesystem provides up to 32 groups of a running process, if this is not sufficient the mount option resolve-gids can be used Gluster/NFS needs nfs.server-aux-gids when users accessing a Gluster volume over NFS belong to more than 16 groups For all of the above options counts that the system doing the group resolving must be configured ( nsswitch , sssd , ..) to be able to get all groups when only a UID is known.","title":"tl;dr"},{"location":"Administrator-Guide/Handling-of-users-with-many-groups/#limit-in-the-glusterfs-protocol","text":"When a Gluster client does some action on a Gluster volume, the operation is sent in an RPC packet. This RPC packet contains an header with the credentials of the user. The server-side receives the RPC packet and uses the credentials from the RPC header to perform ownership operations and allow/deny checks. The RPC header used by the GlusterFS protocols can contain at most ~93 groups. In order to pass this limit, the server process (brick) receiving the RPC procedure can do the resolving of the groups locally, and ignore the (too few) groups from the RPC header. This requires that the service process can resolve all of the users groups by the UID of the client process. Most environments that have many groups, already use a configuration where users and groups are maintained in a central location. for enterprises it is common to manage users and their groups in LDAP, Active Directory, NIS or similar. To have the groups of a user resolved on the server-side (brick), the volume option server.manage-gids needs to be set. Once this option is enabled, the brick processes will not use the groups that the Gluster clients send, but will use the POSIX getgrouplist() function to fetch them. Because this is a protocol limitation, all clients, including FUSE mounts, Gluster/NFS server and libgfapi applications are affected by this.","title":"Limit in the GlusterFS protocol"},{"location":"Administrator-Guide/Handling-of-users-with-many-groups/#group-limit-with-fuse","text":"The FUSE client gets the groups of the process that does the I/O by reading the information from /proc/$pid/status . This file only contains up to 32 groups. If client-side xlators rely on all groups of a process/user (like posix-acl), these 32 groups could limit functionality. For that reason a mount option has been added. With the resolve-gids mount option, the FUSE client calls the POSIX getgrouplist() function instead of reading /proc/$pid/status .","title":"Group limit with FUSE"},{"location":"Administrator-Guide/Handling-of-users-with-many-groups/#group-limit-for-nfs","text":"The NFS protocol (actually the AUTH_SYS/AUTH_UNIX RPC header) allows up to 16 groups. These are the groups that the NFS-server receives from NFS-clients. Similar to the way the brick processes can resolve the groups on the server-side, the NFS-server can take the UID passed by the NFS-client and use that to resolve the groups. the volume option for that is nfs.server-aux-gids . Other NFS-servers offer options like this too. The Linux kernel nfsd server uses rpc.mountd --manage-gids . NFS-Ganesha has the configuration option Manage_Gids .","title":"Group limit for NFS"},{"location":"Administrator-Guide/Handling-of-users-with-many-groups/#implications-of-these-solutions","text":"All of the mentioned options are disabled by default. one of the reasons is that resolving groups is an expensive operation. in many cases there is no need for supporting many groups and there could be a performance hit. When groups are resolved, the list is cached. the validity of the cache is configurable. the Gluster processes are not the only ones that cache these groups. nscd or sssd also keep a cache when they handle the getgroupslist() requests. When there are many requests, and querying the groups from a centralized management system takes long, caches might benefit from a longer validity. An other, less obvious difference might be noticed too. Many processes that are written with security in mind reduce the groups that the process can effectively use. This is normally done with the setegids() function. When storage processes do not honour the fewer groups that are effective, and the processes use the UID to resolve all groups of a process, the groups that got dropped with setegids() are added back again. this could lead to permissions that the process should not have.","title":"Implications of these solutions"},{"location":"Administrator-Guide/Hook-scripts/","text":"Managing GlusterFS Volume Life-Cycle Extensions with Hook Scripts Glusterfs allows automation of operations by user-written scripts. For every operation, you can execute a pre and a post script. Pre Scripts These scripts are run before the occurrence of the event. You can write a script to automate activities like managing system-wide services. For example, you can write a script to stop exporting the SMB share corresponding to the volume before you stop the volume. Post Scripts These scripts are run after execution of the event. For example, you can write a script to export the SMB share corresponding to the volume after you start the volume. You can run scripts for the following events: Creating a volume Starting a volume Adding a brick Removing a brick Tuning volume options Stopping a volume Deleting a volume Naming Convention While creating the file names of your scripts, you must follow the naming convention followed in your underlying file system like XFS. Note: To enable the script, the name of the script must start with an S . Scripts run in lexicographic order of their names. Location of Scripts This section provides information on the folders where the scripts must be placed. When you create a trusted storage pool, the following directories are created: /var/lib/glusterd/hooks/1/create/ /var/lib/glusterd/hooks/1/delete/ /var/lib/glusterd/hooks/1/start/ /var/lib/glusterd/hooks/1/stop/ /var/lib/glusterd/hooks/1/set/ /var/lib/glusterd/hooks/1/add-brick/ /var/lib/glusterd/hooks/1/remove-brick/ After creating a script, you must ensure to save the script in its respective folder on all the nodes of the trusted storage pool. The location of the script dictates whether the script must be executed before or after an event. Scripts are provided with the command line argument --volname=VOLNAME to specify the volume. Command-specific additional arguments are provided for the following volume operations: Start volume --first=yes, if the volume is the first to be started --first=no, for otherwise Stop volume --last=yes, if the volume is to be stopped last. --last=no, for otherwise Set volume -o key=value For every key, value is specified in volume set command. Prepackaged Scripts Gluster provides scripts to export Samba (SMB) share when you start a volume and to remove the share when you stop the volume. These scripts are available at: /var/lib/glusterd/hooks/1/start/post and /var/lib/glusterd/hooks/1/stop/pre . By default, the scripts are enabled. When you start a volume using gluster volume start VOLNAME , the S30samba-start.sh script performs the following: Adds Samba share configuration details of the volume to the smb.conf file Mounts the volume through FUSE and adds an entry in /etc/fstab for the same. Restarts Samba to run with updated configuration When you stop the volume using gluster volume stop VOLNAME , the S30samba-stop.sh script performs the following: Removes the Samba share details of the volume from the smb.conf file Unmounts the FUSE mount point and removes the corresponding entry in /etc/fstab Restarts Samba to run with updated configuration","title":"Managing GlusterFS Volume Life-Cycle Extensions with Hook Scripts"},{"location":"Administrator-Guide/Hook-scripts/#managing-glusterfs-volume-life-cycle-extensions-with-hook-scripts","text":"Glusterfs allows automation of operations by user-written scripts. For every operation, you can execute a pre and a post script.","title":"Managing GlusterFS Volume Life-Cycle Extensions with Hook Scripts"},{"location":"Administrator-Guide/Hook-scripts/#pre-scripts","text":"These scripts are run before the occurrence of the event. You can write a script to automate activities like managing system-wide services. For example, you can write a script to stop exporting the SMB share corresponding to the volume before you stop the volume.","title":"Pre Scripts"},{"location":"Administrator-Guide/Hook-scripts/#post-scripts","text":"These scripts are run after execution of the event. For example, you can write a script to export the SMB share corresponding to the volume after you start the volume. You can run scripts for the following events: Creating a volume Starting a volume Adding a brick Removing a brick Tuning volume options Stopping a volume Deleting a volume","title":"Post Scripts"},{"location":"Administrator-Guide/Hook-scripts/#naming-convention","text":"While creating the file names of your scripts, you must follow the naming convention followed in your underlying file system like XFS. Note: To enable the script, the name of the script must start with an S . Scripts run in lexicographic order of their names.","title":"Naming Convention"},{"location":"Administrator-Guide/Hook-scripts/#location-of-scripts","text":"This section provides information on the folders where the scripts must be placed. When you create a trusted storage pool, the following directories are created: /var/lib/glusterd/hooks/1/create/ /var/lib/glusterd/hooks/1/delete/ /var/lib/glusterd/hooks/1/start/ /var/lib/glusterd/hooks/1/stop/ /var/lib/glusterd/hooks/1/set/ /var/lib/glusterd/hooks/1/add-brick/ /var/lib/glusterd/hooks/1/remove-brick/ After creating a script, you must ensure to save the script in its respective folder on all the nodes of the trusted storage pool. The location of the script dictates whether the script must be executed before or after an event. Scripts are provided with the command line argument --volname=VOLNAME to specify the volume. Command-specific additional arguments are provided for the following volume operations: Start volume --first=yes, if the volume is the first to be started --first=no, for otherwise Stop volume --last=yes, if the volume is to be stopped last. --last=no, for otherwise Set volume -o key=value For every key, value is specified in volume set command.","title":"Location of Scripts"},{"location":"Administrator-Guide/Hook-scripts/#prepackaged-scripts","text":"Gluster provides scripts to export Samba (SMB) share when you start a volume and to remove the share when you stop the volume. These scripts are available at: /var/lib/glusterd/hooks/1/start/post and /var/lib/glusterd/hooks/1/stop/pre . By default, the scripts are enabled. When you start a volume using gluster volume start VOLNAME , the S30samba-start.sh script performs the following: Adds Samba share configuration details of the volume to the smb.conf file Mounts the volume through FUSE and adds an entry in /etc/fstab for the same. Restarts Samba to run with updated configuration When you stop the volume using gluster volume stop VOLNAME , the S30samba-stop.sh script performs the following: Removes the Samba share details of the volume from the smb.conf file Unmounts the FUSE mount point and removes the corresponding entry in /etc/fstab Restarts Samba to run with updated configuration","title":"Prepackaged Scripts"},{"location":"Administrator-Guide/Linux-Kernel-Tuning/","text":"Linux kernel tuning for GlusterFS Every now and then, questions come up here internally and with many enthusiasts on what Gluster has to say about kernel tuning, if anything. The rarity of kernel tuning is on account of the Linux kernel doing a pretty good job on most workloads. But there is a flip side to this design. The Linux kernel historically has eagerly eaten up a lot of RAM, provided there is some, or driven towards caching as the primary way to improve performance. For most cases, this is fine, but as the amount of workload increases over time and clustered load is thrown upon the servers, this turns out to be troublesome, leading to catastrophic failures of jobs etc. Having had a fair bit of experience looking at large memory systems with heavily loaded regressions, be it CAD, EDA or similar tools, we've sometimes encountered stability problems with Gluster. We had to carefully analyse the memory footprint and amount of disk wait times over days. This gave us a rather remarkable story of disk trashing, huge iowaits, kernel oops, disk hangs etc. This article is the result of my many experiences with tuning options which were performed on many sites. The tuning not only helped with overall responsiveness, but it dramatically stabilized the cluster overall. When it comes to memory tuning the journey starts with the 'VM' subsystem which has a bizarre number of options, which can cause a lot of confusion. vm.swappiness vm.swappiness is a tunable kernel parameter that controls how much the kernel favors swap over RAM. At the source code level, it\u2019s also defined as the tendency to steal mapped memory. A high swappiness value means that the kernel will be more apt to unmap mapped pages. A low swappiness value means the opposite, the kernel will be less apt to unmap mapped pages. In other words, the higher the vm.swappiness value, the more the system will swap. High system swapping has very undesirable effects when there are huge chunks of data being swapped in and out of RAM. Many have argued for the value to be set high, but in my experience, setting the value to '0' causes a performance increase. Conforming with the details here - http://lwn.net/Articles/100978/ But again these changes should be driven by testing and due diligence from the user for their own applications. Heavily loaded, streaming apps should set this value to '0'. By changing this value to '0', the system's responsiveness improves. vm.vfs_cache_pressure This option controls the tendency of the kernel to reclaim the memory which is used for caching of directory and inode objects. At the default value of vfs_cache_pressure=100 the kernel will attempt to reclaim dentries and inodes at a \"fair\" rate with respect to pagecache and swapcache reclaim. Decreasing vfs_cache_pressure causes the kernel to prefer to retain dentry and inode caches. When vfs_cache_pressure=0, the kernel will never reclaim dentries and inodes due to memory pressure and this can easily lead to out-of-memory conditions. Increasing vfs_cache_pressure beyond 100 causes the kernel to prefer to reclaim dentries and inodes. With GlusterFS, many users with a lot of storage and many small files easily end up using a lot of RAM on the server side due to 'inode/dentry' caching, leading to decreased performance when the kernel keeps crawling through data-structures on a 40GB RAM system. Changing this value higher than 100 has helped many users to achieve fair caching and more responsiveness from the kernel. vm.dirty_background_ratio vm.dirty_ratio The first of the two (vm.dirty_background_ratio) defines the percentage of memory that can become dirty before a background flushing of the pages to disk starts. Until this percentage is reached no pages are flushed to disk. However when the flushing starts, then it's done in the background without disrupting any of the running processes in the foreground. Now the second of the two parameters (vm.dirty_ratio) defines the percentage of memory which can be occupied by dirty pages before a forced flush starts. If the percentage of dirty pages reaches this threshold, then all processes become synchronous, and they are not allowed to continue until the io operation they have requested is actually performed and the data is on disk. In cases of high performance I/O machines, this causes a problem as the data caching is cut away and all of the processes doing I/O become blocked to wait for I/O. This will cause a large number of hanging processes, which leads to high load, which leads to an unstable system and crappy performance. Lowering them from standard values causes everything to be flushed to disk rather than storing much in RAM. It helps large memory systems, which would normally flush a 45G-90G pagecache to disk, causing huge wait times for front-end applications, decreasing overall responsiveness and interactivity. \"1\" > /proc/sys/vm/pagecache Page Cache is a disk cache which holds data from files and executable programs, i.e. pages with actual contents of files or block devices. Page Cache (disk cache) is used to reduce the number of disk reads. A value of '1' indicates 1% of the RAM is used for this, so that most of them are fetched from disk rather than RAM. This value is somewhat fishy after the above values have been changed. Changing this option is not necessary, but if you are still paranoid about controlling the pagecache, this value should help. \"deadline\" > /sys/block/sdc/queue/scheduler The I/O scheduler is a component of the Linux kernel which decides how the read and write buffers are to be queued for the underlying device. Theoretically 'noop' is better with a smart RAID controller because Linux knows nothing about (physical) disk geometry, therefore it can be efficient to let the controller, well aware of disk geometry, handle the requests as soon as possible. But 'deadline' seems to enhance performance. You can read more about them in the Linux kernel source documentation: linux/Documentation/block/*iosched.txt . I have also seen 'read' throughput increase during mixed-operations (many writes). \"256\" > /sys/block/sdc/queue/nr_requests This is the size of I/O requests which are buffered before they are communicated to the disk by the Scheduler. The internal queue size of some controllers (queue_depth) is larger than the I/O scheduler's nr_requests so that the I/O scheduler doesn't get much of a chance to properly order and merge the requests. Deadline or CFQ scheduler likes to have nr_requests to be set 2 times the value of queue_depth, which is the default for a given controller. Merging the order and requests helps the scheduler to be more responsive during huge load. echo \"16\" > /proc/sys/vm/page-cluster page-cluster controls the number of pages which are written to swap in a single attempt. It defines the swap I/O size, in the above example adding '16' as per the RAID stripe size of 64k. This wouldn't make sense after you have used swappiness=0, but if you defined swappiness=10 or 20, then using this value helps when your have a RAID stripe size of 64k. blockdev --setra 4096 /dev/ (eg:- sdb, hdc or dev_mapper) Default block device settings often result in terrible performance for many RAID controllers. Adding the above option, which sets read-ahead to 4096 * 512-byte sectors, at least for the streamed copy, increases the speed, saturating the HD's integrated cache by reading ahead during the period used by the kernel to prepare I/O. It may put in cached data which will be requested by the next read. Too much read-ahead may kill random I/O on huge files if it uses potentially useful drive time or loads data beyond caches. A few other miscellaneous changes which are recommended at filesystem level but haven't been tested yet are the following. Make sure that your filesystem knows about the stripe size and number of disks in the array. E.g. for a raid5 array with a stripe size of 64K and 6 disks (effectively 5, because in every stripe-set there is one disk doing parity). These are built on theoretical assumptions and gathered from various other blogs/articles provided by RAID experts. -> ext4 fs, 5 disks, 64K stripe, units in 4K blocks mkfs -text4 -E stride=\\$((64/4)) -> xfs, 5 disks, 64K stripe, units in 512-byte sectors mkfs -txfs -d sunit=\\$((64*2)) -d swidth=\\$((5*64*2)) You may want to consider increasing the above stripe sizes for streaming large files. WARNING: Above changes are highly subjective with certain types of applications. This article doesn't guarantee any benefits whatsoever without prior due diligence from the user for their respective applications. It should only be applied at the behest of an expected increase in overall system responsiveness or if it resolves ongoing issues. More informative and interesting articles/emails/blogs to read http://dom.as/2008/02/05/linux-io-schedulers/ http://www.nextre.it/oracledocs/oraclemyths.html https://lkml.org/lkml/2006/11/15/40 http://misterd77.blogspot.com/2007/11/3ware-hardware-raid-vs-linux-software.html Last updated by: User:y4m4 comment:jdarcy Some additional tuning ideas: * The choice of scheduler is *really* hardware- and workload-dependent, and some schedulers have unique features other than performance. For example, last time I looked cgroups support was limited to the cfq scheduler. Different tests regularly do best on any of cfq, deadline, or noop. The best advice here is not to use a particular scheduler but to try them all for a specific need. * It's worth checking to make sure that /sys/.../max_sectors_kb matches max_hw_sectors_kb. I haven't seen this problem for a while, but back when I used to work on Lustre I often saw that these didn't match and performance suffered. * For read-heavy workloads, experimenting with /sys/.../readahead_kb is definitely worthwhile. * Filesystems should be built with -I 512 or similar so that more xattrs can be stored in the inode instead of requiring an extra seek. * Mounting with noatime or relatime is usually good for performance. reply:y4m4 Agreed i was about write those parameters you mentioned. I should write another elaborate article on FS changes. y4m4 comment:eco 1 year ago \\ This article is the model on which all articles should be written. Detailed information, solid examples and a great selection of references to let readers go more in depth on topics they choose. Great benchmark for others to strive to attain. \\ Eco \\ comment:y4m4 sysctl -w net.core.{r,w}mem_max = 4096000 - this helped us to Reach 800MB/sec with replicated GlusterFS on 10gige - Thanks to Ben England for these test results. \\ y4m4 comment:bengland After testing Gluster 3.2.4 performance with RHEL6.1, I'd suggest some changes to this article's recommendations: vm.swappiness=10 not 0 -- I think 0 is a bit extreme and might lead to out-of-memory conditions, but 10 will avoid just about all paging/swapping. If you still see swapping, you need to probably focus on restricting dirty pages with vm.dirty_ratio. vfs_cache_pressure > 100 -- why? I thought this was a percentage. vm.pagecache=1 -- some distros (e.g. RHEL6) don't have vm.pagecache parameter. vm.dirty_background_ratio=1 not 10 (kernel default?) -- the kernel default is a bit dependent on choice of Linux distro, but for most workloads it's better to set this parameter very low to cause Linux to push dirty pages out to storage sooner. It means that if dirty pages exceed 1% of RAM then it will start to asynchronously write dirty pages to storage. The only workload where this is really bad: apps that write temp files and then quickly delete them (compiles) -- and you should probably be using local storage for such files anyway. Choice of vm.dirty_ratio is more dependent upon the workload, but in other contexts I have observed that response time fairness and stability is much better if you lower dirty ratio so that it doesn't take more than 2-5 seconds to flush all dirty pages to storage. block device parameters: I'm not aware of any case where cfq scheduler actually helps Gluster server. Unless server I/O threads correspond directly to end-users, I don't see how cfq can help you. Deadline scheduler is a good choice. I/O request queue has to be deep enough to allow scheduler to reorder requests to optimize away disk seeks. The parameters max_sectors_kb and nr_requests are relevant for this. For read-ahead, consider increasing it to the point where you prefetch for longer period of time than a disk seek (on order of 10 msec), so that you can avoid unnecessary disk seeks for multi-stream workloads. This comes at the expense of I/O latency so don't overdo it. network: jumbo frames can increase throughput significantly for 10-GbE networks. Raise net.core.{r,w}mem_max to 540000 from default of 131071 (not 4 MB above, my previous recommendation). Gluster 3.2 does setsockopt() call to use 1/2 MB mem for TCP socket buffer space. \\ bengland \\ comment:hjmangalam Thanks very much for noting this info - the descriptions are VERY good.. I'm in the midst of debugging a misbehaving gluster that can't seem to handle small writes over IPoIB and this contains some useful pointers. Some suggestions that might make this more immediately useful: - I'm assuming that this discussion refers to the gluster server nodes, not to the gluster native client nodes, yes? If that's the case, are there are also kernel parameters or recommended settings for the client nodes? \\ - While there are some cases where you mention that a value should be changed to a particular # or %, in a number of cases you advise just increasing/decreasing the values, which for something like a kernel parameter is probably not a useful suggestion. Do I raise it by 10? 10% 2x? 10x? I also ran across a complimentary page, which might be of interest - it explains more of the vm variables, especially as it relates to writing. \\ \"Theory of Operation and Tuning for Write-Heavy Loads\" \\ `` and refs therein. hjmangalam comment:bengland Here are some additional suggestions based on recent testing: \\ - scaling out number of clients -- you need to increase the size of the ARP tables on Gluster server if you want to support more than 1K clients mounting a gluster volume. The defaults for RHEL6.3 were too low to support this, we used this: net.ipv4.neigh.default.gc_thresh2 = 2048 \\ net.ipv4.neigh.default.gc_thresh3 = 4096 In addition, tunings common to webservers become relevant at this number of clients as well, such as netdev_max_backlog, tcp_fin_timeout, and somaxconn. Bonding mode 6 has been observed to increase replication write performance, I have no experience with bonding mode 4 but it should work if switch is properly configured, other bonding modes are a waste of time. bengland \\ 3 months ago","title":"Linux Kernel Tuning"},{"location":"Administrator-Guide/Linux-Kernel-Tuning/#linux-kernel-tuning-for-glusterfs","text":"Every now and then, questions come up here internally and with many enthusiasts on what Gluster has to say about kernel tuning, if anything. The rarity of kernel tuning is on account of the Linux kernel doing a pretty good job on most workloads. But there is a flip side to this design. The Linux kernel historically has eagerly eaten up a lot of RAM, provided there is some, or driven towards caching as the primary way to improve performance. For most cases, this is fine, but as the amount of workload increases over time and clustered load is thrown upon the servers, this turns out to be troublesome, leading to catastrophic failures of jobs etc. Having had a fair bit of experience looking at large memory systems with heavily loaded regressions, be it CAD, EDA or similar tools, we've sometimes encountered stability problems with Gluster. We had to carefully analyse the memory footprint and amount of disk wait times over days. This gave us a rather remarkable story of disk trashing, huge iowaits, kernel oops, disk hangs etc. This article is the result of my many experiences with tuning options which were performed on many sites. The tuning not only helped with overall responsiveness, but it dramatically stabilized the cluster overall. When it comes to memory tuning the journey starts with the 'VM' subsystem which has a bizarre number of options, which can cause a lot of confusion.","title":"Linux kernel tuning for GlusterFS"},{"location":"Administrator-Guide/Linux-Kernel-Tuning/#vmswappiness","text":"vm.swappiness is a tunable kernel parameter that controls how much the kernel favors swap over RAM. At the source code level, it\u2019s also defined as the tendency to steal mapped memory. A high swappiness value means that the kernel will be more apt to unmap mapped pages. A low swappiness value means the opposite, the kernel will be less apt to unmap mapped pages. In other words, the higher the vm.swappiness value, the more the system will swap. High system swapping has very undesirable effects when there are huge chunks of data being swapped in and out of RAM. Many have argued for the value to be set high, but in my experience, setting the value to '0' causes a performance increase. Conforming with the details here - http://lwn.net/Articles/100978/ But again these changes should be driven by testing and due diligence from the user for their own applications. Heavily loaded, streaming apps should set this value to '0'. By changing this value to '0', the system's responsiveness improves.","title":"vm.swappiness"},{"location":"Administrator-Guide/Linux-Kernel-Tuning/#vmvfs_cache_pressure","text":"This option controls the tendency of the kernel to reclaim the memory which is used for caching of directory and inode objects. At the default value of vfs_cache_pressure=100 the kernel will attempt to reclaim dentries and inodes at a \"fair\" rate with respect to pagecache and swapcache reclaim. Decreasing vfs_cache_pressure causes the kernel to prefer to retain dentry and inode caches. When vfs_cache_pressure=0, the kernel will never reclaim dentries and inodes due to memory pressure and this can easily lead to out-of-memory conditions. Increasing vfs_cache_pressure beyond 100 causes the kernel to prefer to reclaim dentries and inodes. With GlusterFS, many users with a lot of storage and many small files easily end up using a lot of RAM on the server side due to 'inode/dentry' caching, leading to decreased performance when the kernel keeps crawling through data-structures on a 40GB RAM system. Changing this value higher than 100 has helped many users to achieve fair caching and more responsiveness from the kernel.","title":"vm.vfs_cache_pressure"},{"location":"Administrator-Guide/Linux-Kernel-Tuning/#vmdirty_background_ratio","text":"","title":"vm.dirty_background_ratio"},{"location":"Administrator-Guide/Linux-Kernel-Tuning/#vmdirty_ratio","text":"The first of the two (vm.dirty_background_ratio) defines the percentage of memory that can become dirty before a background flushing of the pages to disk starts. Until this percentage is reached no pages are flushed to disk. However when the flushing starts, then it's done in the background without disrupting any of the running processes in the foreground. Now the second of the two parameters (vm.dirty_ratio) defines the percentage of memory which can be occupied by dirty pages before a forced flush starts. If the percentage of dirty pages reaches this threshold, then all processes become synchronous, and they are not allowed to continue until the io operation they have requested is actually performed and the data is on disk. In cases of high performance I/O machines, this causes a problem as the data caching is cut away and all of the processes doing I/O become blocked to wait for I/O. This will cause a large number of hanging processes, which leads to high load, which leads to an unstable system and crappy performance. Lowering them from standard values causes everything to be flushed to disk rather than storing much in RAM. It helps large memory systems, which would normally flush a 45G-90G pagecache to disk, causing huge wait times for front-end applications, decreasing overall responsiveness and interactivity.","title":"vm.dirty_ratio"},{"location":"Administrator-Guide/Linux-Kernel-Tuning/#1-procsysvmpagecache","text":"Page Cache is a disk cache which holds data from files and executable programs, i.e. pages with actual contents of files or block devices. Page Cache (disk cache) is used to reduce the number of disk reads. A value of '1' indicates 1% of the RAM is used for this, so that most of them are fetched from disk rather than RAM. This value is somewhat fishy after the above values have been changed. Changing this option is not necessary, but if you are still paranoid about controlling the pagecache, this value should help.","title":"\"1\" > /proc/sys/vm/pagecache"},{"location":"Administrator-Guide/Linux-Kernel-Tuning/#deadline-sysblocksdcqueuescheduler","text":"The I/O scheduler is a component of the Linux kernel which decides how the read and write buffers are to be queued for the underlying device. Theoretically 'noop' is better with a smart RAID controller because Linux knows nothing about (physical) disk geometry, therefore it can be efficient to let the controller, well aware of disk geometry, handle the requests as soon as possible. But 'deadline' seems to enhance performance. You can read more about them in the Linux kernel source documentation: linux/Documentation/block/*iosched.txt . I have also seen 'read' throughput increase during mixed-operations (many writes).","title":"\"deadline\" > /sys/block/sdc/queue/scheduler"},{"location":"Administrator-Guide/Linux-Kernel-Tuning/#256-sysblocksdcqueuenr_requests","text":"This is the size of I/O requests which are buffered before they are communicated to the disk by the Scheduler. The internal queue size of some controllers (queue_depth) is larger than the I/O scheduler's nr_requests so that the I/O scheduler doesn't get much of a chance to properly order and merge the requests. Deadline or CFQ scheduler likes to have nr_requests to be set 2 times the value of queue_depth, which is the default for a given controller. Merging the order and requests helps the scheduler to be more responsive during huge load.","title":"\"256\" > /sys/block/sdc/queue/nr_requests"},{"location":"Administrator-Guide/Linux-Kernel-Tuning/#echo-16-procsysvmpage-cluster","text":"page-cluster controls the number of pages which are written to swap in a single attempt. It defines the swap I/O size, in the above example adding '16' as per the RAID stripe size of 64k. This wouldn't make sense after you have used swappiness=0, but if you defined swappiness=10 or 20, then using this value helps when your have a RAID stripe size of 64k.","title":"echo \"16\" > /proc/sys/vm/page-cluster"},{"location":"Administrator-Guide/Linux-Kernel-Tuning/#blockdev-setra-4096-dev-eg-sdb-hdc-or-dev_mapper","text":"Default block device settings often result in terrible performance for many RAID controllers. Adding the above option, which sets read-ahead to 4096 * 512-byte sectors, at least for the streamed copy, increases the speed, saturating the HD's integrated cache by reading ahead during the period used by the kernel to prepare I/O. It may put in cached data which will be requested by the next read. Too much read-ahead may kill random I/O on huge files if it uses potentially useful drive time or loads data beyond caches. A few other miscellaneous changes which are recommended at filesystem level but haven't been tested yet are the following. Make sure that your filesystem knows about the stripe size and number of disks in the array. E.g. for a raid5 array with a stripe size of 64K and 6 disks (effectively 5, because in every stripe-set there is one disk doing parity). These are built on theoretical assumptions and gathered from various other blogs/articles provided by RAID experts. -> ext4 fs, 5 disks, 64K stripe, units in 4K blocks mkfs -text4 -E stride=\\$((64/4)) -> xfs, 5 disks, 64K stripe, units in 512-byte sectors mkfs -txfs -d sunit=\\$((64*2)) -d swidth=\\$((5*64*2)) You may want to consider increasing the above stripe sizes for streaming large files. WARNING: Above changes are highly subjective with certain types of applications. This article doesn't guarantee any benefits whatsoever without prior due diligence from the user for their respective applications. It should only be applied at the behest of an expected increase in overall system responsiveness or if it resolves ongoing issues. More informative and interesting articles/emails/blogs to read http://dom.as/2008/02/05/linux-io-schedulers/ http://www.nextre.it/oracledocs/oraclemyths.html https://lkml.org/lkml/2006/11/15/40 http://misterd77.blogspot.com/2007/11/3ware-hardware-raid-vs-linux-software.html Last updated by: User:y4m4","title":"blockdev --setra 4096 /dev/ (eg:- sdb, hdc or dev_mapper)"},{"location":"Administrator-Guide/Linux-Kernel-Tuning/#commentjdarcy","text":"Some additional tuning ideas: * The choice of scheduler is *really* hardware- and workload-dependent, and some schedulers have unique features other than performance. For example, last time I looked cgroups support was limited to the cfq scheduler. Different tests regularly do best on any of cfq, deadline, or noop. The best advice here is not to use a particular scheduler but to try them all for a specific need. * It's worth checking to make sure that /sys/.../max_sectors_kb matches max_hw_sectors_kb. I haven't seen this problem for a while, but back when I used to work on Lustre I often saw that these didn't match and performance suffered. * For read-heavy workloads, experimenting with /sys/.../readahead_kb is definitely worthwhile. * Filesystems should be built with -I 512 or similar so that more xattrs can be stored in the inode instead of requiring an extra seek. * Mounting with noatime or relatime is usually good for performance.","title":"comment:jdarcy"},{"location":"Administrator-Guide/Linux-Kernel-Tuning/#replyy4m4","text":"Agreed i was about write those parameters you mentioned. I should write another elaborate article on FS changes. y4m4","title":"reply:y4m4"},{"location":"Administrator-Guide/Linux-Kernel-Tuning/#commenteco","text":"1 year ago \\ This article is the model on which all articles should be written. Detailed information, solid examples and a great selection of references to let readers go more in depth on topics they choose. Great benchmark for others to strive to attain. \\ Eco \\","title":"comment:eco"},{"location":"Administrator-Guide/Linux-Kernel-Tuning/#commenty4m4","text":"sysctl -w net.core.{r,w}mem_max = 4096000 - this helped us to Reach 800MB/sec with replicated GlusterFS on 10gige - Thanks to Ben England for these test results. \\ y4m4","title":"comment:y4m4"},{"location":"Administrator-Guide/Linux-Kernel-Tuning/#commentbengland","text":"After testing Gluster 3.2.4 performance with RHEL6.1, I'd suggest some changes to this article's recommendations: vm.swappiness=10 not 0 -- I think 0 is a bit extreme and might lead to out-of-memory conditions, but 10 will avoid just about all paging/swapping. If you still see swapping, you need to probably focus on restricting dirty pages with vm.dirty_ratio. vfs_cache_pressure > 100 -- why? I thought this was a percentage. vm.pagecache=1 -- some distros (e.g. RHEL6) don't have vm.pagecache parameter. vm.dirty_background_ratio=1 not 10 (kernel default?) -- the kernel default is a bit dependent on choice of Linux distro, but for most workloads it's better to set this parameter very low to cause Linux to push dirty pages out to storage sooner. It means that if dirty pages exceed 1% of RAM then it will start to asynchronously write dirty pages to storage. The only workload where this is really bad: apps that write temp files and then quickly delete them (compiles) -- and you should probably be using local storage for such files anyway. Choice of vm.dirty_ratio is more dependent upon the workload, but in other contexts I have observed that response time fairness and stability is much better if you lower dirty ratio so that it doesn't take more than 2-5 seconds to flush all dirty pages to storage. block device parameters: I'm not aware of any case where cfq scheduler actually helps Gluster server. Unless server I/O threads correspond directly to end-users, I don't see how cfq can help you. Deadline scheduler is a good choice. I/O request queue has to be deep enough to allow scheduler to reorder requests to optimize away disk seeks. The parameters max_sectors_kb and nr_requests are relevant for this. For read-ahead, consider increasing it to the point where you prefetch for longer period of time than a disk seek (on order of 10 msec), so that you can avoid unnecessary disk seeks for multi-stream workloads. This comes at the expense of I/O latency so don't overdo it. network: jumbo frames can increase throughput significantly for 10-GbE networks. Raise net.core.{r,w}mem_max to 540000 from default of 131071 (not 4 MB above, my previous recommendation). Gluster 3.2 does setsockopt() call to use 1/2 MB mem for TCP socket buffer space. \\ bengland \\","title":"comment:bengland"},{"location":"Administrator-Guide/Linux-Kernel-Tuning/#commenthjmangalam","text":"Thanks very much for noting this info - the descriptions are VERY good.. I'm in the midst of debugging a misbehaving gluster that can't seem to handle small writes over IPoIB and this contains some useful pointers. Some suggestions that might make this more immediately useful: - I'm assuming that this discussion refers to the gluster server nodes, not to the gluster native client nodes, yes? If that's the case, are there are also kernel parameters or recommended settings for the client nodes? \\ - While there are some cases where you mention that a value should be changed to a particular # or %, in a number of cases you advise just increasing/decreasing the values, which for something like a kernel parameter is probably not a useful suggestion. Do I raise it by 10? 10% 2x? 10x? I also ran across a complimentary page, which might be of interest - it explains more of the vm variables, especially as it relates to writing. \\ \"Theory of Operation and Tuning for Write-Heavy Loads\" \\ `` and refs therein. hjmangalam","title":"comment:hjmangalam"},{"location":"Administrator-Guide/Linux-Kernel-Tuning/#commentbengland_1","text":"Here are some additional suggestions based on recent testing: \\ - scaling out number of clients -- you need to increase the size of the ARP tables on Gluster server if you want to support more than 1K clients mounting a gluster volume. The defaults for RHEL6.3 were too low to support this, we used this: net.ipv4.neigh.default.gc_thresh2 = 2048 \\ net.ipv4.neigh.default.gc_thresh3 = 4096 In addition, tunings common to webservers become relevant at this number of clients as well, such as netdev_max_backlog, tcp_fin_timeout, and somaxconn. Bonding mode 6 has been observed to increase replication write performance, I have no experience with bonding mode 4 but it should work if switch is properly configured, other bonding modes are a waste of time. bengland \\ 3 months ago","title":"comment:bengland"},{"location":"Administrator-Guide/Logging/","text":"GlusterFS service Logs and locations Below lists the component, services, and functionality based logs in the GlusterFS Server. As per the File System Hierarchy Standards (FHS) all the log files are placed in the /var/log directory. \u2060 Glusterd: glusterd logs are located at /var/log/glusterfs/glusterd.log . One glusterd log file per server. This log file also contains the snapshot and user logs. Gluster cli command: gluster cli logs are located at /var/log/glusterfs/cli.log . Gluster commands executed on a node in a GlusterFS Trusted Storage Pool is logged in /var/log/glusterfs/cmd_history.log . Bricks: Bricks logs are located at /var/log/glusterfs/bricks/<path extraction of brick path>.log . One log file per brick on the server Rebalance: rebalance logs are located at /var/log/glusterfs/VOLNAME-rebalance.log . One log file per volume on the server. Self heal deamon: self heal deamon are logged at /var/log/glusterfs/glustershd.log . One log file per server Quota: /var/log/glusterfs/quotad.log are log of the quota daemons running on each node. /var/log/glusterfs/quota-crawl.log Whenever quota is enabled, a file system crawl is performed and the corresponding log is stored in this file. /var/log/glusterfs/quota-mount- VOLNAME.log An auxiliary FUSE client is mounted in /VOLNAME of the glusterFS and the corresponding client logs found in this file. One log file per server (and per volume from quota-mount. Gluster NFS: /var/log/glusterfs/nfs.log One log file per server SAMBA Gluster: /var/log/samba/glusterfs-VOLNAME-<ClientIP>.log . If the client mounts this on a glusterFS server node, the actual log file or the mount point may not be found. In such a case, the mount outputs of all the glusterFS type mount operations need to be considered. Ganesha NFS : /var/log/nfs-ganesha.log FUSE Mount: /var/log/glusterfs/<mountpoint path extraction>.log Geo-replication: /var/log/glusterfs/geo-replication/<primary> /var/log/glusterfs/geo-replication-secondary Gluster volume heal VOLNAME info command: /var/log/glusterfs/glfsheal-VOLNAME.log . One log file per server on which the command is executed. Gluster-swift: /var/log/messages SwiftKrbAuth: /var/log/httpd/error_log","title":"Logging"},{"location":"Administrator-Guide/Logging/#glusterfs-service-logs-and-locations","text":"Below lists the component, services, and functionality based logs in the GlusterFS Server. As per the File System Hierarchy Standards (FHS) all the log files are placed in the /var/log directory. \u2060","title":"GlusterFS service Logs and locations"},{"location":"Administrator-Guide/Logging/#glusterd","text":"glusterd logs are located at /var/log/glusterfs/glusterd.log . One glusterd log file per server. This log file also contains the snapshot and user logs.","title":"Glusterd:"},{"location":"Administrator-Guide/Logging/#gluster-cli-command","text":"gluster cli logs are located at /var/log/glusterfs/cli.log . Gluster commands executed on a node in a GlusterFS Trusted Storage Pool is logged in /var/log/glusterfs/cmd_history.log .","title":"Gluster cli command:"},{"location":"Administrator-Guide/Logging/#bricks","text":"Bricks logs are located at /var/log/glusterfs/bricks/<path extraction of brick path>.log . One log file per brick on the server","title":"Bricks:"},{"location":"Administrator-Guide/Logging/#rebalance","text":"rebalance logs are located at /var/log/glusterfs/VOLNAME-rebalance.log . One log file per volume on the server.","title":"Rebalance:"},{"location":"Administrator-Guide/Logging/#self-heal-deamon","text":"self heal deamon are logged at /var/log/glusterfs/glustershd.log . One log file per server","title":"Self heal deamon:"},{"location":"Administrator-Guide/Logging/#quota","text":"/var/log/glusterfs/quotad.log are log of the quota daemons running on each node. /var/log/glusterfs/quota-crawl.log Whenever quota is enabled, a file system crawl is performed and the corresponding log is stored in this file. /var/log/glusterfs/quota-mount- VOLNAME.log An auxiliary FUSE client is mounted in /VOLNAME of the glusterFS and the corresponding client logs found in this file. One log file per server (and per volume from quota-mount.","title":"Quota:"},{"location":"Administrator-Guide/Logging/#gluster-nfs","text":"/var/log/glusterfs/nfs.log One log file per server","title":"Gluster NFS:"},{"location":"Administrator-Guide/Logging/#samba-gluster","text":"/var/log/samba/glusterfs-VOLNAME-<ClientIP>.log . If the client mounts this on a glusterFS server node, the actual log file or the mount point may not be found. In such a case, the mount outputs of all the glusterFS type mount operations need to be considered.","title":"SAMBA Gluster:"},{"location":"Administrator-Guide/Logging/#ganesha-nfs","text":"/var/log/nfs-ganesha.log","title":"Ganesha NFS :"},{"location":"Administrator-Guide/Logging/#fuse-mount","text":"/var/log/glusterfs/<mountpoint path extraction>.log","title":"FUSE Mount:"},{"location":"Administrator-Guide/Logging/#geo-replication","text":"/var/log/glusterfs/geo-replication/<primary> /var/log/glusterfs/geo-replication-secondary","title":"Geo-replication:"},{"location":"Administrator-Guide/Logging/#gluster-volume-heal-volname-info-command","text":"/var/log/glusterfs/glfsheal-VOLNAME.log . One log file per server on which the command is executed.","title":"Gluster volume heal VOLNAME info command:"},{"location":"Administrator-Guide/Logging/#gluster-swift","text":"/var/log/messages","title":"Gluster-swift:"},{"location":"Administrator-Guide/Logging/#swiftkrbauth","text":"/var/log/httpd/error_log","title":"SwiftKrbAuth:"},{"location":"Administrator-Guide/Managing-Snapshots/","text":"Managing GlusterFS Volume Snapshots This section describes how to perform common GlusterFS volume snapshot management operations Pre-requisites GlusterFS volume snapshot feature is based on thinly provisioned LVM snapshot. To make use of snapshot feature GlusterFS volume should fulfill following pre-requisites: Each brick should be on an independent thinly provisioned LVM. Brick LVM should not contain any other data other than brick. None of the brick should be on a thick LVM. gluster version should be 3.6 and above. Details of how to create thin volume can be found at the following link. https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/7/html/Logical_Volume_Manager_Administration/LV.html#thinly_provisioned_volume_creation Few features of snapshot are: Crash Consistency when a snapshot is taken at a particular point-in-time, it is made sure that the taken snapshot is crash consistent. when the taken snapshot is restored, then the data is identical as it was at the time of taking a snapshot. Online Snapshot When the snapshot is being taken the file system and its associated data continue to be available for the clients. Barrier During snapshot creation some of the fops are blocked to guarantee crash consistency. There is a default time-out of 2 minutes, if snapshot creation is not complete within that span then fops are unbarried. If unbarrier happens before the snapshot creation is complete then the snapshot creation operation fails. This to ensure that the snapshot is in a consistent state. Snapshot Management Snapshot creation gluster snapshot create <snapname> <volname> [no-timestamp] [description <description>] Creates a snapshot of a GlusterFS volume. User can provide a snap-name and a description to identify the snap. The description cannot be more than 1024 characters. Snapshot will be created by appending timestamp with user provided snap name. User can override this behaviour by giving no-timestamp flag. NOTE : To be able to take a snapshot, volume should be present and it should be in started state. All the bricks used in creating the snapshot have to be online in order to successfully create a snapshot as the force option is now deprecated. Snapshot clone gluster snapshot clone <clonename> <snapname> Creates a clone of a snapshot. Upon successful completion, a new GlusterFS volume will be created from snapshot. The clone will be a space efficient clone, i.e, the snapshot and the clone will share the backend disk. NOTE : To be able to take a clone from snapshot, snapshot should be present and it should be in activated state. Restoring snaps gluster snapshot restore <snapname> Restores an already taken snapshot of a GlusterFS volume. Snapshot restore is an offline activity therefore if the volume is online (in started state) then the restore operation will fail. Once the snapshot is restored it will not be available in the list of snapshots. Deleting snaps gluster snapshot delete (all | <snapname> | volume <volname>) If snapname is specified then mentioned snapshot is deleted. If volname is specified then all snapshots belonging to that particular volume is deleted. If keyword all is used then all snapshots belonging to the system is deleted. Listing of available snaps gluster snapshot list [volname] Lists all snapshots taken. If volname is provided, then only the snapshots belonging to that particular volume is listed. Information of available snaps gluster snapshot info [(snapname | volume <volname>)] This command gives information such as snapshot name, snapshot UUID, time at which snapshot was created, and it lists down the snap-volume-name, number of snapshots already taken and number of snapshots still available for that particular volume, and the state of the snapshot. Status of snapshots gluster snapshot status [(snapname | volume <volname>)] This command gives status of the snapshot. The details included are snapshot brick path, volume group(LVM details), status of the snapshot bricks, PID of the bricks, data percentage filled for that particular volume group to which the snapshots belong to, and total size of the logical volume. If snapname is specified then status of the mentioned snapshot is displayed. If volname is specified then status of all snapshots belonging to that volume is displayed. If both snapname and volname is not specified then status of all the snapshots present in the system are displayed. Configuring the snapshot behavior snapshot config [volname] ([snap-max-hard-limit <count>] [snap-max-soft-limit <percent>]) | ([auto-delete <enable|disable>]) | ([activate-on-create <enable|disable>]) Displays and sets the snapshot config values. snapshot config without any keywords displays the snapshot config values of all volumes in the system. If volname is provided, then the snapshot config values of that volume is displayed. Snapshot config command along with keywords can be used to change the existing config values. If volname is provided then config value of that volume is changed, else it will set/change the system limit. snap-max-soft-limit and auto-delete are global options, that will be inherited by all volumes in the system and cannot be set to individual volumes. The system limit takes precedence over the volume specific limit. When auto-delete feature is enabled, then upon reaching the soft-limit, with every successful snapshot creation, the oldest snapshot will be deleted. When auto-delete feature is disabled, then upon reaching the soft-limit, the user gets a warning with every successful snapshot creation. Upon reaching the hard-limit, further snapshot creations will not be allowed. activate-on-create is disabled by default. If you enable activate-on-create, then further snapshot will be activated during the time of snapshot creation. Activating a snapshot gluster snapshot activate <snapname> Activates the mentioned snapshot. Note : By default the snapshot will not be activated during snapshot creation. Deactivate a snapshot gluster snapshot deactivate <snapname> Deactivates the mentioned snapshot. Accessing the snapshot Snapshots can be accessed in 2 ways. Mounting the snapshot: The snapshot can be accessed via FUSE mount (only fuse). To do that it has to be mounted first. A snapshot can be mounted via fuse by below command mount -t glusterfs <hostname>:/snaps/<snap-name>/<volume-name> <mount-path> i.e. say \"host1\" is one of the peers. Let \"vol\" be the volume name and \"my-snap\" be the snapshot name. In this case a snapshot can be mounted via this command # mount -t glusterfs host1:/snaps/my-snap/vol /mnt/snapshot User serviceability: Apart from the above method of mounting the snapshot, a list of available snapshots and the contents of each snapshot can be viewed from any of the mount points accessing the glusterfs volume (either FUSE or NFS or SMB). For having user serviceable snapshots, it has to be enabled for a volume first. User serviceability can be enabled for a volume using the below command. gluster volume set <volname> features.uss enable Once enabled, from any of the directory (including root of the filesystem) an access point will be created to the snapshot world. The access point is a hidden directory cding into which will make the user enter the snapshot world. By default the hidden directory is \".snaps\". Once user serviceability is enabled, one will be able to cd into .snaps from any directory. Doing \"ls\" on that directory shows a list of directories which are nothing but the snapshots present for that volume. Say if there are 3 snapshots (\"snap1\", \"snap2\", \"snap3\"), then doing ls in .snaps directory will show those 3 names as the directory entries. They represent the state of the directory from which .snaps was entered, at different points in time. NOTE : The access to the snapshots are read-only. The snapshot needs to be activated for it to be accessible inside .snaps directory. Also, the name of the hidden directory (or the access point to the snapshot world) can be changed using the below command. gluster volume set <volname> snapshot-directory <new-name> Accessing from windows: The glusterfs volumes can be made accessible by windows via samba. (the glusterfs plugin for samba helps achieve this, without having to re-export a fuse mounted glusterfs volume). The snapshots of a glusterfs volume can also be viewed in the windows explorer. There are 2 ways: * Give the path of the entry point directory ( <hostname><samba-share><directory><entry-point path> ) in the run command window Go to the samba share via windows explorer. Make hidden files and folders visible so that in the root of the samba share a folder icon for the entry point can be seen. NOTE : From the explorer, snapshot world can be entered via entry point only from the root of the samba share. If snapshots have to be seen from subfolders, then the path should be provided in the run command window. For snapshots to be accessible from windows, below 2 options can be used. The glusterfs plugin for samba should give the option \"snapdir-entry-path\" while starting. The option is an indication to glusterfs, that samba is loading it and the value of the option should be the path that is being used as the share for windows. Ex: Say, there is a glusterfs volume and a directory called \"export\" from the root of the volume is being used as the samba share, then samba has to load glusterfs with this option as well. ret = glfs_set_xlator_option( fs, \"*-snapview-client\", \"snapdir-entry-path\", \"/export\" ); The xlator option \"snapdir-entry-path\" is not exposed via volume set options, cannot be changed from CLI. Its an option that has to be provided at the time of mounting glusterfs or when samba loads glusterfs. The accessibility of snapshots via root of the samba share from windows is configurable. By default it is turned off. It is a volume set option which can be changed via CLI. gluster volume set <volname> features.show-snapshot-directory <on/off> . By default it is off. Only when both the above options have been provided (i.e snapdir-entry-path contains a valid unix path that is exported and show-snapshot-directory option is set to true), snapshots can accessed via windows explorer. If only 1st option (i.e. snapdir-entry-path) is set via samba and 2nd option (i.e. show-snapshot-directory) is off, then snapshots can be accessed from windows via the run command window, but not via the explorer.","title":"Snapshots"},{"location":"Administrator-Guide/Managing-Snapshots/#managing-glusterfs-volume-snapshots","text":"This section describes how to perform common GlusterFS volume snapshot management operations","title":"Managing GlusterFS Volume Snapshots"},{"location":"Administrator-Guide/Managing-Snapshots/#pre-requisites","text":"GlusterFS volume snapshot feature is based on thinly provisioned LVM snapshot. To make use of snapshot feature GlusterFS volume should fulfill following pre-requisites: Each brick should be on an independent thinly provisioned LVM. Brick LVM should not contain any other data other than brick. None of the brick should be on a thick LVM. gluster version should be 3.6 and above. Details of how to create thin volume can be found at the following link. https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/7/html/Logical_Volume_Manager_Administration/LV.html#thinly_provisioned_volume_creation","title":"Pre-requisites"},{"location":"Administrator-Guide/Managing-Snapshots/#few-features-of-snapshot-are","text":"Crash Consistency when a snapshot is taken at a particular point-in-time, it is made sure that the taken snapshot is crash consistent. when the taken snapshot is restored, then the data is identical as it was at the time of taking a snapshot. Online Snapshot When the snapshot is being taken the file system and its associated data continue to be available for the clients. Barrier During snapshot creation some of the fops are blocked to guarantee crash consistency. There is a default time-out of 2 minutes, if snapshot creation is not complete within that span then fops are unbarried. If unbarrier happens before the snapshot creation is complete then the snapshot creation operation fails. This to ensure that the snapshot is in a consistent state.","title":"Few features of snapshot are:"},{"location":"Administrator-Guide/Managing-Snapshots/#snapshot-management","text":"","title":"Snapshot Management"},{"location":"Administrator-Guide/Managing-Snapshots/#snapshot-creation","text":"gluster snapshot create <snapname> <volname> [no-timestamp] [description <description>] Creates a snapshot of a GlusterFS volume. User can provide a snap-name and a description to identify the snap. The description cannot be more than 1024 characters. Snapshot will be created by appending timestamp with user provided snap name. User can override this behaviour by giving no-timestamp flag. NOTE : To be able to take a snapshot, volume should be present and it should be in started state. All the bricks used in creating the snapshot have to be online in order to successfully create a snapshot as the force option is now deprecated.","title":"Snapshot creation"},{"location":"Administrator-Guide/Managing-Snapshots/#snapshot-clone","text":"gluster snapshot clone <clonename> <snapname> Creates a clone of a snapshot. Upon successful completion, a new GlusterFS volume will be created from snapshot. The clone will be a space efficient clone, i.e, the snapshot and the clone will share the backend disk. NOTE : To be able to take a clone from snapshot, snapshot should be present and it should be in activated state.","title":"Snapshot clone"},{"location":"Administrator-Guide/Managing-Snapshots/#restoring-snaps","text":"gluster snapshot restore <snapname> Restores an already taken snapshot of a GlusterFS volume. Snapshot restore is an offline activity therefore if the volume is online (in started state) then the restore operation will fail. Once the snapshot is restored it will not be available in the list of snapshots.","title":"Restoring snaps"},{"location":"Administrator-Guide/Managing-Snapshots/#deleting-snaps","text":"gluster snapshot delete (all | <snapname> | volume <volname>) If snapname is specified then mentioned snapshot is deleted. If volname is specified then all snapshots belonging to that particular volume is deleted. If keyword all is used then all snapshots belonging to the system is deleted.","title":"Deleting snaps"},{"location":"Administrator-Guide/Managing-Snapshots/#listing-of-available-snaps","text":"gluster snapshot list [volname] Lists all snapshots taken. If volname is provided, then only the snapshots belonging to that particular volume is listed.","title":"Listing of available snaps"},{"location":"Administrator-Guide/Managing-Snapshots/#information-of-available-snaps","text":"gluster snapshot info [(snapname | volume <volname>)] This command gives information such as snapshot name, snapshot UUID, time at which snapshot was created, and it lists down the snap-volume-name, number of snapshots already taken and number of snapshots still available for that particular volume, and the state of the snapshot.","title":"Information of available snaps"},{"location":"Administrator-Guide/Managing-Snapshots/#status-of-snapshots","text":"gluster snapshot status [(snapname | volume <volname>)] This command gives status of the snapshot. The details included are snapshot brick path, volume group(LVM details), status of the snapshot bricks, PID of the bricks, data percentage filled for that particular volume group to which the snapshots belong to, and total size of the logical volume. If snapname is specified then status of the mentioned snapshot is displayed. If volname is specified then status of all snapshots belonging to that volume is displayed. If both snapname and volname is not specified then status of all the snapshots present in the system are displayed.","title":"Status of snapshots"},{"location":"Administrator-Guide/Managing-Snapshots/#configuring-the-snapshot-behavior","text":"snapshot config [volname] ([snap-max-hard-limit <count>] [snap-max-soft-limit <percent>]) | ([auto-delete <enable|disable>]) | ([activate-on-create <enable|disable>]) Displays and sets the snapshot config values. snapshot config without any keywords displays the snapshot config values of all volumes in the system. If volname is provided, then the snapshot config values of that volume is displayed. Snapshot config command along with keywords can be used to change the existing config values. If volname is provided then config value of that volume is changed, else it will set/change the system limit. snap-max-soft-limit and auto-delete are global options, that will be inherited by all volumes in the system and cannot be set to individual volumes. The system limit takes precedence over the volume specific limit. When auto-delete feature is enabled, then upon reaching the soft-limit, with every successful snapshot creation, the oldest snapshot will be deleted. When auto-delete feature is disabled, then upon reaching the soft-limit, the user gets a warning with every successful snapshot creation. Upon reaching the hard-limit, further snapshot creations will not be allowed. activate-on-create is disabled by default. If you enable activate-on-create, then further snapshot will be activated during the time of snapshot creation.","title":"Configuring the snapshot behavior"},{"location":"Administrator-Guide/Managing-Snapshots/#activating-a-snapshot","text":"gluster snapshot activate <snapname> Activates the mentioned snapshot. Note : By default the snapshot will not be activated during snapshot creation.","title":"Activating a snapshot"},{"location":"Administrator-Guide/Managing-Snapshots/#deactivate-a-snapshot","text":"gluster snapshot deactivate <snapname> Deactivates the mentioned snapshot.","title":"Deactivate a snapshot"},{"location":"Administrator-Guide/Managing-Snapshots/#accessing-the-snapshot","text":"Snapshots can be accessed in 2 ways. Mounting the snapshot: The snapshot can be accessed via FUSE mount (only fuse). To do that it has to be mounted first. A snapshot can be mounted via fuse by below command mount -t glusterfs <hostname>:/snaps/<snap-name>/<volume-name> <mount-path> i.e. say \"host1\" is one of the peers. Let \"vol\" be the volume name and \"my-snap\" be the snapshot name. In this case a snapshot can be mounted via this command # mount -t glusterfs host1:/snaps/my-snap/vol /mnt/snapshot User serviceability: Apart from the above method of mounting the snapshot, a list of available snapshots and the contents of each snapshot can be viewed from any of the mount points accessing the glusterfs volume (either FUSE or NFS or SMB). For having user serviceable snapshots, it has to be enabled for a volume first. User serviceability can be enabled for a volume using the below command. gluster volume set <volname> features.uss enable Once enabled, from any of the directory (including root of the filesystem) an access point will be created to the snapshot world. The access point is a hidden directory cding into which will make the user enter the snapshot world. By default the hidden directory is \".snaps\". Once user serviceability is enabled, one will be able to cd into .snaps from any directory. Doing \"ls\" on that directory shows a list of directories which are nothing but the snapshots present for that volume. Say if there are 3 snapshots (\"snap1\", \"snap2\", \"snap3\"), then doing ls in .snaps directory will show those 3 names as the directory entries. They represent the state of the directory from which .snaps was entered, at different points in time. NOTE : The access to the snapshots are read-only. The snapshot needs to be activated for it to be accessible inside .snaps directory. Also, the name of the hidden directory (or the access point to the snapshot world) can be changed using the below command. gluster volume set <volname> snapshot-directory <new-name> Accessing from windows: The glusterfs volumes can be made accessible by windows via samba. (the glusterfs plugin for samba helps achieve this, without having to re-export a fuse mounted glusterfs volume). The snapshots of a glusterfs volume can also be viewed in the windows explorer. There are 2 ways: * Give the path of the entry point directory ( <hostname><samba-share><directory><entry-point path> ) in the run command window Go to the samba share via windows explorer. Make hidden files and folders visible so that in the root of the samba share a folder icon for the entry point can be seen. NOTE : From the explorer, snapshot world can be entered via entry point only from the root of the samba share. If snapshots have to be seen from subfolders, then the path should be provided in the run command window. For snapshots to be accessible from windows, below 2 options can be used. The glusterfs plugin for samba should give the option \"snapdir-entry-path\" while starting. The option is an indication to glusterfs, that samba is loading it and the value of the option should be the path that is being used as the share for windows. Ex: Say, there is a glusterfs volume and a directory called \"export\" from the root of the volume is being used as the samba share, then samba has to load glusterfs with this option as well. ret = glfs_set_xlator_option( fs, \"*-snapview-client\", \"snapdir-entry-path\", \"/export\" ); The xlator option \"snapdir-entry-path\" is not exposed via volume set options, cannot be changed from CLI. Its an option that has to be provided at the time of mounting glusterfs or when samba loads glusterfs. The accessibility of snapshots via root of the samba share from windows is configurable. By default it is turned off. It is a volume set option which can be changed via CLI. gluster volume set <volname> features.show-snapshot-directory <on/off> . By default it is off. Only when both the above options have been provided (i.e snapdir-entry-path contains a valid unix path that is exported and show-snapshot-directory option is set to true), snapshots can accessed via windows explorer. If only 1st option (i.e. snapdir-entry-path) is set via samba and 2nd option (i.e. show-snapshot-directory) is off, then snapshots can be accessed from windows via the run command window, but not via the explorer.","title":"Accessing the snapshot"},{"location":"Administrator-Guide/Managing-Volumes/","text":"Managing GlusterFS Volumes This section describes how to perform common GlusterFS management operations, including the following: Tuning Volume Options Configuring Transport Types for a Volume Expanding Volumes Shrinking Volumes Replacing Bricks Rebalancing Volumes Stopping Volumes Deleting Volumes Triggering Self-Heal on Replicate Non Uniform File Allocation(NUFA) Configuring Transport Types for a Volume A volume can support one or more transport types for communication between clients and brick processes. There are three types of supported transport, which are tcp, rdma, and tcp,rdma. To change the supported transport types of a volume, follow the procedure: Unmount the volume on all the clients using the following command: # umount mount-point Stop the volumes using the following command: # gluster volume stop <VOLNAME> Change the transport type. For example, to enable both tcp and rdma execute the followimg command: # gluster volume set test-volume config.transport tcp,rdma OR tcp OR rdma Mount the volume on all the clients. For example, to mount using rdma transport, use the following command: # mount -t glusterfs -o transport=rdma server1:/test-volume /mnt/glusterfs Expanding Volumes You can expand volumes, as needed, while the cluster is online and available. For example, you might want to add a brick to a distributed volume, thereby increasing the distribution and adding to the capacity of the GlusterFS volume. Similarly, you might want to add a group of bricks to a distributed replicated volume, increasing the capacity of the GlusterFS volume. Note When expanding distributed replicated and distributed dispersed volumes, you need to add a number of bricks that is a multiple of the replica or disperse count. For example, to expand a distributed replicated volume with a replica count of 2, you need to add bricks in multiples of 2 (such as 4, 6, 8, etc.). To expand a volume If they are not already part of the TSP, probe the servers which contain the bricks you want to add to the volume using the following command: # gluster peer probe <SERVERNAME> For example: # gluster peer probe server4 Probe successful Add the brick using the following command: # gluster volume add-brick <VOLNAME> <NEW-BRICK> For example: # gluster volume add-brick test-volume server4:/exp4 Add Brick successful Check the volume information using the following command: # gluster volume info <VOLNAME> The command displays information similar to the following: Volume Name: test-volume Type: Distribute Status: Started Number of Bricks: 4 Bricks: Brick1: server1:/exp1 Brick2: server2:/exp2 Brick3: server3:/exp3 Brick4: server4:/exp4 Rebalance the volume to ensure that files are distributed to the new brick. You can use the rebalance command as described in Rebalancing Volumes Shrinking Volumes You can shrink volumes, as needed, while the cluster is online and available. For example, you might need to remove a brick that has become inaccessible in a distributed volume due to hardware or network failure. Note Data residing on the brick that you are removing will no longer be accessible at the Gluster mount point. Note however that only the configuration information is removed - you can continue to access the data directly from the brick, as necessary. When shrinking distributed replicated and distributed dispersed volumes, you need to remove a number of bricks that is a multiple of the replica or stripe count. For example, to shrink a distributed replicate volume with a replica count of 2, you need to remove bricks in multiples of 2 (such as 4, 6, 8, etc.). In addition, the bricks you are trying to remove must be from the same sub-volume (the same replica or disperse set). Running remove-brick with the start option will automatically trigger a rebalance operation to migrate data from the removed-bricks to the rest of the volume. To shrink a volume Remove the brick using the following command: # gluster volume remove-brick <VOLNAME> <BRICKNAME> start For example, to remove server2:/exp2: # gluster volume remove-brick test-volume server2:/exp2 start volume remove-brick start: success View the status of the remove brick operation using the following command: # gluster volume remove-brick <VOLNAME> <BRICKNAME> status For example, to view the status of remove brick operation on server2:/exp2 brick: # gluster volume remove-brick test-volume server2:/exp2 status Node Rebalanced-files size scanned status --------- ---------------- ---- ------- ----------- 617c923e-6450-4065-8e33-865e28d9428f 34 340 162 in progress Once the status displays \"completed\", commit the remove-brick operation # gluster volume remove-brick <VOLNAME> <BRICKNAME> commit In this example: # gluster volume remove-brick test-volume server2:/exp2 commit Removing brick(s) can result in data loss. Do you want to Continue? (y/n) y volume remove-brick commit: success Check the removed bricks to ensure all files are migrated. If files with data are found on the brick path, copy them via a gluster mount point before re-purposing the removed brick. Check the volume information using the following command: # gluster volume info The command displays information similar to the following: # gluster volume info Volume Name: test-volume Type: Distribute Status: Started Number of Bricks: 3 Bricks: Brick1: server1:/exp1 Brick3: server3:/exp3 Brick4: server4:/exp4 Replace faulty brick Replacing a brick in a pure distribute volume To replace a brick on a distribute only volume, add the new brick and then remove the brick you want to replace. This will trigger a rebalance operation which will move data from the removed brick. NOTE: Replacing a brick using the 'replace-brick' command in gluster is supported only for distributed-replicate or pure replicate volumes. Steps to remove brick Server1:/home/gfs/r2_1 and add Server1:/home/gfs/r2_2: Here is the initial volume configuration: Volume Name: r2 Type: Distribute Volume ID: 25b4e313-7b36-445d-b524-c3daebb91188 Status: Started Number of Bricks: 2 Transport-type: tcp Bricks: Brick1: Server1:/home/gfs/r2_0 Brick2: Server1:/home/gfs/r2_1 Here are the files that are present on the mount: # ls 1 10 2 3 4 5 6 7 8 9 Add the new brick - Server1:/home/gfs/r2_2 now: # gluster volume add-brick r2 Server1:/home/gfs/r2_2 volume add-brick: success Start remove-brick using the following command: # gluster volume remove-brick r2 Server1:/home/gfs/r2_1 start volume remove-brick start: success ID: fba0a488-21a4-42b7-8a41-b27ebaa8e5f4 Wait until remove-brick status indicates that it is complete. # gluster volume remove-brick r2 Server1:/home/gfs/r2_1 status Node Rebalanced-files size scanned failures skipped status run time in secs --------- ----------- ----------- ----------- ----------- ----------- ------------ -------------- localhost 5 20Bytes 15 0 0 completed 0.00 Now we can safely remove the old brick, so commit the changes: # gluster volume remove-brick r2 Server1:/home/gfs/r2_1 commit Removing brick(s) can result in data loss. Do you want to Continue? (y/n) y volume remove-brick commit: success Here is the new volume configuration. Volume Name: r2 Type: Distribute Volume ID: 25b4e313-7b36-445d-b524-c3daebb91188 Status: Started Number of Bricks: 2 Transport-type: tcp Bricks: Brick1: Server1:/home/gfs/r2_0 Brick2: Server1:/home/gfs/r2_2 Check the contents of the mount: # ls 1 10 2 3 4 5 6 7 8 9 Replacing bricks in Replicate/Distributed Replicate volumes This section of the document describes how brick: Server1:/home/gfs/r2_0 is replaced with brick: Server1:/home/gfs/r2_5 in volume r2 with replica count 2 . Volume Name: r2 Type: Distributed-Replicate Volume ID: 24a0437a-daa0-4044-8acf-7aa82efd76fd Status: Started Number of Bricks: 2 x 2 = 4 Transport-type: tcp Bricks: Brick1: Server1:/home/gfs/r2_0 Brick2: Server2:/home/gfs/r2_1 Brick3: Server1:/home/gfs/r2_2 Brick4: Server2:/home/gfs/r2_3 Steps: Make sure there is no data in the new brick Server1:/home/gfs/r2_5 Check that all the bricks are running. It is okay if the brick that is going to be replaced is down. Bring the brick that is going to be replaced down if not already. Get the pid of the brick by executing 'gluster volume status' # gluster volume status Status of volume: r2 Gluster process Port Online Pid ------------------------------------------------------------------------------ Brick Server1:/home/gfs/r2_0 49152 Y 5342 Brick Server2:/home/gfs/r2_1 49153 Y 5354 Brick Server1:/home/gfs/r2_2 49154 Y 5365 Brick Server2:/home/gfs/r2_3 49155 Y 5376 Login to the machine where the brick is running and kill the brick. # kill -15 5342 Confirm that the brick is not running anymore and the other bricks are running fine. # gluster volume status Status of volume: r2 Gluster process Port Online Pid ------------------------------------------------------------------------------ Brick Server1:/home/gfs/r2_0 N/A N 5342 <<---- brick is not running, others are running fine. Brick Server2:/home/gfs/r2_1 49153 Y 5354 Brick Server1:/home/gfs/r2_2 49154 Y 5365 Brick Server2:/home/gfs/r2_3 49155 Y 5376 Using the gluster volume fuse mount (In this example: /mnt/r2 ) set up metadata so that data will be synced to new brick (In this case it is from Server1:/home/gfs/r2_1 to Server1:/home/gfs/r2_5 ) Create a directory on the mount point that doesn't already exist. Then delete that directory, do the same for metadata changelog by doing setfattr. This operation marks the pending changelog which will tell self-heal damon/mounts to perform self-heal from /home/gfs/r2_1 to /home/gfs/r2_5 . mkdir /mnt/r2/<name-of-nonexistent-dir> rmdir /mnt/r2/<name-of-nonexistent-dir> setfattr -n trusted.non-existent-key -v abc /mnt/r2 setfattr -x trusted.non-existent-key /mnt/r2 Check that there are pending xattrs on the replica of the brick that is being replaced: getfattr -d -m. -e hex /home/gfs/r2_1 # file: home/gfs/r2_1 security.selinux=0x756e636f6e66696e65645f753a6f626a6563745f723a66696c655f743a733000 trusted.afr.r2-client-0=0x000000000000000300000002 <<---- xattrs are marked from source brick Server2:/home/gfs/r2_1 trusted.afr.r2-client-1=0x000000000000000000000000 trusted.gfid=0x00000000000000000000000000000001 trusted.glusterfs.dht=0x0000000100000000000000007ffffffe trusted.glusterfs.volume-id=0xde822e25ebd049ea83bfaa3c4be2b440 Volume heal info will show that '/' needs healing.(There could be more entries based on the work load. But '/' must exist) # gluster volume heal r2 info Brick Server1:/home/gfs/r2_0 Status: Transport endpoint is not connected Brick Server2:/home/gfs/r2_1 / Number of entries: 1 Brick Server1:/home/gfs/r2_2 Number of entries: 0 Brick Server2:/home/gfs/r2_3 Number of entries: 0 Replace the brick with 'commit force' option. Please note that other variants of replace-brick command are not supported. Execute replace-brick command # gluster volume replace-brick r2 Server1:/home/gfs/r2_0 Server1:/home/gfs/r2_5 commit force volume replace-brick: success: replace-brick commit successful Check that the new brick is now online # gluster volume status Status of volume: r2 Gluster process Port Online Pid ------------------------------------------------------------------------------ Brick Server1:/home/gfs/r2_5 49156 Y 5731 <<<---- new brick is online Brick Server2:/home/gfs/r2_1 49153 Y 5354 Brick Server1:/home/gfs/r2_2 49154 Y 5365 Brick Server2:/home/gfs/r2_3 49155 Y 5376 Users can track the progress of self-heal using: gluster volume heal [volname] info . Once self-heal completes the changelogs will be removed. # getfattr -d -m. -e hex /home/gfs/r2_1 getfattr: Removing leading '/' from absolute path names # file: home/gfs/r2_1 security.selinux=0x756e636f6e66696e65645f753a6f626a6563745f723a66696c655f743a733000 trusted.afr.r2-client-0=0x000000000000000000000000 <<---- Pending changelogs are cleared. trusted.afr.r2-client-1=0x000000000000000000000000 trusted.gfid=0x00000000000000000000000000000001 trusted.glusterfs.dht=0x0000000100000000000000007ffffffe trusted.glusterfs.volume-id=0xde822e25ebd049ea83bfaa3c4be2b440 # gluster volume heal <VOLNAME> info will show that no heal is required. # gluster volume heal r2 info Brick Server1:/home/gfs/r2_5 Number of entries: 0 Brick Server2:/home/gfs/r2_1 Number of entries: 0 Brick Server1:/home/gfs/r2_2 Number of entries: 0 Brick Server2:/home/gfs/r2_3 Number of entries: 0 Rebalancing Volumes After expanding a volume using the add-brick command, you may need to rebalance the data among the servers. New directories created after expanding or shrinking of the volume will be evenly distributed automatically. For all the existing directories, the distribution can be fixed by rebalancing the layout and/or data. This section describes how to rebalance GlusterFS volumes in your storage environment, using the following common scenarios: Fix Layout - Fixes the layout to use the new volume topology so that files can be distributed to newly added nodes. Fix Layout and Migrate Data - Rebalances volume by fixing the layout to use the new volume topology and migrating the existing data. Rebalancing Volume to Fix Layout Changes Fixing the layout is necessary because the layout structure is static for a given directory. Even after new bricks are added to the volume, newly created files in existing directories will still be distributed only among the original bricks. The command gluster volume rebalance <volname> fix-layout start will fix the layout information so that the files can be created on the newly added bricks. When this command is issued, all the file stat information which is already cached will get revalidated. As of GlusterFS 3.6, the assignment of files to bricks will take into account the sizes of the bricks. For example, a 20TB brick will be assigned twice as many files as a 10TB brick. In versions before 3.6, the two bricks were treated as equal regardless of size, and would have been assigned an equal share of files. A fix-layout rebalance will only fix the layout changes and does not migrate data. If you want to migrate the existing data, use gluster volume rebalance <volume> start command to rebalance data among the servers. To rebalance a volume to fix layout Start the rebalance operation on any Gluster server using the following command: # gluster volume rebalance <VOLNAME> fix-layout start For example: # gluster volume rebalance test-volume fix-layout start Starting rebalance on volume test-volume has been successful Rebalancing Volume to Fix Layout and Migrate Data After expanding a volume using the add-brick respectively, you need to rebalance the data among the servers. A remove-brick command will automatically trigger a rebalance. To rebalance a volume to fix layout and migrate the existing data Start the rebalance operation on any one of the server using the following command: # gluster volume rebalance <VOLNAME> start For example: # gluster volume rebalance test-volume start Starting rebalancing on volume test-volume has been successful Start the migration operation forcefully on any one of the servers using the following command: # gluster volume rebalance <VOLNAME> start force For example: # gluster volume rebalance test-volume start force Starting rebalancing on volume test-volume has been successful A rebalance operation will attempt to balance the diskusage across nodes, therefore it will skip files where the move will result in a less balanced volume. This leads to link files that are still left behind in the system and hence may cause performance issues. The behaviour can be overridden with the force argument. Displaying the Status of Rebalance Operation You can display the status information about rebalance volume operation, as needed. Check the status of the rebalance operation, using the following command: # gluster volume rebalance <VOLNAME> status For example: # gluster volume rebalance test-volume status Node Rebalanced-files size scanned status --------- ---------------- ---- ------- ----------- 617c923e-6450-4065-8e33-865e28d9428f 416 1463 312 in progress The time to complete the rebalance operation depends on the number of files on the volume along with the corresponding file sizes. Continue checking the rebalance status, verifying that the number of files rebalanced or total files scanned keeps increasing. For example, running the status command again might display a result similar to the following: # gluster volume rebalance test-volume status Node Rebalanced-files size scanned status --------- ---------------- ---- ------- ----------- 617c923e-6450-4065-8e33-865e28d9428f 498 1783 378 in progress The rebalance status displays the following when the rebalance is complete: # gluster volume rebalance test-volume status Node Rebalanced-files size scanned status --------- ---------------- ---- ------- ----------- 617c923e-6450-4065-8e33-865e28d9428f 502 1873 334 completed Stopping an Ongoing Rebalance Operation You can stop the rebalance operation, if needed. Stop the rebalance operation using the following command: # gluster volume rebalance <VOLNAME> stop For example: # gluster volume rebalance test-volume stop Node Rebalanced-files size scanned status --------- ---------------- ---- ------- ----------- 617c923e-6450-4065-8e33-865e28d9428f 59 590 244 stopped Stopped rebalance process on volume test-volume Stopping Volumes Stop the volume using the following command: # gluster volume stop <VOLNAME> For example, to stop test-volume: # gluster volume stop test-volume Stopping volume will make its data inaccessible. Do you want to continue? (y/n) Enter y to confirm the operation. The output of the command displays the following: Stopping volume test-volume has been successful Deleting Volumes Delete the volume using the following command: # gluster volume delete <VOLNAME> For example, to delete test-volume: # gluster volume delete test-volume Deleting volume will erase all information about the volume. Do you want to continue? (y/n) Enter y to confirm the operation. The command displays the following: Deleting volume test-volume has been successful Triggering Self-Heal on Replicate In replicate module, previously you had to manually trigger a self-heal when a brick goes offline and comes back online, to bring all the replicas in sync. Now the pro-active self-heal daemon runs in the background, diagnoses issues and automatically initiates self-healing every 10 minutes on the files which requires healing . You can view the list of files that need healing , the list of files which are currently/previously healed , list of files which are in split-brain state, and you can manually trigger self-heal on the entire volume or only on the files which need healing . Trigger self-heal only on the files which requires healing : # gluster volume heal <VOLNAME> For example, to trigger self-heal on files which requires healing of test-volume: # gluster volume heal test-volume Heal operation on volume test-volume has been successful Trigger self-heal on all the files of a volume: # gluster volume heal <VOLNAME> full For example, to trigger self-heal on all the files of of test-volume: # gluster volume heal test-volume full Heal operation on volume test-volume has been successful View the list of files that needs healing : # gluster volume heal <VOLNAME> info For example, to view the list of files on test-volume that needs healing : # gluster volume heal test-volume info Brick server1:/gfs/test-volume_0 Number of entries: 0 Brick server2:/gfs/test-volume_1 Number of entries: 101 /95.txt /32.txt /66.txt /35.txt /18.txt /26.txt /47.txt /55.txt /85.txt ... View the list of files that are self-healed: # gluster volume heal <VOLNAME> info healed For example, to view the list of files on test-volume that are self-healed: # gluster volume heal test-volume info healed Brick Server1:/gfs/test-volume_0 Number of entries: 0 Brick Server2:/gfs/test-volume_1 Number of entries: 69 /99.txt /93.txt /76.txt /11.txt /27.txt /64.txt /80.txt /19.txt /41.txt /29.txt /37.txt /46.txt ... View the list of files of a particular volume on which the self-heal failed: # gluster volume heal <VOLNAME> info failed For example, to view the list of files of test-volume that are not self-healed: # gluster volume heal test-volume info failed Brick Server1:/gfs/test-volume_0 Number of entries: 0 Brick Server2:/gfs/test-volume_3 Number of entries: 72 /90.txt /95.txt /77.txt /71.txt /87.txt /24.txt ... View the list of files of a particular volume which are in split-brain state: # gluster volume heal <VOLNAME> info split-brain For example, to view the list of files of test-volume which are in split-brain state: # gluster volume heal test-volume info split-brain Brick Server1:/gfs/test-volume_2 Number of entries: 12 /83.txt /28.txt /69.txt ... Brick Server2:/gfs/test-volume_3 Number of entries: 12 /83.txt /28.txt /69.txt ... Non Uniform File Allocation NUFA translator or Non Uniform File Access translator is designed for giving higher preference to a local drive when used in a HPC type of environment. It can be applied to Distribute and Replica translators; in the latter case it ensures that one copy is local if space permits. When a client on a server creates files, the files are allocated to a brick in the volume based on the file name. This allocation may not be ideal, as there is higher latency and unnecessary network traffic for read/write operations to a non-local brick or export directory. NUFA ensures that the files are created in the local export directory of the server, and as a result, reduces latency and conserves bandwidth for that server accessing that file. This can also be useful for applications running on mount points on the storage server. If the local brick runs out of space or reaches the minimum disk free limit, instead of allocating files to the local brick, NUFA distributes files to other bricks in the same volume if there is space available on those bricks. NUFA should be enabled before creating any data in the volume. Use the following command to enable NUFA: # gluster volume set <VOLNAME> cluster.nufa enable Important NUFA is supported under the following conditions: Volumes with only one brick per server. For use with a FUSE client. NUFA is not supported with NFS or SMB . A client that is mounting a NUFA-enabled volume must be present within the trusted storage pool. The NUFA scheduler also exists, for use with the Unify translator; see below. volume bricks type cluster/nufa option local-volume-name brick1 subvolumes brick1 brick2 brick3 brick4 brick5 brick6 brick7 end-volume NUFA additional options lookup-unhashed This is an advanced option where files are looked up in all subvolumes if they are missing on the subvolume matching the hash value of the filename. The default is on. local-volume-name The volume name to consider local and prefer file creations on. The default is to search for a volume matching the hostname of the system. subvolumes This option lists the subvolumes that are part of this 'cluster/nufa' volume. This translator requires more than one subvolume. BitRot Detection With BitRot detection in Gluster, it's possible to identify \"insidious\" type of disk errors where data is silently corrupted with no indication from the disk to the storage software layer than an error has occured. This also helps in catching \"backend\" tinkering of bricks (where data is directly manipulated on the bricks without going through FUSE, NFS or any other access protocol(s). BitRot detection is disbled by default and needs to be enabled to make use of other sub-commands. To enable bitrot detection for a given volume : # gluster volume bitrot <VOLNAME> enable and similarly to disable bitrot use: # gluster volume bitrot <VOLNAME> disable NOTE: Enabling bitrot spawns the Signer & Scrubber daemon per node. Signer is responsible for signing (calculating checksum for each file) an object and scrubber verifies the calculated checksum against the objects data. Scrubber daemon has three (3) throttling modes that adjusts the rate at which objects are verified. # volume bitrot <VOLNAME> scrub-throttle lazy # volume bitrot <VOLNAME> scrub-throttle normal # volume bitrot <VOLNAME> scrub-throttle aggressive By default scrubber scrubs the filesystem biweekly. It's possible to tune it to scrub based on predefined frequency such as monthly, etc. This can be done as shown below: # volume bitrot <VOLNAME> scrub-frequency daily # volume bitrot <VOLNAME> scrub-frequency weekly # volume bitrot <VOLNAME> scrub-frequency biweekly # volume bitrot <VOLNAME> scrub-frequency monthly NOTE: Daily scrubbing would not be available with GA release. Scrubber daemon can be paused and later resumed when required. This can be done as shown below: # volume bitrot <VOLNAME> scrub pause and to resume scrubbing: `# volume bitrot <VOLNAME> scrub resume` NOTE: Signing cannot be paused (and resumed) and would always be active as long as bitrot is enabled for that particular volume.","title":"Managing Volumes"},{"location":"Administrator-Guide/Managing-Volumes/#managing-glusterfs-volumes","text":"This section describes how to perform common GlusterFS management operations, including the following: Tuning Volume Options Configuring Transport Types for a Volume Expanding Volumes Shrinking Volumes Replacing Bricks Rebalancing Volumes Stopping Volumes Deleting Volumes Triggering Self-Heal on Replicate Non Uniform File Allocation(NUFA)","title":"Managing GlusterFS Volumes"},{"location":"Administrator-Guide/Managing-Volumes/#configuring-transport-types-for-a-volume","text":"A volume can support one or more transport types for communication between clients and brick processes. There are three types of supported transport, which are tcp, rdma, and tcp,rdma. To change the supported transport types of a volume, follow the procedure: Unmount the volume on all the clients using the following command: # umount mount-point Stop the volumes using the following command: # gluster volume stop <VOLNAME> Change the transport type. For example, to enable both tcp and rdma execute the followimg command: # gluster volume set test-volume config.transport tcp,rdma OR tcp OR rdma Mount the volume on all the clients. For example, to mount using rdma transport, use the following command: # mount -t glusterfs -o transport=rdma server1:/test-volume /mnt/glusterfs","title":"Configuring Transport Types for a Volume"},{"location":"Administrator-Guide/Managing-Volumes/#expanding-volumes","text":"You can expand volumes, as needed, while the cluster is online and available. For example, you might want to add a brick to a distributed volume, thereby increasing the distribution and adding to the capacity of the GlusterFS volume. Similarly, you might want to add a group of bricks to a distributed replicated volume, increasing the capacity of the GlusterFS volume. Note When expanding distributed replicated and distributed dispersed volumes, you need to add a number of bricks that is a multiple of the replica or disperse count. For example, to expand a distributed replicated volume with a replica count of 2, you need to add bricks in multiples of 2 (such as 4, 6, 8, etc.). To expand a volume If they are not already part of the TSP, probe the servers which contain the bricks you want to add to the volume using the following command: # gluster peer probe <SERVERNAME> For example: # gluster peer probe server4 Probe successful Add the brick using the following command: # gluster volume add-brick <VOLNAME> <NEW-BRICK> For example: # gluster volume add-brick test-volume server4:/exp4 Add Brick successful Check the volume information using the following command: # gluster volume info <VOLNAME> The command displays information similar to the following: Volume Name: test-volume Type: Distribute Status: Started Number of Bricks: 4 Bricks: Brick1: server1:/exp1 Brick2: server2:/exp2 Brick3: server3:/exp3 Brick4: server4:/exp4 Rebalance the volume to ensure that files are distributed to the new brick. You can use the rebalance command as described in Rebalancing Volumes","title":"Expanding Volumes"},{"location":"Administrator-Guide/Managing-Volumes/#shrinking-volumes","text":"You can shrink volumes, as needed, while the cluster is online and available. For example, you might need to remove a brick that has become inaccessible in a distributed volume due to hardware or network failure. Note Data residing on the brick that you are removing will no longer be accessible at the Gluster mount point. Note however that only the configuration information is removed - you can continue to access the data directly from the brick, as necessary. When shrinking distributed replicated and distributed dispersed volumes, you need to remove a number of bricks that is a multiple of the replica or stripe count. For example, to shrink a distributed replicate volume with a replica count of 2, you need to remove bricks in multiples of 2 (such as 4, 6, 8, etc.). In addition, the bricks you are trying to remove must be from the same sub-volume (the same replica or disperse set). Running remove-brick with the start option will automatically trigger a rebalance operation to migrate data from the removed-bricks to the rest of the volume. To shrink a volume Remove the brick using the following command: # gluster volume remove-brick <VOLNAME> <BRICKNAME> start For example, to remove server2:/exp2: # gluster volume remove-brick test-volume server2:/exp2 start volume remove-brick start: success View the status of the remove brick operation using the following command: # gluster volume remove-brick <VOLNAME> <BRICKNAME> status For example, to view the status of remove brick operation on server2:/exp2 brick: # gluster volume remove-brick test-volume server2:/exp2 status Node Rebalanced-files size scanned status --------- ---------------- ---- ------- ----------- 617c923e-6450-4065-8e33-865e28d9428f 34 340 162 in progress Once the status displays \"completed\", commit the remove-brick operation # gluster volume remove-brick <VOLNAME> <BRICKNAME> commit In this example: # gluster volume remove-brick test-volume server2:/exp2 commit Removing brick(s) can result in data loss. Do you want to Continue? (y/n) y volume remove-brick commit: success Check the removed bricks to ensure all files are migrated. If files with data are found on the brick path, copy them via a gluster mount point before re-purposing the removed brick. Check the volume information using the following command: # gluster volume info The command displays information similar to the following: # gluster volume info Volume Name: test-volume Type: Distribute Status: Started Number of Bricks: 3 Bricks: Brick1: server1:/exp1 Brick3: server3:/exp3 Brick4: server4:/exp4","title":"Shrinking Volumes"},{"location":"Administrator-Guide/Managing-Volumes/#replace-faulty-brick","text":"Replacing a brick in a pure distribute volume To replace a brick on a distribute only volume, add the new brick and then remove the brick you want to replace. This will trigger a rebalance operation which will move data from the removed brick. NOTE: Replacing a brick using the 'replace-brick' command in gluster is supported only for distributed-replicate or pure replicate volumes. Steps to remove brick Server1:/home/gfs/r2_1 and add Server1:/home/gfs/r2_2: Here is the initial volume configuration: Volume Name: r2 Type: Distribute Volume ID: 25b4e313-7b36-445d-b524-c3daebb91188 Status: Started Number of Bricks: 2 Transport-type: tcp Bricks: Brick1: Server1:/home/gfs/r2_0 Brick2: Server1:/home/gfs/r2_1 Here are the files that are present on the mount: # ls 1 10 2 3 4 5 6 7 8 9 Add the new brick - Server1:/home/gfs/r2_2 now: # gluster volume add-brick r2 Server1:/home/gfs/r2_2 volume add-brick: success Start remove-brick using the following command: # gluster volume remove-brick r2 Server1:/home/gfs/r2_1 start volume remove-brick start: success ID: fba0a488-21a4-42b7-8a41-b27ebaa8e5f4 Wait until remove-brick status indicates that it is complete. # gluster volume remove-brick r2 Server1:/home/gfs/r2_1 status Node Rebalanced-files size scanned failures skipped status run time in secs --------- ----------- ----------- ----------- ----------- ----------- ------------ -------------- localhost 5 20Bytes 15 0 0 completed 0.00 Now we can safely remove the old brick, so commit the changes: # gluster volume remove-brick r2 Server1:/home/gfs/r2_1 commit Removing brick(s) can result in data loss. Do you want to Continue? (y/n) y volume remove-brick commit: success Here is the new volume configuration. Volume Name: r2 Type: Distribute Volume ID: 25b4e313-7b36-445d-b524-c3daebb91188 Status: Started Number of Bricks: 2 Transport-type: tcp Bricks: Brick1: Server1:/home/gfs/r2_0 Brick2: Server1:/home/gfs/r2_2 Check the contents of the mount: # ls 1 10 2 3 4 5 6 7 8 9 Replacing bricks in Replicate/Distributed Replicate volumes This section of the document describes how brick: Server1:/home/gfs/r2_0 is replaced with brick: Server1:/home/gfs/r2_5 in volume r2 with replica count 2 . Volume Name: r2 Type: Distributed-Replicate Volume ID: 24a0437a-daa0-4044-8acf-7aa82efd76fd Status: Started Number of Bricks: 2 x 2 = 4 Transport-type: tcp Bricks: Brick1: Server1:/home/gfs/r2_0 Brick2: Server2:/home/gfs/r2_1 Brick3: Server1:/home/gfs/r2_2 Brick4: Server2:/home/gfs/r2_3 Steps: Make sure there is no data in the new brick Server1:/home/gfs/r2_5 Check that all the bricks are running. It is okay if the brick that is going to be replaced is down. Bring the brick that is going to be replaced down if not already. Get the pid of the brick by executing 'gluster volume status' # gluster volume status Status of volume: r2 Gluster process Port Online Pid ------------------------------------------------------------------------------ Brick Server1:/home/gfs/r2_0 49152 Y 5342 Brick Server2:/home/gfs/r2_1 49153 Y 5354 Brick Server1:/home/gfs/r2_2 49154 Y 5365 Brick Server2:/home/gfs/r2_3 49155 Y 5376 Login to the machine where the brick is running and kill the brick. # kill -15 5342 Confirm that the brick is not running anymore and the other bricks are running fine. # gluster volume status Status of volume: r2 Gluster process Port Online Pid ------------------------------------------------------------------------------ Brick Server1:/home/gfs/r2_0 N/A N 5342 <<---- brick is not running, others are running fine. Brick Server2:/home/gfs/r2_1 49153 Y 5354 Brick Server1:/home/gfs/r2_2 49154 Y 5365 Brick Server2:/home/gfs/r2_3 49155 Y 5376 Using the gluster volume fuse mount (In this example: /mnt/r2 ) set up metadata so that data will be synced to new brick (In this case it is from Server1:/home/gfs/r2_1 to Server1:/home/gfs/r2_5 ) Create a directory on the mount point that doesn't already exist. Then delete that directory, do the same for metadata changelog by doing setfattr. This operation marks the pending changelog which will tell self-heal damon/mounts to perform self-heal from /home/gfs/r2_1 to /home/gfs/r2_5 . mkdir /mnt/r2/<name-of-nonexistent-dir> rmdir /mnt/r2/<name-of-nonexistent-dir> setfattr -n trusted.non-existent-key -v abc /mnt/r2 setfattr -x trusted.non-existent-key /mnt/r2 Check that there are pending xattrs on the replica of the brick that is being replaced: getfattr -d -m. -e hex /home/gfs/r2_1 # file: home/gfs/r2_1 security.selinux=0x756e636f6e66696e65645f753a6f626a6563745f723a66696c655f743a733000 trusted.afr.r2-client-0=0x000000000000000300000002 <<---- xattrs are marked from source brick Server2:/home/gfs/r2_1 trusted.afr.r2-client-1=0x000000000000000000000000 trusted.gfid=0x00000000000000000000000000000001 trusted.glusterfs.dht=0x0000000100000000000000007ffffffe trusted.glusterfs.volume-id=0xde822e25ebd049ea83bfaa3c4be2b440 Volume heal info will show that '/' needs healing.(There could be more entries based on the work load. But '/' must exist) # gluster volume heal r2 info Brick Server1:/home/gfs/r2_0 Status: Transport endpoint is not connected Brick Server2:/home/gfs/r2_1 / Number of entries: 1 Brick Server1:/home/gfs/r2_2 Number of entries: 0 Brick Server2:/home/gfs/r2_3 Number of entries: 0 Replace the brick with 'commit force' option. Please note that other variants of replace-brick command are not supported. Execute replace-brick command # gluster volume replace-brick r2 Server1:/home/gfs/r2_0 Server1:/home/gfs/r2_5 commit force volume replace-brick: success: replace-brick commit successful Check that the new brick is now online # gluster volume status Status of volume: r2 Gluster process Port Online Pid ------------------------------------------------------------------------------ Brick Server1:/home/gfs/r2_5 49156 Y 5731 <<<---- new brick is online Brick Server2:/home/gfs/r2_1 49153 Y 5354 Brick Server1:/home/gfs/r2_2 49154 Y 5365 Brick Server2:/home/gfs/r2_3 49155 Y 5376 Users can track the progress of self-heal using: gluster volume heal [volname] info . Once self-heal completes the changelogs will be removed. # getfattr -d -m. -e hex /home/gfs/r2_1 getfattr: Removing leading '/' from absolute path names # file: home/gfs/r2_1 security.selinux=0x756e636f6e66696e65645f753a6f626a6563745f723a66696c655f743a733000 trusted.afr.r2-client-0=0x000000000000000000000000 <<---- Pending changelogs are cleared. trusted.afr.r2-client-1=0x000000000000000000000000 trusted.gfid=0x00000000000000000000000000000001 trusted.glusterfs.dht=0x0000000100000000000000007ffffffe trusted.glusterfs.volume-id=0xde822e25ebd049ea83bfaa3c4be2b440 # gluster volume heal <VOLNAME> info will show that no heal is required. # gluster volume heal r2 info Brick Server1:/home/gfs/r2_5 Number of entries: 0 Brick Server2:/home/gfs/r2_1 Number of entries: 0 Brick Server1:/home/gfs/r2_2 Number of entries: 0 Brick Server2:/home/gfs/r2_3 Number of entries: 0","title":"Replace faulty brick"},{"location":"Administrator-Guide/Managing-Volumes/#rebalancing-volumes","text":"After expanding a volume using the add-brick command, you may need to rebalance the data among the servers. New directories created after expanding or shrinking of the volume will be evenly distributed automatically. For all the existing directories, the distribution can be fixed by rebalancing the layout and/or data. This section describes how to rebalance GlusterFS volumes in your storage environment, using the following common scenarios: Fix Layout - Fixes the layout to use the new volume topology so that files can be distributed to newly added nodes. Fix Layout and Migrate Data - Rebalances volume by fixing the layout to use the new volume topology and migrating the existing data.","title":"Rebalancing Volumes"},{"location":"Administrator-Guide/Managing-Volumes/#rebalancing-volume-to-fix-layout-changes","text":"Fixing the layout is necessary because the layout structure is static for a given directory. Even after new bricks are added to the volume, newly created files in existing directories will still be distributed only among the original bricks. The command gluster volume rebalance <volname> fix-layout start will fix the layout information so that the files can be created on the newly added bricks. When this command is issued, all the file stat information which is already cached will get revalidated. As of GlusterFS 3.6, the assignment of files to bricks will take into account the sizes of the bricks. For example, a 20TB brick will be assigned twice as many files as a 10TB brick. In versions before 3.6, the two bricks were treated as equal regardless of size, and would have been assigned an equal share of files. A fix-layout rebalance will only fix the layout changes and does not migrate data. If you want to migrate the existing data, use gluster volume rebalance <volume> start command to rebalance data among the servers. To rebalance a volume to fix layout Start the rebalance operation on any Gluster server using the following command: # gluster volume rebalance <VOLNAME> fix-layout start For example: # gluster volume rebalance test-volume fix-layout start Starting rebalance on volume test-volume has been successful","title":"Rebalancing Volume to Fix Layout Changes"},{"location":"Administrator-Guide/Managing-Volumes/#rebalancing-volume-to-fix-layout-and-migrate-data","text":"After expanding a volume using the add-brick respectively, you need to rebalance the data among the servers. A remove-brick command will automatically trigger a rebalance. To rebalance a volume to fix layout and migrate the existing data Start the rebalance operation on any one of the server using the following command: # gluster volume rebalance <VOLNAME> start For example: # gluster volume rebalance test-volume start Starting rebalancing on volume test-volume has been successful Start the migration operation forcefully on any one of the servers using the following command: # gluster volume rebalance <VOLNAME> start force For example: # gluster volume rebalance test-volume start force Starting rebalancing on volume test-volume has been successful A rebalance operation will attempt to balance the diskusage across nodes, therefore it will skip files where the move will result in a less balanced volume. This leads to link files that are still left behind in the system and hence may cause performance issues. The behaviour can be overridden with the force argument.","title":"Rebalancing Volume to Fix Layout and Migrate Data"},{"location":"Administrator-Guide/Managing-Volumes/#displaying-the-status-of-rebalance-operation","text":"You can display the status information about rebalance volume operation, as needed. Check the status of the rebalance operation, using the following command: # gluster volume rebalance <VOLNAME> status For example: # gluster volume rebalance test-volume status Node Rebalanced-files size scanned status --------- ---------------- ---- ------- ----------- 617c923e-6450-4065-8e33-865e28d9428f 416 1463 312 in progress The time to complete the rebalance operation depends on the number of files on the volume along with the corresponding file sizes. Continue checking the rebalance status, verifying that the number of files rebalanced or total files scanned keeps increasing. For example, running the status command again might display a result similar to the following: # gluster volume rebalance test-volume status Node Rebalanced-files size scanned status --------- ---------------- ---- ------- ----------- 617c923e-6450-4065-8e33-865e28d9428f 498 1783 378 in progress The rebalance status displays the following when the rebalance is complete: # gluster volume rebalance test-volume status Node Rebalanced-files size scanned status --------- ---------------- ---- ------- ----------- 617c923e-6450-4065-8e33-865e28d9428f 502 1873 334 completed","title":"Displaying the Status of Rebalance Operation"},{"location":"Administrator-Guide/Managing-Volumes/#stopping-an-ongoing-rebalance-operation","text":"You can stop the rebalance operation, if needed. Stop the rebalance operation using the following command: # gluster volume rebalance <VOLNAME> stop For example: # gluster volume rebalance test-volume stop Node Rebalanced-files size scanned status --------- ---------------- ---- ------- ----------- 617c923e-6450-4065-8e33-865e28d9428f 59 590 244 stopped Stopped rebalance process on volume test-volume","title":"Stopping an Ongoing Rebalance Operation"},{"location":"Administrator-Guide/Managing-Volumes/#stopping-volumes","text":"Stop the volume using the following command: # gluster volume stop <VOLNAME> For example, to stop test-volume: # gluster volume stop test-volume Stopping volume will make its data inaccessible. Do you want to continue? (y/n) Enter y to confirm the operation. The output of the command displays the following: Stopping volume test-volume has been successful","title":"Stopping Volumes"},{"location":"Administrator-Guide/Managing-Volumes/#deleting-volumes","text":"Delete the volume using the following command: # gluster volume delete <VOLNAME> For example, to delete test-volume: # gluster volume delete test-volume Deleting volume will erase all information about the volume. Do you want to continue? (y/n) Enter y to confirm the operation. The command displays the following: Deleting volume test-volume has been successful","title":"Deleting Volumes"},{"location":"Administrator-Guide/Managing-Volumes/#triggering-self-heal-on-replicate","text":"In replicate module, previously you had to manually trigger a self-heal when a brick goes offline and comes back online, to bring all the replicas in sync. Now the pro-active self-heal daemon runs in the background, diagnoses issues and automatically initiates self-healing every 10 minutes on the files which requires healing . You can view the list of files that need healing , the list of files which are currently/previously healed , list of files which are in split-brain state, and you can manually trigger self-heal on the entire volume or only on the files which need healing . Trigger self-heal only on the files which requires healing : # gluster volume heal <VOLNAME> For example, to trigger self-heal on files which requires healing of test-volume: # gluster volume heal test-volume Heal operation on volume test-volume has been successful Trigger self-heal on all the files of a volume: # gluster volume heal <VOLNAME> full For example, to trigger self-heal on all the files of of test-volume: # gluster volume heal test-volume full Heal operation on volume test-volume has been successful View the list of files that needs healing : # gluster volume heal <VOLNAME> info For example, to view the list of files on test-volume that needs healing : # gluster volume heal test-volume info Brick server1:/gfs/test-volume_0 Number of entries: 0 Brick server2:/gfs/test-volume_1 Number of entries: 101 /95.txt /32.txt /66.txt /35.txt /18.txt /26.txt /47.txt /55.txt /85.txt ... View the list of files that are self-healed: # gluster volume heal <VOLNAME> info healed For example, to view the list of files on test-volume that are self-healed: # gluster volume heal test-volume info healed Brick Server1:/gfs/test-volume_0 Number of entries: 0 Brick Server2:/gfs/test-volume_1 Number of entries: 69 /99.txt /93.txt /76.txt /11.txt /27.txt /64.txt /80.txt /19.txt /41.txt /29.txt /37.txt /46.txt ... View the list of files of a particular volume on which the self-heal failed: # gluster volume heal <VOLNAME> info failed For example, to view the list of files of test-volume that are not self-healed: # gluster volume heal test-volume info failed Brick Server1:/gfs/test-volume_0 Number of entries: 0 Brick Server2:/gfs/test-volume_3 Number of entries: 72 /90.txt /95.txt /77.txt /71.txt /87.txt /24.txt ... View the list of files of a particular volume which are in split-brain state: # gluster volume heal <VOLNAME> info split-brain For example, to view the list of files of test-volume which are in split-brain state: # gluster volume heal test-volume info split-brain Brick Server1:/gfs/test-volume_2 Number of entries: 12 /83.txt /28.txt /69.txt ... Brick Server2:/gfs/test-volume_3 Number of entries: 12 /83.txt /28.txt /69.txt ...","title":"Triggering Self-Heal on Replicate"},{"location":"Administrator-Guide/Managing-Volumes/#non-uniform-file-allocation","text":"NUFA translator or Non Uniform File Access translator is designed for giving higher preference to a local drive when used in a HPC type of environment. It can be applied to Distribute and Replica translators; in the latter case it ensures that one copy is local if space permits. When a client on a server creates files, the files are allocated to a brick in the volume based on the file name. This allocation may not be ideal, as there is higher latency and unnecessary network traffic for read/write operations to a non-local brick or export directory. NUFA ensures that the files are created in the local export directory of the server, and as a result, reduces latency and conserves bandwidth for that server accessing that file. This can also be useful for applications running on mount points on the storage server. If the local brick runs out of space or reaches the minimum disk free limit, instead of allocating files to the local brick, NUFA distributes files to other bricks in the same volume if there is space available on those bricks. NUFA should be enabled before creating any data in the volume. Use the following command to enable NUFA: # gluster volume set <VOLNAME> cluster.nufa enable Important NUFA is supported under the following conditions: Volumes with only one brick per server. For use with a FUSE client. NUFA is not supported with NFS or SMB . A client that is mounting a NUFA-enabled volume must be present within the trusted storage pool. The NUFA scheduler also exists, for use with the Unify translator; see below. volume bricks type cluster/nufa option local-volume-name brick1 subvolumes brick1 brick2 brick3 brick4 brick5 brick6 brick7 end-volume","title":"Non Uniform File Allocation"},{"location":"Administrator-Guide/Managing-Volumes/#nufa-additional-options","text":"lookup-unhashed This is an advanced option where files are looked up in all subvolumes if they are missing on the subvolume matching the hash value of the filename. The default is on. local-volume-name The volume name to consider local and prefer file creations on. The default is to search for a volume matching the hostname of the system. subvolumes This option lists the subvolumes that are part of this 'cluster/nufa' volume. This translator requires more than one subvolume.","title":"NUFA additional options"},{"location":"Administrator-Guide/Managing-Volumes/#bitrot-detection","text":"With BitRot detection in Gluster, it's possible to identify \"insidious\" type of disk errors where data is silently corrupted with no indication from the disk to the storage software layer than an error has occured. This also helps in catching \"backend\" tinkering of bricks (where data is directly manipulated on the bricks without going through FUSE, NFS or any other access protocol(s). BitRot detection is disbled by default and needs to be enabled to make use of other sub-commands. To enable bitrot detection for a given volume : # gluster volume bitrot <VOLNAME> enable and similarly to disable bitrot use: # gluster volume bitrot <VOLNAME> disable NOTE: Enabling bitrot spawns the Signer & Scrubber daemon per node. Signer is responsible for signing (calculating checksum for each file) an object and scrubber verifies the calculated checksum against the objects data. Scrubber daemon has three (3) throttling modes that adjusts the rate at which objects are verified. # volume bitrot <VOLNAME> scrub-throttle lazy # volume bitrot <VOLNAME> scrub-throttle normal # volume bitrot <VOLNAME> scrub-throttle aggressive By default scrubber scrubs the filesystem biweekly. It's possible to tune it to scrub based on predefined frequency such as monthly, etc. This can be done as shown below: # volume bitrot <VOLNAME> scrub-frequency daily # volume bitrot <VOLNAME> scrub-frequency weekly # volume bitrot <VOLNAME> scrub-frequency biweekly # volume bitrot <VOLNAME> scrub-frequency monthly NOTE: Daily scrubbing would not be available with GA release. Scrubber daemon can be paused and later resumed when required. This can be done as shown below: # volume bitrot <VOLNAME> scrub pause and to resume scrubbing: `# volume bitrot <VOLNAME> scrub resume` NOTE: Signing cannot be paused (and resumed) and would always be active as long as bitrot is enabled for that particular volume.","title":"BitRot Detection"},{"location":"Administrator-Guide/Mandatory-Locks/","text":"Mandatory Locks Support for mandatory locks inside GlusterFS does not converge all by itself to what Linux kernel provides to user space file systems. Here we enforce core mandatory lock semantics with and without the help of file mode bits. Please read through the design specification which explains the whole concept behind the mandatory locks implementation done for GlusterFS. Implications and Usage By default, mandatory locking will be disabled for a volume and a volume set options is available to configure volume to operate under 3 different mandatory locking modes. Volume Option gluster volume set <VOLNAME> locks.mandatory-locking <off / file / forced / optimal> off - Disable mandatory locking for specified volume. file - Enable Linux kernel style mandatory locking semantics with the help of mode bits (not well tested) forced - Check for conflicting byte range locks for every data modifying operation in a volume optimal - Combinational mode where POSIX clients can live with their advisory lock semantics which will still honour the mandatory locks acquired by other clients like SMB. Note :- Please refer the design doc for more information on these key values. Points to be remembered Valid key values available with mandatory-locking volume set option are taken into effect only after a subsequent start/restart of the volume. Due to some outstanding issues, it is recommended to turn off the performance translators in order to have the complete functionality of mandatory-locks when volume is configured in any one of the above described mandatory-locking modes. Please see the 'Known issue' section below for more details. Known issues Since the whole logic of mandatory-locks are implemented within the locks translator loaded at the server side, early success returned to fops like open, read, write to upper/application layer by performance translators residing at the client side will impact the intended functionality of mandatory-locks. One such issue is being tracked in the following bugzilla report: https://bugzilla.redhat.com/show_bug.cgi?id=1194546 There is a possible race window uncovered with respect to mandatory locks and an ongoing read/write operation. For more details refer the bug report given below: https://bugzilla.redhat.com/show_bug.cgi?id=1287099","title":"Mandatory Locks"},{"location":"Administrator-Guide/Mandatory-Locks/#mandatory-locks","text":"Support for mandatory locks inside GlusterFS does not converge all by itself to what Linux kernel provides to user space file systems. Here we enforce core mandatory lock semantics with and without the help of file mode bits. Please read through the design specification which explains the whole concept behind the mandatory locks implementation done for GlusterFS.","title":"Mandatory Locks"},{"location":"Administrator-Guide/Mandatory-Locks/#implications-and-usage","text":"By default, mandatory locking will be disabled for a volume and a volume set options is available to configure volume to operate under 3 different mandatory locking modes.","title":"Implications and Usage"},{"location":"Administrator-Guide/Mandatory-Locks/#volume-option","text":"gluster volume set <VOLNAME> locks.mandatory-locking <off / file / forced / optimal> off - Disable mandatory locking for specified volume. file - Enable Linux kernel style mandatory locking semantics with the help of mode bits (not well tested) forced - Check for conflicting byte range locks for every data modifying operation in a volume optimal - Combinational mode where POSIX clients can live with their advisory lock semantics which will still honour the mandatory locks acquired by other clients like SMB. Note :- Please refer the design doc for more information on these key values.","title":"Volume Option"},{"location":"Administrator-Guide/Mandatory-Locks/#points-to-be-remembered","text":"Valid key values available with mandatory-locking volume set option are taken into effect only after a subsequent start/restart of the volume. Due to some outstanding issues, it is recommended to turn off the performance translators in order to have the complete functionality of mandatory-locks when volume is configured in any one of the above described mandatory-locking modes. Please see the 'Known issue' section below for more details.","title":"Points to be remembered"},{"location":"Administrator-Guide/Mandatory-Locks/#known-issues","text":"Since the whole logic of mandatory-locks are implemented within the locks translator loaded at the server side, early success returned to fops like open, read, write to upper/application layer by performance translators residing at the client side will impact the intended functionality of mandatory-locks. One such issue is being tracked in the following bugzilla report: https://bugzilla.redhat.com/show_bug.cgi?id=1194546 There is a possible race window uncovered with respect to mandatory locks and an ongoing read/write operation. For more details refer the bug report given below: https://bugzilla.redhat.com/show_bug.cgi?id=1287099","title":"Known issues"},{"location":"Administrator-Guide/Monitoring-Workload/","text":"Monitoring your GlusterFS Workload You can monitor the GlusterFS volumes on different parameters. Monitoring volumes helps in capacity planning and performance tuning tasks of the GlusterFS volume. Using these information, you can identify and troubleshoot issues. You can use Volume Top and Profile commands to view the performance and identify bottlenecks/hotspots of each brick of a volume. This helps system administrators to get vital performance information whenever performance needs to be probed. You can also perform statedump of the brick processes and nfs server process of a volume, and also view volume status and volume information. Running GlusterFS Volume Profile Command GlusterFS Volume Profile command provides an interface to get the per-brick I/O information for each File Operation (FOP) of a volume. The per brick information helps in identifying bottlenecks in the storage system. This section describes how to run GlusterFS Volume Profile command by performing the following operations: Start Profiling Displaying the I/0 Information Stop Profiling Start Profiling You must start the Profiling to view the File Operation information for each brick. To start profiling, use following command: # gluster volume profile start For example, to start profiling on test-volume: # gluster volume profile test-volume start Profiling started on test-volume When profiling on the volume is started, the following additional options are displayed in the Volume Info: diagnostics.count-fop-hits: on diagnostics.latency-measurement: on Displaying the I/0 Information You can view the I/O information of each brick by using the following command: # gluster volume profile info For example, to see the I/O information on test-volume: # gluster volume profile test-volume info Brick: Test:/export/2 Cumulative Stats: Block 1b+ 32b+ 64b+ Size: Read: 0 0 0 Write: 908 28 8 Block 128b+ 256b+ 512b+ Size: Read: 0 6 4 Write: 5 23 16 Block 1024b+ 2048b+ 4096b+ Size: Read: 0 52 17 Write: 15 120 846 Block 8192b+ 16384b+ 32768b+ Size: Read: 52 8 34 Write: 234 134 286 Block 65536b+ 131072b+ Size: Read: 118 622 Write: 1341 594 %-latency Avg- Min- Max- calls Fop latency Latency Latency ___________________________________________________________ 4.82 1132.28 21.00 800970.00 4575 WRITE 5.70 156.47 9.00 665085.00 39163 READDIRP 11.35 315.02 9.00 1433947.00 38698 LOOKUP 11.88 1729.34 21.00 2569638.00 7382 FXATTROP 47.35 104235.02 2485.00 7789367.00 488 FSYNC ------------------ ------------------ Duration : 335 BytesRead : 94505058 BytesWritten : 195571980 Stop Profiling You can stop profiling the volume, if you do not need profiling information anymore. Stop profiling using the following command: # gluster volume profile stop For example, to stop profiling on test-volume: # gluster volume profile stop Profiling stopped on test-volume Running GlusterFS Volume TOP Command GlusterFS Volume Top command allows you to view the glusterfs bricks\u2019 performance metrics like read, write, file open calls, file read calls, file write calls, directory open calls, and directory real calls. The top command displays up to 100 results. This section describes how to run and view the results for the following GlusterFS Top commands: Viewing Open fd Count and Maximum fd Count Viewing Highest File Read Calls Viewing Highest File Write Calls Viewing Highest Open Calls on Directories Viewing Highest Read Calls on Directory Viewing List of Read Performance on each Brick Viewing List of Write Performance on each Brick Viewing Open fd Count and Maximum fd Count You can view both current open fd count (list of files that are currently the most opened and the count) on the brick and the maximum open fd count (count of files that are the currently open and the count of maximum number of files opened at any given point of time, since the servers are up and running). If the brick name is not specified, then open fd metrics of all the bricks belonging to the volume will be displayed. View open fd count and maximum fd count using the following command: # gluster volume top open [brick ] [list-cnt ] For example, to view open fd count and maximum fd count on brick server:/export of test-volume and list top 10 open calls: # gluster volume top open brick list-cnt Brick: server:/export/dir1 Current open fd's: 34 Max open fd's: 209 ==========Open file stats======== open file name call count 2 /clients/client0/~dmtmp/PARADOX/ COURSES.DB 11 /clients/client0/~dmtmp/PARADOX/ ENROLL.DB 11 /clients/client0/~dmtmp/PARADOX/ STUDENTS.DB 10 /clients/client0/~dmtmp/PWRPNT/ TIPS.PPT 10 /clients/client0/~dmtmp/PWRPNT/ PCBENCHM.PPT 9 /clients/client7/~dmtmp/PARADOX/ STUDENTS.DB 9 /clients/client1/~dmtmp/PARADOX/ STUDENTS.DB 9 /clients/client2/~dmtmp/PARADOX/ STUDENTS.DB 9 /clients/client0/~dmtmp/PARADOX/ STUDENTS.DB 9 /clients/client8/~dmtmp/PARADOX/ STUDENTS.DB Viewing Highest File Read Calls You can view highest read calls on each brick. If brick name is not specified, then by default, list of 100 files will be displayed. View highest file Read calls using the following command: # gluster volume top read [brick ] [list-cnt ] For example, to view highest Read calls on brick server:/export of test-volume: # gluster volume top read brick list-cnt Brick: server:/export/dir1 ==========Read file stats======== read filename call count 116 /clients/client0/~dmtmp/SEED/LARGE.FIL 64 /clients/client0/~dmtmp/SEED/MEDIUM.FIL 54 /clients/client2/~dmtmp/SEED/LARGE.FIL 54 /clients/client6/~dmtmp/SEED/LARGE.FIL 54 /clients/client5/~dmtmp/SEED/LARGE.FIL 54 /clients/client0/~dmtmp/SEED/LARGE.FIL 54 /clients/client3/~dmtmp/SEED/LARGE.FIL 54 /clients/client4/~dmtmp/SEED/LARGE.FIL 54 /clients/client9/~dmtmp/SEED/LARGE.FIL 54 /clients/client8/~dmtmp/SEED/LARGE.FIL Viewing Highest File Write Calls You can view list of files which has highest file write calls on each brick. If brick name is not specified, then by default, list of 100 files will be displayed. View highest file Write calls using the following command: # gluster volume top write [brick ] [list-cnt ] For example, to view highest Write calls on brick server:/export of test-volume: # gluster volume top write brick list-cnt Brick: server:/export/dir1 ==========Write file stats======== write call count filename 83 /clients/client0/~dmtmp/SEED/LARGE.FIL 59 /clients/client7/~dmtmp/SEED/LARGE.FIL 59 /clients/client1/~dmtmp/SEED/LARGE.FIL 59 /clients/client2/~dmtmp/SEED/LARGE.FIL 59 /clients/client0/~dmtmp/SEED/LARGE.FIL 59 /clients/client8/~dmtmp/SEED/LARGE.FIL 59 /clients/client5/~dmtmp/SEED/LARGE.FIL 59 /clients/client4/~dmtmp/SEED/LARGE.FIL 59 /clients/client6/~dmtmp/SEED/LARGE.FIL 59 /clients/client3/~dmtmp/SEED/LARGE.FIL Viewing Highest Open Calls on Directories You can view list of files which has highest open calls on directories of each brick. If brick name is not specified, then the metrics of all the bricks belonging to that volume will be displayed. View list of open calls on each directory using the following command: # gluster volume top opendir [brick ] [list-cnt ] For example, to view open calls on brick server:/export/ of test-volume: # gluster volume top opendir brick list-cnt Brick: server:/export/dir1 ==========Directory open stats======== Opendir count directory name 1001 /clients/client0/~dmtmp 454 /clients/client8/~dmtmp 454 /clients/client2/~dmtmp 454 /clients/client6/~dmtmp 454 /clients/client5/~dmtmp 454 /clients/client9/~dmtmp 443 /clients/client0/~dmtmp/PARADOX 408 /clients/client1/~dmtmp 408 /clients/client7/~dmtmp 402 /clients/client4/~dmtmp Viewing Highest Read Calls on Directory You can view list of files which has highest directory read calls on each brick. If brick name is not specified, then the metrics of all the bricks belonging to that volume will be displayed. View list of highest directory read calls on each brick using the following command: # gluster volume top test-volume readdir [brick BRICK] [list-cnt {0..100}] For example, to view highest directory read calls on brick server:/export of test-volume: # gluster volume top test-volume readdir brick server:/export list-cnt 10 Brick: ==========Directory readdirp stats======== readdirp count directory name 1996 /clients/client0/~dmtmp 1083 /clients/client0/~dmtmp/PARADOX 904 /clients/client8/~dmtmp 904 /clients/client2/~dmtmp 904 /clients/client6/~dmtmp 904 /clients/client5/~dmtmp 904 /clients/client9/~dmtmp 812 /clients/client1/~dmtmp 812 /clients/client7/~dmtmp 800 /clients/client4/~dmtmp Viewing List of Read Performance on each Brick You can view the read throughput of files on each brick. If brick name is not specified, then the metrics of all the bricks belonging to that volume will be displayed. The output will be the read throughput. ==========Read throughput file stats======== read filename Time through put(MBp s) 2570.00 /clients/client0/~dmtmp/PWRPNT/ -2011-01-31 TRIDOTS.POT 15:38:36.894610 2570.00 /clients/client0/~dmtmp/PWRPNT/ -2011-01-31 PCBENCHM.PPT 15:38:39.815310 2383.00 /clients/client2/~dmtmp/SEED/ -2011-01-31 MEDIUM.FIL 15:52:53.631499 2340.00 /clients/client0/~dmtmp/SEED/ -2011-01-31 MEDIUM.FIL 15:38:36.926198 2299.00 /clients/client0/~dmtmp/SEED/ -2011-01-31 LARGE.FIL 15:38:36.930445 2259.00 /clients/client0/~dmtmp/PARADOX/ -2011-01-31 COURSES.X04 15:38:40.549919 2221.00 /clients/client0/~dmtmp/PARADOX/ -2011-01-31 STUDENTS.VAL 15:52:53.298766 2221.00 /clients/client3/~dmtmp/SEED/ -2011-01-31 COURSES.DB 15:39:11.776780 2184.00 /clients/client3/~dmtmp/SEED/ -2011-01-31 MEDIUM.FIL 15:39:10.251764 2184.00 /clients/client5/~dmtmp/WORD/ -2011-01-31 BASEMACH.DOC 15:39:09.336572 This command will initiate a dd for the specified count and block size and measures the corresponding throughput. View list of read performance on each brick using the following command: # gluster volume top read-perf [bs count ] [brick ] [list-cnt ] For example, to view read performance on brick server:/export/ of test-volume, 256 block size of count 1, and list count 10: # gluster volume top read-perf bs 256 count 1 brick list-cnt Brick: server:/export/dir1 256 bytes (256 B) copied, Throughput: 4.1 MB/s ==========Read throughput file stats======== read filename Time through put(MBp s) 2912.00 /clients/client0/~dmtmp/PWRPNT/ -2011-01-31 TRIDOTS.POT 15:38:36.896486 2570.00 /clients/client0/~dmtmp/PWRPNT/ -2011-01-31 PCBENCHM.PPT 15:38:39.815310 2383.00 /clients/client2/~dmtmp/SEED/ -2011-01-31 MEDIUM.FIL 15:52:53.631499 2340.00 /clients/client0/~dmtmp/SEED/ -2011-01-31 MEDIUM.FIL 15:38:36.926198 2299.00 /clients/client0/~dmtmp/SEED/ -2011-01-31 LARGE.FIL 15:38:36.930445 2259.00 /clients/client0/~dmtmp/PARADOX/ -2011-01-31 COURSES.X04 15:38:40.549919 2221.00 /clients/client9/~dmtmp/PARADOX/ -2011-01-31 STUDENTS.VAL 15:52:53.298766 2221.00 /clients/client8/~dmtmp/PARADOX/ -2011-01-31 COURSES.DB 15:39:11.776780 2184.00 /clients/client3/~dmtmp/SEED/ -2011-01-31 MEDIUM.FIL 15:39:10.251764 2184.00 /clients/client5/~dmtmp/WORD/ -2011-01-31 BASEMACH.DOC 15:39:09.336572 Viewing List of Write Performance on each Brick You can view list of write throughput of files on each brick. If brick name is not specified, then the metrics of all the bricks belonging to that volume will be displayed. The output will be the write throughput. This command will initiate a dd for the specified count and block size and measures the corresponding throughput. To view list of write performance on each brick: View list of write performance on each brick using the following command: # gluster volume top write-perf [bs count ] [brick ] [list-cnt ] For example, to view write performance on brick server:/export/ of test-volume, 256 block size of count 1, and list count 10: # gluster volume top write-perf bs 256 count 1 brick list-cnt Brick : server:/export/dir1 256 bytes (256 B) copied, Throughput: 2.8 MB/s ==========Write throughput file stats======== write filename Time throughput (MBps) 1170.00 /clients/client0/~dmtmp/SEED/ -2011-01-31 SMALL.FIL 15:39:09.171494 1008.00 /clients/client6/~dmtmp/SEED/ -2011-01-31 LARGE.FIL 15:39:09.73189 949.00 /clients/client0/~dmtmp/SEED/ -2011-01-31 MEDIUM.FIL 15:38:36.927426 936.00 /clients/client0/~dmtmp/SEED/ -2011-01-31 LARGE.FIL 15:38:36.933177 897.00 /clients/client5/~dmtmp/SEED/ -2011-01-31 MEDIUM.FIL 15:39:09.33628 897.00 /clients/client6/~dmtmp/SEED/ -2011-01-31 MEDIUM.FIL 15:39:09.27713 885.00 /clients/client0/~dmtmp/SEED/ -2011-01-31 SMALL.FIL 15:38:36.924271 528.00 /clients/client5/~dmtmp/SEED/ -2011-01-31 LARGE.FIL 15:39:09.81893 516.00 /clients/client6/~dmtmp/ACCESS/ -2011-01-31 FASTENER.MDB 15:39:01.797317 Displaying Volume Information You can display information about a specific volume, or all volumes, as needed. Display information about a specific volume using the following command: # gluster volume info VOLNAME For example, to display information about test-volume: # gluster volume info test-volume Volume Name: test-volume Type: Distribute Status: Created Number of Bricks: 4 Bricks: Brick1: server1:/exp1 Brick2: server2:/exp2 Brick3: server3:/exp3 Brick4: server4:/exp4 Display information about all volumes using the following command: # gluster volume info all # gluster volume info all Volume Name: test-volume Type: Distribute Status: Created Number of Bricks: 4 Bricks: Brick1: server1:/exp1 Brick2: server2:/exp2 Brick3: server3:/exp3 Brick4: server4:/exp4 Volume Name: mirror Type: Distributed-Replicate Status: Started Number of Bricks: 2 X 2 = 4 Bricks: Brick1: server1:/brick1 Brick2: server2:/brick2 Brick3: server3:/brick3 Brick4: server4:/brick4 Volume Name: Vol Type: Distribute Status: Started Number of Bricks: 1 Bricks: Brick: server:/brick6 Performing Statedump on a Volume Statedump is a mechanism through which you can get details of all internal variables and state of the glusterfs process at the time of issuing the command.You can perform statedumps of the brick processes and nfs server process of a volume using the statedump command. The following options can be used to determine what information is to be dumped: mem - Dumps the memory usage and memory pool details of the bricks. iobuf - Dumps iobuf details of the bricks. priv - Dumps private information of loaded translators. callpool - Dumps the pending calls of the volume. fd - Dumps the open fd tables of the volume. inode - Dumps the inode tables of the volume. To display volume statedump Display statedump of a volume or NFS server using the following command: # gluster volume statedump [nfs] [all|mem|iobuf|callpool|priv|fd|inode] For example, to display statedump of test-volume: # gluster volume statedump test-volume Volume statedump successful The statedump files are created on the brick servers in the /tmp directory or in the directory set using server.statedump-path volume option. The naming convention of the dump file is <brick-path>.<brick-pid>.dump . By defult, the output of the statedump is stored at /tmp/<brickname.PID.dump> file on that particular server. Change the directory of the statedump file using the following command: # gluster volume set server.statedump-path For example, to change the location of the statedump file of test-volume: # gluster volume set test-volume server.statedump-path /usr/local/var/log/glusterfs/dumps/ Set volume successful You can view the changed path of the statedump file using the following command: # gluster volume info Displaying Volume Status You can display the status information about a specific volume, brick or all volumes, as needed. Status information can be used to understand the current status of the brick, nfs processes, and overall file system. Status information can also be used to monitor and debug the volume information. You can view status of the volume along with the following details: detail - Displays additional information about the bricks. clients - Displays the list of clients connected to the volume. mem - Displays the memory usage and memory pool details of the bricks. inode - Displays the inode tables of the volume. fd - Displays the open fd (file descriptors) tables of the volume. callpool - Displays the pending calls of the volume. To display volume status Display information about a specific volume using the following command: # gluster volume status [all| []] [detail|clients|mem|inode|fd|callpool] For example, to display information about test-volume: # gluster volume status test-volume STATUS OF VOLUME: test-volume BRICK PORT ONLINE PID -------------------------------------------------------- arch:/export/1 24009 Y 22445 -------------------------------------------------------- arch:/export/2 24010 Y 22450 Display information about all volumes using the following command: # gluster volume status all # gluster volume status all STATUS OF VOLUME: volume-test BRICK PORT ONLINE PID -------------------------------------------------------- arch:/export/4 24010 Y 22455 STATUS OF VOLUME: test-volume BRICK PORT ONLINE PID -------------------------------------------------------- arch:/export/1 24009 Y 22445 -------------------------------------------------------- arch:/export/2 24010 Y 22450 Display additional information about the bricks using the following command: # gluster volume status detail For example, to display additional information about the bricks of test-volume: # gluster volume status test-volume details STATUS OF VOLUME: test-volume ------------------------------------------- Brick : arch:/export/1 Port : 24009 Online : Y Pid : 16977 File System : rootfs Device : rootfs Mount Options : rw Disk Space Free : 13.8GB Total Disk Space : 46.5GB Inode Size : N/A Inode Count : N/A Free Inodes : N/A Number of Bricks: 1 Bricks: Brick: server:/brick6 Display the list of clients accessing the volumes using the following command: # gluster volume status test-volume clients For example, to display the list of clients connected to test-volume: # gluster volume status test-volume clients Brick : arch:/export/1 Clients connected : 2 Hostname Bytes Read BytesWritten -------- --------- ------------ 127.0.0.1:1013 776 676 127.0.0.1:1012 50440 51200 Display the memory usage and memory pool details of the bricks using the following command: # gluster volume status test-volume mem For example, to display the memory usage and memory pool details of the bricks of test-volume: Memory status for volume : test-volume ---------------------------------------------- Brick : arch:/export/1 Mallinfo -------- Arena : 434176 Ordblks : 2 Smblks : 0 Hblks : 12 Hblkhd : 40861696 Usmblks : 0 Fsmblks : 0 Uordblks : 332416 Fordblks : 101760 Keepcost : 100400 Mempool Stats ------------- Name HotCount ColdCount PaddedSizeof AllocCount MaxAlloc ---- -------- --------- ------------ ---------- -------- test-volume-server:fd_t 0 16384 92 57 5 test-volume-server:dentry_t 59 965 84 59 59 test-volume-server:inode_t 60 964 148 60 60 test-volume-server:rpcsvc_request_t 0 525 6372 351 2 glusterfs:struct saved_frame 0 4096 124 2 2 glusterfs:struct rpc_req 0 4096 2236 2 2 glusterfs:rpcsvc_request_t 1 524 6372 2 1 glusterfs:call_stub_t 0 1024 1220 288 1 glusterfs:call_stack_t 0 8192 2084 290 2 glusterfs:call_frame_t 0 16384 172 1728 6 Display the inode tables of the volume using the following command: # gluster volume status inode For example, to display the inode tables of the test-volume: # gluster volume status test-volume inode inode tables for volume test-volume ---------------------------------------------- Brick : arch:/export/1 Active inodes: GFID Lookups Ref IA type ---- ------- --- ------- 6f3fe173-e07a-4209-abb6-484091d75499 1 9 2 370d35d7-657e-44dc-bac4-d6dd800ec3d3 1 1 2 LRU inodes: GFID Lookups Ref IA type ---- ------- --- ------- 80f98abe-cdcf-4c1d-b917-ae564cf55763 1 0 1 3a58973d-d549-4ea6-9977-9aa218f233de 1 0 1 2ce0197d-87a9-451b-9094-9baa38121155 1 0 2 Display the open fd tables of the volume using the following command: # gluster volume status fd For example, to display the open fd tables of the test-volume: # gluster volume status test-volume fd FD tables for volume test-volume ---------------------------------------------- Brick : arch:/export/1 Connection 1: RefCount = 0 MaxFDs = 128 FirstFree = 4 FD Entry PID RefCount Flags -------- --- -------- ----- 0 26311 1 2 1 26310 3 2 2 26310 1 2 3 26311 3 2 Connection 2: RefCount = 0 MaxFDs = 128 FirstFree = 0 No open fds Connection 3: RefCount = 0 MaxFDs = 128 FirstFree = 0 No open fds Display the pending calls of the volume using the following command: # gluster volume status callpool Each call has a call stack containing call frames. For example, to display the pending calls of test-volume: # gluster volume status test-volume Pending calls for volume test-volume ---------------------------------------------- Brick : arch:/export/1 Pending calls: 2 Call Stack1 UID : 0 GID : 0 PID : 26338 Unique : 192138 Frames : 7 Frame 1 Ref Count = 1 Translator = test-volume-server Completed = No Frame 2 Ref Count = 0 Translator = test-volume-posix Completed = No Parent = test-volume-access-control Wind From = default_fsync Wind To = FIRST_CHILD(this)->fops->fsync Frame 3 Ref Count = 1 Translator = test-volume-access-control Completed = No Parent = repl-locks Wind From = default_fsync Wind To = FIRST_CHILD(this)->fops->fsync Frame 4 Ref Count = 1 Translator = test-volume-locks Completed = No Parent = test-volume-io-threads Wind From = iot_fsync_wrapper Wind To = FIRST_CHILD (this)->fops->fsync Frame 5 Ref Count = 1 Translator = test-volume-io-threads Completed = No Parent = test-volume-marker Wind From = default_fsync Wind To = FIRST_CHILD(this)->fops->fsync Frame 6 Ref Count = 1 Translator = test-volume-marker Completed = No Parent = /export/1 Wind From = io_stats_fsync Wind To = FIRST_CHILD(this)->fops->fsync Frame 7 Ref Count = 1 Translator = /export/1 Completed = No Parent = test-volume-server Wind From = server_fsync_resume Wind To = bound_xl->fops->fsync","title":"Monitoring Workload"},{"location":"Administrator-Guide/Monitoring-Workload/#monitoring-your-glusterfs-workload","text":"You can monitor the GlusterFS volumes on different parameters. Monitoring volumes helps in capacity planning and performance tuning tasks of the GlusterFS volume. Using these information, you can identify and troubleshoot issues. You can use Volume Top and Profile commands to view the performance and identify bottlenecks/hotspots of each brick of a volume. This helps system administrators to get vital performance information whenever performance needs to be probed. You can also perform statedump of the brick processes and nfs server process of a volume, and also view volume status and volume information.","title":"Monitoring your GlusterFS Workload"},{"location":"Administrator-Guide/Monitoring-Workload/#running-glusterfs-volume-profile-command","text":"GlusterFS Volume Profile command provides an interface to get the per-brick I/O information for each File Operation (FOP) of a volume. The per brick information helps in identifying bottlenecks in the storage system. This section describes how to run GlusterFS Volume Profile command by performing the following operations: Start Profiling Displaying the I/0 Information Stop Profiling","title":"Running GlusterFS Volume Profile Command"},{"location":"Administrator-Guide/Monitoring-Workload/#start-profiling","text":"You must start the Profiling to view the File Operation information for each brick. To start profiling, use following command: # gluster volume profile start For example, to start profiling on test-volume: # gluster volume profile test-volume start Profiling started on test-volume When profiling on the volume is started, the following additional options are displayed in the Volume Info: diagnostics.count-fop-hits: on diagnostics.latency-measurement: on","title":"Start Profiling"},{"location":"Administrator-Guide/Monitoring-Workload/#displaying-the-i0-information","text":"You can view the I/O information of each brick by using the following command: # gluster volume profile info For example, to see the I/O information on test-volume: # gluster volume profile test-volume info Brick: Test:/export/2 Cumulative Stats: Block 1b+ 32b+ 64b+ Size: Read: 0 0 0 Write: 908 28 8 Block 128b+ 256b+ 512b+ Size: Read: 0 6 4 Write: 5 23 16 Block 1024b+ 2048b+ 4096b+ Size: Read: 0 52 17 Write: 15 120 846 Block 8192b+ 16384b+ 32768b+ Size: Read: 52 8 34 Write: 234 134 286 Block 65536b+ 131072b+ Size: Read: 118 622 Write: 1341 594 %-latency Avg- Min- Max- calls Fop latency Latency Latency ___________________________________________________________ 4.82 1132.28 21.00 800970.00 4575 WRITE 5.70 156.47 9.00 665085.00 39163 READDIRP 11.35 315.02 9.00 1433947.00 38698 LOOKUP 11.88 1729.34 21.00 2569638.00 7382 FXATTROP 47.35 104235.02 2485.00 7789367.00 488 FSYNC ------------------ ------------------ Duration : 335 BytesRead : 94505058 BytesWritten : 195571980","title":"Displaying the I/0 Information"},{"location":"Administrator-Guide/Monitoring-Workload/#stop-profiling","text":"You can stop profiling the volume, if you do not need profiling information anymore. Stop profiling using the following command: # gluster volume profile stop For example, to stop profiling on test-volume: # gluster volume profile stop Profiling stopped on test-volume","title":"Stop Profiling"},{"location":"Administrator-Guide/Monitoring-Workload/#running-glusterfs-volume-top-command","text":"GlusterFS Volume Top command allows you to view the glusterfs bricks\u2019 performance metrics like read, write, file open calls, file read calls, file write calls, directory open calls, and directory real calls. The top command displays up to 100 results. This section describes how to run and view the results for the following GlusterFS Top commands: Viewing Open fd Count and Maximum fd Count Viewing Highest File Read Calls Viewing Highest File Write Calls Viewing Highest Open Calls on Directories Viewing Highest Read Calls on Directory Viewing List of Read Performance on each Brick Viewing List of Write Performance on each Brick","title":"Running GlusterFS Volume TOP Command"},{"location":"Administrator-Guide/Monitoring-Workload/#viewing-open-fd-count-and-maximum-fd-count","text":"You can view both current open fd count (list of files that are currently the most opened and the count) on the brick and the maximum open fd count (count of files that are the currently open and the count of maximum number of files opened at any given point of time, since the servers are up and running). If the brick name is not specified, then open fd metrics of all the bricks belonging to the volume will be displayed. View open fd count and maximum fd count using the following command: # gluster volume top open [brick ] [list-cnt ] For example, to view open fd count and maximum fd count on brick server:/export of test-volume and list top 10 open calls: # gluster volume top open brick list-cnt Brick: server:/export/dir1 Current open fd's: 34 Max open fd's: 209 ==========Open file stats======== open file name call count 2 /clients/client0/~dmtmp/PARADOX/ COURSES.DB 11 /clients/client0/~dmtmp/PARADOX/ ENROLL.DB 11 /clients/client0/~dmtmp/PARADOX/ STUDENTS.DB 10 /clients/client0/~dmtmp/PWRPNT/ TIPS.PPT 10 /clients/client0/~dmtmp/PWRPNT/ PCBENCHM.PPT 9 /clients/client7/~dmtmp/PARADOX/ STUDENTS.DB 9 /clients/client1/~dmtmp/PARADOX/ STUDENTS.DB 9 /clients/client2/~dmtmp/PARADOX/ STUDENTS.DB 9 /clients/client0/~dmtmp/PARADOX/ STUDENTS.DB 9 /clients/client8/~dmtmp/PARADOX/ STUDENTS.DB","title":"Viewing Open fd Count and Maximum fd Count"},{"location":"Administrator-Guide/Monitoring-Workload/#viewing-highest-file-read-calls","text":"You can view highest read calls on each brick. If brick name is not specified, then by default, list of 100 files will be displayed. View highest file Read calls using the following command: # gluster volume top read [brick ] [list-cnt ] For example, to view highest Read calls on brick server:/export of test-volume: # gluster volume top read brick list-cnt Brick: server:/export/dir1 ==========Read file stats======== read filename call count 116 /clients/client0/~dmtmp/SEED/LARGE.FIL 64 /clients/client0/~dmtmp/SEED/MEDIUM.FIL 54 /clients/client2/~dmtmp/SEED/LARGE.FIL 54 /clients/client6/~dmtmp/SEED/LARGE.FIL 54 /clients/client5/~dmtmp/SEED/LARGE.FIL 54 /clients/client0/~dmtmp/SEED/LARGE.FIL 54 /clients/client3/~dmtmp/SEED/LARGE.FIL 54 /clients/client4/~dmtmp/SEED/LARGE.FIL 54 /clients/client9/~dmtmp/SEED/LARGE.FIL 54 /clients/client8/~dmtmp/SEED/LARGE.FIL","title":"Viewing Highest File Read Calls"},{"location":"Administrator-Guide/Monitoring-Workload/#viewing-highest-file-write-calls","text":"You can view list of files which has highest file write calls on each brick. If brick name is not specified, then by default, list of 100 files will be displayed. View highest file Write calls using the following command: # gluster volume top write [brick ] [list-cnt ] For example, to view highest Write calls on brick server:/export of test-volume: # gluster volume top write brick list-cnt Brick: server:/export/dir1 ==========Write file stats======== write call count filename 83 /clients/client0/~dmtmp/SEED/LARGE.FIL 59 /clients/client7/~dmtmp/SEED/LARGE.FIL 59 /clients/client1/~dmtmp/SEED/LARGE.FIL 59 /clients/client2/~dmtmp/SEED/LARGE.FIL 59 /clients/client0/~dmtmp/SEED/LARGE.FIL 59 /clients/client8/~dmtmp/SEED/LARGE.FIL 59 /clients/client5/~dmtmp/SEED/LARGE.FIL 59 /clients/client4/~dmtmp/SEED/LARGE.FIL 59 /clients/client6/~dmtmp/SEED/LARGE.FIL 59 /clients/client3/~dmtmp/SEED/LARGE.FIL","title":"Viewing Highest File Write Calls"},{"location":"Administrator-Guide/Monitoring-Workload/#viewing-highest-open-calls-on-directories","text":"You can view list of files which has highest open calls on directories of each brick. If brick name is not specified, then the metrics of all the bricks belonging to that volume will be displayed. View list of open calls on each directory using the following command: # gluster volume top opendir [brick ] [list-cnt ] For example, to view open calls on brick server:/export/ of test-volume: # gluster volume top opendir brick list-cnt Brick: server:/export/dir1 ==========Directory open stats======== Opendir count directory name 1001 /clients/client0/~dmtmp 454 /clients/client8/~dmtmp 454 /clients/client2/~dmtmp 454 /clients/client6/~dmtmp 454 /clients/client5/~dmtmp 454 /clients/client9/~dmtmp 443 /clients/client0/~dmtmp/PARADOX 408 /clients/client1/~dmtmp 408 /clients/client7/~dmtmp 402 /clients/client4/~dmtmp","title":"Viewing Highest Open Calls on Directories"},{"location":"Administrator-Guide/Monitoring-Workload/#viewing-highest-read-calls-on-directory","text":"You can view list of files which has highest directory read calls on each brick. If brick name is not specified, then the metrics of all the bricks belonging to that volume will be displayed. View list of highest directory read calls on each brick using the following command: # gluster volume top test-volume readdir [brick BRICK] [list-cnt {0..100}] For example, to view highest directory read calls on brick server:/export of test-volume: # gluster volume top test-volume readdir brick server:/export list-cnt 10 Brick: ==========Directory readdirp stats======== readdirp count directory name 1996 /clients/client0/~dmtmp 1083 /clients/client0/~dmtmp/PARADOX 904 /clients/client8/~dmtmp 904 /clients/client2/~dmtmp 904 /clients/client6/~dmtmp 904 /clients/client5/~dmtmp 904 /clients/client9/~dmtmp 812 /clients/client1/~dmtmp 812 /clients/client7/~dmtmp 800 /clients/client4/~dmtmp","title":"Viewing Highest Read Calls on Directory"},{"location":"Administrator-Guide/Monitoring-Workload/#viewing-list-of-read-performance-on-each-brick","text":"You can view the read throughput of files on each brick. If brick name is not specified, then the metrics of all the bricks belonging to that volume will be displayed. The output will be the read throughput. ==========Read throughput file stats======== read filename Time through put(MBp s) 2570.00 /clients/client0/~dmtmp/PWRPNT/ -2011-01-31 TRIDOTS.POT 15:38:36.894610 2570.00 /clients/client0/~dmtmp/PWRPNT/ -2011-01-31 PCBENCHM.PPT 15:38:39.815310 2383.00 /clients/client2/~dmtmp/SEED/ -2011-01-31 MEDIUM.FIL 15:52:53.631499 2340.00 /clients/client0/~dmtmp/SEED/ -2011-01-31 MEDIUM.FIL 15:38:36.926198 2299.00 /clients/client0/~dmtmp/SEED/ -2011-01-31 LARGE.FIL 15:38:36.930445 2259.00 /clients/client0/~dmtmp/PARADOX/ -2011-01-31 COURSES.X04 15:38:40.549919 2221.00 /clients/client0/~dmtmp/PARADOX/ -2011-01-31 STUDENTS.VAL 15:52:53.298766 2221.00 /clients/client3/~dmtmp/SEED/ -2011-01-31 COURSES.DB 15:39:11.776780 2184.00 /clients/client3/~dmtmp/SEED/ -2011-01-31 MEDIUM.FIL 15:39:10.251764 2184.00 /clients/client5/~dmtmp/WORD/ -2011-01-31 BASEMACH.DOC 15:39:09.336572 This command will initiate a dd for the specified count and block size and measures the corresponding throughput. View list of read performance on each brick using the following command: # gluster volume top read-perf [bs count ] [brick ] [list-cnt ] For example, to view read performance on brick server:/export/ of test-volume, 256 block size of count 1, and list count 10: # gluster volume top read-perf bs 256 count 1 brick list-cnt Brick: server:/export/dir1 256 bytes (256 B) copied, Throughput: 4.1 MB/s ==========Read throughput file stats======== read filename Time through put(MBp s) 2912.00 /clients/client0/~dmtmp/PWRPNT/ -2011-01-31 TRIDOTS.POT 15:38:36.896486 2570.00 /clients/client0/~dmtmp/PWRPNT/ -2011-01-31 PCBENCHM.PPT 15:38:39.815310 2383.00 /clients/client2/~dmtmp/SEED/ -2011-01-31 MEDIUM.FIL 15:52:53.631499 2340.00 /clients/client0/~dmtmp/SEED/ -2011-01-31 MEDIUM.FIL 15:38:36.926198 2299.00 /clients/client0/~dmtmp/SEED/ -2011-01-31 LARGE.FIL 15:38:36.930445 2259.00 /clients/client0/~dmtmp/PARADOX/ -2011-01-31 COURSES.X04 15:38:40.549919 2221.00 /clients/client9/~dmtmp/PARADOX/ -2011-01-31 STUDENTS.VAL 15:52:53.298766 2221.00 /clients/client8/~dmtmp/PARADOX/ -2011-01-31 COURSES.DB 15:39:11.776780 2184.00 /clients/client3/~dmtmp/SEED/ -2011-01-31 MEDIUM.FIL 15:39:10.251764 2184.00 /clients/client5/~dmtmp/WORD/ -2011-01-31 BASEMACH.DOC 15:39:09.336572","title":"Viewing List of Read Performance on each Brick"},{"location":"Administrator-Guide/Monitoring-Workload/#viewing-list-of-write-performance-on-each-brick","text":"You can view list of write throughput of files on each brick. If brick name is not specified, then the metrics of all the bricks belonging to that volume will be displayed. The output will be the write throughput. This command will initiate a dd for the specified count and block size and measures the corresponding throughput. To view list of write performance on each brick: View list of write performance on each brick using the following command: # gluster volume top write-perf [bs count ] [brick ] [list-cnt ] For example, to view write performance on brick server:/export/ of test-volume, 256 block size of count 1, and list count 10: # gluster volume top write-perf bs 256 count 1 brick list-cnt Brick : server:/export/dir1 256 bytes (256 B) copied, Throughput: 2.8 MB/s ==========Write throughput file stats======== write filename Time throughput (MBps) 1170.00 /clients/client0/~dmtmp/SEED/ -2011-01-31 SMALL.FIL 15:39:09.171494 1008.00 /clients/client6/~dmtmp/SEED/ -2011-01-31 LARGE.FIL 15:39:09.73189 949.00 /clients/client0/~dmtmp/SEED/ -2011-01-31 MEDIUM.FIL 15:38:36.927426 936.00 /clients/client0/~dmtmp/SEED/ -2011-01-31 LARGE.FIL 15:38:36.933177 897.00 /clients/client5/~dmtmp/SEED/ -2011-01-31 MEDIUM.FIL 15:39:09.33628 897.00 /clients/client6/~dmtmp/SEED/ -2011-01-31 MEDIUM.FIL 15:39:09.27713 885.00 /clients/client0/~dmtmp/SEED/ -2011-01-31 SMALL.FIL 15:38:36.924271 528.00 /clients/client5/~dmtmp/SEED/ -2011-01-31 LARGE.FIL 15:39:09.81893 516.00 /clients/client6/~dmtmp/ACCESS/ -2011-01-31 FASTENER.MDB 15:39:01.797317","title":"Viewing List of Write Performance on each Brick"},{"location":"Administrator-Guide/Monitoring-Workload/#displaying-volume-information","text":"You can display information about a specific volume, or all volumes, as needed. Display information about a specific volume using the following command: # gluster volume info VOLNAME For example, to display information about test-volume: # gluster volume info test-volume Volume Name: test-volume Type: Distribute Status: Created Number of Bricks: 4 Bricks: Brick1: server1:/exp1 Brick2: server2:/exp2 Brick3: server3:/exp3 Brick4: server4:/exp4 Display information about all volumes using the following command: # gluster volume info all # gluster volume info all Volume Name: test-volume Type: Distribute Status: Created Number of Bricks: 4 Bricks: Brick1: server1:/exp1 Brick2: server2:/exp2 Brick3: server3:/exp3 Brick4: server4:/exp4 Volume Name: mirror Type: Distributed-Replicate Status: Started Number of Bricks: 2 X 2 = 4 Bricks: Brick1: server1:/brick1 Brick2: server2:/brick2 Brick3: server3:/brick3 Brick4: server4:/brick4 Volume Name: Vol Type: Distribute Status: Started Number of Bricks: 1 Bricks: Brick: server:/brick6","title":"Displaying Volume Information"},{"location":"Administrator-Guide/Monitoring-Workload/#performing-statedump-on-a-volume","text":"Statedump is a mechanism through which you can get details of all internal variables and state of the glusterfs process at the time of issuing the command.You can perform statedumps of the brick processes and nfs server process of a volume using the statedump command. The following options can be used to determine what information is to be dumped: mem - Dumps the memory usage and memory pool details of the bricks. iobuf - Dumps iobuf details of the bricks. priv - Dumps private information of loaded translators. callpool - Dumps the pending calls of the volume. fd - Dumps the open fd tables of the volume. inode - Dumps the inode tables of the volume. To display volume statedump Display statedump of a volume or NFS server using the following command: # gluster volume statedump [nfs] [all|mem|iobuf|callpool|priv|fd|inode] For example, to display statedump of test-volume: # gluster volume statedump test-volume Volume statedump successful The statedump files are created on the brick servers in the /tmp directory or in the directory set using server.statedump-path volume option. The naming convention of the dump file is <brick-path>.<brick-pid>.dump . By defult, the output of the statedump is stored at /tmp/<brickname.PID.dump> file on that particular server. Change the directory of the statedump file using the following command: # gluster volume set server.statedump-path For example, to change the location of the statedump file of test-volume: # gluster volume set test-volume server.statedump-path /usr/local/var/log/glusterfs/dumps/ Set volume successful You can view the changed path of the statedump file using the following command: # gluster volume info","title":"Performing Statedump on a Volume"},{"location":"Administrator-Guide/Monitoring-Workload/#displaying-volume-status","text":"You can display the status information about a specific volume, brick or all volumes, as needed. Status information can be used to understand the current status of the brick, nfs processes, and overall file system. Status information can also be used to monitor and debug the volume information. You can view status of the volume along with the following details: detail - Displays additional information about the bricks. clients - Displays the list of clients connected to the volume. mem - Displays the memory usage and memory pool details of the bricks. inode - Displays the inode tables of the volume. fd - Displays the open fd (file descriptors) tables of the volume. callpool - Displays the pending calls of the volume. To display volume status Display information about a specific volume using the following command: # gluster volume status [all| []] [detail|clients|mem|inode|fd|callpool] For example, to display information about test-volume: # gluster volume status test-volume STATUS OF VOLUME: test-volume BRICK PORT ONLINE PID -------------------------------------------------------- arch:/export/1 24009 Y 22445 -------------------------------------------------------- arch:/export/2 24010 Y 22450 Display information about all volumes using the following command: # gluster volume status all # gluster volume status all STATUS OF VOLUME: volume-test BRICK PORT ONLINE PID -------------------------------------------------------- arch:/export/4 24010 Y 22455 STATUS OF VOLUME: test-volume BRICK PORT ONLINE PID -------------------------------------------------------- arch:/export/1 24009 Y 22445 -------------------------------------------------------- arch:/export/2 24010 Y 22450 Display additional information about the bricks using the following command: # gluster volume status detail For example, to display additional information about the bricks of test-volume: # gluster volume status test-volume details STATUS OF VOLUME: test-volume ------------------------------------------- Brick : arch:/export/1 Port : 24009 Online : Y Pid : 16977 File System : rootfs Device : rootfs Mount Options : rw Disk Space Free : 13.8GB Total Disk Space : 46.5GB Inode Size : N/A Inode Count : N/A Free Inodes : N/A Number of Bricks: 1 Bricks: Brick: server:/brick6 Display the list of clients accessing the volumes using the following command: # gluster volume status test-volume clients For example, to display the list of clients connected to test-volume: # gluster volume status test-volume clients Brick : arch:/export/1 Clients connected : 2 Hostname Bytes Read BytesWritten -------- --------- ------------ 127.0.0.1:1013 776 676 127.0.0.1:1012 50440 51200 Display the memory usage and memory pool details of the bricks using the following command: # gluster volume status test-volume mem For example, to display the memory usage and memory pool details of the bricks of test-volume: Memory status for volume : test-volume ---------------------------------------------- Brick : arch:/export/1 Mallinfo -------- Arena : 434176 Ordblks : 2 Smblks : 0 Hblks : 12 Hblkhd : 40861696 Usmblks : 0 Fsmblks : 0 Uordblks : 332416 Fordblks : 101760 Keepcost : 100400 Mempool Stats ------------- Name HotCount ColdCount PaddedSizeof AllocCount MaxAlloc ---- -------- --------- ------------ ---------- -------- test-volume-server:fd_t 0 16384 92 57 5 test-volume-server:dentry_t 59 965 84 59 59 test-volume-server:inode_t 60 964 148 60 60 test-volume-server:rpcsvc_request_t 0 525 6372 351 2 glusterfs:struct saved_frame 0 4096 124 2 2 glusterfs:struct rpc_req 0 4096 2236 2 2 glusterfs:rpcsvc_request_t 1 524 6372 2 1 glusterfs:call_stub_t 0 1024 1220 288 1 glusterfs:call_stack_t 0 8192 2084 290 2 glusterfs:call_frame_t 0 16384 172 1728 6 Display the inode tables of the volume using the following command: # gluster volume status inode For example, to display the inode tables of the test-volume: # gluster volume status test-volume inode inode tables for volume test-volume ---------------------------------------------- Brick : arch:/export/1 Active inodes: GFID Lookups Ref IA type ---- ------- --- ------- 6f3fe173-e07a-4209-abb6-484091d75499 1 9 2 370d35d7-657e-44dc-bac4-d6dd800ec3d3 1 1 2 LRU inodes: GFID Lookups Ref IA type ---- ------- --- ------- 80f98abe-cdcf-4c1d-b917-ae564cf55763 1 0 1 3a58973d-d549-4ea6-9977-9aa218f233de 1 0 1 2ce0197d-87a9-451b-9094-9baa38121155 1 0 2 Display the open fd tables of the volume using the following command: # gluster volume status fd For example, to display the open fd tables of the test-volume: # gluster volume status test-volume fd FD tables for volume test-volume ---------------------------------------------- Brick : arch:/export/1 Connection 1: RefCount = 0 MaxFDs = 128 FirstFree = 4 FD Entry PID RefCount Flags -------- --- -------- ----- 0 26311 1 2 1 26310 3 2 2 26310 1 2 3 26311 3 2 Connection 2: RefCount = 0 MaxFDs = 128 FirstFree = 0 No open fds Connection 3: RefCount = 0 MaxFDs = 128 FirstFree = 0 No open fds Display the pending calls of the volume using the following command: # gluster volume status callpool Each call has a call stack containing call frames. For example, to display the pending calls of test-volume: # gluster volume status test-volume Pending calls for volume test-volume ---------------------------------------------- Brick : arch:/export/1 Pending calls: 2 Call Stack1 UID : 0 GID : 0 PID : 26338 Unique : 192138 Frames : 7 Frame 1 Ref Count = 1 Translator = test-volume-server Completed = No Frame 2 Ref Count = 0 Translator = test-volume-posix Completed = No Parent = test-volume-access-control Wind From = default_fsync Wind To = FIRST_CHILD(this)->fops->fsync Frame 3 Ref Count = 1 Translator = test-volume-access-control Completed = No Parent = repl-locks Wind From = default_fsync Wind To = FIRST_CHILD(this)->fops->fsync Frame 4 Ref Count = 1 Translator = test-volume-locks Completed = No Parent = test-volume-io-threads Wind From = iot_fsync_wrapper Wind To = FIRST_CHILD (this)->fops->fsync Frame 5 Ref Count = 1 Translator = test-volume-io-threads Completed = No Parent = test-volume-marker Wind From = default_fsync Wind To = FIRST_CHILD(this)->fops->fsync Frame 6 Ref Count = 1 Translator = test-volume-marker Completed = No Parent = /export/1 Wind From = io_stats_fsync Wind To = FIRST_CHILD(this)->fops->fsync Frame 7 Ref Count = 1 Translator = /export/1 Completed = No Parent = test-volume-server Wind From = server_fsync_resume Wind To = bound_xl->fops->fsync","title":"Displaying Volume Status"},{"location":"Administrator-Guide/NFS-Ganesha-GlusterFS-Integration/","text":"Configuring NFS-Ganesha over GlusterFS NFS-Ganesha is a user-space file server for the NFS protocol with support for NFSv3, v4, v4.1, pNFS. It provides a FUSE-compatible File System Abstraction Layer(FSAL) to allow the file-system developers to plug in their storage mechanism and access it from any NFS client. NFS-Ganesha can access the FUSE filesystems directly through its FSAL without copying any data to or from the kernel, thus potentially improving response times. Installing nfs-ganesha Gluster RPMs (>= 3.10) glusterfs-server glusterfs-api glusterfs-ganesha Ganesha RPMs (>= 2.5) nfs-ganesha nfs-ganesha-gluster Start NFS-Ganesha manually To start NFS-Ganesha manually, use the command: service nfs-ganesha start where: /var/log/ganesha.log is the default log file for the ganesha process. /etc/ganesha/ganesha.conf is the default configuration file NIV_EVENT is the default log level. If the user wants to run ganesha in a preferred mode, execute the following command : #ganesha.nfsd -f \\<location_of_nfs-ganesha.conf_file> -L \\<location_of_log_file> -N \\<log_level> For example: #ganesha.nfsd -f nfs-ganesha.conf -L nfs-ganesha.log -N NIV_DEBUG where: nfs-ganesha.log is the log file for the ganesha.nfsd process. nfs-ganesha.conf is the configuration file NIV_DEBUG is the log level. By default, the export list for the server will be Null Note : include following parameters in ganesha configuration file for exporting gluster volumes NFS_Core_Param { #Use supplied name other tha IP In NSM operations NSM_Use_Caller_Name = true; #Copy lock states into \"/var/lib/nfs/ganesha\" dir Clustered = false; #Use a non-privileged port for RQuota Rquota_Port = 875; #please note add below option for Mac clients #Enable_RQUOTA = false; } Step by step procedures to exporting GlusterFS volume via NFS-Ganesha step 1 : To export any GlusterFS volume or directory inside a volume, create the EXPORT block for each of those entries in an export configuration file. The following parameters are required to export any entry. - #cat export.conf EXPORT{ Export_Id = 1 ; # Export ID unique to each export Path = \"volume_path\"; # Path of the volume to be exported. Eg: \"/test_volume\" FSAL { name = GLUSTER; hostname = \"10.xx.xx.xx\"; # IP of one of the nodes in the trusted pool volume = \"volume_name\"; # Volume name. Eg: \"test_volume\" } Access_type = RW; # Access permissions Squash = No_root_squash; # To enable/disable root squashing Disable_ACL = TRUE; # To enable/disable ACL Pseudo = \"pseudo_path\"; # NFSv4 pseudo path for this export. Eg: \"/test_volume_pseudo\" Protocols = \"3\",\"4\" ; # NFS protocols supported Transports = \"UDP\",\"TCP\" ; # Transport protocols supported SecType = \"sys\"; # Security flavors supported } step 2 : Now include the export configuration file in the ganesha configuration file (by default). This can be done by adding the line below at the end of file - %include \u201c\\<path of export configuration>\u201d Note : The above two steps can be done with following script #/usr/libexec/ganesha/create-export-ganesha.sh <ganesha directory> on <volume name> By default ganesha directory is \"/etc/ganesha\" This will create export configuration file in <ganesha directory>/exports/export.<volume name>.conf Also, it will add the above entry to ganesha.conf step 3 : Turn on features.cache-invalidation for that volume - gluster volume set \\<volume name> features.cache-invalidation on step 4 : dbus commands are used to export/unexport volume - export - #dbus-send --system --print-reply --dest=org.ganesha.nfsd /org/ganesha/nfsd/ExportMgr org.ganesha.nfsd.exportmgr.AddExport string: /exports/export. .conf string:\"EXPORT(Path=/\\<volume name>)\" unexport #dbus-send --system --dest=org.ganesha.nfsd /org/ganesha/nfsd/ExportMgr org.ganesha.nfsd.exportmgr.RemoveExport uint16:\\<export id> Note : Step 4 can be performed via following script #/usr/libexec/ganesha/dbus-send.sh <ganesha directory> [on|off] <volume name> Above scripts (mentioned in step 3 and step 4) are available in glusterfs 3.10 rpms. You can download it from here step 5 : To check if the volume is exported, run #showmount -e localhost Or else use the following dbus command #dbus-send --type=method_call --print-reply --system --dest=org.ganesha.nfsd /org/ganesha/nfsd/ExportMgr org.ganesha.nfsd.exportmgr.ShowExports To see clients #dbus-send --type=method_call --print-reply --system --dest=org.ganesha.nfsd /org/ganesha/nfsd/ClientMgr org.ganesha.nfsd.clientmgr.ShowClients Using Highly Available Active-Active NFS-Ganesha And GlusterFS cli Please Note currently HA solution for nfs-ganesha is available in 3.10. From 3.12 onwards HA will be handled by a different project known as storhaug which is under development. In a highly available active-active environment, if an NFS-Ganesha server that is connected to an NFS client running a particular application crashes, the application/NFS client is seamlessly connected to another NFS-Ganesha server without any administrative intervention. The cluster is maintained using Pacemaker and Corosync. Pacemaker acts as a resource manager and Corosync provides the communication layer of the cluster. Data coherency across the multi-head NFS-Ganesha servers in the cluster is achieved using the UPCALL infrastructure. UPCALL infrastructure is a generic and extensible framework that sends notifications to the respective glusterfs clients (in this case NFS-Ganesha server) in case of any changes detected in the backend filesystem. The Highly Available cluster is configured in the following three stages: Creating the ganesha-ha.conf file The ganesha-ha.conf.example is created in the following location /etc/ganesha when Gluster Storage is installed. Rename the file to ganesha-ha.conf and make the changes as suggested in the following example: sample ganesha-ha.conf file: # Name of the HA cluster created. must be unique within the subnet HA_NAME=\"ganesha-ha-360\" # The subset of nodes of the Gluster Trusted Pool that form the ganesha HA cluster. # Hostname is specified. HA_CLUSTER_NODES=\"server1,server2,...\" #HA_CLUSTER_NODES=\"server1.lab.redhat.com,server2.lab.redhat.com,...\" # Virtual IPs for each of the nodes specified above. VIP_server1=\"10.0.2.1\" VIP_server2=\"10.0.2.2\" Configuring NFS-Ganesha using gluster CLI The HA cluster can be set up or torn down using gluster CLI. Also, it can export and unexport specific volumes. For more information, see section Configuring NFS-Ganesha using gluster CLI. Modifying the HA cluster using the ganesha-ha.sh script Post the cluster creation any further modification can be done using the ganesha-ha.sh script. For more information, see the section Modifying the HA cluster using the ganesha-ha.sh script. Step-by-step guide Configuring NFS-Ganesha using Gluster CLI\u2060 Pre-requisites to run NFS-Ganesha Ensure that the following pre-requisites are taken into consideration before you run NFS-Ganesha in your environment: A Gluster Storage volume must be available for export and NFS-Ganesha rpms are installed on all the nodes. IPv6 must be enabled on the host interface which is used by the NFS-Ganesha daemon. To enable IPv6 support, perform the following steps: Comment or remove the line options ipv6 disable=1 in the /etc/modprobe.d/ipv6.conf file. Reboot the system. Ensure that all the nodes in the cluster are DNS resolvable. For example, you can populate the /etc/hosts with the details of all the nodes in the cluster. Disable and stop NetworkManager service. Enable and start network service on all machines. Create and mount a gluster shared volume. gluster volume set all cluster.enable-shared-storage enable Install Pacemaker and Corosync on all machines. Set the cluster auth password on all the machines. Passwordless ssh needs to be enabled on all the HA nodes. Follow these steps, On one (primary) node in the cluster, run: ssh-keygen -f /var/lib/glusterd/nfs/secret.pem Deploy the pubkey ~root/.ssh/authorized keys on all nodes, run: ssh-copy-id -i /var/lib/glusterd/nfs/secret.pem.pub root@$node Copy the keys to all nodes in the cluster, run: scp /var/lib/glusterd/nfs/secret.* $node:/var/lib/glusterd/nfs/ Create a directory named \"nfs-ganesha\" in shared storage path and create ganesha.conf & ganesha-ha.conf in it (from glusterfs 3.9 onwards) Configuring the HA Cluster To set up the HA cluster, enable NFS-Ganesha by executing the following command: #gluster nfs-ganesha enable To tear down the HA cluster, execute the following command: #gluster nfs-ganesha disable Note : Enable command performs the following * create a symlink ganesha.conf in /etc/ganesha using ganesha.conf in shared storage * start nfs-ganesha process on nodes part of ganesha cluster * set up ha cluster and disable does the reversal of enable Also if gluster nfs-ganesha [enable/disable] fails of please check following logs * /var/log/glusterfs/glusterd.log * /var/log/messages (and grep for pcs commands) * /var/log/pcsd/pcsd.log Exporting Volumes through NFS-Ganesha using cli To export a Red Hat Gluster Storage volume, execute the following command: #gluster volume set <volname> ganesha.enable on To unexport a Red Hat Gluster Storage volume, execute the following command: #gluster volume set <volname> ganesha.enable off This command unexports the Red Hat Gluster Storage volume without affecting other exports. To verify the status of the volume set options, follow the guidelines mentioned below: Check if NFS-Ganesha is started by executing the following command: ps aux | grep ganesha.nfsd Check if the volume is exported. showmount -e localhost The logs of ganesha.nfsd daemon is written to /var/log/ganesha.log. Check the log file on noticing any unexpected behavior. Modifying the HA cluster using the ganesha-ha.sh script To modify the existing HA cluster and to change the default values of the exports use the ganesha-ha.sh script located at /usr/libexec/ganesha/. Adding a node to the cluster Before adding a node to the cluster, ensure all the prerequisites mentioned in section Pre-requisites to run NFS-Ganesha are met. To add a node to the cluster. execute the following command on any of the nodes in the existing NFS-Ganesha cluster: #./ganesha-ha.sh --add <HA_CONF_DIR> <HOSTNAME> <NODE-VIP> where, HA_CONF_DIR: The directory path containing the ganesha-ha.conf file. HOSTNAME: Hostname of the new node to be added NODE-VIP: Virtual IP of the new node to be added. Deleting a node in the cluster To delete a node from the cluster, execute the following command on any of the nodes in the existing NFS-Ganesha cluster: #./ganesha-ha.sh --delete <HA_CONF_DIR> <HOSTNAME> where, HA_CONF_DIR: The directory path containing the ganesha-ha.conf file. HOSTNAME: Hostname of the new node to be added Modifying the default export configuration To modify the default export configurations perform the following steps on any of the nodes in the existing ganesha cluster: Edit/add the required fields in the corresponding export file located at /etc/ganesha/exports . Execute the following command: #./ganesha-ha.sh --refresh-config <HA_CONFDIR> <volname> where, HA_CONF_DIR: The directory path containing the ganesha-ha.conf file. volname: The name of the volume whose export configuration has to be changed. Note: The export ID must not be changed. \u2060 Configure ganesha ha cluster outside of gluster nodes Currently, ganesha HA cluster creating tightly integrated with glusterd. So here user needs to create another TSP using ganesha nodes. Then create ganesha HA cluster using above mentioned steps till executing \"gluster nfs-ganesha enable\" Exporting/Unexporting should be performed without using glusterd cli (follow the manual steps, before performing step 4 replace localhost with required hostname/ip \"hostname=localhost;\" in the export configuration file) Configuring Gluster volume for pNFS The Parallel Network File System (pNFS) is part of the NFS v4.1 protocol that allows computing clients to access storage devices directly and in parallel. The pNFS cluster consists of MDS (Meta-Data-Server) and DS (Data-Server). The client sends all the read/write requests directly to DS and all other operations are handle by the MDS. Step by step guide Turn on feature.cache-invalidation for the volume. gluster v set \\<volname> features.cache-invalidation on Select one of the nodes in the cluster as MDS and configure it adding the following block to ganesha configuration file GLUSTER { PNFS_MDS = true; } Manually start NFS-Ganesha in every node in the cluster. Check whether the volume is exported via nfs-ganesha in all the nodes. #showmount -e localhost Mount the volume using NFS version 4.1 protocol with the ip of MDS #mount -t nfs4 -o minorversion=1 \\<ip of MDS>:/\\<volume name> \\<mount path> Points to be Noted The current architecture supports only a single MDS and multiple DS. The server with which client mounts will act as MDS and all servers including MDS can act as DS. Currently, HA is not supported for pNFS (more specifically MDS). Although it is configurable, consistency is guaranteed across the cluster. If any of the DS goes down, then MDS will handle those I/O's. Hereafter, all the subsequent NFS clients need to use the same server for mounting that volume via pNFS. i.e more than one MDS for a volume is not preferred pNFS support is only tested with distributed, replicated, or distribute-replicate volumes It is tested and verified with RHEL 6.5 , fedora 20, fedora 21 nfs clients. It is always better to use latest nfs-clients","title":"Configuring NFS-Ganesha server"},{"location":"Administrator-Guide/NFS-Ganesha-GlusterFS-Integration/#configuring-nfs-ganesha-over-glusterfs","text":"NFS-Ganesha is a user-space file server for the NFS protocol with support for NFSv3, v4, v4.1, pNFS. It provides a FUSE-compatible File System Abstraction Layer(FSAL) to allow the file-system developers to plug in their storage mechanism and access it from any NFS client. NFS-Ganesha can access the FUSE filesystems directly through its FSAL without copying any data to or from the kernel, thus potentially improving response times.","title":"Configuring NFS-Ganesha over GlusterFS"},{"location":"Administrator-Guide/NFS-Ganesha-GlusterFS-Integration/#installing-nfs-ganesha","text":"","title":"Installing nfs-ganesha"},{"location":"Administrator-Guide/NFS-Ganesha-GlusterFS-Integration/#gluster-rpms-310","text":"glusterfs-server glusterfs-api glusterfs-ganesha","title":"Gluster RPMs (&gt;= 3.10)"},{"location":"Administrator-Guide/NFS-Ganesha-GlusterFS-Integration/#ganesha-rpms-25","text":"nfs-ganesha nfs-ganesha-gluster","title":"Ganesha RPMs (&gt;= 2.5)"},{"location":"Administrator-Guide/NFS-Ganesha-GlusterFS-Integration/#start-nfs-ganesha-manually","text":"To start NFS-Ganesha manually, use the command: service nfs-ganesha start where: /var/log/ganesha.log is the default log file for the ganesha process. /etc/ganesha/ganesha.conf is the default configuration file NIV_EVENT is the default log level. If the user wants to run ganesha in a preferred mode, execute the following command : #ganesha.nfsd -f \\<location_of_nfs-ganesha.conf_file> -L \\<location_of_log_file> -N \\<log_level> For example: #ganesha.nfsd -f nfs-ganesha.conf -L nfs-ganesha.log -N NIV_DEBUG where: nfs-ganesha.log is the log file for the ganesha.nfsd process. nfs-ganesha.conf is the configuration file NIV_DEBUG is the log level. By default, the export list for the server will be Null Note : include following parameters in ganesha configuration file for exporting gluster volumes NFS_Core_Param { #Use supplied name other tha IP In NSM operations NSM_Use_Caller_Name = true; #Copy lock states into \"/var/lib/nfs/ganesha\" dir Clustered = false; #Use a non-privileged port for RQuota Rquota_Port = 875; #please note add below option for Mac clients #Enable_RQUOTA = false; }","title":"Start NFS-Ganesha manually"},{"location":"Administrator-Guide/NFS-Ganesha-GlusterFS-Integration/#step-by-step-procedures-to-exporting-glusterfs-volume-via-nfs-ganesha","text":"","title":"Step by step procedures to exporting GlusterFS volume via NFS-Ganesha"},{"location":"Administrator-Guide/NFS-Ganesha-GlusterFS-Integration/#step-1","text":"To export any GlusterFS volume or directory inside a volume, create the EXPORT block for each of those entries in an export configuration file. The following parameters are required to export any entry. - #cat export.conf EXPORT{ Export_Id = 1 ; # Export ID unique to each export Path = \"volume_path\"; # Path of the volume to be exported. Eg: \"/test_volume\" FSAL { name = GLUSTER; hostname = \"10.xx.xx.xx\"; # IP of one of the nodes in the trusted pool volume = \"volume_name\"; # Volume name. Eg: \"test_volume\" } Access_type = RW; # Access permissions Squash = No_root_squash; # To enable/disable root squashing Disable_ACL = TRUE; # To enable/disable ACL Pseudo = \"pseudo_path\"; # NFSv4 pseudo path for this export. Eg: \"/test_volume_pseudo\" Protocols = \"3\",\"4\" ; # NFS protocols supported Transports = \"UDP\",\"TCP\" ; # Transport protocols supported SecType = \"sys\"; # Security flavors supported }","title":"step 1 :"},{"location":"Administrator-Guide/NFS-Ganesha-GlusterFS-Integration/#step-2","text":"Now include the export configuration file in the ganesha configuration file (by default). This can be done by adding the line below at the end of file - %include \u201c\\<path of export configuration>\u201d Note : The above two steps can be done with following script #/usr/libexec/ganesha/create-export-ganesha.sh <ganesha directory> on <volume name> By default ganesha directory is \"/etc/ganesha\" This will create export configuration file in <ganesha directory>/exports/export.<volume name>.conf Also, it will add the above entry to ganesha.conf","title":"step 2 :"},{"location":"Administrator-Guide/NFS-Ganesha-GlusterFS-Integration/#step-3","text":"Turn on features.cache-invalidation for that volume - gluster volume set \\<volume name> features.cache-invalidation on","title":"step 3 :"},{"location":"Administrator-Guide/NFS-Ganesha-GlusterFS-Integration/#step-4","text":"dbus commands are used to export/unexport volume - export - #dbus-send --system --print-reply --dest=org.ganesha.nfsd /org/ganesha/nfsd/ExportMgr org.ganesha.nfsd.exportmgr.AddExport string: /exports/export. .conf string:\"EXPORT(Path=/\\<volume name>)\" unexport #dbus-send --system --dest=org.ganesha.nfsd /org/ganesha/nfsd/ExportMgr org.ganesha.nfsd.exportmgr.RemoveExport uint16:\\<export id> Note : Step 4 can be performed via following script #/usr/libexec/ganesha/dbus-send.sh <ganesha directory> [on|off] <volume name> Above scripts (mentioned in step 3 and step 4) are available in glusterfs 3.10 rpms. You can download it from here","title":"step 4 :"},{"location":"Administrator-Guide/NFS-Ganesha-GlusterFS-Integration/#step-5","text":"To check if the volume is exported, run #showmount -e localhost Or else use the following dbus command #dbus-send --type=method_call --print-reply --system --dest=org.ganesha.nfsd /org/ganesha/nfsd/ExportMgr org.ganesha.nfsd.exportmgr.ShowExports To see clients #dbus-send --type=method_call --print-reply --system --dest=org.ganesha.nfsd /org/ganesha/nfsd/ClientMgr org.ganesha.nfsd.clientmgr.ShowClients","title":"step 5 :"},{"location":"Administrator-Guide/NFS-Ganesha-GlusterFS-Integration/#using-highly-available-active-active-nfs-ganesha-and-glusterfs-cli","text":"Please Note currently HA solution for nfs-ganesha is available in 3.10. From 3.12 onwards HA will be handled by a different project known as storhaug which is under development. In a highly available active-active environment, if an NFS-Ganesha server that is connected to an NFS client running a particular application crashes, the application/NFS client is seamlessly connected to another NFS-Ganesha server without any administrative intervention. The cluster is maintained using Pacemaker and Corosync. Pacemaker acts as a resource manager and Corosync provides the communication layer of the cluster. Data coherency across the multi-head NFS-Ganesha servers in the cluster is achieved using the UPCALL infrastructure. UPCALL infrastructure is a generic and extensible framework that sends notifications to the respective glusterfs clients (in this case NFS-Ganesha server) in case of any changes detected in the backend filesystem. The Highly Available cluster is configured in the following three stages:","title":"Using Highly Available Active-Active NFS-Ganesha And GlusterFS cli"},{"location":"Administrator-Guide/NFS-Ganesha-GlusterFS-Integration/#creating-the-ganesha-haconf-file","text":"The ganesha-ha.conf.example is created in the following location /etc/ganesha when Gluster Storage is installed. Rename the file to ganesha-ha.conf and make the changes as suggested in the following example: sample ganesha-ha.conf file: # Name of the HA cluster created. must be unique within the subnet HA_NAME=\"ganesha-ha-360\" # The subset of nodes of the Gluster Trusted Pool that form the ganesha HA cluster. # Hostname is specified. HA_CLUSTER_NODES=\"server1,server2,...\" #HA_CLUSTER_NODES=\"server1.lab.redhat.com,server2.lab.redhat.com,...\" # Virtual IPs for each of the nodes specified above. VIP_server1=\"10.0.2.1\" VIP_server2=\"10.0.2.2\"","title":"Creating the ganesha-ha.conf file"},{"location":"Administrator-Guide/NFS-Ganesha-GlusterFS-Integration/#configuring-nfs-ganesha-using-gluster-cli","text":"The HA cluster can be set up or torn down using gluster CLI. Also, it can export and unexport specific volumes. For more information, see section Configuring NFS-Ganesha using gluster CLI.","title":"Configuring NFS-Ganesha using gluster CLI"},{"location":"Administrator-Guide/NFS-Ganesha-GlusterFS-Integration/#modifying-the-ha-cluster-using-the-ganesha-hash-script","text":"Post the cluster creation any further modification can be done using the ganesha-ha.sh script. For more information, see the section Modifying the HA cluster using the ganesha-ha.sh script.","title":"Modifying the HA cluster using the ganesha-ha.sh script"},{"location":"Administrator-Guide/NFS-Ganesha-GlusterFS-Integration/#step-by-step-guide","text":"","title":"Step-by-step guide"},{"location":"Administrator-Guide/NFS-Ganesha-GlusterFS-Integration/#configuring-nfs-ganesha-using-gluster-cli_1","text":"","title":"Configuring NFS-Ganesha using Gluster CLI\u2060"},{"location":"Administrator-Guide/NFS-Ganesha-GlusterFS-Integration/#pre-requisites-to-run-nfs-ganesha","text":"Ensure that the following pre-requisites are taken into consideration before you run NFS-Ganesha in your environment: A Gluster Storage volume must be available for export and NFS-Ganesha rpms are installed on all the nodes. IPv6 must be enabled on the host interface which is used by the NFS-Ganesha daemon. To enable IPv6 support, perform the following steps: Comment or remove the line options ipv6 disable=1 in the /etc/modprobe.d/ipv6.conf file. Reboot the system. Ensure that all the nodes in the cluster are DNS resolvable. For example, you can populate the /etc/hosts with the details of all the nodes in the cluster. Disable and stop NetworkManager service. Enable and start network service on all machines. Create and mount a gluster shared volume. gluster volume set all cluster.enable-shared-storage enable Install Pacemaker and Corosync on all machines. Set the cluster auth password on all the machines. Passwordless ssh needs to be enabled on all the HA nodes. Follow these steps, On one (primary) node in the cluster, run: ssh-keygen -f /var/lib/glusterd/nfs/secret.pem Deploy the pubkey ~root/.ssh/authorized keys on all nodes, run: ssh-copy-id -i /var/lib/glusterd/nfs/secret.pem.pub root@$node Copy the keys to all nodes in the cluster, run: scp /var/lib/glusterd/nfs/secret.* $node:/var/lib/glusterd/nfs/ Create a directory named \"nfs-ganesha\" in shared storage path and create ganesha.conf & ganesha-ha.conf in it (from glusterfs 3.9 onwards)","title":"Pre-requisites to run NFS-Ganesha"},{"location":"Administrator-Guide/NFS-Ganesha-GlusterFS-Integration/#configuring-the-ha-cluster","text":"To set up the HA cluster, enable NFS-Ganesha by executing the following command: #gluster nfs-ganesha enable To tear down the HA cluster, execute the following command: #gluster nfs-ganesha disable Note : Enable command performs the following * create a symlink ganesha.conf in /etc/ganesha using ganesha.conf in shared storage * start nfs-ganesha process on nodes part of ganesha cluster * set up ha cluster and disable does the reversal of enable Also if gluster nfs-ganesha [enable/disable] fails of please check following logs * /var/log/glusterfs/glusterd.log * /var/log/messages (and grep for pcs commands) * /var/log/pcsd/pcsd.log","title":"Configuring the HA Cluster"},{"location":"Administrator-Guide/NFS-Ganesha-GlusterFS-Integration/#exporting-volumes-through-nfs-ganesha-using-cli","text":"To export a Red Hat Gluster Storage volume, execute the following command: #gluster volume set <volname> ganesha.enable on To unexport a Red Hat Gluster Storage volume, execute the following command: #gluster volume set <volname> ganesha.enable off This command unexports the Red Hat Gluster Storage volume without affecting other exports. To verify the status of the volume set options, follow the guidelines mentioned below: Check if NFS-Ganesha is started by executing the following command: ps aux | grep ganesha.nfsd Check if the volume is exported. showmount -e localhost The logs of ganesha.nfsd daemon is written to /var/log/ganesha.log. Check the log file on noticing any unexpected behavior.","title":"Exporting Volumes through NFS-Ganesha using cli"},{"location":"Administrator-Guide/NFS-Ganesha-GlusterFS-Integration/#modifying-the-ha-cluster-using-the-ganesha-hash-script_1","text":"To modify the existing HA cluster and to change the default values of the exports use the ganesha-ha.sh script located at /usr/libexec/ganesha/.","title":"Modifying the HA cluster using the ganesha-ha.sh script"},{"location":"Administrator-Guide/NFS-Ganesha-GlusterFS-Integration/#adding-a-node-to-the-cluster","text":"Before adding a node to the cluster, ensure all the prerequisites mentioned in section Pre-requisites to run NFS-Ganesha are met. To add a node to the cluster. execute the following command on any of the nodes in the existing NFS-Ganesha cluster: #./ganesha-ha.sh --add <HA_CONF_DIR> <HOSTNAME> <NODE-VIP> where, HA_CONF_DIR: The directory path containing the ganesha-ha.conf file. HOSTNAME: Hostname of the new node to be added NODE-VIP: Virtual IP of the new node to be added.","title":"Adding a node to the cluster"},{"location":"Administrator-Guide/NFS-Ganesha-GlusterFS-Integration/#deleting-a-node-in-the-cluster","text":"To delete a node from the cluster, execute the following command on any of the nodes in the existing NFS-Ganesha cluster: #./ganesha-ha.sh --delete <HA_CONF_DIR> <HOSTNAME> where, HA_CONF_DIR: The directory path containing the ganesha-ha.conf file. HOSTNAME: Hostname of the new node to be added","title":"Deleting a node in the cluster"},{"location":"Administrator-Guide/NFS-Ganesha-GlusterFS-Integration/#modifying-the-default-export-configuration","text":"To modify the default export configurations perform the following steps on any of the nodes in the existing ganesha cluster: Edit/add the required fields in the corresponding export file located at /etc/ganesha/exports . Execute the following command: #./ganesha-ha.sh --refresh-config <HA_CONFDIR> <volname> where, HA_CONF_DIR: The directory path containing the ganesha-ha.conf file. volname: The name of the volume whose export configuration has to be changed. Note: The export ID must not be changed. \u2060","title":"Modifying the default export configuration"},{"location":"Administrator-Guide/NFS-Ganesha-GlusterFS-Integration/#configure-ganesha-ha-cluster-outside-of-gluster-nodes","text":"Currently, ganesha HA cluster creating tightly integrated with glusterd. So here user needs to create another TSP using ganesha nodes. Then create ganesha HA cluster using above mentioned steps till executing \"gluster nfs-ganesha enable\" Exporting/Unexporting should be performed without using glusterd cli (follow the manual steps, before performing step 4 replace localhost with required hostname/ip \"hostname=localhost;\" in the export configuration file)","title":"Configure ganesha ha cluster outside of gluster nodes"},{"location":"Administrator-Guide/NFS-Ganesha-GlusterFS-Integration/#configuring-gluster-volume-for-pnfs","text":"The Parallel Network File System (pNFS) is part of the NFS v4.1 protocol that allows computing clients to access storage devices directly and in parallel. The pNFS cluster consists of MDS (Meta-Data-Server) and DS (Data-Server). The client sends all the read/write requests directly to DS and all other operations are handle by the MDS.","title":"Configuring Gluster volume for pNFS"},{"location":"Administrator-Guide/NFS-Ganesha-GlusterFS-Integration/#step-by-step-guide_1","text":"Turn on feature.cache-invalidation for the volume. gluster v set \\<volname> features.cache-invalidation on Select one of the nodes in the cluster as MDS and configure it adding the following block to ganesha configuration file GLUSTER { PNFS_MDS = true; } Manually start NFS-Ganesha in every node in the cluster. Check whether the volume is exported via nfs-ganesha in all the nodes. #showmount -e localhost Mount the volume using NFS version 4.1 protocol with the ip of MDS #mount -t nfs4 -o minorversion=1 \\<ip of MDS>:/\\<volume name> \\<mount path>","title":"Step by step guide"},{"location":"Administrator-Guide/NFS-Ganesha-GlusterFS-Integration/#points-to-be-noted","text":"The current architecture supports only a single MDS and multiple DS. The server with which client mounts will act as MDS and all servers including MDS can act as DS. Currently, HA is not supported for pNFS (more specifically MDS). Although it is configurable, consistency is guaranteed across the cluster. If any of the DS goes down, then MDS will handle those I/O's. Hereafter, all the subsequent NFS clients need to use the same server for mounting that volume via pNFS. i.e more than one MDS for a volume is not preferred pNFS support is only tested with distributed, replicated, or distribute-replicate volumes It is tested and verified with RHEL 6.5 , fedora 20, fedora 21 nfs clients. It is always better to use latest nfs-clients","title":"Points to be Noted"},{"location":"Administrator-Guide/Network-Configurations-Techniques/","text":"Network Configurations Techniques Bonding best practices Bonded network interfaces incorporate multiple physical interfaces into a single logical bonded interface, with a single IP addr. An N-way bonded interface can survive loss of N-1 physical interfaces, and performance can be improved in some cases. When to bond? Need high availability for network link Workload: sequential access to large files (most time spent reading/writing) Network throughput limit of client/server \\<\\< storage throughput limit 1 GbE (almost always) 10-Gbps links or faster -- for writes, replication doubles the load on the network and replicas are usually on different peers to which the client can transmit in parallel. LIMITATION: Bonding mode 6 doesn't improve throughput if network peers are not on the same VLAN. How to configure Bonding-howto Best bonding mode for Gluster client is mode 6 (balance-alb), this allows client to transmit writes in parallel on separate NICs much of the time. A peak throughput of 750 MB/s on writes from a single client was observed with bonding mode 6 on 2 10-GbE NICs with jumbo frames. That's 1.5 GB/s of network traffic. Another way to balance both transmit and receive traffic is bonding mode 4 (802.3ad) but this requires switch configuration (trunking commands) Still another way to load balance is bonding mode 2 (balance-xor) with option \"xmit_hash_policy=layer3+4\". The bonding modes 6 and 2 will not improve single-connection throughput, but improve aggregate throughput across all connections. Jumbo frames Jumbo frames are Ethernet (or Infiniband) frames with size greater than the default of 1500 bytes (Infiniband default is around 2000 bytes). Increasing frame size reduces load on operating system and hardware, which must process interrupts and protocol messages per frame. When to configure? Any network faster than 1-GbE Workload is sequential large-file reads/writes LIMITATION: Requires all network switches in VLAN must be configured to handle jumbo frames, do not configure otherwise. How to configure? Edit network interface file at /etc/sysconfig/network-scripts/ifcfg-your-interface Ethernet (on ixgbe driver): add \"MTU=9000\" (MTU means \"maximum transfer unit\") record to network interface file Infiniband (on mlx4 driver): add \"CONNECTED_MODE=yes\" and \"MTU=65520\" records to network interface file ifdown your-interface; ifup your-interface Test with \"ping -s 16384 other-host-on-VLAN\" Switch requires max frame size larger than MTU because of protocol headers, usually 9216 bytes Configuring a backend network for storage This method lets you add network capacity for multi-protocol sites by segregating traffic for different protocols on different network interfaces. This method can lower latency and improve throughput. For example, this method can keep self-heal and rebalancing traffic from competing with non-Gluster client traffic for a network interface, and will better support multi-stream I/O. When to configure? For non-Gluster services such as NFS, Swift (REST), CIFS being provided on Gluster servers. It will not help Gluster clients (external nodes with Gluster mountpoints on them). Network port is over-utilized. How to configure? Most network cards have multiple ports on them -- make port 1 the non-Gluster port and port 2 the Gluster port. Separate Gluster ports onto a separate VLAN from non-Gluster ports, to simplify configuration.","title":"Network Configuration Techniques"},{"location":"Administrator-Guide/Network-Configurations-Techniques/#network-configurations-techniques","text":"","title":"Network Configurations Techniques"},{"location":"Administrator-Guide/Network-Configurations-Techniques/#bonding-best-practices","text":"Bonded network interfaces incorporate multiple physical interfaces into a single logical bonded interface, with a single IP addr. An N-way bonded interface can survive loss of N-1 physical interfaces, and performance can be improved in some cases.","title":"Bonding best practices"},{"location":"Administrator-Guide/Network-Configurations-Techniques/#when-to-bond","text":"Need high availability for network link Workload: sequential access to large files (most time spent reading/writing) Network throughput limit of client/server \\<\\< storage throughput limit 1 GbE (almost always) 10-Gbps links or faster -- for writes, replication doubles the load on the network and replicas are usually on different peers to which the client can transmit in parallel. LIMITATION: Bonding mode 6 doesn't improve throughput if network peers are not on the same VLAN.","title":"When to bond?"},{"location":"Administrator-Guide/Network-Configurations-Techniques/#how-to-configure","text":"Bonding-howto Best bonding mode for Gluster client is mode 6 (balance-alb), this allows client to transmit writes in parallel on separate NICs much of the time. A peak throughput of 750 MB/s on writes from a single client was observed with bonding mode 6 on 2 10-GbE NICs with jumbo frames. That's 1.5 GB/s of network traffic. Another way to balance both transmit and receive traffic is bonding mode 4 (802.3ad) but this requires switch configuration (trunking commands) Still another way to load balance is bonding mode 2 (balance-xor) with option \"xmit_hash_policy=layer3+4\". The bonding modes 6 and 2 will not improve single-connection throughput, but improve aggregate throughput across all connections.","title":"How to configure"},{"location":"Administrator-Guide/Network-Configurations-Techniques/#jumbo-frames","text":"Jumbo frames are Ethernet (or Infiniband) frames with size greater than the default of 1500 bytes (Infiniband default is around 2000 bytes). Increasing frame size reduces load on operating system and hardware, which must process interrupts and protocol messages per frame.","title":"Jumbo frames"},{"location":"Administrator-Guide/Network-Configurations-Techniques/#when-to-configure","text":"Any network faster than 1-GbE Workload is sequential large-file reads/writes LIMITATION: Requires all network switches in VLAN must be configured to handle jumbo frames, do not configure otherwise.","title":"When to configure?"},{"location":"Administrator-Guide/Network-Configurations-Techniques/#how-to-configure_1","text":"Edit network interface file at /etc/sysconfig/network-scripts/ifcfg-your-interface Ethernet (on ixgbe driver): add \"MTU=9000\" (MTU means \"maximum transfer unit\") record to network interface file Infiniband (on mlx4 driver): add \"CONNECTED_MODE=yes\" and \"MTU=65520\" records to network interface file ifdown your-interface; ifup your-interface Test with \"ping -s 16384 other-host-on-VLAN\" Switch requires max frame size larger than MTU because of protocol headers, usually 9216 bytes","title":"How to configure?"},{"location":"Administrator-Guide/Network-Configurations-Techniques/#configuring-a-backend-network-for-storage","text":"This method lets you add network capacity for multi-protocol sites by segregating traffic for different protocols on different network interfaces. This method can lower latency and improve throughput. For example, this method can keep self-heal and rebalancing traffic from competing with non-Gluster client traffic for a network interface, and will better support multi-stream I/O.","title":"Configuring a backend network for storage"},{"location":"Administrator-Guide/Network-Configurations-Techniques/#when-to-configure_1","text":"For non-Gluster services such as NFS, Swift (REST), CIFS being provided on Gluster servers. It will not help Gluster clients (external nodes with Gluster mountpoints on them). Network port is over-utilized.","title":"When to configure?"},{"location":"Administrator-Guide/Network-Configurations-Techniques/#how-to-configure_2","text":"Most network cards have multiple ports on them -- make port 1 the non-Gluster port and port 2 the Gluster port. Separate Gluster ports onto a separate VLAN from non-Gluster ports, to simplify configuration.","title":"How to configure?"},{"location":"Administrator-Guide/Object-Storage/","text":"SwiftOnFile SwiftOnFile project enables GlusterFS volume to be used as backend for Openstack Swift - a distributed object store. This allows objects PUT over Swift's RESTful API to be accessed as files over filesystem interface and vice versa i.e files created over filesystem interface (NFS/FUSE/native) can be accessed as objects over Swift's RESTful API. SwiftOnFile project was formerly known as gluster-swift and also as UFO (Unified File and Object) before that. More information about SwiftOnFile can be found here . There are differences in working of gluster-swift (now obsolete) and swiftonfile projects. The older gluster-swift code and relevant documentation can be found in icehouse branch of swiftonfile repo. SwiftOnFile vs gluster-swift Gluster-Swift SwiftOnFile One GlusterFS volume maps to and stores only one Swift account. Mountpoint Hierarchy: container/object One GlusterFS volume or XFS partition can have multiple accounts. Mountpoint Hierarchy: acc/container/object Over-rides account server, container server and object server. We need to keep in sync with upstream Swift and often may need code changes or workarounds to support new Swift features Implements only object-server. Very less need to catch-up to Swift as new features at proxy,container and account level would very likely be compatible with SwiftOnFile as it's just a storage policy. Does not use DBs for accounts and container.A container listing involves a filesystem crawl.A HEAD on account/container gives inaccurate or stale results without FS crawl. Uses Swift's DBs to store account and container information. An account or container listing does not involve FS crawl. Accurate info on HEAD to account/container \u2013 ability to support account quotas. GET on a container and account lists actual files in filesystem. GET on a container and account only lists objects PUT over Swift. Files created over filesystem interface do not appear in container and object listings. Standalone deployment required and does not integrate with existing Swift cluster. Integrates with any existing Swift deployment as a Storage Policy.","title":"Object Storage"},{"location":"Administrator-Guide/Object-Storage/#swiftonfile","text":"SwiftOnFile project enables GlusterFS volume to be used as backend for Openstack Swift - a distributed object store. This allows objects PUT over Swift's RESTful API to be accessed as files over filesystem interface and vice versa i.e files created over filesystem interface (NFS/FUSE/native) can be accessed as objects over Swift's RESTful API. SwiftOnFile project was formerly known as gluster-swift and also as UFO (Unified File and Object) before that. More information about SwiftOnFile can be found here . There are differences in working of gluster-swift (now obsolete) and swiftonfile projects. The older gluster-swift code and relevant documentation can be found in icehouse branch of swiftonfile repo.","title":"SwiftOnFile"},{"location":"Administrator-Guide/Object-Storage/#swiftonfile-vs-gluster-swift","text":"Gluster-Swift SwiftOnFile One GlusterFS volume maps to and stores only one Swift account. Mountpoint Hierarchy: container/object One GlusterFS volume or XFS partition can have multiple accounts. Mountpoint Hierarchy: acc/container/object Over-rides account server, container server and object server. We need to keep in sync with upstream Swift and often may need code changes or workarounds to support new Swift features Implements only object-server. Very less need to catch-up to Swift as new features at proxy,container and account level would very likely be compatible with SwiftOnFile as it's just a storage policy. Does not use DBs for accounts and container.A container listing involves a filesystem crawl.A HEAD on account/container gives inaccurate or stale results without FS crawl. Uses Swift's DBs to store account and container information. An account or container listing does not involve FS crawl. Accurate info on HEAD to account/container \u2013 ability to support account quotas. GET on a container and account lists actual files in filesystem. GET on a container and account only lists objects PUT over Swift. Files created over filesystem interface do not appear in container and object listings. Standalone deployment required and does not integrate with existing Swift cluster. Integrates with any existing Swift deployment as a Storage Policy.","title":"SwiftOnFile vs gluster-swift"},{"location":"Administrator-Guide/Performance-Testing/","text":"Gluster performance testing Once you have created a Gluster volume, you need to verify that it has adequate performance for your application, and if it does not, you need a way to isolate the root cause of the problem. There are two kinds of workloads: synthetic - run a test program such as ones below application - run existing application Profiling tools Ideally it's best to use the actual application that you want to run on Gluster, but applications often don't tell the sysadmin much about where the performance problems are, particularly latency (response-time) problems. So there are non-invasive profiling tools built into Gluster that can measure performance as seen by the application, without changing the application. Gluster profiling methods at present are based on the io-stats translator, and include: client-side profiling - instrument a Gluster mountpoint or libgfapi process to sample profiling data. In this case, the io-stats translator is at the \"top\" of the translator stack, so the profile data truly represents what the application (or FUSE mountpoint) is asking Gluster to do. For example, a single application write is counted once as a WRITE FOP (file operation) call, and the latency for that WRITE FOP includes latency of the data replication done by the AFR translator lower in the stack. server-side profiling - this is done using the \"gluster volume profile\" command (and \"gluster volume top\" can be used to identify particular hot files in use as well). Server-side profiling can measure the throughput of an entire Gluster volume over time, and can measure server-side latencies. However, it does not incorporate network or client-side latencies. It is also hard to infer application behavior because of client-side translators that alter the I/O workload (examples: erasure coding, cache tiering). In short, use client-side profiling for understanding \"why is my application unresponsive\"? and use server-side profiling for understanding how busy your Gluster volume is, what kind of workload is being applied to it (i.e. is it mostly-read? is it small-file?), and how well the I/O load is spread across the volume. client-side profiling To run client-side profiling, gluster volume profile your-volume start setfattr -n trusted.io-stats-dump -v io-stats-pre.txt /your/mountpoint This will generate the specified file ( /var/run/gluster/io-stats-pre.txt ) on the client. A script like gvp-client.sh can automate collection of this data. TBS: what the different FOPs are and what they mean. server-side profiling To run it: gluster volume profile your-volume start repeat this command periodically: gluster volume profile your-volume info gluster volume profile your-volume stop A script like gvp.sh can help you automate this procedure. Scripts to post-process this data are in development now, let us know what you need and what would be a useful format for presenting the data. Testing tools In this section, we suggest some basic workload tests that can be used to measure Gluster performance in an application-independent way for a wide variety of POSIX-like operating systems and runtime environments. We then provide some terminology and conceptual framework for interpreting these results. The tools that we suggest here are designed to run in a distributed filesystem. This is still a relatively rare attribute for filesystem benchmarks, even now! There is a much larger set of benchmarks available that can be run from a single system. While single-system results are important, they are far from a definitive measure of the performance capabilities of a distributed filesystem. fio - for large file I/O tests. smallfile - for pure-workload small-file tests iozone - for pure-workload large-file tests parallel-libgfapi - for pure-workload libgfapi tests The \"netmist\" mixed-workload generator of SPECsfs2014 may be suitable in some cases, but is not technically an open-source tool. This tool was written by Don Capps, who was an author of iozone. fio fio is extremely powerful and is easily installed from traditional distros, unlike iozone, and has increasingly powerful distributed test capabilities described in its --client parameter upstream as of May 2015. To use this mode, start by launching an fio \"server\" instance on each workload generator host using: fio --server --daemonize=/var/run/fio-svr.pid And make sure your firewall allows port 8765 through for it. You can now run tests on sets of hosts using syntax like: fio --client=workload-generator.list --output-format=json my-workload.fiojob You can also use it for distributed testing, however, by launching fio instances on separate hosts, taking care to start all fio instances as close to the same time as possible, limiting per-thread throughput, and specifying the run duration rather than the amount of data, so that all fio instances end at around the same time. You can then aggregate the fio results from different hosts to get a meaningful aggregate result. fio also has different I/O engines, in particular Huamin Chen authored the libgfapi engine for fio so that you can use fio to test Gluster performance without using FUSE. Limitations of fio in distributed mode: stonewalling - fio calculates throughput based on when the last thread finishes a test run. In contrast, iozone calculates throughput by default based on when the FIRST thread finishes the workload. This can lead to (deceptively?) higher throughput results for iozone, since there are inevitably some \"straggler\" threads limping to the finish line later than others. It is possible in some cases to overcome this limitation by specifying a time limit for the test. This works well for random I/O tests, where typically you do not want to read/write the entire file/device anyway. inaccuracy when response times > 1 sec - at least in some cases fio has reported excessively high IOPS when fio threads encounter response times much greater than 1 second, this can happen for distributed storage when there is unfairness in the implementation. io engines are not integrated. smallfile Distributed I/O Benchmark Smallfile is a python-based small-file distributed POSIX workload generator which can be used to quickly measure performance for a variety of metadata-intensive workloads across an entire cluster. It has no dependencies on any specific filesystem or implementation AFAIK. It runs on Linux, Windows and should work on most Unixes too. It is intended to complement use of iozone benchmark for measuring performance of large-file workloads, and borrows certain concepts from iozone and Ric Wheeler's fs_mark. It was developed by Ben England starting in March 2009, and is now open-source (Apache License v2). Here is a typical simple sequence of tests where files laid down in an initial create test are then used in subsequent tests. There are many more smallfile operation types than these 5 (see doc), but these are the most commonly used ones. SMF=\"./smallfile_cli.py --top /mnt/glusterfs/smf --host-set h1,h2,h3,h4 --threads 8 --file-size 4 --files 10000 --response-times Y \" $SMF --operation create for s in $SERVERS ; do ssh $h 'echo 3 > /proc/sys/vm/drop_caches' ; done $SMF --operation read $SMF --operation append $SMF --operation rename $SMF --operation delete iozone This tool has limitations but does distributed testing well using -+m option (below). The \"-a\" option for automated testing of all use cases is discouraged, because: this does not allow you to drop the read cache in server before a test. most of the data points being measured will be irrelevant to the problem you are solving. Single-thread testing is an important use case, but to fully utilize the available hardware you typically need to do multi-thread and even multi-host testing. Consider using \"-c -e\" options to measure the time it takes for data to reach persistent storage. \"-C\" option lets you see how much each thread participated in the test. \"-+n\" allows you to save time by skipping re-read and re-write tests. \"-w\" option tells iozone not to delete any files that it accessed, so that subsequent tests can use them. Specify these options with each test: -i -- test type, 0=write, 1=read, 2=random read/write -r -- data transfer size -- allows you to simulate I/O size used by application -s -- per-thread file size -- choose this to be large enough for the system to reach steady state (typically multiple GB needed) -t -- number of threads -- how many subprocesses will be concurrently issuing I/O requests -F -- list of files -- what files to write/read. If you do not specify then the filenames iozone.DUMMY.* will be used in the default directory. Example of an 8-thread sequential write test with 64-KB transfer size and file size of 1 GB to shared Gluster mountpoint directory /mnt/glusterfs , including time to fsync() and close() the files in the throughput calculation: iozone -w -c -e -i 0 -+n -C -r 64k -s 1g -t 8 -F /mnt/glusterfs/f{0,1,2,3,4,5,6,7,8}.ioz WARNING: random I/O testing in iozone is heavily restricted by the iozone constraint that it must randomly read then randomly write the entire file! This is not what we want - instead it should randomly read/write for some fraction of file size or time duration, allowing us to spread out more on the disk while not waiting too long for test to finish. This is why fio (below) is the preferred test tool for random I/O workloads. Distributed testing is a strength of the iozone utility, but this requires use of \"-+m\" option in place of \"-F\" option. The configuration file passed with \"-+m\" option contains a series of records that look like this: hostname directory iozone-pathname Where hostname is a host name or IP address of a test driver machine that iozone can use, directory is the pathname of a directory to use within that host, and iozone-pathname is the full pathname of the iozone executable to use on that host. Be sure that every target host can resolve the hostname of host where the iozone command was run. All target hosts must permit password-less ssh access from the host running the command. For example: (Here, my-ip-address refers to the machine from where the iozone is being run) export RSH=ssh iozone -+m ioz.cfg -+h my-ip-address -w -c -e -i 0 -+n -C -r 64k -s 1g -t 4 And the file ioz.cfg contains these records (where /mnt/glusterfs is the Gluster mountpoint on each test machine and test-client-ip is the IP address of a client). Also note that, Each record in the file is a thread in IOZone terminology. Since we have defined the number of threads to be 4 in the above example, we have four records(threads) for a single client. test-client-ip /mnt/glusterfs /usr/local/bin/iozone test-client-ip /mnt/glusterfs /usr/local/bin/iozone test-client-ip /mnt/glusterfs /usr/local/bin/iozone test-client-ip /mnt/glusterfs /usr/local/bin/iozone Restriction: Since iozone uses non-privileged ports it may be necessary to temporarily shut down or alter iptables on some/all of the hosts. Secondary machines must support password-less access from Primary machine via ssh. Note that the -+h option is undocumented but it tells the secondary host what IP address to use so that the secondary does not have to be able to resolve the hostname of the test driver. my-ip-address is the IP address that the secondary should connect to in order to report results back to the host. This need not be the same as the host's hostname. Typically you run the sequential write test first to lay down the file, drop cache on the servers (and clients if necessary), do the sequential read test, drop cache, do random I/O test if desired. Using above example: export RSH=ssh IOZ=\"iozone -+m ioz.cfg -+h my-ip-address -w -C -c -e -r 64k -+n \" hosts=\"`awk '{ print $1 }' ioz.cfg`\" $IOZ -i 0 -s 1g -t 4`\\ for n in $hosts $servers ; do \\ ssh $n 'sync; echo 1 > /proc/sys/vm/drop_caches' ; done $IOZ -i 1 -s 1g -t 4 for n in $hosts $servers ; do \\ ssh $n 'sync; echo 1 > /proc/sys/vm/drop_caches' ; done $IOZ -i 2 -s 1g -t 4 If you use client with buffered I/O (the default), drop cache on the client machines first, then the server machines also as shown above. parallel-libgfapi This test exercises Gluster performance using the libgfapi API, bypassing FUSE - no mountpoints are used. Available here . To use it, you edit the script parameters in parallel_gfapi_test.sh script - all of them are above the comment \"NO EDITABLE PARAMETERS BELOW THIS LINE\". These include such things as the Gluster volume name, a host serving that volume, number of files, etc. You then make sure that the gfapi_perf_test executable is distributed to the client machines at the specified directory, and then run the script. The script starts all libgfapi workload generator processes in parallel in such a way that they all start the test at the same time. It waits until they all complete, and then it collects and aggregates the results for you. Note that libgfapi processes consume one socket per brick, so in Gluster volumes with high brick counts, there can be constraints on the number of libgfapi processes that can run concurrently. Specifically, each host can only support up to about 30000 concurrent TCP ports. You may need to adjust \"ulimit -n\" parameter (see /etc/security/limits.conf \"nofile\" parameter for persistent tuning). Object Store tools COSBench was developed by Intel employees and is very useful for both Swift and S3 workload generation. ssbench is part of OpenStack Swift toolset and is command-line tool with a workload definition file format. Workload An application can be as simple as writing some files, or it can be as complex as running a cloud on top of Gluster. But all applications have performance requirements, whether the users are aware of them or not, and if these requirements aren't met, the system as a whole is not functional from the user's perspective. The activities that the application spends most of its time doing with Gluster are called the \"workload\" below. For the Gluster filesystem, the \"workload\" consists of the filesystem requests being delivered to Gluster by the application. There are two ways to look at workload: top-down - what is the application trying to get the filesystem to do? bottom-up - what requests is the application actually generating to the filesystem? data vs metadata In this page we frequently refer to \"large-file\" or \"small-file\" workloads. But what do we mean by the terms \"large-file\" or \"small-file\"? \"large-file\" is a deliberately vague but descriptive term that refers to workloads where most of the application time is spent reading/writing the file. This is in contrast to a \"small-file\" workload, where most of the application's time is spent opening/closing the file or accessing metadata about the file. Metadata means \"data about data\", so it is information that describes the state of the file, rather than the contents of the file. For example, a filename is a type of metadata, as are directories and extended attributes. Top-down workload analysis Often this is what users will be able to help you with -- for example, a workload might consist of ingesting a billion .mp3 files. Typical questions that need to be answered (approximately) are: what is file size distribution? Averages are often not enough - file size distributions can be bi-modal (i.e. consist mostly of the very large and very small file sizes). TBS: provide pointers to scripts that can collect this. what fraction of file accesses are reads vs writes? how cache-friendly is the workload? Do the same files get read repeatedly by different Gluster clients, or by different processes/threads on these clients? for large-file workloads, what fraction of accesses are sequential/random? Sequential file access means that the application thread reads/writes the file from start to finish in byte offset order, and random file access is the exact opposite -- the thread may read/write from any offset at any time. Virtual machine disk images are typically accessed randomly, since the VM's filesystem is embedded in a Gluster file. Why do these questions matter? For example, if you have a large-file sequential read workload, network configuration + Gluster and Linux readahead is important. If you have a small-file workload, storage configuration is important, and so on. You will not know what tuning is appropriate for Gluster unless you have a basic understanding the workload. Bottom-up analysis Even a complex application may have a very simple workload from the point of view of the filesystem servicing its requests. If you don't know what your application spends its time doing, you can start by running the \"gluster volume profile\" and \"gluster volume top\" commands. These extremely useful tools will help you understand both the workload and the bottlenecks which are limiting performance of that workload. TBS: links to documentation for these tools and scripts that reduce the data to usable form. Configuration There are 4 basic hardware dimensions to a Gluster server, listed here in order of importance: network - possibly the most important hardware component of a Gluster site access protocol - what kind of client is used to get to the files/objects? storage - this is absolutely critical to get right up front cpu - on client, look for hot threads (see below) memory - can impact performance of read-intensive, cacheable workloads network testing Network configuration has a huge impact on performance of distributed storage, but is often not given the attention it deserves during the planning and installation phases of the cluster lifecycle. Fortunately, network configuration can be enhanced significantly, often without additional hardware. To measure network performance, consider use of a netperf-based script. The purpose of these two tools is to characterize the capacity of your entire network infrastructure to support the desired level of traffic induced by distributed storage, using multiple network connections in parallel. The latter script is probably the most realistic network workload for distributed storage. The two most common hardware problems impacting distributed storage are, not surprisingly, disk drive failures and network failures. Some of these failures do not cause hard errors, but instead cause performance degradation. For example, with a bonded network interface containing two physical network interfaces, if one of the physical interfaces fails (either port on NIC/switch, or cable), then the bonded interface will stay up, but will have less performance (how much less depends on the bonding mode). Another error would be failure of an 10-GbE Ethernet interface to autonegotiate speed to 10-Gbps -- sometimes network interfaces auto-negotiate to 1-Gbps instead. If the TCP connection is experiencing a high rate of packet loss or is not tuned correctly, it may not reach the full network speed supported by the hardware. So why run parallel netperf sessions instead of just one? There are a variety of network performance problems relating to network topology (the way in which hosts are interconnected), particularly network switch and router topology, that only manifest when several pairs of hosts are attempting to transmit traffic across the same shared resource, which could be a trunk connecting top-of-rack switches or a blade-based switch with insufficient bandwidth to switch backplane, for example. Individual netperf/iperf sessions will not find these problems, but this script will. This test can be used to simulate flow of data through a distributed filesystem, for example. If you want to simulate 4 Gluster clients, call them c1 through c4, writing large files to a set of 2 servers, call them s1 and s2, you can specify these (sender, receiver) pairs: (c1,s1), (c2, s2), (c3, s1), (c4, s2) If on the other hand you want to simulate reads, you can use these (sender, receiver) pairs: (s1, c1), (s2, c2), (s1, c3), (s2, c4) To simulate a mixed read-write workload, use both sets of pairs: (c1,s1), (c2, s2), (c3, s1), (c4, s2), (s1, c1), (s2, c2), (s1, c3), (s2, c4) More complicated flows can model behavior of non-native protocols, where a cluster node acts as a proxy server- it is a server (for non-native protocol) and a client (for native protocol). For example, such protocols often induce full-duplex traffic which can stress the network differently than unidirectional in/out traffic. For example, try adding this set of flows to preceding flow: (s1, s2),.(s2, s3),.(s3, s4),.(s4, s1) The comments at the top of the script describe the input syntax, but here are some suggestions on how to best utilize it. You typically run this script from a head node or test driver that has password-less ssh access to the set of machines being tested. The hosts running the test do not need ssh access to each other -- they only have to allow password-less ssh access from the head node. The script does not rely on root privileges, so you can run it from a non-root account. Just create a public key on the head node in the right account (usually in \\$HOME/.ssh/id_rsa.pub ) and then append this public key to \\$HOME/.ssh/authorized_keys on each host participating in the test. We input senders and receivers using separate text files, 1 host per line. For pair (sender[j], receiver[j]), you get sender[j] from line j in the sender file, and receiver[j] from line j in the receiver file. You have to use the IP address/name that corresponds to the interface you want to test, and you have to be able to ssh to each host from the head node using this interface. Results There are 3 basic forms of performance results, not in order of importance: throughput -- how much work is done in a unit of time? Best metrics typically are workload-dependent: for large-file random: IOPS for large-file sequential: MB/s for small-file: files/sec response time -- IMPORTANT, how long does it take for filesystem request to complete? utilization -- how busy is the hardware while the workload is running? scalability -- can we linearly scale throughput without sacrificing response time as we add servers to a Gluster volume? Typically throughput results get the most attention, but in a distributed-storage environment, the hardest goal to achieve may well be CONSISTENTLY LOW RESPONSE TIME, not throughput. While there are non-interactive workloads where response time does not matter as much, you should pay attention to response time in any situation where a user has to directly interact with the filesystem. Tuning the filesystem to achieve the absolute highest throughput can result in a filesystem that is unusable because of high response time. Unless you are in a benchmarking situation, you want to achieve a balance of good throughput and response time. Typically an interactive user wants to see a response time under 5 seconds always, with most response times much lower than this. To keep response times under control (including system management!), you do not want any hardware component to run at maximum utilization, typically 60-80% utilization is a good peak utilization target. On the other hand, to avoid wasting hardware, you want all of the hardware to be utilized to some extent.","title":"Performance Testing"},{"location":"Administrator-Guide/Performance-Testing/#gluster-performance-testing","text":"Once you have created a Gluster volume, you need to verify that it has adequate performance for your application, and if it does not, you need a way to isolate the root cause of the problem. There are two kinds of workloads: synthetic - run a test program such as ones below application - run existing application","title":"Gluster performance testing"},{"location":"Administrator-Guide/Performance-Testing/#profiling-tools","text":"Ideally it's best to use the actual application that you want to run on Gluster, but applications often don't tell the sysadmin much about where the performance problems are, particularly latency (response-time) problems. So there are non-invasive profiling tools built into Gluster that can measure performance as seen by the application, without changing the application. Gluster profiling methods at present are based on the io-stats translator, and include: client-side profiling - instrument a Gluster mountpoint or libgfapi process to sample profiling data. In this case, the io-stats translator is at the \"top\" of the translator stack, so the profile data truly represents what the application (or FUSE mountpoint) is asking Gluster to do. For example, a single application write is counted once as a WRITE FOP (file operation) call, and the latency for that WRITE FOP includes latency of the data replication done by the AFR translator lower in the stack. server-side profiling - this is done using the \"gluster volume profile\" command (and \"gluster volume top\" can be used to identify particular hot files in use as well). Server-side profiling can measure the throughput of an entire Gluster volume over time, and can measure server-side latencies. However, it does not incorporate network or client-side latencies. It is also hard to infer application behavior because of client-side translators that alter the I/O workload (examples: erasure coding, cache tiering). In short, use client-side profiling for understanding \"why is my application unresponsive\"? and use server-side profiling for understanding how busy your Gluster volume is, what kind of workload is being applied to it (i.e. is it mostly-read? is it small-file?), and how well the I/O load is spread across the volume.","title":"Profiling tools"},{"location":"Administrator-Guide/Performance-Testing/#client-side-profiling","text":"To run client-side profiling, gluster volume profile your-volume start setfattr -n trusted.io-stats-dump -v io-stats-pre.txt /your/mountpoint This will generate the specified file ( /var/run/gluster/io-stats-pre.txt ) on the client. A script like gvp-client.sh can automate collection of this data. TBS: what the different FOPs are and what they mean.","title":"client-side profiling"},{"location":"Administrator-Guide/Performance-Testing/#server-side-profiling","text":"To run it: gluster volume profile your-volume start repeat this command periodically: gluster volume profile your-volume info gluster volume profile your-volume stop A script like gvp.sh can help you automate this procedure. Scripts to post-process this data are in development now, let us know what you need and what would be a useful format for presenting the data.","title":"server-side profiling"},{"location":"Administrator-Guide/Performance-Testing/#testing-tools","text":"In this section, we suggest some basic workload tests that can be used to measure Gluster performance in an application-independent way for a wide variety of POSIX-like operating systems and runtime environments. We then provide some terminology and conceptual framework for interpreting these results. The tools that we suggest here are designed to run in a distributed filesystem. This is still a relatively rare attribute for filesystem benchmarks, even now! There is a much larger set of benchmarks available that can be run from a single system. While single-system results are important, they are far from a definitive measure of the performance capabilities of a distributed filesystem. fio - for large file I/O tests. smallfile - for pure-workload small-file tests iozone - for pure-workload large-file tests parallel-libgfapi - for pure-workload libgfapi tests The \"netmist\" mixed-workload generator of SPECsfs2014 may be suitable in some cases, but is not technically an open-source tool. This tool was written by Don Capps, who was an author of iozone.","title":"Testing tools"},{"location":"Administrator-Guide/Performance-Testing/#fio","text":"fio is extremely powerful and is easily installed from traditional distros, unlike iozone, and has increasingly powerful distributed test capabilities described in its --client parameter upstream as of May 2015. To use this mode, start by launching an fio \"server\" instance on each workload generator host using: fio --server --daemonize=/var/run/fio-svr.pid And make sure your firewall allows port 8765 through for it. You can now run tests on sets of hosts using syntax like: fio --client=workload-generator.list --output-format=json my-workload.fiojob You can also use it for distributed testing, however, by launching fio instances on separate hosts, taking care to start all fio instances as close to the same time as possible, limiting per-thread throughput, and specifying the run duration rather than the amount of data, so that all fio instances end at around the same time. You can then aggregate the fio results from different hosts to get a meaningful aggregate result. fio also has different I/O engines, in particular Huamin Chen authored the libgfapi engine for fio so that you can use fio to test Gluster performance without using FUSE. Limitations of fio in distributed mode: stonewalling - fio calculates throughput based on when the last thread finishes a test run. In contrast, iozone calculates throughput by default based on when the FIRST thread finishes the workload. This can lead to (deceptively?) higher throughput results for iozone, since there are inevitably some \"straggler\" threads limping to the finish line later than others. It is possible in some cases to overcome this limitation by specifying a time limit for the test. This works well for random I/O tests, where typically you do not want to read/write the entire file/device anyway. inaccuracy when response times > 1 sec - at least in some cases fio has reported excessively high IOPS when fio threads encounter response times much greater than 1 second, this can happen for distributed storage when there is unfairness in the implementation. io engines are not integrated.","title":"fio"},{"location":"Administrator-Guide/Performance-Testing/#smallfile-distributed-io-benchmark","text":"Smallfile is a python-based small-file distributed POSIX workload generator which can be used to quickly measure performance for a variety of metadata-intensive workloads across an entire cluster. It has no dependencies on any specific filesystem or implementation AFAIK. It runs on Linux, Windows and should work on most Unixes too. It is intended to complement use of iozone benchmark for measuring performance of large-file workloads, and borrows certain concepts from iozone and Ric Wheeler's fs_mark. It was developed by Ben England starting in March 2009, and is now open-source (Apache License v2). Here is a typical simple sequence of tests where files laid down in an initial create test are then used in subsequent tests. There are many more smallfile operation types than these 5 (see doc), but these are the most commonly used ones. SMF=\"./smallfile_cli.py --top /mnt/glusterfs/smf --host-set h1,h2,h3,h4 --threads 8 --file-size 4 --files 10000 --response-times Y \" $SMF --operation create for s in $SERVERS ; do ssh $h 'echo 3 > /proc/sys/vm/drop_caches' ; done $SMF --operation read $SMF --operation append $SMF --operation rename $SMF --operation delete","title":"smallfile Distributed I/O Benchmark"},{"location":"Administrator-Guide/Performance-Testing/#iozone","text":"This tool has limitations but does distributed testing well using -+m option (below). The \"-a\" option for automated testing of all use cases is discouraged, because: this does not allow you to drop the read cache in server before a test. most of the data points being measured will be irrelevant to the problem you are solving. Single-thread testing is an important use case, but to fully utilize the available hardware you typically need to do multi-thread and even multi-host testing. Consider using \"-c -e\" options to measure the time it takes for data to reach persistent storage. \"-C\" option lets you see how much each thread participated in the test. \"-+n\" allows you to save time by skipping re-read and re-write tests. \"-w\" option tells iozone not to delete any files that it accessed, so that subsequent tests can use them. Specify these options with each test: -i -- test type, 0=write, 1=read, 2=random read/write -r -- data transfer size -- allows you to simulate I/O size used by application -s -- per-thread file size -- choose this to be large enough for the system to reach steady state (typically multiple GB needed) -t -- number of threads -- how many subprocesses will be concurrently issuing I/O requests -F -- list of files -- what files to write/read. If you do not specify then the filenames iozone.DUMMY.* will be used in the default directory. Example of an 8-thread sequential write test with 64-KB transfer size and file size of 1 GB to shared Gluster mountpoint directory /mnt/glusterfs , including time to fsync() and close() the files in the throughput calculation: iozone -w -c -e -i 0 -+n -C -r 64k -s 1g -t 8 -F /mnt/glusterfs/f{0,1,2,3,4,5,6,7,8}.ioz WARNING: random I/O testing in iozone is heavily restricted by the iozone constraint that it must randomly read then randomly write the entire file! This is not what we want - instead it should randomly read/write for some fraction of file size or time duration, allowing us to spread out more on the disk while not waiting too long for test to finish. This is why fio (below) is the preferred test tool for random I/O workloads. Distributed testing is a strength of the iozone utility, but this requires use of \"-+m\" option in place of \"-F\" option. The configuration file passed with \"-+m\" option contains a series of records that look like this: hostname directory iozone-pathname Where hostname is a host name or IP address of a test driver machine that iozone can use, directory is the pathname of a directory to use within that host, and iozone-pathname is the full pathname of the iozone executable to use on that host. Be sure that every target host can resolve the hostname of host where the iozone command was run. All target hosts must permit password-less ssh access from the host running the command. For example: (Here, my-ip-address refers to the machine from where the iozone is being run) export RSH=ssh iozone -+m ioz.cfg -+h my-ip-address -w -c -e -i 0 -+n -C -r 64k -s 1g -t 4 And the file ioz.cfg contains these records (where /mnt/glusterfs is the Gluster mountpoint on each test machine and test-client-ip is the IP address of a client). Also note that, Each record in the file is a thread in IOZone terminology. Since we have defined the number of threads to be 4 in the above example, we have four records(threads) for a single client. test-client-ip /mnt/glusterfs /usr/local/bin/iozone test-client-ip /mnt/glusterfs /usr/local/bin/iozone test-client-ip /mnt/glusterfs /usr/local/bin/iozone test-client-ip /mnt/glusterfs /usr/local/bin/iozone Restriction: Since iozone uses non-privileged ports it may be necessary to temporarily shut down or alter iptables on some/all of the hosts. Secondary machines must support password-less access from Primary machine via ssh. Note that the -+h option is undocumented but it tells the secondary host what IP address to use so that the secondary does not have to be able to resolve the hostname of the test driver. my-ip-address is the IP address that the secondary should connect to in order to report results back to the host. This need not be the same as the host's hostname. Typically you run the sequential write test first to lay down the file, drop cache on the servers (and clients if necessary), do the sequential read test, drop cache, do random I/O test if desired. Using above example: export RSH=ssh IOZ=\"iozone -+m ioz.cfg -+h my-ip-address -w -C -c -e -r 64k -+n \" hosts=\"`awk '{ print $1 }' ioz.cfg`\" $IOZ -i 0 -s 1g -t 4`\\ for n in $hosts $servers ; do \\ ssh $n 'sync; echo 1 > /proc/sys/vm/drop_caches' ; done $IOZ -i 1 -s 1g -t 4 for n in $hosts $servers ; do \\ ssh $n 'sync; echo 1 > /proc/sys/vm/drop_caches' ; done $IOZ -i 2 -s 1g -t 4 If you use client with buffered I/O (the default), drop cache on the client machines first, then the server machines also as shown above.","title":"iozone"},{"location":"Administrator-Guide/Performance-Testing/#parallel-libgfapi","text":"This test exercises Gluster performance using the libgfapi API, bypassing FUSE - no mountpoints are used. Available here . To use it, you edit the script parameters in parallel_gfapi_test.sh script - all of them are above the comment \"NO EDITABLE PARAMETERS BELOW THIS LINE\". These include such things as the Gluster volume name, a host serving that volume, number of files, etc. You then make sure that the gfapi_perf_test executable is distributed to the client machines at the specified directory, and then run the script. The script starts all libgfapi workload generator processes in parallel in such a way that they all start the test at the same time. It waits until they all complete, and then it collects and aggregates the results for you. Note that libgfapi processes consume one socket per brick, so in Gluster volumes with high brick counts, there can be constraints on the number of libgfapi processes that can run concurrently. Specifically, each host can only support up to about 30000 concurrent TCP ports. You may need to adjust \"ulimit -n\" parameter (see /etc/security/limits.conf \"nofile\" parameter for persistent tuning).","title":"parallel-libgfapi"},{"location":"Administrator-Guide/Performance-Testing/#object-store-tools","text":"COSBench was developed by Intel employees and is very useful for both Swift and S3 workload generation. ssbench is part of OpenStack Swift toolset and is command-line tool with a workload definition file format.","title":"Object Store tools"},{"location":"Administrator-Guide/Performance-Testing/#workload","text":"An application can be as simple as writing some files, or it can be as complex as running a cloud on top of Gluster. But all applications have performance requirements, whether the users are aware of them or not, and if these requirements aren't met, the system as a whole is not functional from the user's perspective. The activities that the application spends most of its time doing with Gluster are called the \"workload\" below. For the Gluster filesystem, the \"workload\" consists of the filesystem requests being delivered to Gluster by the application. There are two ways to look at workload: top-down - what is the application trying to get the filesystem to do? bottom-up - what requests is the application actually generating to the filesystem?","title":"Workload"},{"location":"Administrator-Guide/Performance-Testing/#data-vs-metadata","text":"In this page we frequently refer to \"large-file\" or \"small-file\" workloads. But what do we mean by the terms \"large-file\" or \"small-file\"? \"large-file\" is a deliberately vague but descriptive term that refers to workloads where most of the application time is spent reading/writing the file. This is in contrast to a \"small-file\" workload, where most of the application's time is spent opening/closing the file or accessing metadata about the file. Metadata means \"data about data\", so it is information that describes the state of the file, rather than the contents of the file. For example, a filename is a type of metadata, as are directories and extended attributes.","title":"data vs metadata"},{"location":"Administrator-Guide/Performance-Testing/#top-down-workload-analysis","text":"Often this is what users will be able to help you with -- for example, a workload might consist of ingesting a billion .mp3 files. Typical questions that need to be answered (approximately) are: what is file size distribution? Averages are often not enough - file size distributions can be bi-modal (i.e. consist mostly of the very large and very small file sizes). TBS: provide pointers to scripts that can collect this. what fraction of file accesses are reads vs writes? how cache-friendly is the workload? Do the same files get read repeatedly by different Gluster clients, or by different processes/threads on these clients? for large-file workloads, what fraction of accesses are sequential/random? Sequential file access means that the application thread reads/writes the file from start to finish in byte offset order, and random file access is the exact opposite -- the thread may read/write from any offset at any time. Virtual machine disk images are typically accessed randomly, since the VM's filesystem is embedded in a Gluster file. Why do these questions matter? For example, if you have a large-file sequential read workload, network configuration + Gluster and Linux readahead is important. If you have a small-file workload, storage configuration is important, and so on. You will not know what tuning is appropriate for Gluster unless you have a basic understanding the workload.","title":"Top-down workload analysis"},{"location":"Administrator-Guide/Performance-Testing/#bottom-up-analysis","text":"Even a complex application may have a very simple workload from the point of view of the filesystem servicing its requests. If you don't know what your application spends its time doing, you can start by running the \"gluster volume profile\" and \"gluster volume top\" commands. These extremely useful tools will help you understand both the workload and the bottlenecks which are limiting performance of that workload. TBS: links to documentation for these tools and scripts that reduce the data to usable form.","title":"Bottom-up analysis"},{"location":"Administrator-Guide/Performance-Testing/#configuration","text":"There are 4 basic hardware dimensions to a Gluster server, listed here in order of importance: network - possibly the most important hardware component of a Gluster site access protocol - what kind of client is used to get to the files/objects? storage - this is absolutely critical to get right up front cpu - on client, look for hot threads (see below) memory - can impact performance of read-intensive, cacheable workloads","title":"Configuration"},{"location":"Administrator-Guide/Performance-Testing/#network-testing","text":"Network configuration has a huge impact on performance of distributed storage, but is often not given the attention it deserves during the planning and installation phases of the cluster lifecycle. Fortunately, network configuration can be enhanced significantly, often without additional hardware. To measure network performance, consider use of a netperf-based script. The purpose of these two tools is to characterize the capacity of your entire network infrastructure to support the desired level of traffic induced by distributed storage, using multiple network connections in parallel. The latter script is probably the most realistic network workload for distributed storage. The two most common hardware problems impacting distributed storage are, not surprisingly, disk drive failures and network failures. Some of these failures do not cause hard errors, but instead cause performance degradation. For example, with a bonded network interface containing two physical network interfaces, if one of the physical interfaces fails (either port on NIC/switch, or cable), then the bonded interface will stay up, but will have less performance (how much less depends on the bonding mode). Another error would be failure of an 10-GbE Ethernet interface to autonegotiate speed to 10-Gbps -- sometimes network interfaces auto-negotiate to 1-Gbps instead. If the TCP connection is experiencing a high rate of packet loss or is not tuned correctly, it may not reach the full network speed supported by the hardware. So why run parallel netperf sessions instead of just one? There are a variety of network performance problems relating to network topology (the way in which hosts are interconnected), particularly network switch and router topology, that only manifest when several pairs of hosts are attempting to transmit traffic across the same shared resource, which could be a trunk connecting top-of-rack switches or a blade-based switch with insufficient bandwidth to switch backplane, for example. Individual netperf/iperf sessions will not find these problems, but this script will. This test can be used to simulate flow of data through a distributed filesystem, for example. If you want to simulate 4 Gluster clients, call them c1 through c4, writing large files to a set of 2 servers, call them s1 and s2, you can specify these (sender, receiver) pairs: (c1,s1), (c2, s2), (c3, s1), (c4, s2) If on the other hand you want to simulate reads, you can use these (sender, receiver) pairs: (s1, c1), (s2, c2), (s1, c3), (s2, c4) To simulate a mixed read-write workload, use both sets of pairs: (c1,s1), (c2, s2), (c3, s1), (c4, s2), (s1, c1), (s2, c2), (s1, c3), (s2, c4) More complicated flows can model behavior of non-native protocols, where a cluster node acts as a proxy server- it is a server (for non-native protocol) and a client (for native protocol). For example, such protocols often induce full-duplex traffic which can stress the network differently than unidirectional in/out traffic. For example, try adding this set of flows to preceding flow: (s1, s2),.(s2, s3),.(s3, s4),.(s4, s1) The comments at the top of the script describe the input syntax, but here are some suggestions on how to best utilize it. You typically run this script from a head node or test driver that has password-less ssh access to the set of machines being tested. The hosts running the test do not need ssh access to each other -- they only have to allow password-less ssh access from the head node. The script does not rely on root privileges, so you can run it from a non-root account. Just create a public key on the head node in the right account (usually in \\$HOME/.ssh/id_rsa.pub ) and then append this public key to \\$HOME/.ssh/authorized_keys on each host participating in the test. We input senders and receivers using separate text files, 1 host per line. For pair (sender[j], receiver[j]), you get sender[j] from line j in the sender file, and receiver[j] from line j in the receiver file. You have to use the IP address/name that corresponds to the interface you want to test, and you have to be able to ssh to each host from the head node using this interface.","title":"network testing"},{"location":"Administrator-Guide/Performance-Testing/#results","text":"There are 3 basic forms of performance results, not in order of importance: throughput -- how much work is done in a unit of time? Best metrics typically are workload-dependent: for large-file random: IOPS for large-file sequential: MB/s for small-file: files/sec response time -- IMPORTANT, how long does it take for filesystem request to complete? utilization -- how busy is the hardware while the workload is running? scalability -- can we linearly scale throughput without sacrificing response time as we add servers to a Gluster volume? Typically throughput results get the most attention, but in a distributed-storage environment, the hardest goal to achieve may well be CONSISTENTLY LOW RESPONSE TIME, not throughput. While there are non-interactive workloads where response time does not matter as much, you should pay attention to response time in any situation where a user has to directly interact with the filesystem. Tuning the filesystem to achieve the absolute highest throughput can result in a filesystem that is unusable because of high response time. Unless you are in a benchmarking situation, you want to achieve a balance of good throughput and response time. Typically an interactive user wants to see a response time under 5 seconds always, with most response times much lower than this. To keep response times under control (including system management!), you do not want any hardware component to run at maximum utilization, typically 60-80% utilization is a good peak utilization target. On the other hand, to avoid wasting hardware, you want all of the hardware to be utilized to some extent.","title":"Results"},{"location":"Administrator-Guide/Performance-Tuning/","text":"Performance tuning Enable Metadata cache Metadata caching improves performance in almost all the workloads, except for use cases with most of the workload accessing a file sumultaneously from multiple clients. 1. Execute the following command to enable metadata caching and cache invalidation: # gluster volume set <volname> group metadata-cache This group command enables caching of stat and xattr information of a file or directory. The caching is refreshed every 10 min, and cache-invalidation is enabled to ensure cache consistency. To increase the number of files that can be cached, execute the following command: # gluster volume set <volname> network.inode-lru-limit <n> n, is set to 50000. It can be increased if the number of active files in the volume is very high. Increasing this number increases the memory footprint of the brick processes. Execute the following command to enable samba specific metadata caching: # gluster volume set <volname> cache-samba-metadata on By default, some xattrs are cached by gluster like: capability xattrs, ima xattrs ACLs, etc. If there are any other xattrs that are used by the application using the Gluster storage, execute the following command to add these xattrs to the metadata cache list: # gluster volume set <volname> xattr-cache-list \"comma separated xattr list\" Eg: # gluster volume set <volname> xattr-cache-list \"user.org.netatalk.*,user.swift.metadata\" Directory operations Along with enabling the metadata caching, the following options can be set to increase performance of directory operations: ### Directory listing Performance: Enable parallel-readdir # gluster volume set <VOLNAME> performance.readdir-ahead on # gluster volume set <VOLNAME> performance.parallel-readdir on ### File/Directory Create Performance Enable nl-cache # gluster volume set <volname> group nl-cache # gluster volume set <volname> nl-cache-positive-entry on The above command also enables cache invalidation and increases the timeout to 10 minutes Small file Read operations For use cases with dominant small file reads, enable the following options # gluster volume set <volname> performance.cache-invalidation on # gluster volume set <volname> features.cache-invalidation on # gluster volume set <volname> performance.qr-cache-timeout 600 --> 10 min recommended setting # gluster volume set <volname> cache-invalidation-timeout 600 --> 10 min recommended setting This command enables caching of the content of small file, in the client cache. Enabling cache invalidation ensures cache consistency. The total cache size can be set using # gluster volume set <volname> cache-size <size> By default, the files with size <=64KB are cached. To change this value: # gluster volume set <volname> performance.cache-max-file-size <size> Note that the size arguments use SI unit suffixes, e.g. 64KB or 2MB .","title":"Performance Tuning"},{"location":"Administrator-Guide/Performance-Tuning/#performance-tuning","text":"","title":"Performance tuning"},{"location":"Administrator-Guide/Performance-Tuning/#enable-metadata-cache","text":"Metadata caching improves performance in almost all the workloads, except for use cases with most of the workload accessing a file sumultaneously from multiple clients. 1. Execute the following command to enable metadata caching and cache invalidation: # gluster volume set <volname> group metadata-cache This group command enables caching of stat and xattr information of a file or directory. The caching is refreshed every 10 min, and cache-invalidation is enabled to ensure cache consistency. To increase the number of files that can be cached, execute the following command: # gluster volume set <volname> network.inode-lru-limit <n> n, is set to 50000. It can be increased if the number of active files in the volume is very high. Increasing this number increases the memory footprint of the brick processes. Execute the following command to enable samba specific metadata caching: # gluster volume set <volname> cache-samba-metadata on By default, some xattrs are cached by gluster like: capability xattrs, ima xattrs ACLs, etc. If there are any other xattrs that are used by the application using the Gluster storage, execute the following command to add these xattrs to the metadata cache list: # gluster volume set <volname> xattr-cache-list \"comma separated xattr list\" Eg: # gluster volume set <volname> xattr-cache-list \"user.org.netatalk.*,user.swift.metadata\"","title":"Enable Metadata cache"},{"location":"Administrator-Guide/Performance-Tuning/#directory-operations","text":"Along with enabling the metadata caching, the following options can be set to increase performance of directory operations: ### Directory listing Performance: Enable parallel-readdir # gluster volume set <VOLNAME> performance.readdir-ahead on # gluster volume set <VOLNAME> performance.parallel-readdir on ### File/Directory Create Performance Enable nl-cache # gluster volume set <volname> group nl-cache # gluster volume set <volname> nl-cache-positive-entry on The above command also enables cache invalidation and increases the timeout to 10 minutes","title":"Directory operations"},{"location":"Administrator-Guide/Performance-Tuning/#small-file-read-operations","text":"For use cases with dominant small file reads, enable the following options # gluster volume set <volname> performance.cache-invalidation on # gluster volume set <volname> features.cache-invalidation on # gluster volume set <volname> performance.qr-cache-timeout 600 --> 10 min recommended setting # gluster volume set <volname> cache-invalidation-timeout 600 --> 10 min recommended setting This command enables caching of the content of small file, in the client cache. Enabling cache invalidation ensures cache consistency. The total cache size can be set using # gluster volume set <volname> cache-size <size> By default, the files with size <=64KB are cached. To change this value: # gluster volume set <volname> performance.cache-max-file-size <size> Note that the size arguments use SI unit suffixes, e.g. 64KB or 2MB .","title":"Small file Read operations"},{"location":"Administrator-Guide/Puppet/","text":"Puppet-Gluster A GlusterFS Puppet module by James Available from: https://github.com/purpleidea/puppet-gluster/ Table of Contents Overview Module description - What the module does Setup - Getting started with Puppet-Gluster What can Puppet-Gluster manage? Simple setup Elastic setup Advanced setup Usage/FAQ - Notes on management and frequently asked questions Reference - Class and type reference gluster::simple gluster::elastic gluster::server gluster::host gluster::brick gluster::volume gluster::volume::property Examples - Example configurations Limitations - Puppet versions, OS compatibility, etc... Development - Background on module development Author - Author and contact information Overview The Puppet-Gluster module installs, configures, and manages a GlusterFS cluster. Module Description This Puppet-Gluster module handles installation, configuration, and management of GlusterFS across all of the hosts in the cluster. Setup What can Puppet-Gluster manage? Puppet-Gluster is designed to be able to manage as much or as little of your GlusterFS cluster as you wish. All features are optional. If there is a feature that doesn't appear to be optional, and you believe it should be, please let me know. Having said that, it makes good sense to me to have Puppet-Gluster manage as much of your GlusterFS infrastructure as it can. At the moment, it cannot rack new servers, but I am accepting funding to explore this feature ;) At the moment it can manage: GlusterFS packages (rpm) GlusterFS configuration files (/var/lib/glusterd/) GlusterFS host peering (gluster peer probe) GlusterFS storage partitioning (fdisk) GlusterFS storage formatting (mkfs) GlusterFS brick creation (mkdir) GlusterFS services (glusterd) GlusterFS firewalling (whitelisting) GlusterFS volume creation (gluster volume create) GlusterFS volume state (started/stopped) GlusterFS volume properties (gluster volume set) And much more... Simple setup include '::gluster::simple' is enough to get you up and running. When using the gluster::simple class, or with any other Puppet-Gluster configuration, identical definitions must be used on all hosts in the cluster. The simplest way to accomplish this is with a single shared puppet host definition like: node /^annex\\d+$/ { # annex{1,2,..N} class { '::gluster::simple': } } If you wish to pass in different parameters, you can specify them in the class before you provision your hosts: class { '::gluster::simple': replica => 2, volume => ['volume1', 'volume2', 'volumeN'], } Elastic setup The gluster::elastic class is not yet available. Stay tuned! Advanced setup Some system administrators may wish to manually itemize each of the required components for the Puppet-Gluster deployment. This happens automatically with the higher level modules, but may still be a desirable feature, particularly for non-elastic storage pools where the configuration isn't expected to change very often (if ever). To put together your cluster piece by piece, you must manually include and define each class and type that you wish to use. If there are certain aspects that you wish to manage yourself, you can omit them from your configuration. See the reference section below for the specifics. Here is one possible example: class { '::gluster::server': shorewall => true, } gluster::host { 'annex1.example.com': # use uuidgen to make these uuid => '1f660ca2-2c78-4aa0-8f4d-21608218c69c', } # note that this is using a folder on your existing file system... # this can be useful for prototyping gluster using virtual machines # if this isn't a separate partition, remember that your root fs will # run out of space when your gluster volume does! gluster::brick { 'annex1.example.com:/data/gluster-storage1': areyousure => true, } gluster::host { 'annex2.example.com': # NOTE: specifying a host uuid is now optional! # if you don't choose one, one will be assigned #uuid => '2fbe6e2f-f6bc-4c2d-a301-62fa90c459f8', } gluster::brick { 'annex2.example.com:/data/gluster-storage2': areyousure => true, } $brick_list = [ 'annex1.example.com:/data/gluster-storage1', 'annex2.example.com:/data/gluster-storage2', ] gluster::volume { 'examplevol': replica => 2, bricks => $brick_list, start => undef, # i'll start this myself } # namevar must be: <VOLNAME>#<KEY> gluster::volume::property { 'examplevol#auth.reject': value => ['192.0.2.13', '198.51.100.42', '203.0.113.69'], } Usage and frequently asked questions All management should be done by manipulating the arguments on the appropriate Puppet-Gluster classes and types. Since certain manipulations are either not yet possible with Puppet-Gluster, or are not supported by GlusterFS, attempting to manipulate the Puppet configuration in an unsupported way will result in undefined behaviour, and possible even data loss, however this is unlikely. How do I change the replica count? You must set this before volume creation. This is a limitation of GlusterFS. There are certain situations where you can change the replica count by adding a multiple of the existing brick count to get this desired effect. These cases are not yet supported by Puppet-Gluster. If you want to use Puppet-Gluster before and / or after this transition, you can do so, but you'll have to do the changes manually. Do I need to use a virtual IP? Using a virtual IP (VIP) is strongly recommended as a distributed lock manager (DLM) and also to provide a highly-available (HA) IP address for your clients to connect to. For a more detailed explanation of the reasoning please see: https://ttboj.wordpress.com/2012/08/23/how-to-avoid-cluster-race-conditions-or-how-to-implement-a-distributed-lock-manager-in-puppet/ Remember that even if you're using a hosted solution (such as AWS) that doesn't provide an additional IP address, or you want to avoid using an additional IP, and you're okay not having full HA client mounting, you can use an unused private RFC1918 IP address as the DLM VIP. Remember that a layer 3 IP can co-exist on the same layer 2 network with the layer 3 network that is used by your cluster. Is it possible to have Puppet-Gluster complete in a single run? No. This is a limitation of Puppet, and is related to how GlusterFS operates. For example, it is not reliably possible to predict which ports a particular GlusterFS volume will run on until after the volume is started. As a result, this module will initially whitelist connections from GlusterFS host IP addresses, and then further restrict this to only allow individual ports once this information is known. This is possible in conjunction with the puppet-shorewall module. You should notice that each run should complete without error. If you do see an error, it means that either something is wrong with your system and / or configuration, or because there is a bug in Puppet-Gluster. Can you integrate this with vagrant? Not until vagrant properly supports libvirt/KVM. I have no desire to use VirtualBox for fun. Awesome work, but it's missing support for a feature and/or platform! Since this is an Open Source / Free Software project that I also give away for free (as in beer, free as in gratis, free as in libre), I'm unable to provide unlimited support. Please consider donating funds, hardware, virtual machines, and other resources. For specific needs, you could perhaps sponsor a feature! You didn't answer my question, or I have a question! Contact me through my technical blog and I'll do my best to help. If you have a good question, please remind me to add my answer to this documentation! Reference Please note that there are a number of undocumented options. For more information on these options, please view the source at: https://github.com/purpleidea/puppet-gluster/ . If you feel that a well used option needs documenting here, please contact me. Overview of classes and types gluster::simple : Simple Puppet-Gluster deployment. gluster::elastic : Under construction. gluster::server : Base class for server hosts. gluster::host : Host type for each participating host. gluster::brick : Brick type for each defined brick, per host. gluster::volume : Volume type for each defined volume. gluster::volume::property : Manages properties for each volume. gluster::simple This is gluster::simple. It should probably take care of 80% of all use cases. It is particularly useful for deploying quick test clusters. It uses a finite-state machine (FSM) to decide when the cluster has settled and volume creation can begin. For more information on the FSM in Puppet-Gluster see: https://ttboj.wordpress.com/2013/09/28/finite-state-machines-in-puppet/ replica The replica count. Can't be changed automatically after initial deployment. volume The volume name or list of volume names to create. path The valid brick path for each host. Defaults to local file system. If you need a different path per host, then Gluster::Simple will not meet your needs. vip The virtual IP address to be used for the cluster distributed lock manager. shorewall Boolean to specify whether puppet-shorewall integration should be used or not. gluster::elastic Under construction. gluster::server Main server class for the cluster. Must be included when building the GlusterFS cluster manually. Wrapper classes such as gluster::simple include this automatically. vip The virtual IP address to be used for the cluster distributed lock manager. shorewall Boolean to specify whether puppet-shorewall integration should be used or not. gluster::host Main host type for the cluster. Each host participating in the GlusterFS cluster must define this type on itself, and on every other host. As a result, this is not a singleton like the gluster::server class. ip Specify which IP address this host is using. This defaults to the $::ipaddress variable. Be sure to set this manually if you're declaring this yourself on each host without using exported resources. If each host thinks the other hosts should have the same IP address as itself, then Puppet-Gluster and GlusterFS won't work correctly. uuid Universally unique identifier (UUID) for the host. If empty, Puppet-Gluster will generate this automatically for the host. You can generate your own manually with uuidgen , and set them yourself. I found this particularly useful for testing, because I would pick easy to recognize UUID's like: aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaaaaaa , bbbbbbbb-bbbb-bbbb-bbbb-bbbbbbbbbbbb , and so on. If you set a UUID manually, and Puppet-Gluster has a chance to run, then it will remember your choice, and store it locally to be used again if you no longer specify the UUID. This is particularly useful for upgrading an existing un-managed GlusterFS installation to a Puppet-Gluster managed one, without changing any UUID's. gluster::brick Main brick type for the cluster. Each brick is an individual storage segment to be used on a host. Each host must have at least one brick to participate in the cluster, but usually a host will have multiple bricks. A brick can be as simple as a file system folder, or it can be a separate file system. Please read the official GlusterFS documentation, if you aren't entirely comfortable with the concept of a brick. For most test clusters, and for experimentation, it is easiest to use a directory on the root file system. You can even use a /tmp sub folder if you don't care about the persistence of your data. For more serious clusters, you might want to create separate file systems for your data. On self-hosted iron, it is not uncommon to create multiple RAID-6 drive pools, and to then create a separate file system per virtual drive. Each file system can then be used as a single brick. So that each volume in GlusterFS has the maximum ability to grow, without having to partition storage separately, the bricks in Puppet-Gluster are actually folders (on whatever backing store you wish) which then contain sub folders-- one for each volume. As a result, all the volumes on a given GlusterFS cluster can share the total available storage space. If you wish to limit the storage used by each volume, you can setup quotas. Alternatively, you can buy more hardware, and elastically grow your GlusterFS volumes, since the price per GB will be significantly less than any proprietary storage system. The one downside to this brick sharing, is that if you have chosen the brick per host count specifically to match your performance requirements, and each GlusterFS volume on the same cluster has drastically different brick per host performance requirements, then this won't suit your needs. I doubt that anyone actually has such requirements, but if you do insist on needing this compartmentalization, then you can probably use the Puppet-Gluster grouping feature to accomplish this goal. Please let me know about your use-case, and be warned that the grouping feature hasn't been extensively tested. To prove to you that I care about automation, this type offers the ability to automatically partition and format your file systems. This means you can plug in new iron, boot, provision and configure the entire system automatically. Regrettably, I don't have a lot of test hardware to routinely use this feature. If you'd like to donate some, I'd be happy to test this thoroughly. Having said that, I have used this feature, I consider it to be extremely safe, and it has never caused me to lose data. If you're uncertain, feel free to look at the code, or avoid using this feature entirely. If you think there's a way to make it even safer, then feel free to let me know. dev Block device, such as /dev/sdc or /dev/disk/by-id/scsi-0123456789abcdef . By default, Puppet-Gluster will assume you're using a folder to store the brick data, if you don't specify this parameter. fsuuid File system UUID. This ensures we can distinctly identify a file system. You can set this to be used with automatic file system creation, or you can specify the file system UUID that you'd like to use. labeltype Only gpt is supported. Other options include msdos , but this has never been used because of it's size limitations. fstype This should be xfs or ext4 . Using xfs is recommended, but ext4 is also quite common. This only affects a file system that is getting created by this module. If you provision a new machine, with a root file system of ext4 , and the brick you create is a root file system path, then this option does nothing. xfs_inode64 Set inode64 mount option when using the xfs fstype. Choose true to set. xfs_nobarrier Set nobarrier mount option when using the xfs fstype. Choose true to set. ro Whether the file system should be mounted read only. For emergencies only. force If true , this will overwrite any xfs file system it sees. This is useful for rebuilding GlusterFS repeatedly and wiping data. There are other safeties in place to stop this. In general, you probably don't ever want to touch this. areyousure Do you want to allow Puppet-Gluster to do dangerous things? You have to set this to true to allow Puppet-Gluster to fdisk and mkfs your file system. gluster::volume Main volume type for the cluster. This is where a lot of the magic happens. Remember that changing some of these parameters after the volume has been created won't work, and you'll experience undefined behaviour. There could be FSM based error checking to verify that no changes occur, but it has been left out so that this code base can eventually support such changes, and so that the user can manually change a parameter if they know that it is safe to do so. bricks List of bricks to use for this volume. If this is left at the default value of true , then this list is built automatically. The algorithm that determines this order does not support all possible situations, and most likely can't handle certain corner cases. It is possible to examine the FSM to view the selected brick order before it has a chance to create the volume. The volume creation script won't run until there is a stable brick list as seen by the FSM running on the host that has the DLM. If you specify this list of bricks manually, you must choose the order to match your desired volume layout. If you aren't sure about how to order the bricks, you should review the GlusterFS documentation first. transport Only tcp is supported. Possible values can include rdma , but this won't get any testing if I don't have access to infiniband hardware. Donations welcome. replica Replica count. Usually you'll want to set this to 2 . Some users choose 3 . Other values are seldom seen. A value of 1 can be used for simply testing a distributed setup, when you don't care about your data or high availability. A value greater than 4 is probably wasteful and unnecessary. It might even cause performance issues if a synchronous write is waiting on a slow fourth server. stripe Stripe count. Thoroughly unsupported and untested option. Not recommended for use by GlusterFS. ping Do we want to include ping checks with fping ? settle Do we want to run settle checks? start Requested state for the volume. Valid values include: true (start), false (stop), or undef (un-managed start/stop state). gluster::volume::property Main volume property type for the cluster. This allows you to manage GlusterFS volume specific properties. There are a wide range of properties that volumes support. For the full list of properties, you should consult the GlusterFS documentation, or run the gluster volume set help command. To set a property you must use the special name pattern of: volume # key . The value argument is used to set the associated value. It is smart enough to accept values in the most logical format for that specific property. Some properties aren't yet supported, so please report any problems you have with this functionality. Because this feature is an awesome way to document as code the volume specific optimizations that you've made, make sure you use this feature even if you don't use all the others. value The value to be used for this volume property. Examples For example configurations, please consult the examples/ directory in the git source repository. It is available from: https://github.com/purpleidea/puppet-gluster/tree/master/examples Limitations This module has been tested against open source Puppet 3.2.4 and higher. The module has been tested on: CentOS 6.4 It will probably work without incident or without major modification on: CentOS 5.x/6.x RHEL 5.x/6.x It will most likely work with other Puppet versions and on other platforms, but testing under other conditions has been light due to lack of resources. It will most likely not work on Debian/Ubuntu systems without modification. I would really love to add support for these operating systems, but I do not have any test resources to do so. Please sponsor this if you'd like to see it happen. Development This is my personal project that I work on in my free time. Donations of funding, hardware, virtual machines, and other resources are appreciated. Please contact me if you'd like to sponsor a feature, invite me to talk/teach or for consulting. You can follow along on my technical blog . Author Copyright (C) 2010-2013+ James Shubin github @purpleidea https://ttboj.wordpress.com/","title":"Puppet Gluster"},{"location":"Administrator-Guide/Puppet/#puppet-gluster","text":"","title":"Puppet-Gluster"},{"location":"Administrator-Guide/Puppet/#a-glusterfs-puppet-module-by-james","text":"","title":"A GlusterFS Puppet module by James"},{"location":"Administrator-Guide/Puppet/#available-from","text":"","title":"Available from:"},{"location":"Administrator-Guide/Puppet/#httpsgithubcompurpleideapuppet-gluster","text":"","title":"https://github.com/purpleidea/puppet-gluster/"},{"location":"Administrator-Guide/Puppet/#table-of-contents","text":"Overview Module description - What the module does Setup - Getting started with Puppet-Gluster What can Puppet-Gluster manage? Simple setup Elastic setup Advanced setup Usage/FAQ - Notes on management and frequently asked questions Reference - Class and type reference gluster::simple gluster::elastic gluster::server gluster::host gluster::brick gluster::volume gluster::volume::property Examples - Example configurations Limitations - Puppet versions, OS compatibility, etc... Development - Background on module development Author - Author and contact information","title":"Table of Contents"},{"location":"Administrator-Guide/Puppet/#overview","text":"The Puppet-Gluster module installs, configures, and manages a GlusterFS cluster.","title":"Overview"},{"location":"Administrator-Guide/Puppet/#module-description","text":"This Puppet-Gluster module handles installation, configuration, and management of GlusterFS across all of the hosts in the cluster.","title":"Module Description"},{"location":"Administrator-Guide/Puppet/#setup","text":"","title":"Setup"},{"location":"Administrator-Guide/Puppet/#what-can-puppet-gluster-manage","text":"Puppet-Gluster is designed to be able to manage as much or as little of your GlusterFS cluster as you wish. All features are optional. If there is a feature that doesn't appear to be optional, and you believe it should be, please let me know. Having said that, it makes good sense to me to have Puppet-Gluster manage as much of your GlusterFS infrastructure as it can. At the moment, it cannot rack new servers, but I am accepting funding to explore this feature ;) At the moment it can manage: GlusterFS packages (rpm) GlusterFS configuration files (/var/lib/glusterd/) GlusterFS host peering (gluster peer probe) GlusterFS storage partitioning (fdisk) GlusterFS storage formatting (mkfs) GlusterFS brick creation (mkdir) GlusterFS services (glusterd) GlusterFS firewalling (whitelisting) GlusterFS volume creation (gluster volume create) GlusterFS volume state (started/stopped) GlusterFS volume properties (gluster volume set) And much more...","title":"What can Puppet-Gluster manage?"},{"location":"Administrator-Guide/Puppet/#simple-setup","text":"include '::gluster::simple' is enough to get you up and running. When using the gluster::simple class, or with any other Puppet-Gluster configuration, identical definitions must be used on all hosts in the cluster. The simplest way to accomplish this is with a single shared puppet host definition like: node /^annex\\d+$/ { # annex{1,2,..N} class { '::gluster::simple': } } If you wish to pass in different parameters, you can specify them in the class before you provision your hosts: class { '::gluster::simple': replica => 2, volume => ['volume1', 'volume2', 'volumeN'], }","title":"Simple setup"},{"location":"Administrator-Guide/Puppet/#elastic-setup","text":"The gluster::elastic class is not yet available. Stay tuned!","title":"Elastic setup"},{"location":"Administrator-Guide/Puppet/#advanced-setup","text":"Some system administrators may wish to manually itemize each of the required components for the Puppet-Gluster deployment. This happens automatically with the higher level modules, but may still be a desirable feature, particularly for non-elastic storage pools where the configuration isn't expected to change very often (if ever). To put together your cluster piece by piece, you must manually include and define each class and type that you wish to use. If there are certain aspects that you wish to manage yourself, you can omit them from your configuration. See the reference section below for the specifics. Here is one possible example: class { '::gluster::server': shorewall => true, } gluster::host { 'annex1.example.com': # use uuidgen to make these uuid => '1f660ca2-2c78-4aa0-8f4d-21608218c69c', } # note that this is using a folder on your existing file system... # this can be useful for prototyping gluster using virtual machines # if this isn't a separate partition, remember that your root fs will # run out of space when your gluster volume does! gluster::brick { 'annex1.example.com:/data/gluster-storage1': areyousure => true, } gluster::host { 'annex2.example.com': # NOTE: specifying a host uuid is now optional! # if you don't choose one, one will be assigned #uuid => '2fbe6e2f-f6bc-4c2d-a301-62fa90c459f8', } gluster::brick { 'annex2.example.com:/data/gluster-storage2': areyousure => true, } $brick_list = [ 'annex1.example.com:/data/gluster-storage1', 'annex2.example.com:/data/gluster-storage2', ] gluster::volume { 'examplevol': replica => 2, bricks => $brick_list, start => undef, # i'll start this myself } # namevar must be: <VOLNAME>#<KEY> gluster::volume::property { 'examplevol#auth.reject': value => ['192.0.2.13', '198.51.100.42', '203.0.113.69'], }","title":"Advanced setup"},{"location":"Administrator-Guide/Puppet/#usage-and-frequently-asked-questions","text":"All management should be done by manipulating the arguments on the appropriate Puppet-Gluster classes and types. Since certain manipulations are either not yet possible with Puppet-Gluster, or are not supported by GlusterFS, attempting to manipulate the Puppet configuration in an unsupported way will result in undefined behaviour, and possible even data loss, however this is unlikely.","title":"Usage and frequently asked questions"},{"location":"Administrator-Guide/Puppet/#how-do-i-change-the-replica-count","text":"You must set this before volume creation. This is a limitation of GlusterFS. There are certain situations where you can change the replica count by adding a multiple of the existing brick count to get this desired effect. These cases are not yet supported by Puppet-Gluster. If you want to use Puppet-Gluster before and / or after this transition, you can do so, but you'll have to do the changes manually.","title":"How do I change the replica count?"},{"location":"Administrator-Guide/Puppet/#do-i-need-to-use-a-virtual-ip","text":"Using a virtual IP (VIP) is strongly recommended as a distributed lock manager (DLM) and also to provide a highly-available (HA) IP address for your clients to connect to. For a more detailed explanation of the reasoning please see: https://ttboj.wordpress.com/2012/08/23/how-to-avoid-cluster-race-conditions-or-how-to-implement-a-distributed-lock-manager-in-puppet/ Remember that even if you're using a hosted solution (such as AWS) that doesn't provide an additional IP address, or you want to avoid using an additional IP, and you're okay not having full HA client mounting, you can use an unused private RFC1918 IP address as the DLM VIP. Remember that a layer 3 IP can co-exist on the same layer 2 network with the layer 3 network that is used by your cluster.","title":"Do I need to use a virtual IP?"},{"location":"Administrator-Guide/Puppet/#is-it-possible-to-have-puppet-gluster-complete-in-a-single-run","text":"No. This is a limitation of Puppet, and is related to how GlusterFS operates. For example, it is not reliably possible to predict which ports a particular GlusterFS volume will run on until after the volume is started. As a result, this module will initially whitelist connections from GlusterFS host IP addresses, and then further restrict this to only allow individual ports once this information is known. This is possible in conjunction with the puppet-shorewall module. You should notice that each run should complete without error. If you do see an error, it means that either something is wrong with your system and / or configuration, or because there is a bug in Puppet-Gluster.","title":"Is it possible to have Puppet-Gluster complete in a single run?"},{"location":"Administrator-Guide/Puppet/#can-you-integrate-this-with-vagrant","text":"Not until vagrant properly supports libvirt/KVM. I have no desire to use VirtualBox for fun.","title":"Can you integrate this with vagrant?"},{"location":"Administrator-Guide/Puppet/#awesome-work-but-its-missing-support-for-a-feature-andor-platform","text":"Since this is an Open Source / Free Software project that I also give away for free (as in beer, free as in gratis, free as in libre), I'm unable to provide unlimited support. Please consider donating funds, hardware, virtual machines, and other resources. For specific needs, you could perhaps sponsor a feature!","title":"Awesome work, but it's missing support for a feature and/or platform!"},{"location":"Administrator-Guide/Puppet/#you-didnt-answer-my-question-or-i-have-a-question","text":"Contact me through my technical blog and I'll do my best to help. If you have a good question, please remind me to add my answer to this documentation!","title":"You didn't answer my question, or I have a question!"},{"location":"Administrator-Guide/Puppet/#reference","text":"Please note that there are a number of undocumented options. For more information on these options, please view the source at: https://github.com/purpleidea/puppet-gluster/ . If you feel that a well used option needs documenting here, please contact me.","title":"Reference"},{"location":"Administrator-Guide/Puppet/#overview-of-classes-and-types","text":"gluster::simple : Simple Puppet-Gluster deployment. gluster::elastic : Under construction. gluster::server : Base class for server hosts. gluster::host : Host type for each participating host. gluster::brick : Brick type for each defined brick, per host. gluster::volume : Volume type for each defined volume. gluster::volume::property : Manages properties for each volume.","title":"Overview of classes and types"},{"location":"Administrator-Guide/Puppet/#glustersimple","text":"This is gluster::simple. It should probably take care of 80% of all use cases. It is particularly useful for deploying quick test clusters. It uses a finite-state machine (FSM) to decide when the cluster has settled and volume creation can begin. For more information on the FSM in Puppet-Gluster see: https://ttboj.wordpress.com/2013/09/28/finite-state-machines-in-puppet/","title":"gluster::simple"},{"location":"Administrator-Guide/Puppet/#replica","text":"The replica count. Can't be changed automatically after initial deployment.","title":"replica"},{"location":"Administrator-Guide/Puppet/#volume","text":"The volume name or list of volume names to create.","title":"volume"},{"location":"Administrator-Guide/Puppet/#path","text":"The valid brick path for each host. Defaults to local file system. If you need a different path per host, then Gluster::Simple will not meet your needs.","title":"path"},{"location":"Administrator-Guide/Puppet/#vip","text":"The virtual IP address to be used for the cluster distributed lock manager.","title":"vip"},{"location":"Administrator-Guide/Puppet/#shorewall","text":"Boolean to specify whether puppet-shorewall integration should be used or not.","title":"shorewall"},{"location":"Administrator-Guide/Puppet/#glusterelastic","text":"Under construction.","title":"gluster::elastic"},{"location":"Administrator-Guide/Puppet/#glusterserver","text":"Main server class for the cluster. Must be included when building the GlusterFS cluster manually. Wrapper classes such as gluster::simple include this automatically.","title":"gluster::server"},{"location":"Administrator-Guide/Puppet/#vip_1","text":"The virtual IP address to be used for the cluster distributed lock manager.","title":"vip"},{"location":"Administrator-Guide/Puppet/#shorewall_1","text":"Boolean to specify whether puppet-shorewall integration should be used or not.","title":"shorewall"},{"location":"Administrator-Guide/Puppet/#glusterhost","text":"Main host type for the cluster. Each host participating in the GlusterFS cluster must define this type on itself, and on every other host. As a result, this is not a singleton like the gluster::server class.","title":"gluster::host"},{"location":"Administrator-Guide/Puppet/#ip","text":"Specify which IP address this host is using. This defaults to the $::ipaddress variable. Be sure to set this manually if you're declaring this yourself on each host without using exported resources. If each host thinks the other hosts should have the same IP address as itself, then Puppet-Gluster and GlusterFS won't work correctly.","title":"ip"},{"location":"Administrator-Guide/Puppet/#uuid","text":"Universally unique identifier (UUID) for the host. If empty, Puppet-Gluster will generate this automatically for the host. You can generate your own manually with uuidgen , and set them yourself. I found this particularly useful for testing, because I would pick easy to recognize UUID's like: aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaaaaaa , bbbbbbbb-bbbb-bbbb-bbbb-bbbbbbbbbbbb , and so on. If you set a UUID manually, and Puppet-Gluster has a chance to run, then it will remember your choice, and store it locally to be used again if you no longer specify the UUID. This is particularly useful for upgrading an existing un-managed GlusterFS installation to a Puppet-Gluster managed one, without changing any UUID's.","title":"uuid"},{"location":"Administrator-Guide/Puppet/#glusterbrick","text":"Main brick type for the cluster. Each brick is an individual storage segment to be used on a host. Each host must have at least one brick to participate in the cluster, but usually a host will have multiple bricks. A brick can be as simple as a file system folder, or it can be a separate file system. Please read the official GlusterFS documentation, if you aren't entirely comfortable with the concept of a brick. For most test clusters, and for experimentation, it is easiest to use a directory on the root file system. You can even use a /tmp sub folder if you don't care about the persistence of your data. For more serious clusters, you might want to create separate file systems for your data. On self-hosted iron, it is not uncommon to create multiple RAID-6 drive pools, and to then create a separate file system per virtual drive. Each file system can then be used as a single brick. So that each volume in GlusterFS has the maximum ability to grow, without having to partition storage separately, the bricks in Puppet-Gluster are actually folders (on whatever backing store you wish) which then contain sub folders-- one for each volume. As a result, all the volumes on a given GlusterFS cluster can share the total available storage space. If you wish to limit the storage used by each volume, you can setup quotas. Alternatively, you can buy more hardware, and elastically grow your GlusterFS volumes, since the price per GB will be significantly less than any proprietary storage system. The one downside to this brick sharing, is that if you have chosen the brick per host count specifically to match your performance requirements, and each GlusterFS volume on the same cluster has drastically different brick per host performance requirements, then this won't suit your needs. I doubt that anyone actually has such requirements, but if you do insist on needing this compartmentalization, then you can probably use the Puppet-Gluster grouping feature to accomplish this goal. Please let me know about your use-case, and be warned that the grouping feature hasn't been extensively tested. To prove to you that I care about automation, this type offers the ability to automatically partition and format your file systems. This means you can plug in new iron, boot, provision and configure the entire system automatically. Regrettably, I don't have a lot of test hardware to routinely use this feature. If you'd like to donate some, I'd be happy to test this thoroughly. Having said that, I have used this feature, I consider it to be extremely safe, and it has never caused me to lose data. If you're uncertain, feel free to look at the code, or avoid using this feature entirely. If you think there's a way to make it even safer, then feel free to let me know.","title":"gluster::brick"},{"location":"Administrator-Guide/Puppet/#dev","text":"Block device, such as /dev/sdc or /dev/disk/by-id/scsi-0123456789abcdef . By default, Puppet-Gluster will assume you're using a folder to store the brick data, if you don't specify this parameter.","title":"dev"},{"location":"Administrator-Guide/Puppet/#fsuuid","text":"File system UUID. This ensures we can distinctly identify a file system. You can set this to be used with automatic file system creation, or you can specify the file system UUID that you'd like to use.","title":"fsuuid"},{"location":"Administrator-Guide/Puppet/#labeltype","text":"Only gpt is supported. Other options include msdos , but this has never been used because of it's size limitations.","title":"labeltype"},{"location":"Administrator-Guide/Puppet/#fstype","text":"This should be xfs or ext4 . Using xfs is recommended, but ext4 is also quite common. This only affects a file system that is getting created by this module. If you provision a new machine, with a root file system of ext4 , and the brick you create is a root file system path, then this option does nothing.","title":"fstype"},{"location":"Administrator-Guide/Puppet/#xfs_inode64","text":"Set inode64 mount option when using the xfs fstype. Choose true to set.","title":"xfs_inode64"},{"location":"Administrator-Guide/Puppet/#xfs_nobarrier","text":"Set nobarrier mount option when using the xfs fstype. Choose true to set.","title":"xfs_nobarrier"},{"location":"Administrator-Guide/Puppet/#ro","text":"Whether the file system should be mounted read only. For emergencies only.","title":"ro"},{"location":"Administrator-Guide/Puppet/#force","text":"If true , this will overwrite any xfs file system it sees. This is useful for rebuilding GlusterFS repeatedly and wiping data. There are other safeties in place to stop this. In general, you probably don't ever want to touch this.","title":"force"},{"location":"Administrator-Guide/Puppet/#areyousure","text":"Do you want to allow Puppet-Gluster to do dangerous things? You have to set this to true to allow Puppet-Gluster to fdisk and mkfs your file system.","title":"areyousure"},{"location":"Administrator-Guide/Puppet/#glustervolume","text":"Main volume type for the cluster. This is where a lot of the magic happens. Remember that changing some of these parameters after the volume has been created won't work, and you'll experience undefined behaviour. There could be FSM based error checking to verify that no changes occur, but it has been left out so that this code base can eventually support such changes, and so that the user can manually change a parameter if they know that it is safe to do so.","title":"gluster::volume"},{"location":"Administrator-Guide/Puppet/#bricks","text":"List of bricks to use for this volume. If this is left at the default value of true , then this list is built automatically. The algorithm that determines this order does not support all possible situations, and most likely can't handle certain corner cases. It is possible to examine the FSM to view the selected brick order before it has a chance to create the volume. The volume creation script won't run until there is a stable brick list as seen by the FSM running on the host that has the DLM. If you specify this list of bricks manually, you must choose the order to match your desired volume layout. If you aren't sure about how to order the bricks, you should review the GlusterFS documentation first.","title":"bricks"},{"location":"Administrator-Guide/Puppet/#transport","text":"Only tcp is supported. Possible values can include rdma , but this won't get any testing if I don't have access to infiniband hardware. Donations welcome.","title":"transport"},{"location":"Administrator-Guide/Puppet/#replica_1","text":"Replica count. Usually you'll want to set this to 2 . Some users choose 3 . Other values are seldom seen. A value of 1 can be used for simply testing a distributed setup, when you don't care about your data or high availability. A value greater than 4 is probably wasteful and unnecessary. It might even cause performance issues if a synchronous write is waiting on a slow fourth server.","title":"replica"},{"location":"Administrator-Guide/Puppet/#stripe","text":"Stripe count. Thoroughly unsupported and untested option. Not recommended for use by GlusterFS.","title":"stripe"},{"location":"Administrator-Guide/Puppet/#ping","text":"Do we want to include ping checks with fping ?","title":"ping"},{"location":"Administrator-Guide/Puppet/#settle","text":"Do we want to run settle checks?","title":"settle"},{"location":"Administrator-Guide/Puppet/#start","text":"Requested state for the volume. Valid values include: true (start), false (stop), or undef (un-managed start/stop state).","title":"start"},{"location":"Administrator-Guide/Puppet/#glustervolumeproperty","text":"Main volume property type for the cluster. This allows you to manage GlusterFS volume specific properties. There are a wide range of properties that volumes support. For the full list of properties, you should consult the GlusterFS documentation, or run the gluster volume set help command. To set a property you must use the special name pattern of: volume # key . The value argument is used to set the associated value. It is smart enough to accept values in the most logical format for that specific property. Some properties aren't yet supported, so please report any problems you have with this functionality. Because this feature is an awesome way to document as code the volume specific optimizations that you've made, make sure you use this feature even if you don't use all the others.","title":"gluster::volume::property"},{"location":"Administrator-Guide/Puppet/#value","text":"The value to be used for this volume property.","title":"value"},{"location":"Administrator-Guide/Puppet/#examples","text":"For example configurations, please consult the examples/ directory in the git source repository. It is available from: https://github.com/purpleidea/puppet-gluster/tree/master/examples","title":"Examples"},{"location":"Administrator-Guide/Puppet/#limitations","text":"This module has been tested against open source Puppet 3.2.4 and higher. The module has been tested on: CentOS 6.4 It will probably work without incident or without major modification on: CentOS 5.x/6.x RHEL 5.x/6.x It will most likely work with other Puppet versions and on other platforms, but testing under other conditions has been light due to lack of resources. It will most likely not work on Debian/Ubuntu systems without modification. I would really love to add support for these operating systems, but I do not have any test resources to do so. Please sponsor this if you'd like to see it happen.","title":"Limitations"},{"location":"Administrator-Guide/Puppet/#development","text":"This is my personal project that I work on in my free time. Donations of funding, hardware, virtual machines, and other resources are appreciated. Please contact me if you'd like to sponsor a feature, invite me to talk/teach or for consulting. You can follow along on my technical blog .","title":"Development"},{"location":"Administrator-Guide/Puppet/#author","text":"Copyright (C) 2010-2013+ James Shubin github @purpleidea https://ttboj.wordpress.com/","title":"Author"},{"location":"Administrator-Guide/RDMA-Transport/","text":"NOTE: FEATURE DEPRECATED THE RDMA is no longer supported in Gluster builds. This has been removed from release 8 onwards. Currently we dont have 1. The expertise to support RDMA 2. Infrastructure to test/verify the performances each release The options are getting discussed here - https://github.com/gluster/glusterfs/issues/2000 Ready to enable as a compile time option, if there is proper support and testing infrastructure. Introduction GlusterFS supports using RDMA protocol for communication between glusterfs clients and glusterfs bricks. GlusterFS clients include FUSE client, libgfapi clients(Samba and NFS-Ganesha included), gNFS server and other glusterfs processes that communicate with bricks like self-heal daemon, quotad, rebalance process etc. NOTE: As of now only FUSE client and gNFS server would support RDMA transport. NOTE: NFS client to gNFS Server/NFS Ganesha Server communication would still happen over tcp. CIFS Clients/Windows Clients to Samba Server communication would still happen over tcp. Setup Please refer to these external documentation to setup RDMA on your machines http://people.redhat.com/dledford/infiniband_get_started.html Creating Trusted Storage Pool All the servers in the Trusted Storage Pool must have RDMA devices if either RDMA or TCP,RDMA volumes are created in the storage pool. The peer probe must be performed using IP/hostname assigned to the RDMA device. Ports and Firewall Process glusterd will listen on both tcp and rdma if rdma device is found. Port used for rdma is 24008. Similarly, brick processes will also listen on two ports for a volume created with transport \"tcp,rdma\". Make sure you update the firewall to accept packets on these ports. Gluster Volume Create A volume can support one or more transport types for communication between clients and brick processes. There are three types of supported transport, which are, tcp, rdma, and tcp,rdma. Example: To create a distributed volume with four storage servers over InfiniBand: # gluster volume create test-volume transport rdma server1:/exp1 server2:/exp2 server3:/exp3 server4:/exp4 Creation of test-volume has been successful Please start the volume to access data. Changing Transport of Volume To change the supported transport types of a existing volume, follow the procedure: NOTE: This is possible only if the volume was created with IP/hostname assigned to RDMA device. Unmount the volume on all the clients using the following command: # umount mount-point Stop the volumes using the following command: # gluster volume stop volname Change the transport type. For example, to enable both tcp and rdma execute the followimg command: # gluster volume set volname config.transport tcp,rdma Mount the volume on all the clients. For example, to mount using rdma transport, use the following command: # mount -t glusterfs -o transport=rdma server1:/test-volume /mnt/glusterfs NOTE: config.transport option does not have a entry in help of gluster cli. #gluster vol set help | grep config.transport However, the key is a valid one. Mounting a Volume using RDMA You can use the mount option \"transport\" to specify the transport type that FUSE client must use to communicate with bricks. If the volume was created with only one transport type, then that becomes the default when no value is specified. In case of tcp,rdma volume, tcp is the default. For example, to mount using rdma transport, use the following command: # mount -t glusterfs -o transport=rdma server1:/test-volume /mnt/glusterfs Transport used by auxillary processes All the auxillary processes like self-heal daemon, rebalance process etc use the default transport.In case you have a tcp,rdma volume it will use tcp. In case of rdma volume, rdma will be used. Configuration options to select transport used by these processes when volume is tcp,rdma are not yet available and will be coming in later releases.","title":"RDMA Transport"},{"location":"Administrator-Guide/RDMA-Transport/#note-feature-deprecated","text":"THE RDMA is no longer supported in Gluster builds. This has been removed from release 8 onwards. Currently we dont have 1. The expertise to support RDMA 2. Infrastructure to test/verify the performances each release The options are getting discussed here - https://github.com/gluster/glusterfs/issues/2000 Ready to enable as a compile time option, if there is proper support and testing infrastructure.","title":"NOTE: FEATURE DEPRECATED"},{"location":"Administrator-Guide/RDMA-Transport/#introduction","text":"GlusterFS supports using RDMA protocol for communication between glusterfs clients and glusterfs bricks. GlusterFS clients include FUSE client, libgfapi clients(Samba and NFS-Ganesha included), gNFS server and other glusterfs processes that communicate with bricks like self-heal daemon, quotad, rebalance process etc. NOTE: As of now only FUSE client and gNFS server would support RDMA transport. NOTE: NFS client to gNFS Server/NFS Ganesha Server communication would still happen over tcp. CIFS Clients/Windows Clients to Samba Server communication would still happen over tcp.","title":"Introduction"},{"location":"Administrator-Guide/RDMA-Transport/#setup","text":"Please refer to these external documentation to setup RDMA on your machines http://people.redhat.com/dledford/infiniband_get_started.html","title":"Setup"},{"location":"Administrator-Guide/RDMA-Transport/#creating-trusted-storage-pool","text":"All the servers in the Trusted Storage Pool must have RDMA devices if either RDMA or TCP,RDMA volumes are created in the storage pool. The peer probe must be performed using IP/hostname assigned to the RDMA device.","title":"Creating Trusted Storage Pool"},{"location":"Administrator-Guide/RDMA-Transport/#ports-and-firewall","text":"Process glusterd will listen on both tcp and rdma if rdma device is found. Port used for rdma is 24008. Similarly, brick processes will also listen on two ports for a volume created with transport \"tcp,rdma\". Make sure you update the firewall to accept packets on these ports.","title":"Ports and Firewall"},{"location":"Administrator-Guide/RDMA-Transport/#gluster-volume-create","text":"A volume can support one or more transport types for communication between clients and brick processes. There are three types of supported transport, which are, tcp, rdma, and tcp,rdma. Example: To create a distributed volume with four storage servers over InfiniBand: # gluster volume create test-volume transport rdma server1:/exp1 server2:/exp2 server3:/exp3 server4:/exp4 Creation of test-volume has been successful Please start the volume to access data.","title":"Gluster Volume Create"},{"location":"Administrator-Guide/RDMA-Transport/#changing-transport-of-volume","text":"To change the supported transport types of a existing volume, follow the procedure: NOTE: This is possible only if the volume was created with IP/hostname assigned to RDMA device. Unmount the volume on all the clients using the following command: # umount mount-point Stop the volumes using the following command: # gluster volume stop volname Change the transport type. For example, to enable both tcp and rdma execute the followimg command: # gluster volume set volname config.transport tcp,rdma Mount the volume on all the clients. For example, to mount using rdma transport, use the following command: # mount -t glusterfs -o transport=rdma server1:/test-volume /mnt/glusterfs NOTE: config.transport option does not have a entry in help of gluster cli. #gluster vol set help | grep config.transport However, the key is a valid one.","title":"Changing Transport of Volume"},{"location":"Administrator-Guide/RDMA-Transport/#mounting-a-volume-using-rdma","text":"You can use the mount option \"transport\" to specify the transport type that FUSE client must use to communicate with bricks. If the volume was created with only one transport type, then that becomes the default when no value is specified. In case of tcp,rdma volume, tcp is the default. For example, to mount using rdma transport, use the following command: # mount -t glusterfs -o transport=rdma server1:/test-volume /mnt/glusterfs","title":"Mounting a Volume using RDMA"},{"location":"Administrator-Guide/RDMA-Transport/#transport-used-by-auxillary-processes","text":"All the auxillary processes like self-heal daemon, rebalance process etc use the default transport.In case you have a tcp,rdma volume it will use tcp. In case of rdma volume, rdma will be used. Configuration options to select transport used by these processes when volume is tcp,rdma are not yet available and will be coming in later releases.","title":"Transport used by auxillary processes"},{"location":"Administrator-Guide/SSL/","text":"Setting up GlusterFS with SSL/TLS GlusterFS allows its communication to be secured using the Transport Layer Security standard (which supersedes Secure Sockets Layer), using the OpenSSL library. Setting this up requires a basic working knowledge of some SSL/TLS concepts, which can only be briefly summarized here. \"Authentication\" is the process of one entity (e.g. a machine, process, or person) proving its identity to a second entity. \"Authorization\" is the process of checking whether an entity has permission to perform an action. TLS provides authentication and encryption. It does not provide authorization, though GlusterFS can use TLS-authenticated identities to authorize client connections to bricks/volumes. An entity X which must authenticate to a second entity Y does so by sharing with Y a certificate , which contains information sufficient to prove X's identity. X's proof of identity also requires possession of a private key which matches its certificate, but this key is never seen by Y or anyone else. Because the certificate is already public, anyone who has the key can claim that identity. Each certificate contains the identity of its principal (owner) along with the identity of a certifying authority or CA who can verify the integrity of the certificate's contents. The principal and CA can be the same (a \"self-signed certificate\"). If they are different, the CA must sign the certificate by appending information derived from both the certificate contents and the CA's own private key. Certificate-signing relationships can extend through multiple levels. For example, a company X could sign another company Y's certificate, which could then be used to sign a third certificate Z for a specific user or purpose. Anyone who trusts X (and is willing to extend that trust through a certificate depth of two or more) would therefore be able to authenticate Y and Z as well. Any entity willing to accept other entities' authentication attempts must have some sort of database seeded with the certificates that already accept. In GlusterFS's case, a client or server X uses the following files to contain TLS-related information: /etc/ssl/glusterfs.pem X's own certificate /etc/ssl/glusterfs.key X's private key /etc/ssl/glusterfs.ca concatenation of others' certificates GlusterFS always performs mutual authentication , though clients do not currently do anything with the authenticated server identity. Thus, if client X wants to communicate with server Y, then X's certificate (or that of a signer) must be in Y's CA file, and vice versa. For all uses of TLS in GlusterFS, if one side of a connection is configured to use TLS then the other side must use it as well. There is no automatic fallback to non-TLS communication, or allowance for concurrent TLS and non-TLS access to the same resource, because either would be insecure. Instead, any such \"mixed mode\" connections will be rejected by the TLS-using side, sacrificing availability to maintain security. NOTE The TLS certificate verification will fail if the machines' date and time are not in sync with each other. Certificate verification depends on the time of the client as well as the server and if that is not found to be in sync then it is deemed to be an invalid certificate. To get the date and times in sync, tools such as ntpdate can be used. Using Certmonger and FreeIPA to generate and manage certs Certmonger can be used to generate keys, request certs from a CA and then automatically keep the Gluster certificate and the CA bundle updated as required, simplifying deployment. Either a commercial CA or a local CA can be used. E.g., FreeIPA (with dogtag CA) is an open-source CA with user-friendly tooling. If using FreeIPA, first add the host. This is required for FreeIPA to issue certificates. This can be done via the web UI, or the CLI with: ipa host-add <hostname> If the host has been added the following should show the host: ipa host-show <hostname> And it should show a kerberos principal for the host in the form of: host/<hostname> Now use certmonger on the gluster server or client to generate the key (if required), and submit a CSR to the CA. Certmonger will monitor the request, and create and update the files as required. For FreeIPA we need to specify the Kerberos principal from above to -K. E.g.: getcert request -r \\ -K host/$(hostname) \\ -f /etc/ssl/gluster.pem \\ -k /etc/ssl/gluster.key \\ -D $(hostname) \\ -F /etc/ssl/gluster.ca Certmonger should print out an ID for the request, e.g.: New signing request \"20210801190305\" added. You can check the status of the request with this ID: getcert list -i 20210801190147 If the CA approves the CSR and issues the cert, then the previous command should print a status field with: status: MONITORING As this point, the key, the cert and the CA bundle should all be in /etc/ssl ready for Gluster to use. Certmonger will renew the certificates as required for you. You do not need to manually concatenate certs to a trusted cert bundle and distribute them to all servers. You may need to set the certificate depth to allow the CA signed certs to be used, if there are intermediate CAs in the signing path. E.g., on every server and client: echo \"option transport.socket.ssl-cert-depth 3\" > /var/lib/glusterd/secure-access This should not be necessary where a local CA (e.g., FreeIPA) has directly signed the cart. Enabling TLS on the I/O Path To enable authentication and encryption between clients and brick servers, two options must be set: gluster volume set MYVOLUME client.ssl on gluster volume set MYVOLUME server.ssl on Note that the above options affect only the GlusterFS native protocol. For foreign protocols such as NFS, SMB, or Swift the encryption will not be affected between: NFS client and Glusterfs NFS Ganesha Server SMB client and Glusterfs SMB server While it affects the encryption between the following: NFS Ganesha server and Glusterfs bricks Glusterfs SMB server and Glusterfs bricks Using TLS Identities for Authorization Once TLS has been enabled on the I/O path, TLS identities can be used instead of IP addresses or plain usernames to control access to specific volumes. For example: gluster volume set MYVOLUME auth.ssl-allow Zaphod Here, we're allowing the TLS-authenticated identity \"Zaphod\" to access MYVOLUME. This is intentionally identical to the existing \"auth.allow\" option, except that the name is taken from a TLS certificate instead of a command-line string. Note that infelicities in the gluster CLI preclude using names that include spaces, which would otherwise be allowed. Enabling TLS on the Management Path Management-daemon traffic is not controlled by an option. Instead, it is controlled by the presence of a file on each machine: /var/lib/glusterd/secure-access Creating this file will cause glusterd connections made from that machine to use TLS. Note that even clients must do this to communicate with a remote glusterd while mounting, but not thereafter. Additional Options The GlusterFS TLS implementation supports two additional options related to TLS internals. The first option allows the user to set the certificate depth, as mentioned above. gluster volume set MYVOLUME ssl.certificate-depth 2 Here, we're setting our certificate depth to two, as in the introductory example. By default this value is zero, meaning that only certificates which are directly specified in the local CA file will be accepted (i.e. no signed certificates at all). The second option allows the user to specify the set of allowed TLS ciphers. gluster volume set MYVOLUME ssl.cipher-list 'HIGH:!SSLv2' Cipher lists are negotiated between the two parties to a TLS connection so that both sides' security needs are satisfied. In this example, we're setting the initial cipher list to HIGH, representing ciphers that the cryptography community still believes to be unbroken. We are also explicitly disallowing ciphers specific to SSL version 2. The default is based on this example but also excludes CBC-based cipher modes to provide extra mitigation against the POODLE attack.","title":"SSL"},{"location":"Administrator-Guide/SSL/#setting-up-glusterfs-with-ssltls","text":"GlusterFS allows its communication to be secured using the Transport Layer Security standard (which supersedes Secure Sockets Layer), using the OpenSSL library. Setting this up requires a basic working knowledge of some SSL/TLS concepts, which can only be briefly summarized here. \"Authentication\" is the process of one entity (e.g. a machine, process, or person) proving its identity to a second entity. \"Authorization\" is the process of checking whether an entity has permission to perform an action. TLS provides authentication and encryption. It does not provide authorization, though GlusterFS can use TLS-authenticated identities to authorize client connections to bricks/volumes. An entity X which must authenticate to a second entity Y does so by sharing with Y a certificate , which contains information sufficient to prove X's identity. X's proof of identity also requires possession of a private key which matches its certificate, but this key is never seen by Y or anyone else. Because the certificate is already public, anyone who has the key can claim that identity. Each certificate contains the identity of its principal (owner) along with the identity of a certifying authority or CA who can verify the integrity of the certificate's contents. The principal and CA can be the same (a \"self-signed certificate\"). If they are different, the CA must sign the certificate by appending information derived from both the certificate contents and the CA's own private key. Certificate-signing relationships can extend through multiple levels. For example, a company X could sign another company Y's certificate, which could then be used to sign a third certificate Z for a specific user or purpose. Anyone who trusts X (and is willing to extend that trust through a certificate depth of two or more) would therefore be able to authenticate Y and Z as well. Any entity willing to accept other entities' authentication attempts must have some sort of database seeded with the certificates that already accept. In GlusterFS's case, a client or server X uses the following files to contain TLS-related information: /etc/ssl/glusterfs.pem X's own certificate /etc/ssl/glusterfs.key X's private key /etc/ssl/glusterfs.ca concatenation of others' certificates GlusterFS always performs mutual authentication , though clients do not currently do anything with the authenticated server identity. Thus, if client X wants to communicate with server Y, then X's certificate (or that of a signer) must be in Y's CA file, and vice versa. For all uses of TLS in GlusterFS, if one side of a connection is configured to use TLS then the other side must use it as well. There is no automatic fallback to non-TLS communication, or allowance for concurrent TLS and non-TLS access to the same resource, because either would be insecure. Instead, any such \"mixed mode\" connections will be rejected by the TLS-using side, sacrificing availability to maintain security. NOTE The TLS certificate verification will fail if the machines' date and time are not in sync with each other. Certificate verification depends on the time of the client as well as the server and if that is not found to be in sync then it is deemed to be an invalid certificate. To get the date and times in sync, tools such as ntpdate can be used.","title":"Setting up GlusterFS with SSL/TLS"},{"location":"Administrator-Guide/SSL/#using-certmonger-and-freeipa-to-generate-and-manage-certs","text":"Certmonger can be used to generate keys, request certs from a CA and then automatically keep the Gluster certificate and the CA bundle updated as required, simplifying deployment. Either a commercial CA or a local CA can be used. E.g., FreeIPA (with dogtag CA) is an open-source CA with user-friendly tooling. If using FreeIPA, first add the host. This is required for FreeIPA to issue certificates. This can be done via the web UI, or the CLI with: ipa host-add <hostname> If the host has been added the following should show the host: ipa host-show <hostname> And it should show a kerberos principal for the host in the form of: host/<hostname> Now use certmonger on the gluster server or client to generate the key (if required), and submit a CSR to the CA. Certmonger will monitor the request, and create and update the files as required. For FreeIPA we need to specify the Kerberos principal from above to -K. E.g.: getcert request -r \\ -K host/$(hostname) \\ -f /etc/ssl/gluster.pem \\ -k /etc/ssl/gluster.key \\ -D $(hostname) \\ -F /etc/ssl/gluster.ca Certmonger should print out an ID for the request, e.g.: New signing request \"20210801190305\" added. You can check the status of the request with this ID: getcert list -i 20210801190147 If the CA approves the CSR and issues the cert, then the previous command should print a status field with: status: MONITORING As this point, the key, the cert and the CA bundle should all be in /etc/ssl ready for Gluster to use. Certmonger will renew the certificates as required for you. You do not need to manually concatenate certs to a trusted cert bundle and distribute them to all servers. You may need to set the certificate depth to allow the CA signed certs to be used, if there are intermediate CAs in the signing path. E.g., on every server and client: echo \"option transport.socket.ssl-cert-depth 3\" > /var/lib/glusterd/secure-access This should not be necessary where a local CA (e.g., FreeIPA) has directly signed the cart.","title":"Using Certmonger and FreeIPA to generate and manage certs"},{"location":"Administrator-Guide/SSL/#enabling-tls-on-the-io-path","text":"To enable authentication and encryption between clients and brick servers, two options must be set: gluster volume set MYVOLUME client.ssl on gluster volume set MYVOLUME server.ssl on Note that the above options affect only the GlusterFS native protocol. For foreign protocols such as NFS, SMB, or Swift the encryption will not be affected between: NFS client and Glusterfs NFS Ganesha Server SMB client and Glusterfs SMB server While it affects the encryption between the following: NFS Ganesha server and Glusterfs bricks Glusterfs SMB server and Glusterfs bricks","title":"Enabling TLS on the I/O Path"},{"location":"Administrator-Guide/SSL/#using-tls-identities-for-authorization","text":"Once TLS has been enabled on the I/O path, TLS identities can be used instead of IP addresses or plain usernames to control access to specific volumes. For example: gluster volume set MYVOLUME auth.ssl-allow Zaphod Here, we're allowing the TLS-authenticated identity \"Zaphod\" to access MYVOLUME. This is intentionally identical to the existing \"auth.allow\" option, except that the name is taken from a TLS certificate instead of a command-line string. Note that infelicities in the gluster CLI preclude using names that include spaces, which would otherwise be allowed.","title":"Using TLS Identities for Authorization"},{"location":"Administrator-Guide/SSL/#enabling-tls-on-the-management-path","text":"Management-daemon traffic is not controlled by an option. Instead, it is controlled by the presence of a file on each machine: /var/lib/glusterd/secure-access Creating this file will cause glusterd connections made from that machine to use TLS. Note that even clients must do this to communicate with a remote glusterd while mounting, but not thereafter.","title":"Enabling TLS on the Management Path"},{"location":"Administrator-Guide/SSL/#additional-options","text":"The GlusterFS TLS implementation supports two additional options related to TLS internals. The first option allows the user to set the certificate depth, as mentioned above. gluster volume set MYVOLUME ssl.certificate-depth 2 Here, we're setting our certificate depth to two, as in the introductory example. By default this value is zero, meaning that only certificates which are directly specified in the local CA file will be accepted (i.e. no signed certificates at all). The second option allows the user to specify the set of allowed TLS ciphers. gluster volume set MYVOLUME ssl.cipher-list 'HIGH:!SSLv2' Cipher lists are negotiated between the two parties to a TLS connection so that both sides' security needs are satisfied. In this example, we're setting the initial cipher list to HIGH, representing ciphers that the cryptography community still believes to be unbroken. We are also explicitly disallowing ciphers specific to SSL version 2. The default is based on this example but also excludes CBC-based cipher modes to provide extra mitigation against the POODLE attack.","title":"Additional Options"},{"location":"Administrator-Guide/Setting-Up-Clients/","text":"Accessing Data - Setting Up GlusterFS Client You can access gluster volumes in multiple ways. You can use Gluster Native Client method for high concurrency, performance and transparent failover in GNU/Linux clients. You can also use NFS v3 to access gluster volumes. Extensive testing has been done on GNU/Linux clients and NFS implementation in other operating system, such as FreeBSD, and Mac OS X, as well as Windows 7 (Professional and Up) and Windows Server 2003. Other NFS client implementations may work with gluster NFS server. You can use CIFS to access volumes when using Microsoft Windows as well as SAMBA clients. For this access method, Samba packages need to be present on the client side. Gluster Native Client The Gluster Native Client is a FUSE-based client running in user space. Gluster Native Client is the recommended method for accessing volumes when high concurrency and high write performance is required. This section introduces the Gluster Native Client and explains how to install the software on client machines. This section also describes how to mount volumes on clients (both manually and automatically) and how to verify that the volume has mounted successfully. Installing the Gluster Native Client Before you begin installing the Gluster Native Client, you need to verify that the FUSE module is loaded on the client and has access to the required modules as follows: Add the FUSE loadable kernel module (LKM) to the Linux kernel: # modprobe fuse Verify that the FUSE module is loaded: # dmesg | grep -i fuse fuse init (API version 7.13) Installing on Red Hat Package Manager (RPM) Distributions To install Gluster Native Client on RPM distribution-based systems Install required prerequisites on the client using the following command: $ sudo yum -y install openssh-server wget fuse fuse-libs openib libibverbs Ensure that TCP and UDP ports 24007 and 24008 are open on all Gluster servers. Apart from these ports, you need to open one port for each brick starting from port 49152 (instead of 24009 onwards as with previous releases). The brick ports assignment scheme is now compliant with IANA guidelines. For example: if you have five bricks, you need to have ports 49152 to 49156 open. From Gluster-10 onwards, the brick ports will be randomized. A port is randomly selected within the range of base_port to max_port as defined in glusterd.vol file and then assigned to the brick. For example: if you have five bricks, you need to have at least 5 ports open within the given range of base_port and max_port. To reduce the number of open ports (for best security practices), one can lower the max_port value in the glusterd.vol file and restart glusterd to get it into effect. You can use the following chains with iptables: `$ sudo iptables -A RH-Firewall-1-INPUT -m state --state NEW -m tcp -p tcp --dport 24007:24008 -j ACCEPT ` `$ sudo iptables -A RH-Firewall-1-INPUT -m state --state NEW -m tcp -p tcp --dport 49152:49156 -j ACCEPT` > **Note** > > If you already have iptable chains, make sure that the above > ACCEPT rules precede the DROP rules. This can be achieved by > providing a lower rule number than the DROP rule. Download the latest glusterfs, glusterfs-fuse, and glusterfs-rdma RPM files to each client. The glusterfs package contains the Gluster Native Client. The glusterfs-fuse package contains the FUSE translator required for mounting on client systems and the glusterfs-rdma packages contain OpenFabrics verbs RDMA module for Infiniband. You can download the software at GlusterFS download page . Install Gluster Native Client on the client. Note The package versions listed in the example below may not be the latest release. Please refer to the download page to ensure that you have the recently released packages. `$ sudo rpm -i glusterfs-3.8.5-1.x86_64` `$ sudo rpm -i glusterfs-fuse-3.8.5-1.x86_64` `$ sudo rpm -i glusterfs-rdma-3.8.5-1.x86_64` Note: The RDMA module is only required when using Infiniband. Installing on Debian-based Distributions To install Gluster Native Client on Debian-based distributions Install OpenSSH Server on each client using the following command: $ sudo apt-get install openssh-server vim wget Download the latest GlusterFS .deb file and checksum to each client. You can download the software at GlusterFS download page . For each .deb file, get the checksum (using the following command) and compare it against the checksum for that file in the md5sum file. $ md5sum GlusterFS_DEB_file.deb The md5sum of the packages is available at: GlusterFS download page Uninstall GlusterFS v3.1 (or an earlier version) from the client using the following command: $ sudo dpkg -r glusterfs (Optional) Run $ sudo dpkg -purge glusterfs to purge the configuration files. Install Gluster Native Client on the client using the following command: $ sudo dpkg -i GlusterFS_DEB_file For example: $ sudo dpkg -i glusterfs-3.8.x.deb Ensure that TCP and UDP ports 24007 and 24008 are open on all Gluster servers. Apart from these ports, you need to open one port for each brick starting from port 49152 (instead of 24009 onwards as with previous releases). The brick ports assignment scheme is now compliant with IANA guidelines. For example: if you have five bricks, you need to have ports 49152 to 49156 open. From Gluster-10 onwards, the brick ports will be randomized. A port is randomly selected within the range of base_port to max_port as defined in glusterd.vol file and then assigned to the brick. For example: if you have five bricks, you need to have at least 5 ports open within the given range of base_port and max_port. To reduce the number of open ports (for best security practices), one can lower the max_port value in the glusterd.vol file and restart glusterd to get it into effect. You can use the following chains with iptables: `$ sudo iptables -A RH-Firewall-1-INPUT -m state --state NEW -m tcp -p tcp --dport 24007:24008 -j ACCEPT ` `$ sudo iptables -A RH-Firewall-1-INPUT -m state --state NEW -m tcp -p tcp --dport 49152:49156 -j ACCEPT` Note If you already have iptable chains, make sure that the above ACCEPT rules precede the DROP rules. This can be achieved by providing a lower rule number than the DROP rule. Performing a Source Installation To build and install Gluster Native Client from the source code Create a new directory using the following commands: `# mkdir glusterfs ` `# cd glusterfs` Download the source code. You can download the source at link . Extract the source code using the following command: # tar -xvzf SOURCE-FILE Run the configuration utility using the following command: # ./configure GlusterFS configure summary =========================== FUSE client : yes Infiniband verbs : yes epoll IO multiplex : yes argp-standalone : no fusermount : no readline : yes The configuration summary shows the components that will be built with Gluster Native Client. Build the Gluster Native Client software using the following commands: `# make ` `# make install` Verify that the correct version of Gluster Native Client is installed, using the following command: # glusterfs --version Mounting Volumes After installing the Gluster Native Client, you need to mount Gluster volumes to access data. There are two methods you can choose: Manually Mounting Volumes Automatically Mounting Volumes Note Server names selected during creation of Volumes should be resolvable in the client machine. You can use appropriate /etc/hosts entries or DNS server to resolve server names to IP addresses. Manually Mounting Volumes To mount a volume, use the following command: # mount -t glusterfs HOSTNAME-OR-IPADDRESS:/VOLNAME MOUNTDIR For example: # mount -t glusterfs server1:/test-volume /mnt/glusterfs Note The server specified in the mount command is only used to fetch the gluster configuration volfile describing the volume name. Subsequently, the client will communicate directly with the servers mentioned in the volfile (which might not even include the one used for mount). If you see a usage message like \"Usage: mount.glusterfs\", mount usually requires you to create a directory to be used as the mount point. Run \"mkdir /mnt/glusterfs\" before you attempt to run the mount command listed above. Mounting Options You can specify the following options when using the mount -t glusterfs command. Note that you need to separate all options with commas. backupvolfile-server=server-name volfile-max-fetch-attempts=number of attempts log-level=loglevel log-file=logfile transport=transport-type direct-io-mode=[enable|disable] use-readdirp=[yes|no] For example: # mount -t glusterfs -o backupvolfile-server=volfile_server2,use-readdirp=no,volfile-max-fetch-attempts=2,log-level=WARNING,log-file=/var/log/gluster.log server1:/test-volume /mnt/glusterfs If backupvolfile-server option is added while mounting fuse client, when the first volfile server fails, then the server specified in backupvolfile-server option is used as volfile server to mount the client. In volfile-max-fetch-attempts=X option, specify the number of attempts to fetch volume files while mounting a volume. This option is useful when you mount a server with multiple IP addresses or when round-robin DNS is configured for the server-name.. If use-readdirp is set to ON, it forces the use of readdirp mode in fuse kernel module Automatically Mounting Volumes You can configure your system to automatically mount the Gluster volume each time your system starts. The server specified in the mount command is only used to fetch the gluster configuration volfile describing the volume name. Subsequently, the client will communicate directly with the servers mentioned in the volfile (which might not even include the one used for mount). To mount a volume, edit the /etc/fstab file and add the following line: HOSTNAME-OR-IPADDRESS:/VOLNAME MOUNTDIR glusterfs defaults,_netdev 0 0 For example: server1:/test-volume /mnt/glusterfs glusterfs defaults,_netdev 0 0 Mounting Options You can specify the following options when updating the /etc/fstab file. Note that you need to separate all options with commas. log-level=loglevel log-file=logfile transport=transport-type direct-io-mode=[enable|disable] use-readdirp=no For example: HOSTNAME-OR-IPADDRESS:/VOLNAME MOUNTDIR glusterfs defaults,_netdev,log-level=WARNING,log-file=/var/log/gluster.log 0 0 Testing Mounted Volumes To test mounted volumes Use the following command: # mount If the gluster volume was successfully mounted, the output of the mount command on the client will be similar to this example: server1:/test-volume on /mnt/glusterfs type fuse.glusterfs (rw,allow_other,default_permissions,max_read=131072 Use the following command: # df The output of df command on the client will display the aggregated storage space from all the bricks in a volume similar to this example: # df -h /mnt/glusterfs Filesystem Size Used Avail Use% Mounted on server1:/test-volume 28T 22T 5.4T 82% /mnt/glusterfs Change to the directory and list the contents by entering the following: `# cd MOUNTDIR ` `# ls` For example, `# cd /mnt/glusterfs ` `# ls` NFS You can use NFS v3 to access to gluster volumes. Extensive testing has be done on GNU/Linux clients and NFS implementation in other operating system, such as FreeBSD, and Mac OS X, as well as Windows 7 (Professional and Up), Windows Server 2003, and others, may work with gluster NFS server implementation. GlusterFS now includes network lock manager (NLM) v4. NLM enables applications on NFSv3 clients to do record locking on files on NFS server. It is started automatically whenever the NFS server is run. You must install nfs-common package on both servers and clients (only for Debian-based) distribution. This section describes how to use NFS to mount Gluster volumes (both manually and automatically) and how to verify that the volume has been mounted successfully. Using NFS to Mount Volumes You can use either of the following methods to mount Gluster volumes: Manually Mounting Volumes Using NFS Automatically Mounting Volumes Using NFS Prerequisite : Install nfs-common package on both servers and clients (only for Debian-based distribution), using the following command: $ sudo aptitude install nfs-common Manually Mounting Volumes Using NFS To manually mount a Gluster volume using NFS To mount a volume, use the following command: # mount -t nfs -o vers=3 HOSTNAME-OR-IPADDRESS:/VOLNAME MOUNTDIR For example: # mount -t nfs -o vers=3 server1:/test-volume /mnt/glusterfs Note Gluster NFS server does not support UDP. If the NFS client you are using defaults to connecting using UDP, the following message appears: requested NFS version or transport protocol is not supported . To connect using TCP Add the following option to the mount command: -o mountproto=tcp For example: # mount -o mountproto=tcp -t nfs server1:/test-volume /mnt/glusterfs To mount Gluster NFS server from a Solaris client Use the following command: # mount -o proto=tcp,vers=3 nfs://HOSTNAME-OR-IPADDRESS:38467/VOLNAME MOUNTDIR For example: # mount -o proto=tcp,vers=3 nfs://server1:38467/test-volume /mnt/glusterfs Automatically Mounting Volumes Using NFS You can configure your system to automatically mount Gluster volumes using NFS each time the system starts. To automatically mount a Gluster volume using NFS To mount a volume, edit the /etc/fstab file and add the following line: HOSTNAME-OR-IPADDRESS:/VOLNAME MOUNTDIR nfs defaults,_netdev,vers=3 0 0 For example, server1:/test-volume /mnt/glusterfs nfs defaults,_netdev,vers=3 0 0 Note Gluster NFS server does not support UDP. If the NFS client you are using defaults to connecting using UDP, the following message appears: requested NFS version or transport protocol is not supported. To connect using TCP Add the following entry in /etc/fstab file : HOSTNAME-OR-IPADDRESS:/VOLNAME MOUNTDIR nfs defaults,_netdev,mountproto=tcp 0 0 For example, server1:/test-volume /mnt/glusterfs nfs defaults,_netdev,mountproto=tcp 0 0 To automount NFS mounts Gluster supports *nix standard method of automounting NFS mounts. Update the /etc/auto.master and /etc/auto.misc and restart the autofs service. After that, whenever a user or process attempts to access the directory it will be mounted in the background. Testing Volumes Mounted Using NFS You can confirm that Gluster directories are mounting successfully. To test mounted volumes Use the mount command by entering the following: # mount For example, the output of the mount command on the client will display an entry like the following: server1:/test-volume on /mnt/glusterfs type nfs (rw,vers=3,addr=server1) Use the df command by entering the following: # df For example, the output of df command on the client will display the aggregated storage space from all the bricks in a volume. # df -h /mnt/glusterfs Filesystem Size Used Avail Use% Mounted on server1:/test-volume 28T 22T 5.4T 82% /mnt/glusterfs Change to the directory and list the contents by entering the following: # cd MOUNTDIR # ls CIFS You can use CIFS to access to volumes when using Microsoft Windows as well as SAMBA clients. For this access method, Samba packages need to be present on the client side. You can export glusterfs mount point as the samba export, and then mount it using CIFS protocol. This section describes how to mount CIFS shares on Microsoft Windows-based clients (both manually and automatically) and how to verify that the volume has mounted successfully. Note CIFS access using the Mac OS X Finder is not supported, however, you can use the Mac OS X command line to access Gluster volumes using CIFS. Using CIFS to Mount Volumes You can use either of the following methods to mount Gluster volumes: Exporting Gluster Volumes Through Samba Manually Mounting Volumes Using CIFS Automatically Mounting Volumes Using CIFS You can also use Samba for exporting Gluster Volumes through CIFS protocol. Exporting Gluster Volumes Through Samba We recommend you to use Samba for exporting Gluster volumes through the CIFS protocol. To export volumes through CIFS protocol Mount a Gluster volume. Setup Samba configuration to export the mount point of the Gluster volume. For example, if a Gluster volume is mounted on /mnt/gluster, you must edit smb.conf file to enable exporting this through CIFS. Open smb.conf file in an editor and add the following lines for a simple configuration: [glustertest] comment = For testing a Gluster volume exported through CIFS path = /mnt/glusterfs read only = no guest ok = yes Save the changes and start the smb service using your systems init scripts (/etc/init.d/smb [re]start). Abhove steps is needed for doing multiple mount. If you want only samba mount then in your smb.conf you need to add kernel share modes = no kernel oplocks = no map archive = no map hidden = no map read only = no map system = no store dos attributes = yes Note To be able mount from any server in the trusted storage pool, you must repeat these steps on each Gluster node. For more advanced configurations, see Samba documentation. Manually Mounting Volumes Using CIFS You can manually mount Gluster volumes using CIFS on Microsoft Windows-based client machines. To manually mount a Gluster volume using CIFS Using Windows Explorer, choose Tools > Map Network Drive\u2026 from the menu. The Map Network Drive window appears. Choose the drive letter using the Drive drop-down list. Click Browse , select the volume to map to the network drive, and click OK . Click Finish. The network drive (mapped to the volume) appears in the Computer window. Alternatively, to manually mount a Gluster volume using CIFS by going to Start > Run and entering Network path manually. Automatically Mounting Volumes Using CIFS You can configure your system to automatically mount Gluster volumes using CIFS on Microsoft Windows-based clients each time the system starts. To automatically mount a Gluster volume using CIFS The network drive (mapped to the volume) appears in the Computer window and is reconnected each time the system starts. Using Windows Explorer, choose Tools > Map Network Drive\u2026 from the menu. The Map Network Drive window appears. Choose the drive letter using the Drive drop-down list. Click Browse , select the volume to map to the network drive, and click OK . Click the Reconnect at logon checkbox. Click Finish. Testing Volumes Mounted Using CIFS You can confirm that Gluster directories are mounting successfully by navigating to the directory using Windows Explorer.","title":"Setting Up Clients"},{"location":"Administrator-Guide/Setting-Up-Clients/#accessing-data-setting-up-glusterfs-client","text":"You can access gluster volumes in multiple ways. You can use Gluster Native Client method for high concurrency, performance and transparent failover in GNU/Linux clients. You can also use NFS v3 to access gluster volumes. Extensive testing has been done on GNU/Linux clients and NFS implementation in other operating system, such as FreeBSD, and Mac OS X, as well as Windows 7 (Professional and Up) and Windows Server 2003. Other NFS client implementations may work with gluster NFS server. You can use CIFS to access volumes when using Microsoft Windows as well as SAMBA clients. For this access method, Samba packages need to be present on the client side.","title":"Accessing Data - Setting Up GlusterFS Client"},{"location":"Administrator-Guide/Setting-Up-Clients/#gluster-native-client","text":"The Gluster Native Client is a FUSE-based client running in user space. Gluster Native Client is the recommended method for accessing volumes when high concurrency and high write performance is required. This section introduces the Gluster Native Client and explains how to install the software on client machines. This section also describes how to mount volumes on clients (both manually and automatically) and how to verify that the volume has mounted successfully.","title":"Gluster Native Client"},{"location":"Administrator-Guide/Setting-Up-Clients/#installing-the-gluster-native-client","text":"Before you begin installing the Gluster Native Client, you need to verify that the FUSE module is loaded on the client and has access to the required modules as follows: Add the FUSE loadable kernel module (LKM) to the Linux kernel: # modprobe fuse Verify that the FUSE module is loaded: # dmesg | grep -i fuse fuse init (API version 7.13)","title":"Installing the Gluster Native Client"},{"location":"Administrator-Guide/Setting-Up-Clients/#installing-on-red-hat-package-manager-rpm-distributions","text":"To install Gluster Native Client on RPM distribution-based systems Install required prerequisites on the client using the following command: $ sudo yum -y install openssh-server wget fuse fuse-libs openib libibverbs Ensure that TCP and UDP ports 24007 and 24008 are open on all Gluster servers. Apart from these ports, you need to open one port for each brick starting from port 49152 (instead of 24009 onwards as with previous releases). The brick ports assignment scheme is now compliant with IANA guidelines. For example: if you have five bricks, you need to have ports 49152 to 49156 open. From Gluster-10 onwards, the brick ports will be randomized. A port is randomly selected within the range of base_port to max_port as defined in glusterd.vol file and then assigned to the brick. For example: if you have five bricks, you need to have at least 5 ports open within the given range of base_port and max_port. To reduce the number of open ports (for best security practices), one can lower the max_port value in the glusterd.vol file and restart glusterd to get it into effect. You can use the following chains with iptables: `$ sudo iptables -A RH-Firewall-1-INPUT -m state --state NEW -m tcp -p tcp --dport 24007:24008 -j ACCEPT ` `$ sudo iptables -A RH-Firewall-1-INPUT -m state --state NEW -m tcp -p tcp --dport 49152:49156 -j ACCEPT` > **Note** > > If you already have iptable chains, make sure that the above > ACCEPT rules precede the DROP rules. This can be achieved by > providing a lower rule number than the DROP rule. Download the latest glusterfs, glusterfs-fuse, and glusterfs-rdma RPM files to each client. The glusterfs package contains the Gluster Native Client. The glusterfs-fuse package contains the FUSE translator required for mounting on client systems and the glusterfs-rdma packages contain OpenFabrics verbs RDMA module for Infiniband. You can download the software at GlusterFS download page . Install Gluster Native Client on the client. Note The package versions listed in the example below may not be the latest release. Please refer to the download page to ensure that you have the recently released packages. `$ sudo rpm -i glusterfs-3.8.5-1.x86_64` `$ sudo rpm -i glusterfs-fuse-3.8.5-1.x86_64` `$ sudo rpm -i glusterfs-rdma-3.8.5-1.x86_64` Note: The RDMA module is only required when using Infiniband.","title":"Installing on Red Hat Package Manager (RPM) Distributions"},{"location":"Administrator-Guide/Setting-Up-Clients/#installing-on-debian-based-distributions","text":"To install Gluster Native Client on Debian-based distributions Install OpenSSH Server on each client using the following command: $ sudo apt-get install openssh-server vim wget Download the latest GlusterFS .deb file and checksum to each client. You can download the software at GlusterFS download page . For each .deb file, get the checksum (using the following command) and compare it against the checksum for that file in the md5sum file. $ md5sum GlusterFS_DEB_file.deb The md5sum of the packages is available at: GlusterFS download page Uninstall GlusterFS v3.1 (or an earlier version) from the client using the following command: $ sudo dpkg -r glusterfs (Optional) Run $ sudo dpkg -purge glusterfs to purge the configuration files. Install Gluster Native Client on the client using the following command: $ sudo dpkg -i GlusterFS_DEB_file For example: $ sudo dpkg -i glusterfs-3.8.x.deb Ensure that TCP and UDP ports 24007 and 24008 are open on all Gluster servers. Apart from these ports, you need to open one port for each brick starting from port 49152 (instead of 24009 onwards as with previous releases). The brick ports assignment scheme is now compliant with IANA guidelines. For example: if you have five bricks, you need to have ports 49152 to 49156 open. From Gluster-10 onwards, the brick ports will be randomized. A port is randomly selected within the range of base_port to max_port as defined in glusterd.vol file and then assigned to the brick. For example: if you have five bricks, you need to have at least 5 ports open within the given range of base_port and max_port. To reduce the number of open ports (for best security practices), one can lower the max_port value in the glusterd.vol file and restart glusterd to get it into effect. You can use the following chains with iptables: `$ sudo iptables -A RH-Firewall-1-INPUT -m state --state NEW -m tcp -p tcp --dport 24007:24008 -j ACCEPT ` `$ sudo iptables -A RH-Firewall-1-INPUT -m state --state NEW -m tcp -p tcp --dport 49152:49156 -j ACCEPT` Note If you already have iptable chains, make sure that the above ACCEPT rules precede the DROP rules. This can be achieved by providing a lower rule number than the DROP rule.","title":"Installing on Debian-based Distributions"},{"location":"Administrator-Guide/Setting-Up-Clients/#performing-a-source-installation","text":"To build and install Gluster Native Client from the source code Create a new directory using the following commands: `# mkdir glusterfs ` `# cd glusterfs` Download the source code. You can download the source at link . Extract the source code using the following command: # tar -xvzf SOURCE-FILE Run the configuration utility using the following command: # ./configure GlusterFS configure summary =========================== FUSE client : yes Infiniband verbs : yes epoll IO multiplex : yes argp-standalone : no fusermount : no readline : yes The configuration summary shows the components that will be built with Gluster Native Client. Build the Gluster Native Client software using the following commands: `# make ` `# make install` Verify that the correct version of Gluster Native Client is installed, using the following command: # glusterfs --version","title":"Performing a Source Installation"},{"location":"Administrator-Guide/Setting-Up-Clients/#mounting-volumes","text":"After installing the Gluster Native Client, you need to mount Gluster volumes to access data. There are two methods you can choose: Manually Mounting Volumes Automatically Mounting Volumes Note Server names selected during creation of Volumes should be resolvable in the client machine. You can use appropriate /etc/hosts entries or DNS server to resolve server names to IP addresses.","title":"Mounting Volumes"},{"location":"Administrator-Guide/Setting-Up-Clients/#manually-mounting-volumes","text":"To mount a volume, use the following command: # mount -t glusterfs HOSTNAME-OR-IPADDRESS:/VOLNAME MOUNTDIR For example: # mount -t glusterfs server1:/test-volume /mnt/glusterfs Note The server specified in the mount command is only used to fetch the gluster configuration volfile describing the volume name. Subsequently, the client will communicate directly with the servers mentioned in the volfile (which might not even include the one used for mount). If you see a usage message like \"Usage: mount.glusterfs\", mount usually requires you to create a directory to be used as the mount point. Run \"mkdir /mnt/glusterfs\" before you attempt to run the mount command listed above. Mounting Options You can specify the following options when using the mount -t glusterfs command. Note that you need to separate all options with commas. backupvolfile-server=server-name volfile-max-fetch-attempts=number of attempts log-level=loglevel log-file=logfile transport=transport-type direct-io-mode=[enable|disable] use-readdirp=[yes|no] For example: # mount -t glusterfs -o backupvolfile-server=volfile_server2,use-readdirp=no,volfile-max-fetch-attempts=2,log-level=WARNING,log-file=/var/log/gluster.log server1:/test-volume /mnt/glusterfs If backupvolfile-server option is added while mounting fuse client, when the first volfile server fails, then the server specified in backupvolfile-server option is used as volfile server to mount the client. In volfile-max-fetch-attempts=X option, specify the number of attempts to fetch volume files while mounting a volume. This option is useful when you mount a server with multiple IP addresses or when round-robin DNS is configured for the server-name.. If use-readdirp is set to ON, it forces the use of readdirp mode in fuse kernel module","title":"Manually Mounting Volumes"},{"location":"Administrator-Guide/Setting-Up-Clients/#automatically-mounting-volumes","text":"You can configure your system to automatically mount the Gluster volume each time your system starts. The server specified in the mount command is only used to fetch the gluster configuration volfile describing the volume name. Subsequently, the client will communicate directly with the servers mentioned in the volfile (which might not even include the one used for mount). To mount a volume, edit the /etc/fstab file and add the following line: HOSTNAME-OR-IPADDRESS:/VOLNAME MOUNTDIR glusterfs defaults,_netdev 0 0 For example: server1:/test-volume /mnt/glusterfs glusterfs defaults,_netdev 0 0 Mounting Options You can specify the following options when updating the /etc/fstab file. Note that you need to separate all options with commas. log-level=loglevel log-file=logfile transport=transport-type direct-io-mode=[enable|disable] use-readdirp=no For example: HOSTNAME-OR-IPADDRESS:/VOLNAME MOUNTDIR glusterfs defaults,_netdev,log-level=WARNING,log-file=/var/log/gluster.log 0 0","title":"Automatically Mounting Volumes"},{"location":"Administrator-Guide/Setting-Up-Clients/#testing-mounted-volumes","text":"To test mounted volumes Use the following command: # mount If the gluster volume was successfully mounted, the output of the mount command on the client will be similar to this example: server1:/test-volume on /mnt/glusterfs type fuse.glusterfs (rw,allow_other,default_permissions,max_read=131072 Use the following command: # df The output of df command on the client will display the aggregated storage space from all the bricks in a volume similar to this example: # df -h /mnt/glusterfs Filesystem Size Used Avail Use% Mounted on server1:/test-volume 28T 22T 5.4T 82% /mnt/glusterfs Change to the directory and list the contents by entering the following: `# cd MOUNTDIR ` `# ls` For example, `# cd /mnt/glusterfs ` `# ls`","title":"Testing Mounted Volumes"},{"location":"Administrator-Guide/Setting-Up-Clients/#nfs","text":"You can use NFS v3 to access to gluster volumes. Extensive testing has be done on GNU/Linux clients and NFS implementation in other operating system, such as FreeBSD, and Mac OS X, as well as Windows 7 (Professional and Up), Windows Server 2003, and others, may work with gluster NFS server implementation. GlusterFS now includes network lock manager (NLM) v4. NLM enables applications on NFSv3 clients to do record locking on files on NFS server. It is started automatically whenever the NFS server is run. You must install nfs-common package on both servers and clients (only for Debian-based) distribution. This section describes how to use NFS to mount Gluster volumes (both manually and automatically) and how to verify that the volume has been mounted successfully.","title":"NFS"},{"location":"Administrator-Guide/Setting-Up-Clients/#using-nfs-to-mount-volumes","text":"You can use either of the following methods to mount Gluster volumes: Manually Mounting Volumes Using NFS Automatically Mounting Volumes Using NFS Prerequisite : Install nfs-common package on both servers and clients (only for Debian-based distribution), using the following command: $ sudo aptitude install nfs-common","title":"Using NFS to Mount Volumes"},{"location":"Administrator-Guide/Setting-Up-Clients/#manually-mounting-volumes-using-nfs","text":"To manually mount a Gluster volume using NFS To mount a volume, use the following command: # mount -t nfs -o vers=3 HOSTNAME-OR-IPADDRESS:/VOLNAME MOUNTDIR For example: # mount -t nfs -o vers=3 server1:/test-volume /mnt/glusterfs Note Gluster NFS server does not support UDP. If the NFS client you are using defaults to connecting using UDP, the following message appears: requested NFS version or transport protocol is not supported . To connect using TCP Add the following option to the mount command: -o mountproto=tcp For example: # mount -o mountproto=tcp -t nfs server1:/test-volume /mnt/glusterfs To mount Gluster NFS server from a Solaris client Use the following command: # mount -o proto=tcp,vers=3 nfs://HOSTNAME-OR-IPADDRESS:38467/VOLNAME MOUNTDIR For example: # mount -o proto=tcp,vers=3 nfs://server1:38467/test-volume /mnt/glusterfs","title":"Manually Mounting Volumes Using NFS"},{"location":"Administrator-Guide/Setting-Up-Clients/#automatically-mounting-volumes-using-nfs","text":"You can configure your system to automatically mount Gluster volumes using NFS each time the system starts. To automatically mount a Gluster volume using NFS To mount a volume, edit the /etc/fstab file and add the following line: HOSTNAME-OR-IPADDRESS:/VOLNAME MOUNTDIR nfs defaults,_netdev,vers=3 0 0 For example, server1:/test-volume /mnt/glusterfs nfs defaults,_netdev,vers=3 0 0 Note Gluster NFS server does not support UDP. If the NFS client you are using defaults to connecting using UDP, the following message appears: requested NFS version or transport protocol is not supported. To connect using TCP Add the following entry in /etc/fstab file : HOSTNAME-OR-IPADDRESS:/VOLNAME MOUNTDIR nfs defaults,_netdev,mountproto=tcp 0 0 For example, server1:/test-volume /mnt/glusterfs nfs defaults,_netdev,mountproto=tcp 0 0 To automount NFS mounts Gluster supports *nix standard method of automounting NFS mounts. Update the /etc/auto.master and /etc/auto.misc and restart the autofs service. After that, whenever a user or process attempts to access the directory it will be mounted in the background.","title":"Automatically Mounting Volumes Using NFS"},{"location":"Administrator-Guide/Setting-Up-Clients/#testing-volumes-mounted-using-nfs","text":"You can confirm that Gluster directories are mounting successfully. To test mounted volumes Use the mount command by entering the following: # mount For example, the output of the mount command on the client will display an entry like the following: server1:/test-volume on /mnt/glusterfs type nfs (rw,vers=3,addr=server1) Use the df command by entering the following: # df For example, the output of df command on the client will display the aggregated storage space from all the bricks in a volume. # df -h /mnt/glusterfs Filesystem Size Used Avail Use% Mounted on server1:/test-volume 28T 22T 5.4T 82% /mnt/glusterfs Change to the directory and list the contents by entering the following: # cd MOUNTDIR # ls","title":"Testing Volumes Mounted Using NFS"},{"location":"Administrator-Guide/Setting-Up-Clients/#cifs","text":"You can use CIFS to access to volumes when using Microsoft Windows as well as SAMBA clients. For this access method, Samba packages need to be present on the client side. You can export glusterfs mount point as the samba export, and then mount it using CIFS protocol. This section describes how to mount CIFS shares on Microsoft Windows-based clients (both manually and automatically) and how to verify that the volume has mounted successfully. Note CIFS access using the Mac OS X Finder is not supported, however, you can use the Mac OS X command line to access Gluster volumes using CIFS.","title":"CIFS"},{"location":"Administrator-Guide/Setting-Up-Clients/#using-cifs-to-mount-volumes","text":"You can use either of the following methods to mount Gluster volumes: Exporting Gluster Volumes Through Samba Manually Mounting Volumes Using CIFS Automatically Mounting Volumes Using CIFS You can also use Samba for exporting Gluster Volumes through CIFS protocol.","title":"Using CIFS to Mount Volumes"},{"location":"Administrator-Guide/Setting-Up-Clients/#exporting-gluster-volumes-through-samba","text":"We recommend you to use Samba for exporting Gluster volumes through the CIFS protocol. To export volumes through CIFS protocol Mount a Gluster volume. Setup Samba configuration to export the mount point of the Gluster volume. For example, if a Gluster volume is mounted on /mnt/gluster, you must edit smb.conf file to enable exporting this through CIFS. Open smb.conf file in an editor and add the following lines for a simple configuration: [glustertest] comment = For testing a Gluster volume exported through CIFS path = /mnt/glusterfs read only = no guest ok = yes Save the changes and start the smb service using your systems init scripts (/etc/init.d/smb [re]start). Abhove steps is needed for doing multiple mount. If you want only samba mount then in your smb.conf you need to add kernel share modes = no kernel oplocks = no map archive = no map hidden = no map read only = no map system = no store dos attributes = yes Note To be able mount from any server in the trusted storage pool, you must repeat these steps on each Gluster node. For more advanced configurations, see Samba documentation.","title":"Exporting Gluster Volumes Through Samba"},{"location":"Administrator-Guide/Setting-Up-Clients/#manually-mounting-volumes-using-cifs","text":"You can manually mount Gluster volumes using CIFS on Microsoft Windows-based client machines. To manually mount a Gluster volume using CIFS Using Windows Explorer, choose Tools > Map Network Drive\u2026 from the menu. The Map Network Drive window appears. Choose the drive letter using the Drive drop-down list. Click Browse , select the volume to map to the network drive, and click OK . Click Finish. The network drive (mapped to the volume) appears in the Computer window. Alternatively, to manually mount a Gluster volume using CIFS by going to Start > Run and entering Network path manually.","title":"Manually Mounting Volumes Using CIFS"},{"location":"Administrator-Guide/Setting-Up-Clients/#automatically-mounting-volumes-using-cifs","text":"You can configure your system to automatically mount Gluster volumes using CIFS on Microsoft Windows-based clients each time the system starts. To automatically mount a Gluster volume using CIFS The network drive (mapped to the volume) appears in the Computer window and is reconnected each time the system starts. Using Windows Explorer, choose Tools > Map Network Drive\u2026 from the menu. The Map Network Drive window appears. Choose the drive letter using the Drive drop-down list. Click Browse , select the volume to map to the network drive, and click OK . Click the Reconnect at logon checkbox. Click Finish.","title":"Automatically Mounting Volumes Using CIFS"},{"location":"Administrator-Guide/Setting-Up-Clients/#testing-volumes-mounted-using-cifs","text":"You can confirm that Gluster directories are mounting successfully by navigating to the directory using Windows Explorer.","title":"Testing Volumes Mounted Using CIFS"},{"location":"Administrator-Guide/Setting-Up-Volumes/","text":"Setting up GlusterFS Volumes A volume is a logical collection of bricks where each brick is an export directory on a server in the trusted storage pool. To create a new volume in your storage environment, specify the bricks that comprise the volume. After you have created a new volume, you must start it before attempting to mount it. See Setting up Storage for how to set up bricks. Volume Types Volumes of the following types can be created in your storage environment: Distributed - Distributed volumes distribute files across the bricks in the volume. You can use distributed volumes where the requirement is to scale storage and the redundancy is either not important or is provided by other hardware/software layers. Replicated \u2013 Replicated volumes replicate files across bricks in the volume. You can use replicated volumes in environments where high-availability and high-reliability are critical. Distributed Replicated - Distributed replicated volumes distribute files across replicated bricks in the volume. You can use distributed replicated volumes in environments where the requirement is to scale storage and high-reliability is critical. Distributed replicated volumes also offer improved read performance in most environments. Dispersed - Dispersed volumes are based on erasure codes, providing space-efficient protection against disk or server failures. It stores an encoded fragment of the original file to each brick in a way that only a subset of the fragments is needed to recover the original file. The number of bricks that can be missing without losing access to data is configured by the administrator on volume creation time. Distributed Dispersed - Distributed dispersed volumes distribute files across dispersed subvolumes. This has the same advantages of distribute replicate volumes, but using disperse to store the data into the bricks. To create a new volume Create a new volume : # gluster volume create <NEW-VOLNAME> [[replica <COUNT> [arbiter <COUNT>]]|[replica 2 thin-arbiter 1]] [disperse [<COUNT>]] [disperse-data <COUNT>] [redundancy <COUNT>] [transport <tcp|rdma|tcp,rdma>] <NEW-BRICK> <TA-BRICK>... [force] For example, to create a volume called test-volume consisting of server3:/exp3 and server4:/exp4: # gluster volume create test-volume server3:/exp3 server4:/exp4 Creation of test-volume has been successful Please start the volume to access data. Creating Distributed Volumes In a distributed volume files are spread randomly across the bricks in the volume. Use distributed volumes where you need to scale storage and redundancy is either not important or is provided by other hardware/software layers. Note : Disk/server failure in distributed volumes can result in a serious loss of data because directory contents are spread randomly across the bricks in the volume. To create a distributed volume Create a trusted storage pool. Create the distributed volume: # gluster volume create [transport tcp | rdma | tcp,rdma] For example, to create a distributed volume with four storage servers using tcp: # gluster volume create test-volume server1:/exp1 server2:/exp2 server3:/exp3 server4:/exp4 Creation of test-volume has been successful Please start the volume to access data. (Optional) You can display the volume information: # gluster volume info Volume Name: test-volume Type: Distribute Status: Created Number of Bricks: 4 Transport-type: tcp Bricks: Brick1: server1:/exp1 Brick2: server2:/exp2 Brick3: server3:/exp3 Brick4: server4:/exp4 For example, to create a distributed volume with four storage servers over InfiniBand: # gluster volume create test-volume transport rdma server1:/exp1 server2:/exp2 server3:/exp3 server4:/exp4 Creation of test-volume has been successful Please start the volume to access data. If the transport type is not specified, tcp is used as the default. You can also set additional options if required, such as auth.allow or auth.reject. Note : Make sure you start your volumes before you try to mount them or else client operations after the mount will hang. Creating Replicated Volumes Replicated volumes create copies of files across multiple bricks in the volume. You can use replicated volumes in environments where high-availability and high-reliability are critical. Note : The number of bricks should be equal to of the replica count for a replicated volume. To protect against server and disk failures, it is recommended that the bricks of the volume are from different servers. To create a replicated volume Create a trusted storage pool. Create the replicated volume: # gluster volume create [replica ] [transport tcp | rdma | tcp,rdma] For example, to create a replicated volume with two storage servers: # gluster volume create test-volume replica 2 transport tcp server1:/exp1 server2:/exp2 Creation of test-volume has been successful Please start the volume to access data. If the transport type is not specified, tcp is used as the default. You can also set additional options if required, such as auth.allow or auth.reject. Note : Make sure you start your volumes before you try to mount them or else client operations after the mount will hang. GlusterFS will fail to create a replicate volume if more than one brick of a replica set is present on the same peer. For eg. a four node replicated volume where more than one brick of a replica set is present on the same peer. # gluster volume create <volname> replica 4 server1:/brick1 server1:/brick2 server2:/brick3 server4:/brick4 volume create: <volname>: failed: Multiple bricks of a replicate volume are present on the same server. This setup is not optimal. Use 'force' at the end of the command if you want to override this behavior. Use the force option at the end of command if you still want to create the volume with this configuration. Arbiter configuration for replica volumes Arbiter volumes are replica 3 volumes where the 3rd brick acts as the arbiter brick. This configuration has mechanisms that prevent occurrence of split-brains. It can be created with the following command: `# gluster volume create <VOLNAME> replica 2 arbiter 1 host1:brick1 host2:brick2 host3:brick3` More information about this configuration can be found at Administrator-Guide : arbiter-volumes-and-quorum Note that the arbiter configuration for replica 3 can be used to create distributed-replicate volumes as well. Creating Distributed Replicated Volumes Distributes files across replicated bricks in the volume. You can use distributed replicated volumes in environments where the requirement is to scale storage and high-reliability is critical. Distributed replicated volumes also offer improved read performance in most environments. Note : The number of bricks should be a multiple of the replica count for a distributed replicated volume. Also, the order in which bricks are specified has a great effect on data protection. Each replica_count consecutive bricks in the list you give will form a replica set, with all replica sets combined into a volume-wide distribute set. To make sure that replica-set members are not placed on the same node, list the first brick on every server, then the second brick on every server in the same order, and so on. To create a distributed replicated volume Create a trusted storage pool. Create the distributed replicated volume: # gluster volume create [replica ] [transport tcp | rdma | tcp,rdma] For example, a four node distributed (replicated) volume with a two-way mirror: # gluster volume create test-volume replica 2 transport tcp server1:/exp1 server2:/exp2 server3:/exp3 server4:/exp4 Creation of test-volume has been successful Please start the volume to access data. For example, to create a six node distributed (replicated) volume with a two-way mirror: # gluster volume create test-volume replica 2 transport tcp server1:/exp1 server2:/exp2 server3:/exp3 server4:/exp4 server5:/exp5 server6:/exp6 Creation of test-volume has been successful Please start the volume to access data. If the transport type is not specified, tcp is used as the default. You can also set additional options if required, such as auth.allow or auth.reject. Note : - Make sure you start your volumes before you try to mount them or else client operations after the mount will hang. GlusterFS will fail to create a distribute replicate volume if more than one brick of a replica set is present on the same peer. For eg. for a four node distribute (replicated) volume where more than one brick of a replica set is present on the same peer. # gluster volume create <volname> replica 2 server1:/brick1 server1:/brick2 server2:/brick3 server4:/brick4 volume create: <volname>: failed: Multiple bricks of a replicate volume are present on the same server. This setup is not optimal. Use 'force' at the end of the command if you want to override this behavior. Use the force option at the end of command if you want to create the volume in this case. Creating Dispersed Volumes Dispersed volumes are based on erasure codes. It stripes the encoded data of files, with some redundancy added, across multiple bricks in the volume. You can use dispersed volumes to have a configurable level of reliability with minimum space waste. Redundancy Each dispersed volume has a redundancy value defined when the volume is created. This value determines how many bricks can be lost without interrupting the operation of the volume. It also determines the amount of usable space of the volume using this formula: <Usable size> = <Brick size> * (#Bricks - Redundancy) All bricks of a disperse set should have the same capacity, otherwise, when the smallest brick becomes full, no additional data will be allowed in the disperse set. It's important to note that a configuration with 3 bricks and redundancy 1 will have less usable space (66.7% of the total physical space) than a configuration with 10 bricks and redundancy 1 (90%). However the first one will be safer than the second one (roughly the probability of failure of the second configuration if more than 4.5 times bigger than the first one). For example, a dispersed volume composed of 6 bricks of 4TB and a redundancy of 2 will be completely operational even with two bricks inaccessible. However a third inaccessible brick will bring the volume down because it won't be possible to read or write to it. The usable space of the volume will be equal to 16TB. The implementation of erasure codes in GlusterFS limits the redundancy to a value smaller than #Bricks / 2 (or equivalently, redundancy * 2 < #Bricks). Having a redundancy equal to half of the number of bricks would be almost equivalent to a replica-2 volume, and probably a replicated volume will perform better in this case. Optimal volumes One of the worst things erasure codes have in terms of performance is the RMW (Read-Modify-Write) cycle. Erasure codes operate in blocks of a certain size and it cannot work with smaller ones. This means that if a user issues a write of a portion of a file that doesn't fill a full block, it needs to read the remaining portion from the current contents of the file, merge them, compute the updated encoded block and, finally, writing the resulting data. This adds latency, reducing performance when this happens. Some GlusterFS performance xlators can help to reduce or even eliminate this problem for some workloads, but it should be taken into account when using dispersed volumes for a specific use case. Current implementation of dispersed volumes use blocks of a size that depends on the number of bricks and redundancy: 512 * (#Bricks - redundancy) bytes. This value is also known as the stripe size. Using combinations of #Bricks/redundancy that give a power of two for the stripe size will make the disperse volume perform better in most workloads because it's more typical to write information in blocks that are multiple of two (for example databases, virtual machines and many applications). These combinations are considered optimal . For example, a configuration with 6 bricks and redundancy 2 will have a stripe size of 512 * (6 - 2) = 2048 bytes, so it's considered optimal. A configuration with 7 bricks and redundancy 2 would have a stripe size of 2560 bytes, needing a RMW cycle for many writes (of course this always depends on the use case). To create a dispersed volume Create a trusted storage pool. Create the dispersed volume: # gluster volume create [disperse [<count>]] [redundancy <count>] [transport tcp | rdma | tcp,rdma] A dispersed volume can be created by specifying the number of bricks in a disperse set, by specifying the number of redundancy bricks, or both. If disperse is not specified, or the <count> is missing, the entire volume will be treated as a single disperse set composed by all bricks enumerated in the command line. If redundancy is not specified, it is computed automatically to be the optimal value. If this value does not exist, it's assumed to be '1' and a warning message is shown: # gluster volume create test-volume disperse 4 server{1..4}:/bricks/test-volume There isn't an optimal redundancy value for this configuration. Do you want to create the volume with redundancy 1 ? (y/n) In all cases where redundancy is automatically computed and it's not equal to '1', a warning message is displayed: # gluster volume create test-volume disperse 6 server{1..6}:/bricks/test-volume The optimal redundancy for this configuration is 2. Do you want to create the volume with this value ? (y/n) redundancy must be greater than 0, and the total number of bricks must be greater than 2 * redundancy . This means that a dispersed volume must have a minimum of 3 bricks. If the transport type is not specified, tcp is used as the default. You can also set additional options if required, like in the other volume types. Note : Make sure you start your volumes before you try to mount them or else client operations after the mount will hang. GlusterFS will fail with a warning to create a dispersed volume if more than one brick of a disperse set is present on the same peer. # gluster volume create <volname> disperse 3 server1:/brick{1..3} volume create: <volname>: failed: Multiple bricks of a disperse volume are present on the same server. This setup is not optimal. Bricks should be on different nodes to have best fault tolerant configuration. Use 'force' at the end of the command if you want to override this behavior. Creating Distributed Dispersed Volumes Distributed dispersed volumes are the equivalent to distributed replicated volumes, but using dispersed subvolumes instead of replicated ones. To create a distributed dispersed volume Create a trusted storage pool. Create the distributed dispersed volume: # gluster volume create disperse <count> [redundancy <count>] [transport tcp | rdma | tcp,rdma] To create a distributed dispersed volume, the disperse keyword and <count> is mandatory, and the number of bricks specified in the command line must must be a multiple of the disperse count. redundancy is exactly the same as in the dispersed volume. If the transport type is not specified, tcp is used as the default. You can also set additional options if required, like in the other volume types. Note : Make sure you start your volumes before you try to mount them or else client operations after the mount will hang. For distributed disperse volumes bricks can be hosted on same node if they belong to different subvol. # gluster volume create <volname> disperse 3 server1:/br1 server2:/br1 server3:/br1 server1:/br2 server2:/br2 server3:/br2 volume create: : success: please start the volume to access data Starting Volumes You must start your volumes before you try to mount them. To start a volume Start a volume: # gluster volume start <VOLNAME> [force] For example, to start test-volume: # gluster volume start test-volume Starting test-volume has been successful","title":"Setting Up Volumes"},{"location":"Administrator-Guide/Setting-Up-Volumes/#setting-up-glusterfs-volumes","text":"A volume is a logical collection of bricks where each brick is an export directory on a server in the trusted storage pool. To create a new volume in your storage environment, specify the bricks that comprise the volume. After you have created a new volume, you must start it before attempting to mount it. See Setting up Storage for how to set up bricks.","title":"Setting up GlusterFS Volumes"},{"location":"Administrator-Guide/Setting-Up-Volumes/#volume-types","text":"Volumes of the following types can be created in your storage environment: Distributed - Distributed volumes distribute files across the bricks in the volume. You can use distributed volumes where the requirement is to scale storage and the redundancy is either not important or is provided by other hardware/software layers. Replicated \u2013 Replicated volumes replicate files across bricks in the volume. You can use replicated volumes in environments where high-availability and high-reliability are critical. Distributed Replicated - Distributed replicated volumes distribute files across replicated bricks in the volume. You can use distributed replicated volumes in environments where the requirement is to scale storage and high-reliability is critical. Distributed replicated volumes also offer improved read performance in most environments. Dispersed - Dispersed volumes are based on erasure codes, providing space-efficient protection against disk or server failures. It stores an encoded fragment of the original file to each brick in a way that only a subset of the fragments is needed to recover the original file. The number of bricks that can be missing without losing access to data is configured by the administrator on volume creation time. Distributed Dispersed - Distributed dispersed volumes distribute files across dispersed subvolumes. This has the same advantages of distribute replicate volumes, but using disperse to store the data into the bricks. To create a new volume Create a new volume : # gluster volume create <NEW-VOLNAME> [[replica <COUNT> [arbiter <COUNT>]]|[replica 2 thin-arbiter 1]] [disperse [<COUNT>]] [disperse-data <COUNT>] [redundancy <COUNT>] [transport <tcp|rdma|tcp,rdma>] <NEW-BRICK> <TA-BRICK>... [force] For example, to create a volume called test-volume consisting of server3:/exp3 and server4:/exp4: # gluster volume create test-volume server3:/exp3 server4:/exp4 Creation of test-volume has been successful Please start the volume to access data.","title":"Volume Types"},{"location":"Administrator-Guide/Setting-Up-Volumes/#creating-distributed-volumes","text":"In a distributed volume files are spread randomly across the bricks in the volume. Use distributed volumes where you need to scale storage and redundancy is either not important or is provided by other hardware/software layers. Note : Disk/server failure in distributed volumes can result in a serious loss of data because directory contents are spread randomly across the bricks in the volume. To create a distributed volume Create a trusted storage pool. Create the distributed volume: # gluster volume create [transport tcp | rdma | tcp,rdma] For example, to create a distributed volume with four storage servers using tcp: # gluster volume create test-volume server1:/exp1 server2:/exp2 server3:/exp3 server4:/exp4 Creation of test-volume has been successful Please start the volume to access data. (Optional) You can display the volume information: # gluster volume info Volume Name: test-volume Type: Distribute Status: Created Number of Bricks: 4 Transport-type: tcp Bricks: Brick1: server1:/exp1 Brick2: server2:/exp2 Brick3: server3:/exp3 Brick4: server4:/exp4 For example, to create a distributed volume with four storage servers over InfiniBand: # gluster volume create test-volume transport rdma server1:/exp1 server2:/exp2 server3:/exp3 server4:/exp4 Creation of test-volume has been successful Please start the volume to access data. If the transport type is not specified, tcp is used as the default. You can also set additional options if required, such as auth.allow or auth.reject. Note : Make sure you start your volumes before you try to mount them or else client operations after the mount will hang.","title":"Creating Distributed Volumes"},{"location":"Administrator-Guide/Setting-Up-Volumes/#creating-replicated-volumes","text":"Replicated volumes create copies of files across multiple bricks in the volume. You can use replicated volumes in environments where high-availability and high-reliability are critical. Note : The number of bricks should be equal to of the replica count for a replicated volume. To protect against server and disk failures, it is recommended that the bricks of the volume are from different servers. To create a replicated volume Create a trusted storage pool. Create the replicated volume: # gluster volume create [replica ] [transport tcp | rdma | tcp,rdma] For example, to create a replicated volume with two storage servers: # gluster volume create test-volume replica 2 transport tcp server1:/exp1 server2:/exp2 Creation of test-volume has been successful Please start the volume to access data. If the transport type is not specified, tcp is used as the default. You can also set additional options if required, such as auth.allow or auth.reject. Note : Make sure you start your volumes before you try to mount them or else client operations after the mount will hang. GlusterFS will fail to create a replicate volume if more than one brick of a replica set is present on the same peer. For eg. a four node replicated volume where more than one brick of a replica set is present on the same peer. # gluster volume create <volname> replica 4 server1:/brick1 server1:/brick2 server2:/brick3 server4:/brick4 volume create: <volname>: failed: Multiple bricks of a replicate volume are present on the same server. This setup is not optimal. Use 'force' at the end of the command if you want to override this behavior. Use the force option at the end of command if you still want to create the volume with this configuration.","title":"Creating Replicated Volumes"},{"location":"Administrator-Guide/Setting-Up-Volumes/#arbiter-configuration-for-replica-volumes","text":"Arbiter volumes are replica 3 volumes where the 3rd brick acts as the arbiter brick. This configuration has mechanisms that prevent occurrence of split-brains. It can be created with the following command: `# gluster volume create <VOLNAME> replica 2 arbiter 1 host1:brick1 host2:brick2 host3:brick3` More information about this configuration can be found at Administrator-Guide : arbiter-volumes-and-quorum Note that the arbiter configuration for replica 3 can be used to create distributed-replicate volumes as well.","title":"Arbiter configuration for replica volumes"},{"location":"Administrator-Guide/Setting-Up-Volumes/#creating-distributed-replicated-volumes","text":"Distributes files across replicated bricks in the volume. You can use distributed replicated volumes in environments where the requirement is to scale storage and high-reliability is critical. Distributed replicated volumes also offer improved read performance in most environments. Note : The number of bricks should be a multiple of the replica count for a distributed replicated volume. Also, the order in which bricks are specified has a great effect on data protection. Each replica_count consecutive bricks in the list you give will form a replica set, with all replica sets combined into a volume-wide distribute set. To make sure that replica-set members are not placed on the same node, list the first brick on every server, then the second brick on every server in the same order, and so on. To create a distributed replicated volume Create a trusted storage pool. Create the distributed replicated volume: # gluster volume create [replica ] [transport tcp | rdma | tcp,rdma] For example, a four node distributed (replicated) volume with a two-way mirror: # gluster volume create test-volume replica 2 transport tcp server1:/exp1 server2:/exp2 server3:/exp3 server4:/exp4 Creation of test-volume has been successful Please start the volume to access data. For example, to create a six node distributed (replicated) volume with a two-way mirror: # gluster volume create test-volume replica 2 transport tcp server1:/exp1 server2:/exp2 server3:/exp3 server4:/exp4 server5:/exp5 server6:/exp6 Creation of test-volume has been successful Please start the volume to access data. If the transport type is not specified, tcp is used as the default. You can also set additional options if required, such as auth.allow or auth.reject. Note : - Make sure you start your volumes before you try to mount them or else client operations after the mount will hang. GlusterFS will fail to create a distribute replicate volume if more than one brick of a replica set is present on the same peer. For eg. for a four node distribute (replicated) volume where more than one brick of a replica set is present on the same peer. # gluster volume create <volname> replica 2 server1:/brick1 server1:/brick2 server2:/brick3 server4:/brick4 volume create: <volname>: failed: Multiple bricks of a replicate volume are present on the same server. This setup is not optimal. Use 'force' at the end of the command if you want to override this behavior. Use the force option at the end of command if you want to create the volume in this case.","title":"Creating Distributed Replicated Volumes"},{"location":"Administrator-Guide/Setting-Up-Volumes/#creating-dispersed-volumes","text":"Dispersed volumes are based on erasure codes. It stripes the encoded data of files, with some redundancy added, across multiple bricks in the volume. You can use dispersed volumes to have a configurable level of reliability with minimum space waste. Redundancy Each dispersed volume has a redundancy value defined when the volume is created. This value determines how many bricks can be lost without interrupting the operation of the volume. It also determines the amount of usable space of the volume using this formula: <Usable size> = <Brick size> * (#Bricks - Redundancy) All bricks of a disperse set should have the same capacity, otherwise, when the smallest brick becomes full, no additional data will be allowed in the disperse set. It's important to note that a configuration with 3 bricks and redundancy 1 will have less usable space (66.7% of the total physical space) than a configuration with 10 bricks and redundancy 1 (90%). However the first one will be safer than the second one (roughly the probability of failure of the second configuration if more than 4.5 times bigger than the first one). For example, a dispersed volume composed of 6 bricks of 4TB and a redundancy of 2 will be completely operational even with two bricks inaccessible. However a third inaccessible brick will bring the volume down because it won't be possible to read or write to it. The usable space of the volume will be equal to 16TB. The implementation of erasure codes in GlusterFS limits the redundancy to a value smaller than #Bricks / 2 (or equivalently, redundancy * 2 < #Bricks). Having a redundancy equal to half of the number of bricks would be almost equivalent to a replica-2 volume, and probably a replicated volume will perform better in this case. Optimal volumes One of the worst things erasure codes have in terms of performance is the RMW (Read-Modify-Write) cycle. Erasure codes operate in blocks of a certain size and it cannot work with smaller ones. This means that if a user issues a write of a portion of a file that doesn't fill a full block, it needs to read the remaining portion from the current contents of the file, merge them, compute the updated encoded block and, finally, writing the resulting data. This adds latency, reducing performance when this happens. Some GlusterFS performance xlators can help to reduce or even eliminate this problem for some workloads, but it should be taken into account when using dispersed volumes for a specific use case. Current implementation of dispersed volumes use blocks of a size that depends on the number of bricks and redundancy: 512 * (#Bricks - redundancy) bytes. This value is also known as the stripe size. Using combinations of #Bricks/redundancy that give a power of two for the stripe size will make the disperse volume perform better in most workloads because it's more typical to write information in blocks that are multiple of two (for example databases, virtual machines and many applications). These combinations are considered optimal . For example, a configuration with 6 bricks and redundancy 2 will have a stripe size of 512 * (6 - 2) = 2048 bytes, so it's considered optimal. A configuration with 7 bricks and redundancy 2 would have a stripe size of 2560 bytes, needing a RMW cycle for many writes (of course this always depends on the use case). To create a dispersed volume Create a trusted storage pool. Create the dispersed volume: # gluster volume create [disperse [<count>]] [redundancy <count>] [transport tcp | rdma | tcp,rdma] A dispersed volume can be created by specifying the number of bricks in a disperse set, by specifying the number of redundancy bricks, or both. If disperse is not specified, or the <count> is missing, the entire volume will be treated as a single disperse set composed by all bricks enumerated in the command line. If redundancy is not specified, it is computed automatically to be the optimal value. If this value does not exist, it's assumed to be '1' and a warning message is shown: # gluster volume create test-volume disperse 4 server{1..4}:/bricks/test-volume There isn't an optimal redundancy value for this configuration. Do you want to create the volume with redundancy 1 ? (y/n) In all cases where redundancy is automatically computed and it's not equal to '1', a warning message is displayed: # gluster volume create test-volume disperse 6 server{1..6}:/bricks/test-volume The optimal redundancy for this configuration is 2. Do you want to create the volume with this value ? (y/n) redundancy must be greater than 0, and the total number of bricks must be greater than 2 * redundancy . This means that a dispersed volume must have a minimum of 3 bricks. If the transport type is not specified, tcp is used as the default. You can also set additional options if required, like in the other volume types. Note : Make sure you start your volumes before you try to mount them or else client operations after the mount will hang. GlusterFS will fail with a warning to create a dispersed volume if more than one brick of a disperse set is present on the same peer. # gluster volume create <volname> disperse 3 server1:/brick{1..3} volume create: <volname>: failed: Multiple bricks of a disperse volume are present on the same server. This setup is not optimal. Bricks should be on different nodes to have best fault tolerant configuration. Use 'force' at the end of the command if you want to override this behavior.","title":"Creating Dispersed Volumes"},{"location":"Administrator-Guide/Setting-Up-Volumes/#creating-distributed-dispersed-volumes","text":"Distributed dispersed volumes are the equivalent to distributed replicated volumes, but using dispersed subvolumes instead of replicated ones. To create a distributed dispersed volume Create a trusted storage pool. Create the distributed dispersed volume: # gluster volume create disperse <count> [redundancy <count>] [transport tcp | rdma | tcp,rdma] To create a distributed dispersed volume, the disperse keyword and <count> is mandatory, and the number of bricks specified in the command line must must be a multiple of the disperse count. redundancy is exactly the same as in the dispersed volume. If the transport type is not specified, tcp is used as the default. You can also set additional options if required, like in the other volume types. Note : Make sure you start your volumes before you try to mount them or else client operations after the mount will hang. For distributed disperse volumes bricks can be hosted on same node if they belong to different subvol. # gluster volume create <volname> disperse 3 server1:/br1 server2:/br1 server3:/br1 server1:/br2 server2:/br2 server3:/br2 volume create: : success: please start the volume to access data","title":"Creating Distributed Dispersed Volumes"},{"location":"Administrator-Guide/Setting-Up-Volumes/#starting-volumes","text":"You must start your volumes before you try to mount them. To start a volume Start a volume: # gluster volume start <VOLNAME> [force] For example, to start test-volume: # gluster volume start test-volume Starting test-volume has been successful","title":"Starting Volumes"},{"location":"Administrator-Guide/Split-brain-and-ways-to-deal-with-it/","text":"Split brain and the ways to deal with it Split brain: Split brain is a situation where two or more replicated copies of a file become divergent. When a file is in split brain, there is an inconsistency in either data or metadata of the file amongst the bricks of a replica and do not have enough information to authoritatively pick a copy as being pristine and heal the bad copies, despite all bricks being up and online. For a directory, there is also an entry split brain where a file inside it can have different gfid/file-type across the bricks of a replica. Split brain can happen mainly because of 2 reasons: 1. Due to network disconnect: Where a client temporarily loses connection to the bricks. - There is a replica pair of 2 bricks, brick1 on server1 and brick2 on server2. - Client1 loses connection to brick2 and client2 loses connection to brick1 due to network split. - Writes from client1 goes to brick1 and from client2 goes to brick2, which is nothing but split-brain. 2. Gluster brick processes going down or returning error: - Server1 is down and server2 is up: Writes happen on server 2. - Server1 comes up, server2 goes down (Heal not happened / data on server 2 is not replicated on server1): Writes happen on server1. - Server2 comes up: Both server1 and server2 has data independent of each other. If we use the replica 2 volume, it is not possible to prevent split-brain without losing availability. Ways to deal with split brain: In glusterfs there are ways to resolve split brain. You can see the detailed description of how to resolve a split-brain here . Moreover, there are ways to reduce the chances of ending up in split-brain situations. They are: 1. Replica 3 volume 2. Arbiter volume Both of these uses the client-quorum option of glusterfs to avoid the split-brain situations. Client quorum: This is a feature implemented in Automatic File Replication (AFR here on) module, to prevent split-brains in the I/O path for replicate/distributed-replicate volumes. By default, if the client-quorum is not met for a particular replica subvol, it becomes read-only. The other subvols (in a dist-rep volume) will still have R/W access. Here you can see more details about client-quorum. Client quorum in replica 2 volumes: In a replica 2 volume it is not possible to achieve high availability and consistency at the same time, without sacrificing tolerance to partition. If we set the client-quorum option to auto, then the first brick must always be up, irrespective of the status of the second brick. If only the second brick is up, the subvolume becomes read-only. If the quorum-type is set to fixed, and the quorum-count is set to 1, then we may end up in split brain. - Brick1 is up and brick2 is down. Quorum is met and write happens on brick1. - Brick1 goes down and brick2 comes up (No heal happened). Quorum is met, write happens on brick2. - Brick1 comes up. Quorum is met, but both the bricks have independent writes - split-brain. To avoid this we have to set the quorum-count to 2, which will cost the availability. Even if we have one replica brick up and running, the quorum is not met and we end up seeing EROFS. 1. Replica 3 volume: When we create a replicated or distributed replicated volume with replica count 3, the cluster.quorum-type option is set to auto by default. That means at least 2 bricks should be up and running to satisfy the quorum and allow the writes. This is the recommended setting for a replica 3 volume and this should not be changed. Here is how it prevents files from ending up in split brain: B1, B2, and B3 are the 3 bricks of a replica 3 volume. 1. B1 & B2 are up and B3 is down. Quorum is met and write happens on B1 & B2. 2. B3 comes up and B2 is down. Quorum is met and write happens on B1 & B3. 3. B2 comes up and B1 goes down. Quorum is met. But when a write request comes, AFR sees that B2 & B3 are blaming each other (B2 says that some writes are pending on B3 and B3 says that some writes are pending on B2), therefore the write is not allowed and is failed with EIO. Command to create a replica 3 volume: $gluster volume create <volname> replica 3 host1:brick1 host2:brick2 host3:brick3 2. Arbiter volume: Arbiter offers the sweet spot between replica 2 and replica 3, where user wants the split-brain protection offered by replica 3 but does not want to invest in 3x storage space. Arbiter is also an replica 3 volume where the third brick of the replica is automatically configured as an arbiter node. This means that the third brick stores only the file name and metadata, but not any data. This will help in avoiding split brain while providing the same level of consistency as a normal replica 3 volume. Command to create a arbiter volume: $gluster volume create <volname> replica 3 arbiter 1 host1:brick1 host2:brick2 host3:brick3 The only difference in the command is, we need to add one more keyword arbiter 1 after the replica count. Since it is also a replica 3 volume, the cluster.quorum-type option is set to auto by default and at least 2 bricks should be up to satisfy the quorum and allow writes. Since the arbiter brick has only name and metadata of the files, there are some more checks to guarantee consistency. Arbiter works as follows: Clients take full file locks while writing (replica 3 takes range locks). If 2 bricks are up and if one of them is the arbiter, and it blames the other up brick, then all FOPs will fail with ENOTCONN (Transport endpoint is not connected). If the arbiter doesn't blame the other brick, FOPs will be allowed to proceed. If 2 bricks are up and the arbiter is down, then FOPs will be allowed. If only one brick is up, then client-quorum is not met and the volume becomes EROFS. In all cases, if there is only one source before the FOP is initiated and if the FOP fails on that source, the application will receive ENOTCONN. You can find more details on arbiter here . Differences between replica 3 and arbiter volumes: In case of a replica 3 volume, we store the entire file in all the bricks and it is recommended to have bricks of same size. But in case of arbiter, since we do not store data, the size of the arbiter brick is comparatively lesser than the other bricks. Arbiter is a state between replica 2 and replica 3 volume. If we have only arbiter and one of the other brick is up and the arbiter brick blames the other brick, then we can not proceed with the FOPs. Replica 3 gives high availability compared to arbiter, because unlike in arbiter, replica 3 has a full copy of the data in all 3 bricks.","title":"Split brain and ways to deal with it"},{"location":"Administrator-Guide/Split-brain-and-ways-to-deal-with-it/#split-brain-and-the-ways-to-deal-with-it","text":"","title":"Split brain and the ways to deal with it"},{"location":"Administrator-Guide/Split-brain-and-ways-to-deal-with-it/#split-brain","text":"Split brain is a situation where two or more replicated copies of a file become divergent. When a file is in split brain, there is an inconsistency in either data or metadata of the file amongst the bricks of a replica and do not have enough information to authoritatively pick a copy as being pristine and heal the bad copies, despite all bricks being up and online. For a directory, there is also an entry split brain where a file inside it can have different gfid/file-type across the bricks of a replica. Split brain can happen mainly because of 2 reasons: 1. Due to network disconnect: Where a client temporarily loses connection to the bricks. - There is a replica pair of 2 bricks, brick1 on server1 and brick2 on server2. - Client1 loses connection to brick2 and client2 loses connection to brick1 due to network split. - Writes from client1 goes to brick1 and from client2 goes to brick2, which is nothing but split-brain. 2. Gluster brick processes going down or returning error: - Server1 is down and server2 is up: Writes happen on server 2. - Server1 comes up, server2 goes down (Heal not happened / data on server 2 is not replicated on server1): Writes happen on server1. - Server2 comes up: Both server1 and server2 has data independent of each other. If we use the replica 2 volume, it is not possible to prevent split-brain without losing availability.","title":"Split brain:"},{"location":"Administrator-Guide/Split-brain-and-ways-to-deal-with-it/#ways-to-deal-with-split-brain","text":"In glusterfs there are ways to resolve split brain. You can see the detailed description of how to resolve a split-brain here . Moreover, there are ways to reduce the chances of ending up in split-brain situations. They are: 1. Replica 3 volume 2. Arbiter volume Both of these uses the client-quorum option of glusterfs to avoid the split-brain situations.","title":"Ways to deal with split brain:"},{"location":"Administrator-Guide/Split-brain-and-ways-to-deal-with-it/#client-quorum","text":"This is a feature implemented in Automatic File Replication (AFR here on) module, to prevent split-brains in the I/O path for replicate/distributed-replicate volumes. By default, if the client-quorum is not met for a particular replica subvol, it becomes read-only. The other subvols (in a dist-rep volume) will still have R/W access. Here you can see more details about client-quorum.","title":"Client quorum:"},{"location":"Administrator-Guide/Split-brain-and-ways-to-deal-with-it/#client-quorum-in-replica-2-volumes","text":"In a replica 2 volume it is not possible to achieve high availability and consistency at the same time, without sacrificing tolerance to partition. If we set the client-quorum option to auto, then the first brick must always be up, irrespective of the status of the second brick. If only the second brick is up, the subvolume becomes read-only. If the quorum-type is set to fixed, and the quorum-count is set to 1, then we may end up in split brain. - Brick1 is up and brick2 is down. Quorum is met and write happens on brick1. - Brick1 goes down and brick2 comes up (No heal happened). Quorum is met, write happens on brick2. - Brick1 comes up. Quorum is met, but both the bricks have independent writes - split-brain. To avoid this we have to set the quorum-count to 2, which will cost the availability. Even if we have one replica brick up and running, the quorum is not met and we end up seeing EROFS.","title":"Client quorum in replica 2 volumes:"},{"location":"Administrator-Guide/Split-brain-and-ways-to-deal-with-it/#1-replica-3-volume","text":"When we create a replicated or distributed replicated volume with replica count 3, the cluster.quorum-type option is set to auto by default. That means at least 2 bricks should be up and running to satisfy the quorum and allow the writes. This is the recommended setting for a replica 3 volume and this should not be changed. Here is how it prevents files from ending up in split brain: B1, B2, and B3 are the 3 bricks of a replica 3 volume. 1. B1 & B2 are up and B3 is down. Quorum is met and write happens on B1 & B2. 2. B3 comes up and B2 is down. Quorum is met and write happens on B1 & B3. 3. B2 comes up and B1 goes down. Quorum is met. But when a write request comes, AFR sees that B2 & B3 are blaming each other (B2 says that some writes are pending on B3 and B3 says that some writes are pending on B2), therefore the write is not allowed and is failed with EIO. Command to create a replica 3 volume: $gluster volume create <volname> replica 3 host1:brick1 host2:brick2 host3:brick3","title":"1. Replica 3 volume:"},{"location":"Administrator-Guide/Split-brain-and-ways-to-deal-with-it/#2-arbiter-volume","text":"Arbiter offers the sweet spot between replica 2 and replica 3, where user wants the split-brain protection offered by replica 3 but does not want to invest in 3x storage space. Arbiter is also an replica 3 volume where the third brick of the replica is automatically configured as an arbiter node. This means that the third brick stores only the file name and metadata, but not any data. This will help in avoiding split brain while providing the same level of consistency as a normal replica 3 volume. Command to create a arbiter volume: $gluster volume create <volname> replica 3 arbiter 1 host1:brick1 host2:brick2 host3:brick3 The only difference in the command is, we need to add one more keyword arbiter 1 after the replica count. Since it is also a replica 3 volume, the cluster.quorum-type option is set to auto by default and at least 2 bricks should be up to satisfy the quorum and allow writes. Since the arbiter brick has only name and metadata of the files, there are some more checks to guarantee consistency. Arbiter works as follows: Clients take full file locks while writing (replica 3 takes range locks). If 2 bricks are up and if one of them is the arbiter, and it blames the other up brick, then all FOPs will fail with ENOTCONN (Transport endpoint is not connected). If the arbiter doesn't blame the other brick, FOPs will be allowed to proceed. If 2 bricks are up and the arbiter is down, then FOPs will be allowed. If only one brick is up, then client-quorum is not met and the volume becomes EROFS. In all cases, if there is only one source before the FOP is initiated and if the FOP fails on that source, the application will receive ENOTCONN. You can find more details on arbiter here .","title":"2. Arbiter volume:"},{"location":"Administrator-Guide/Split-brain-and-ways-to-deal-with-it/#differences-between-replica-3-and-arbiter-volumes","text":"In case of a replica 3 volume, we store the entire file in all the bricks and it is recommended to have bricks of same size. But in case of arbiter, since we do not store data, the size of the arbiter brick is comparatively lesser than the other bricks. Arbiter is a state between replica 2 and replica 3 volume. If we have only arbiter and one of the other brick is up and the arbiter brick blames the other brick, then we can not proceed with the FOPs. Replica 3 gives high availability compared to arbiter, because unlike in arbiter, replica 3 has a full copy of the data in all 3 bricks.","title":"Differences between replica 3 and arbiter volumes:"},{"location":"Administrator-Guide/Start-Stop-Daemon/","text":"Managing the glusterd Service After installing GlusterFS, you must start glusterd service. The glusterd service serves as the Gluster elastic volume manager, overseeing glusterfs processes, and co-ordinating dynamic volume operations, such as adding and removing volumes across multiple storage servers non-disruptively. This section describes how to start the glusterd service in the following ways: Starting and stopping glusterd manually on distributions using systemd Starting glusterd automatically on distributions using systemd Starting and stopping glusterd manually Starting glusterd Automatically Note : You must start glusterd on all GlusterFS servers. Distributions with systemd Starting and stopping glusterd manually To start glusterd manually: systemctl start glusterd To stop glusterd manually: systemctl stop glusterd Starting glusterd automatically To enable the glusterd service and start it if stopped: systemctl enable --now glusterd To disable the glusterd service and stop it if started: systemctl disable --now glusterd Distributions without systemd Starting and stopping glusterd manually This section describes how to start and stop glusterd manually To start glusterd manually, enter the following command: # /etc/init.d/glusterd start To stop glusterd manually, enter the following command: # /etc/init.d/glusterd stop Starting glusterd Automatically This section describes how to configure the system to automatically start the glusterd service every time the system boots. Red Hat and Fedora distributions To configure Red Hat-based systems to automatically start the glusterd service every time the system boots, enter the following from the command line: # chkconfig glusterd on Debian and derivatives like Ubuntu To configure Debian-based systems to automatically start the glusterd service every time the system boots, enter the following from the command line: # update-rc.d glusterd defaults Systems Other than Red Hat and Debian To configure systems other than Red Hat or Debian to automatically start the glusterd service every time the system boots, enter the following entry to the /etc/rc.local file: # echo \"glusterd\" >> /etc/rc.local","title":"Managing the Gluster Service"},{"location":"Administrator-Guide/Start-Stop-Daemon/#managing-the-glusterd-service","text":"After installing GlusterFS, you must start glusterd service. The glusterd service serves as the Gluster elastic volume manager, overseeing glusterfs processes, and co-ordinating dynamic volume operations, such as adding and removing volumes across multiple storage servers non-disruptively. This section describes how to start the glusterd service in the following ways: Starting and stopping glusterd manually on distributions using systemd Starting glusterd automatically on distributions using systemd Starting and stopping glusterd manually Starting glusterd Automatically Note : You must start glusterd on all GlusterFS servers.","title":"Managing the glusterd Service"},{"location":"Administrator-Guide/Start-Stop-Daemon/#distributions-with-systemd","text":"","title":"Distributions with systemd"},{"location":"Administrator-Guide/Start-Stop-Daemon/#starting-and-stopping-glusterd-manually","text":"To start glusterd manually: systemctl start glusterd To stop glusterd manually: systemctl stop glusterd","title":"Starting and stopping glusterd manually"},{"location":"Administrator-Guide/Start-Stop-Daemon/#starting-glusterd-automatically","text":"To enable the glusterd service and start it if stopped: systemctl enable --now glusterd To disable the glusterd service and stop it if started: systemctl disable --now glusterd","title":"Starting glusterd automatically"},{"location":"Administrator-Guide/Start-Stop-Daemon/#distributions-without-systemd","text":"","title":"Distributions without systemd"},{"location":"Administrator-Guide/Start-Stop-Daemon/#starting-and-stopping-glusterd-manually_1","text":"This section describes how to start and stop glusterd manually To start glusterd manually, enter the following command: # /etc/init.d/glusterd start To stop glusterd manually, enter the following command: # /etc/init.d/glusterd stop","title":"Starting and stopping glusterd manually"},{"location":"Administrator-Guide/Start-Stop-Daemon/#starting-glusterd-automatically_1","text":"This section describes how to configure the system to automatically start the glusterd service every time the system boots.","title":"Starting glusterd Automatically"},{"location":"Administrator-Guide/Start-Stop-Daemon/#red-hat-and-fedora-distributions","text":"To configure Red Hat-based systems to automatically start the glusterd service every time the system boots, enter the following from the command line: # chkconfig glusterd on","title":"Red Hat and Fedora distributions"},{"location":"Administrator-Guide/Start-Stop-Daemon/#debian-and-derivatives-like-ubuntu","text":"To configure Debian-based systems to automatically start the glusterd service every time the system boots, enter the following from the command line: # update-rc.d glusterd defaults","title":"Debian and derivatives like Ubuntu"},{"location":"Administrator-Guide/Start-Stop-Daemon/#systems-other-than-red-hat-and-debian","text":"To configure systems other than Red Hat or Debian to automatically start the glusterd service every time the system boots, enter the following entry to the /etc/rc.local file: # echo \"glusterd\" >> /etc/rc.local","title":"Systems Other than Red Hat and Debian"},{"location":"Administrator-Guide/Storage-Pools/","text":"Managing Trusted Storage Pools Overview A trusted storage pool(TSP) is a trusted network of storage servers. Before you can configure a GlusterFS volume, you must create a trusted storage pool of the storage servers that will provide bricks to the volume by peer probing the servers. The servers in a TSP are peers of each other. After installing Gluster on your servers and before creating a trusted storage pool, each server belongs to a storage pool consisting of only that server. Adding Servers Listing Servers Viewing Peer Status Removing Servers Before you start : The servers used to create the storage pool must be resolvable by hostname. The glusterd daemon must be running on all storage servers that you want to add to the storage pool. See Managing the glusterd Service for details. The firewall on the servers must be configured to allow access to port 24007. The following commands were run on a TSP consisting of 3 servers - server1, server2, and server3. Adding Servers To add a server to a TSP, peer probe it from a server already in the pool. # gluster peer probe <server> For example, to add a new server4 to the cluster described above, probe it from one of the other servers: server1# gluster peer probe server4 Probe successful Verify the peer status from the first server (server1): server1# gluster peer status Number of Peers: 3 Hostname: server2 Uuid: 5e987bda-16dd-43c2-835b-08b7d55e94e5 State: Peer in Cluster (Connected) Hostname: server3 Uuid: 1e0ca3aa-9ef7-4f66-8f15-cbc348f29ff7 State: Peer in Cluster (Connected) Hostname: server4 Uuid: 3e0cabaa-9df7-4f66-8e5d-cbc348f29ff7 State: Peer in Cluster (Connected) Listing Servers To list all nodes in the TSP: server1# gluster pool list UUID Hostname State d18d36c5-533a-4541-ac92-c471241d5418 localhost Connected 5e987bda-16dd-43c2-835b-08b7d55e94e5 server2 Connected 1e0ca3aa-9ef7-4f66-8f15-cbc348f29ff7 server3 Connected 3e0cabaa-9df7-4f66-8e5d-cbc348f29ff7 server4 Connected Viewing Peer Status To view the status of the peers in the TSP: server1# gluster peer status Number of Peers: 3 Hostname: server2 Uuid: 5e987bda-16dd-43c2-835b-08b7d55e94e5 State: Peer in Cluster (Connected) Hostname: server3 Uuid: 1e0ca3aa-9ef7-4f66-8f15-cbc348f29ff7 State: Peer in Cluster (Connected) Hostname: server4 Uuid: 3e0cabaa-9df7-4f66-8e5d-cbc348f29ff7 State: Peer in Cluster (Connected) Removing Servers To remove a server from the TSP, run the following command from another server in the pool: # gluster peer detach <server> For example, to remove server4 from the trusted storage pool: server1# gluster peer detach server4 Detach successful Verify the peer status: server1# gluster peer status Number of Peers: 2 Hostname: server2 Uuid: 5e987bda-16dd-43c2-835b-08b7d55e94e5 State: Peer in Cluster (Connected) Hostname: server3 Uuid: 1e0ca3aa-9ef7-4f66-8f15-cbc348f29ff7 State: Peer in Cluster (Connected)","title":"Managing Trusted Storage Pools"},{"location":"Administrator-Guide/Storage-Pools/#managing-trusted-storage-pools","text":"","title":"Managing Trusted Storage Pools"},{"location":"Administrator-Guide/Storage-Pools/#overview","text":"A trusted storage pool(TSP) is a trusted network of storage servers. Before you can configure a GlusterFS volume, you must create a trusted storage pool of the storage servers that will provide bricks to the volume by peer probing the servers. The servers in a TSP are peers of each other. After installing Gluster on your servers and before creating a trusted storage pool, each server belongs to a storage pool consisting of only that server. Adding Servers Listing Servers Viewing Peer Status Removing Servers Before you start : The servers used to create the storage pool must be resolvable by hostname. The glusterd daemon must be running on all storage servers that you want to add to the storage pool. See Managing the glusterd Service for details. The firewall on the servers must be configured to allow access to port 24007. The following commands were run on a TSP consisting of 3 servers - server1, server2, and server3.","title":"Overview"},{"location":"Administrator-Guide/Storage-Pools/#adding-servers","text":"To add a server to a TSP, peer probe it from a server already in the pool. # gluster peer probe <server> For example, to add a new server4 to the cluster described above, probe it from one of the other servers: server1# gluster peer probe server4 Probe successful Verify the peer status from the first server (server1): server1# gluster peer status Number of Peers: 3 Hostname: server2 Uuid: 5e987bda-16dd-43c2-835b-08b7d55e94e5 State: Peer in Cluster (Connected) Hostname: server3 Uuid: 1e0ca3aa-9ef7-4f66-8f15-cbc348f29ff7 State: Peer in Cluster (Connected) Hostname: server4 Uuid: 3e0cabaa-9df7-4f66-8e5d-cbc348f29ff7 State: Peer in Cluster (Connected)","title":"Adding Servers"},{"location":"Administrator-Guide/Storage-Pools/#listing-servers","text":"To list all nodes in the TSP: server1# gluster pool list UUID Hostname State d18d36c5-533a-4541-ac92-c471241d5418 localhost Connected 5e987bda-16dd-43c2-835b-08b7d55e94e5 server2 Connected 1e0ca3aa-9ef7-4f66-8f15-cbc348f29ff7 server3 Connected 3e0cabaa-9df7-4f66-8e5d-cbc348f29ff7 server4 Connected","title":"Listing Servers"},{"location":"Administrator-Guide/Storage-Pools/#viewing-peer-status","text":"To view the status of the peers in the TSP: server1# gluster peer status Number of Peers: 3 Hostname: server2 Uuid: 5e987bda-16dd-43c2-835b-08b7d55e94e5 State: Peer in Cluster (Connected) Hostname: server3 Uuid: 1e0ca3aa-9ef7-4f66-8f15-cbc348f29ff7 State: Peer in Cluster (Connected) Hostname: server4 Uuid: 3e0cabaa-9df7-4f66-8e5d-cbc348f29ff7 State: Peer in Cluster (Connected)","title":"Viewing Peer Status"},{"location":"Administrator-Guide/Storage-Pools/#removing-servers","text":"To remove a server from the TSP, run the following command from another server in the pool: # gluster peer detach <server> For example, to remove server4 from the trusted storage pool: server1# gluster peer detach server4 Detach successful Verify the peer status: server1# gluster peer status Number of Peers: 2 Hostname: server2 Uuid: 5e987bda-16dd-43c2-835b-08b7d55e94e5 State: Peer in Cluster (Connected) Hostname: server3 Uuid: 1e0ca3aa-9ef7-4f66-8f15-cbc348f29ff7 State: Peer in Cluster (Connected)","title":"Removing Servers"},{"location":"Administrator-Guide/Thin-Arbiter-Volumes/","text":"Thin Arbiter volumes in gluster Thin Arbiter is a new type of quorum node where granularity of what is good and what is bad data is less compared to the traditional arbiter brick. In this type of volume, quorum is taken into account at a brick level rather than per file basis. If there is even one file that is marked bad (i.e. needs healing) on a data brick, that brick is considered bad for all files as a whole. So, even different file, if the write fails on the other data brick but succeeds on this 'bad' brick we will return failure for the write. Why Thin Arbiter? Setting UP Thin Arbiter Volume How Thin Arbiter works Why Thin Arbiter? This is a solution for handling stretch cluster kind of workload, but it can be used for regular workloads as well in case users are satisfied with this kind of quorum in comparison to arbiter/3-way-replication. Thin arbiter node can be placed outside of trusted storage pool i.e, thin arbiter is the \"stretched\" node in the cluster. This node can be placed on cloud or anywhere even if that connection has high latency. As this node will take part only in case of failure (or a brick is down) and to decide the quorum, it will not impact the performance in normal cases. Cost to perform any file operation would be lesser than arbiter if everything is fine. I/O will only go to the data bricks and goes to thin-arbiter only in the case of first failure until heal completes. Setting UP Thin Arbiter Volume The command to run thin-arbiter process on node: #/usr/local/sbin/glusterfsd -N --volfile-id ta-vol -f /var/lib/glusterd/vols/thin-arbiter.vol --brick-port 24007 --xlator-option ta-vol-server.transport.socket.listen-port=24007 Creating a thin arbiter replica 2 volume: #glustercli volume create <volname> --replica 2 <host1>:<brick1> <host2>:<brick2> --thin-arbiter <quorum-host>:<path-to-store-replica-id-file> For example: glustercli volume create testvol --replica 2 server{1..2}:/bricks/brick-{1..2} --thin-arbiter server-3:/bricks/brick_ta --force volume create: testvol: success: please start the volume to access data How Thin Arbiter works There will be only one process running on thin arbiter node which will be used to update replica id file for all replica pairs across all volumes. Replica id file contains the information of good and bad data bricks in the form of xattrs. Replica pairs will use its respective replica-id file that is going to be created during mount. 1) Read Transactions: Reads are allowed when quorum is met. i.e. When all data bricks and thin arbiter are up: Perform lookup on data bricks to figure out good/bad bricks and serve content from the good brick. When one brick is up: Fail FOP with EIO. Two bricks are up: If two data bricks are up, lookup is done on data bricks to figure out good/bad bricks and content will be served from the good brick. One lookup is enough to figure out good/bad copy of that file and keep this in inode context. If one data brick and thin arbiter brick are up, xattrop is done on thin arbiter to get information of source (good) brick. If the data brick, which is UP, has also been marked as source brick on thin arbiter, lookup on this file is done on the data brick to check if the file is really healthy or not. If the file is good, data will be served from this brick else an EIO error would be returned to user. 2) Write transactions: Thin arbiter doesn\u2019t participate in I/O, transaction will choose to wind operations on thin-arbiter brick to make sure the necessary metadata is kept up-to-date in case of failures. Operation failure will lead to updating the replica-id file on thin-arbiter with source/sink information in the xattrs just how it happens in AFR.","title":"Thin Arbiter volumes"},{"location":"Administrator-Guide/Thin-Arbiter-Volumes/#thin-arbiter-volumes-in-gluster","text":"Thin Arbiter is a new type of quorum node where granularity of what is good and what is bad data is less compared to the traditional arbiter brick. In this type of volume, quorum is taken into account at a brick level rather than per file basis. If there is even one file that is marked bad (i.e. needs healing) on a data brick, that brick is considered bad for all files as a whole. So, even different file, if the write fails on the other data brick but succeeds on this 'bad' brick we will return failure for the write. Why Thin Arbiter? Setting UP Thin Arbiter Volume How Thin Arbiter works","title":"Thin Arbiter volumes in gluster"},{"location":"Administrator-Guide/Thin-Arbiter-Volumes/#why-thin-arbiter","text":"This is a solution for handling stretch cluster kind of workload, but it can be used for regular workloads as well in case users are satisfied with this kind of quorum in comparison to arbiter/3-way-replication. Thin arbiter node can be placed outside of trusted storage pool i.e, thin arbiter is the \"stretched\" node in the cluster. This node can be placed on cloud or anywhere even if that connection has high latency. As this node will take part only in case of failure (or a brick is down) and to decide the quorum, it will not impact the performance in normal cases. Cost to perform any file operation would be lesser than arbiter if everything is fine. I/O will only go to the data bricks and goes to thin-arbiter only in the case of first failure until heal completes.","title":"Why Thin Arbiter?"},{"location":"Administrator-Guide/Thin-Arbiter-Volumes/#setting-up-thin-arbiter-volume","text":"The command to run thin-arbiter process on node: #/usr/local/sbin/glusterfsd -N --volfile-id ta-vol -f /var/lib/glusterd/vols/thin-arbiter.vol --brick-port 24007 --xlator-option ta-vol-server.transport.socket.listen-port=24007 Creating a thin arbiter replica 2 volume: #glustercli volume create <volname> --replica 2 <host1>:<brick1> <host2>:<brick2> --thin-arbiter <quorum-host>:<path-to-store-replica-id-file> For example: glustercli volume create testvol --replica 2 server{1..2}:/bricks/brick-{1..2} --thin-arbiter server-3:/bricks/brick_ta --force volume create: testvol: success: please start the volume to access data","title":"Setting UP Thin Arbiter Volume"},{"location":"Administrator-Guide/Thin-Arbiter-Volumes/#how-thin-arbiter-works","text":"There will be only one process running on thin arbiter node which will be used to update replica id file for all replica pairs across all volumes. Replica id file contains the information of good and bad data bricks in the form of xattrs. Replica pairs will use its respective replica-id file that is going to be created during mount. 1) Read Transactions: Reads are allowed when quorum is met. i.e. When all data bricks and thin arbiter are up: Perform lookup on data bricks to figure out good/bad bricks and serve content from the good brick. When one brick is up: Fail FOP with EIO. Two bricks are up: If two data bricks are up, lookup is done on data bricks to figure out good/bad bricks and content will be served from the good brick. One lookup is enough to figure out good/bad copy of that file and keep this in inode context. If one data brick and thin arbiter brick are up, xattrop is done on thin arbiter to get information of source (good) brick. If the data brick, which is UP, has also been marked as source brick on thin arbiter, lookup on this file is done on the data brick to check if the file is really healthy or not. If the file is good, data will be served from this brick else an EIO error would be returned to user. 2) Write transactions: Thin arbiter doesn\u2019t participate in I/O, transaction will choose to wind operations on thin-arbiter brick to make sure the necessary metadata is kept up-to-date in case of failures. Operation failure will lead to updating the replica-id file on thin-arbiter with source/sink information in the xattrs just how it happens in AFR.","title":"How Thin Arbiter works"},{"location":"Administrator-Guide/Trash/","text":"Trash Translator Trash translator will allow users to access deleted or truncated files. Every brick will maintain a hidden .trashcan directory, which will be used to store the files deleted or truncated from the respective brick. The aggregate of all those .trashcan directories can be accessed from the mount point. To avoid name collisions, a timestamp is appended to the original file name while it is being moved to the trash directory. Implications and Usage Apart from the primary use-case of accessing files deleted or truncated by the user, the trash translator can be helpful for internal operations such as self-heal and rebalance. During self-heal and rebalance it is possible to lose crucial data. In those circumstances, the trash translator can assist in the recovery of the lost data. The trash translator is designed to intercept unlink, truncate and ftruncate fops, store a copy of the current file in the trash directory, and then perform the fop on the original file. For the internal operations, the files are stored under the 'internal_op' folder inside the trash directory. Volume Options gluster volume set <VOLNAME> features.trash <on/off> This command can be used to enable a trash translator in a volume. If set to on, a trash directory will be created in every brick inside the volume during the volume start command. By default, a translator is loaded during volume start but remains non-functional. Disabling trash with the help of this option will not remove the trash directory or even its contents from the volume. gluster volume set <VOLNAME> features.trash-dir <name> This command is used to reconfigure the trash directory to a user-specified name. The argument is a valid directory name. The directory will be created inside every brick under this name. If not specified by the user, the trash translator will create the trash directory with the default name \u201c.trashcan\u201d. This can be used only when the trash-translator is on. gluster volume set <VOLNAME> features.trash-max-filesize <size> This command can be used to filter files entering the trash directory based on their size. Files above trash_max_filesize are deleted/truncated directly. Value for size may be followed by multiplicative suffixes as KB(=1024 bytes), MB(=1024*1024 bytes) ,and GB(=1024*1024*1024 bytes). The default size is set to 5MB. gluster volume set <VOLNAME> features.trash-eliminate-path <path1> [ , <path2> , . . . ] This command can be used to set the eliminate pattern for the trash translator. Files residing under this pattern will not be moved to the trash directory during deletion/truncation. The path must be a valid one present in the volume. gluster volume set <VOLNAME> features.trash-internal-op <on/off> This command can be used to enable trash for internal operations like self-heal and re-balance. By default set to off. Sample usage The following steps give illustrates a simple scenario of deletion of a file from a directory Create a simple distributed volume and start it. # gluster volume create test rhs:/home/brick # gluster volume start test Enable trash translator # gluster volume set test features.trash on Mount glusterfs volume via native client as follows. # mount -t glusterfs rhs:test /mnt Create a directory and file in the mount. # mkdir mnt/dir # echo abc > mnt/dir/file Delete the file from the mount. # rm mnt/dir/file -rf Checkout inside the trash directory. # ls mnt/.trashcan We can find the deleted file inside the trash directory with a timestamp appending on its filename. For example, # mount -t glusterfs rh-host:/test /mnt/test # mkdir /mnt/test/abc # touch /mnt/test/abc/file # rm /mnt/test/abc/file remove regular empty file \u2018/mnt/test/abc/file\u2019? y # ls /mnt/test/abc # # ls /mnt/test/.trashcan/abc/ file2014-08-21_123400 Points to be remembered As soon as the volume is started, the trash directory will be created inside the volume and will be visible through the mount. Disabling the trash will not have any impact on its visibility from the mount. Even though deletion of trash-directory is not permitted, currently residing trash contents will be removed on issuing delete on it and only an empty trash-directory exists. Known issue Since trash translator resides on the server side higher translators like AFR, DHT are unaware of rename and truncate operations being done by this translator which eventually moves the files to trash directory. Unless and until a complete-path-based lookup comes on trashed files, those may not be visible from the mount.","title":"Trash for GlusterFS"},{"location":"Administrator-Guide/Trash/#trash-translator","text":"Trash translator will allow users to access deleted or truncated files. Every brick will maintain a hidden .trashcan directory, which will be used to store the files deleted or truncated from the respective brick. The aggregate of all those .trashcan directories can be accessed from the mount point. To avoid name collisions, a timestamp is appended to the original file name while it is being moved to the trash directory.","title":"Trash Translator"},{"location":"Administrator-Guide/Trash/#implications-and-usage","text":"Apart from the primary use-case of accessing files deleted or truncated by the user, the trash translator can be helpful for internal operations such as self-heal and rebalance. During self-heal and rebalance it is possible to lose crucial data. In those circumstances, the trash translator can assist in the recovery of the lost data. The trash translator is designed to intercept unlink, truncate and ftruncate fops, store a copy of the current file in the trash directory, and then perform the fop on the original file. For the internal operations, the files are stored under the 'internal_op' folder inside the trash directory.","title":"Implications and Usage"},{"location":"Administrator-Guide/Trash/#volume-options","text":"gluster volume set <VOLNAME> features.trash <on/off> This command can be used to enable a trash translator in a volume. If set to on, a trash directory will be created in every brick inside the volume during the volume start command. By default, a translator is loaded during volume start but remains non-functional. Disabling trash with the help of this option will not remove the trash directory or even its contents from the volume. gluster volume set <VOLNAME> features.trash-dir <name> This command is used to reconfigure the trash directory to a user-specified name. The argument is a valid directory name. The directory will be created inside every brick under this name. If not specified by the user, the trash translator will create the trash directory with the default name \u201c.trashcan\u201d. This can be used only when the trash-translator is on. gluster volume set <VOLNAME> features.trash-max-filesize <size> This command can be used to filter files entering the trash directory based on their size. Files above trash_max_filesize are deleted/truncated directly. Value for size may be followed by multiplicative suffixes as KB(=1024 bytes), MB(=1024*1024 bytes) ,and GB(=1024*1024*1024 bytes). The default size is set to 5MB. gluster volume set <VOLNAME> features.trash-eliminate-path <path1> [ , <path2> , . . . ] This command can be used to set the eliminate pattern for the trash translator. Files residing under this pattern will not be moved to the trash directory during deletion/truncation. The path must be a valid one present in the volume. gluster volume set <VOLNAME> features.trash-internal-op <on/off> This command can be used to enable trash for internal operations like self-heal and re-balance. By default set to off.","title":"Volume Options"},{"location":"Administrator-Guide/Trash/#sample-usage","text":"The following steps give illustrates a simple scenario of deletion of a file from a directory Create a simple distributed volume and start it. # gluster volume create test rhs:/home/brick # gluster volume start test Enable trash translator # gluster volume set test features.trash on Mount glusterfs volume via native client as follows. # mount -t glusterfs rhs:test /mnt Create a directory and file in the mount. # mkdir mnt/dir # echo abc > mnt/dir/file Delete the file from the mount. # rm mnt/dir/file -rf Checkout inside the trash directory. # ls mnt/.trashcan We can find the deleted file inside the trash directory with a timestamp appending on its filename. For example, # mount -t glusterfs rh-host:/test /mnt/test # mkdir /mnt/test/abc # touch /mnt/test/abc/file # rm /mnt/test/abc/file remove regular empty file \u2018/mnt/test/abc/file\u2019? y # ls /mnt/test/abc # # ls /mnt/test/.trashcan/abc/ file2014-08-21_123400","title":"Sample usage"},{"location":"Administrator-Guide/Trash/#points-to-be-remembered","text":"As soon as the volume is started, the trash directory will be created inside the volume and will be visible through the mount. Disabling the trash will not have any impact on its visibility from the mount. Even though deletion of trash-directory is not permitted, currently residing trash contents will be removed on issuing delete on it and only an empty trash-directory exists.","title":"Points to be remembered"},{"location":"Administrator-Guide/Trash/#known-issue","text":"Since trash translator resides on the server side higher translators like AFR, DHT are unaware of rename and truncate operations being done by this translator which eventually moves the files to trash directory. Unless and until a complete-path-based lookup comes on trashed files, those may not be visible from the mount.","title":"Known issue"},{"location":"Administrator-Guide/Tuning-Volume-Options/","text":"You can tune volume options, as needed, while the cluster is online and available. Note It is recommended to set server.allow-insecure option to ON if there are too many bricks in each volume or if there are too many services which have already utilized all the privileged ports in the system. Turning this option ON allows ports to accept/reject messages from insecure ports. So, use this option only if your deployment requires it. Tune volume options using the following command: # gluster volume set <VOLNAME> <OPT-NAME> <OPT-VALUE> For example, to specify the performance cache size for test-volume: # gluster volume set test-volume performance.cache-size 256MB Set volume successful You can view the changed volume options using command: # gluster volume info The following table lists the Volume options along with its description and default value: Note The default options given here are subject to modification at any given time and may not be the same for all versions. Type Option Description Default Value Available Options auth.allow IP addresses of the clients which should be allowed to access the volume. * (allow all) Valid IP address which includes wild card patterns including *, such as 192.168.1.* auth.reject IP addresses of the clients which should be denied to access the volume. NONE (reject none) Valid IP address which includes wild card patterns including *, such as 192.168.2.* Cluster cluster.self-heal-window-size Specifies the maximum number of blocks per file on which self-heal would happen simultaneously. 1 0 - 1024 blocks cluster.data-self-heal-algorithm Specifies the type of self-heal. If you set the option as \"full\", the entire file is copied from source to destinations. If the option is set to \"diff\" the file blocks that are not in sync are copied to destinations. Reset uses a heuristic model. If the file does not exist on one of the subvolumes, or a zero-byte file exists (created by entry self-heal) the entire content has to be copied anyway, so there is no benefit from using the \"diff\" algorithm. If the file size is about the same as page size, the entire file can be read and written with a few operations, which will be faster than \"diff\" which has to read checksums and then read and write. reset full/diff/reset cluster.min-free-disk Specifies the percentage of disk space that must be kept free. Might be useful for non-uniform bricks 10% Percentage of required minimum free disk space cluster.min-free-inodes Specifies when system has only N% of inodes remaining, warnings starts to appear in log files 10% Percentage of required minimum free inodes cluster.stripe-block-size Specifies the size of the stripe unit that will be read from or written to. 128 KB (for all files) size in bytes cluster.self-heal-daemon Allows you to turn-off proactive self-heal on replicated On On/Off cluster.ensure-durability This option makes sure the data/metadata is durable across abrupt shutdown of the brick. On On/Off cluster.lookup-unhashed This option does a lookup through all the sub-volumes, in case a lookup didn\u2019t return any result from the hashed subvolume. If set to OFF, it does not do a lookup on the remaining subvolumes. on auto, yes/no, enable/disable, 1/0, on/off cluster.lookup-optimize This option enables the optimization of -ve lookups, by not doing a lookup on non-hashed subvolumes for files, in case the hashed subvolume does not return any result. This option disregards the lookup-unhashed setting, when enabled. on on/off cluster.randomize-hash-range-by-gfid Allows to use gfid of directory to determine the subvolume from which hash ranges are allocated starting with 0. Note that we still use a directory/file\u2019s name to determine the subvolume to which it hashes off on/off cluster.rebal-throttle Sets the maximum number of parallel file migrations allowed on a node during the rebalance operation. The default value is normal and allows 2 files to be migrated at a time. Lazy will allow only one file to be migrated at a time and aggressive will allow maxof[(((processing units) - 4) / 2), 4] normal lazy/normal/aggressive cluster.background-self-heal-count Specifies the number of per client self-heal jobs that can perform parallel heals in the background. 8 0-256 cluster.heal-timeout Time interval for checking the need to self-heal in self-heal-daemon 600 5-(signed-int) cluster.eager-lock If eager-lock is off, locks release immediately after file operations complete, improving performance for some operations, but reducing access efficiency on on/off cluster.quorum-type If value is \u201cfixed\u201d only allow writes if quorum-count bricks are present. If value is \u201cauto\u201d only allow writes if more than half of bricks, or exactly half including the first brick, are present none none/auto/fixed cluster.quorum-count If quorum-type is \u201cfixed\u201d only allow writes if this many bricks are present. Other quorum types will OVERWRITE this value null 1-(signed-int) cluster.heal-wait-queue-length Specifies the number of heals that can be queued for the parallel background self heal jobs. 128 0-10000 cluster.favorite-child-policy Specifies which policy can be used to automatically resolve split-brains without user intervention. \u201csize\u201d picks the file with the biggest size as the source. \u201cctime\u201d and \u201cmtime\u201d pick the file with the latest ctime and mtime respectively as the source. \u201cmajority\u201d picks a file with identical mtime and size in more than half the number of bricks in the replica. none none/size/ctime/mtime/majority cluster.use-anonymous-inode Setting this option heals directory renames efficiently no no/yes Disperse disperse.eager-lock If eager-lock is on, the lock remains in place either until lock contention is detected, or for 1 second in order to check if there is another request for that file from the same client. If eager-lock is off, locks release immediately after file operations complete, improving performance for some operations, but reducing access efficiency. on on/off disperse.other-eager-lock This option is equivalent to the disperse.eager-lock option but applicable only for non regular files. When multiple clients access a particular directory, disabling disperse.other-eager-lockoption for the volume can improve performance for directory access without compromising performance of I/O's for regular files. off on/off disperse.shd-max-threads Specifies the number of entries that can be self healed in parallel on each disperse subvolume by self-heal daemon. 1 1 - 64 disperse.shd-wait-qlength Specifies the number of entries that must be kept in the dispersed subvolume's queue for self-heal daemon threads to take up as soon as any of the threads are free to heal. This value should be changed based on how much memory self-heal daemon process can use for keeping the next set of entries that need to be healed. 1024 1 - 655536 disprse.eager-lock-timeout Maximum time (in seconds) that a lock on an inode is kept held if no new operations on the inode are received. 1 1-60 disperse.other-eager-lock-timeout It\u2019s equivalent to eager-lock-timeout option but for non regular files. 1 1-60 disperse.background-heals This option can be used to control number of parallel heals running in background. 8 0-256 disperse.heal-wait-qlength This option can be used to control number of heals that can wait 128 0-65536 disperse.read-policy inode-read fops happen only on \u2018k\u2019 number of bricks in n=k+m disperse subvolume. \u2018round-robin\u2019 selects the read subvolume using round-robin algo. \u2018gfid-hash\u2019 selects read subvolume based on hash of the gfid of that file/directory. gfid-hash round-robin/gfid-hash disperse.self-heal-window-size Maximum number blocks(128KB) per file for which self-heal process would be applied simultaneously. 1 1-1024 disperse.optimistic-change-log This option Set/Unset dirty flag for every update fop at the start of the fop. If OFF, this option impacts performance of entry or metadata operations as it will set dirty flag at the start and unset it at the end of ALL update fop. If ON and all the bricks are good, dirty flag will be set at the start only for file fops, For metadata and entry fops dirty flag will not be set at the start This does not impact performance for metadata operations and entry operation but has a very small window to miss marking entry as dirty in case it is required to be healed. on on/off disperse.parallel-writes This controls if writes can be wound in parallel as long as it doesn\u2019t modify same stripes on on/off disperse.stripe-cache This option will keep the last stripe of write fop in memory. If next write falls in this stripe, we need not to read it again from backend and we can save READ fop going over the network. This will improve performance, specially for sequential writes. However, this will also lead to extra memory consumption, maximum (cache size * stripe size) Bytes per open file 4 0-10 disperse.quorum-count This option can be used to define how many successes on the bricks constitute a success to the application. This count should be in the range [disperse-data-count, disperse-count] (inclusive) 0 0-(signedint) disperse.use-anonymous-inode Setting this option heals renames efficiently off on/off Logging diagnostics.brick-log-level Changes the log-level of the bricks INFO DEBUG/WARNING/ERROR/CRITICAL/NONE/TRACE diagnostics.client-log-level Changes the log-level of the clients. INFO DEBUG/WARNING/ERROR/CRITICAL/NONE/TRACE diagnostics.brick-sys-log-level Depending on the value defined for this option, log messages at and above the defined level are generated in the syslog and the brick log files. CRITICAL INFO/WARNING/ERROR/CRITICAL diagnostics.client-sys-log-level Depending on the value defined for this option, log messages at and above the defined level are generated in the syslog and the client log files. CRITICAL INFO/WARNING/ERROR/CRITICAL diagnostics.brick-log-format Allows you to configure the log format to log either with a message id or without one on the brick. with-msg-id no-msg-id/with-msg-id diagnostics.client-log-format Allows you to configure the log format to log either with a message ID or without one on the client. with-msg-id no-msg-id/with-msg-id diagnostics.brick-log-buf-size The maximum number of unique log messages that can be suppressed until the timeout or buffer overflow, whichever occurs first on the bricks. 5 0 and 20 (0 and 20 included) diagnostics.client-log-buf-size The maximum number of unique log messages that can be suppressed until the timeout or buffer overflow, whichever occurs first on the clients. 5 0 and 20 (0 and 20 included) diagnostics.brick-log-flush-timeout The length of time for which the log messages are buffered, before being flushed to the logging infrastructure (gluster or syslog files) on the bricks. 120 30 - 300 seconds (30 and 300 included) diagnostics.client-log-flush-timeout The length of time for which the log messages are buffered, before being flushed to the logging infrastructure (gluster or syslog files) on the clients. 120 30 - 300 seconds (30 and 300 included) Performance *features.trash Enable/disable trash translator off on/off *performance.readdir-ahead Enable/disable readdir-ahead translator in the volume off on/off *performance.read-ahead Enable/disable read-ahead translator in the volume off on/off *performance.io-cache Enable/disable io-cache translator in the volume off on/off performance.quick-read To enable/disable quick-read translator in the volume. on off/on performance.md-cache Enables and disables md-cache translator. off off/on performance.open-behind Enables and disables open-behind translator. on off/on performance.nl-cache Enables and disables nl-cache translator. off off/on performance.stat-prefetch Enables and disables stat-prefetch translator. on off/on performance.client-io-threads Enables and disables client-io-thread translator. on off/on performance.write-behind Enables and disables write-behind translator. on off/on performance.write-behind-window-size Size of the per-file write-behind buffer. 1MB Write-behind cache size performance.io-thread-count The number of threads in IO threads translator. 16 1-64 performance.flush-behind If this option is set ON, instructs write-behind translator to perform flush in background, by returning success (or any errors, if any of previous writes were failed) to application even before flush is sent to backend filesystem. On On/Off performance.cache-max-file-size Sets the maximum file size cached by the io-cache translator. Can use the normal size descriptors of KB, MB, GB,TB or PB (for example, 6GB). Maximum size uint64. 2 ^ 64 -1 bytes size in bytes performance.cache-min-file-size Sets the minimum file size cached by the io-cache translator. Values same as \"max\" above 0B size in bytes performance.cache-refresh-timeout The cached data for a file will be retained till 'cache-refresh-timeout' seconds, after which data re-validation is performed. 1s 0-61 performance.cache-size Size of the read cache. 32 MB size in bytes performance.lazy-open This option requires open-behind to be on. Perform an open in the backend only when a necessary FOP arrives (for example, write on the file descriptor, unlink of the file). When this option is disabled, perform backend open immediately after an unwinding open. Yes Yes/No performance.md-cache-timeout The time period in seconds which controls when metadata cache has to be refreshed. If the age of cache is greater than this time-period, it is refreshed. Every time cache is refreshed, its age is reset to 0. 1 0-600 seconds performance.nfs-strict-write-ordering Specifies whether to prevent later writes from overtaking earlier writes for NFS, even if the writes do not relate to the same files or locations. off on/off performance.nfs.flush-behind Specifies whether the write-behind translator performs flush operations in the background for NFS by returning (false) success to the application before flush file operations are sent to the backend file system. on on/off performance.nfs.strict-o-direct Specifies whether to attempt to minimize the cache effects of I/O for a file on NFS. When this option is enabled and a file descriptor is opened using the O_DIRECT flag, write-back caching is disabled for writes that affect that file descriptor. When this option is disabled, O_DIRECT has no effect on caching. This option is ignored if performance.write-behind is disabled. off on/off performance.nfs.write-behind-trickling-writes Enables and disables trickling-write strategy for the write-behind translator for NFS clients. on off/on performance.nfs.write-behind-window-size Specifies the size of the write-behind buffer for a single file or inode for NFS. 1 MB 512 KB - 1 GB performance.rda-cache-limit The value specified for this option is the maximum size of cache consumed by the readdir-ahead translator. This value is global and the total memory consumption by readdir-ahead is capped by this value, irrespective of the number/size of directories cached. 10MB 0-1GB performance.rda-request-size The value specified for this option will be the size of buffer holding directory entries in readdirp response. 128KB 4KB-128KB performance.resync-failed-syncs-after-fsync If syncing cached writes that were issued before an fsync operation fails, this option configures whether to reattempt the failed sync operations. off on/off performance.strict-o-direct Specifies whether to attempt to minimize the cache effects of I/O for a file. When this option is enabled and a file descriptor is opened using the O_DIRECT flag, write-back caching is disabled for writes that affect that file descriptor. When this option is disabled, O_DIRECT has no effect on caching. This option is ignored if performance.write-behind is disabled. on on/off performance.strict-write-ordering Specifies whether to prevent later writes from overtaking earlier writes, even if the writes do not relate to the same files or locations. on on/off performance.use-anonymous-fd This option requires open-behind to be on. For read operations, use anonymous file descriptor when the original file descriptor is open-behind and not yet opened in the backend. Yes No/Yes performance.write-behind-trickling-writes Enables and disables trickling-write strategy for the write-behind translator for FUSE clients. on off/on performance.write-behind-window-size Specifies the size of the write-behind buffer for a single file or inode. 1MB 512 KB - 1 GB features.read-only Enables you to mount the entire volume as read-only for all the clients (including NFS clients) accessing it. Off On/Off features.quota-deem-statfs When this option is set to on, it takes the quota limits into consideration while estimating the filesystem size. The limit will be treated as the total size instead of the actual size of filesystem. on on/off features.shard Enables or disables sharding on the volume. Affects files created after volume configuration. disable enable/disable features.shard-block-size Specifies the maximum size of file pieces when sharding is enabled. Affects files created after volume configuration. 64MB 4MB-4TB features.uss This option enable/disable User Serviceable Snapshots on the volume. off on/off geo-replication.indexing Use this option to automatically sync the changes in the filesystem from Primary to Secondary. Off On/Off network.frame-timeout The time frame after which the operation has to be declared as dead, if the server does not respond for a particular operation. 1800 (30 mins) 1800 secs network.ping-timeout The time duration for which the client waits to check if the server is responsive. When a ping timeout happens, there is a network disconnect between the client and server. All resources held by server on behalf of the client get cleaned up. When a reconnection happens, all resources will need to be re-acquired before the client can resume its operations on the server. Additionally, the locks will be acquired and the lock tables updated. This reconnect is a very expensive operation and should be avoided. 42 Secs 42 Secs nfs nfs.enable-ino32 For 32-bit nfs clients or applications that do not support 64-bit inode numbers or large files, use this option from the CLI to make Gluster NFS return 32-bit inode numbers instead of 64-bit inode numbers. Off On/Off nfs.volume-access Set the access type for the specified sub-volume. read-write read-write/read-only nfs.trusted-write If there is an UNSTABLE write from the client, STABLE flag will be returned to force the client to not send a COMMIT request. In some environments, combined with a replicated GlusterFS setup, this option can improve write performance. This flag allows users to trust Gluster replication logic to sync data to the disks and recover when required. COMMIT requests if received will be handled in a default manner by fsyncing. STABLE writes are still handled in a sync manner. Off On/Off nfs.trusted-sync All writes and COMMIT requests are treated as async. This implies that no write requests are guaranteed to be on server disks when the write reply is received at the NFS client. Trusted sync includes trusted-write behavior. Off On/Off nfs.export-dir This option can be used to export specified comma separated subdirectories in the volume. The path must be an absolute path. Along with path allowed list of IPs/hostname can be associated with each subdirectory. If provided connection will allowed only from these IPs. Format: \\<dir>[(hostspec[hostspec...])][,...]. Where hostspec can be an IP address, hostname or an IP range in CIDR notation. Note : Care must be taken while configuring this option as invalid entries and/or unreachable DNS servers can introduce unwanted delay in all the mount calls. No sub directory exported. Absolute path with allowed list of IP/hostname nfs.export-volumes Enable/Disable exporting entire volumes, instead if used in conjunction with nfs3.export-dir, can allow setting up only subdirectories as exports. On On/Off nfs.rpc-auth-unix Enable/Disable the AUTH_UNIX authentication type. This option is enabled by default for better interoperability. However, you can disable it if required. On On/Off nfs.rpc-auth-null Enable/Disable the AUTH_NULL authentication type. It is not recommended to change the default value for this option. On On/Off nfs.rpc-auth-allow\\<IP- Addresses> Allow a comma separated list of addresses and/or hostnames to connect to the server. By default, all clients are disallowed. This allows you to define a general rule for all exported volumes. Reject All IP address or Host name nfs.rpc-auth-reject\\<IP- Addresses> Reject a comma separated list of addresses and/or hostnames from connecting to the server. By default, all connections are disallowed. This allows you to define a general rule for all exported volumes. Reject All IP address or Host name nfs.ports-insecure Allow client connections from unprivileged ports. By default only privileged ports are allowed. This is a global setting in case insecure ports are to be enabled for all exports using a single option. Off On/Off nfs.addr-namelookup Turn-off name lookup for incoming client connections using this option. In some setups, the name server can take too long to reply to DNS queries resulting in timeouts of mount requests. Use this option to turn off name lookups during address authentication. Note, turning this off will prevent you from using hostnames in rpc-auth.addr.* filters. On On/Off nfs.register-with-portmap For systems that need to run multiple NFS servers, you need to prevent more than one from registering with portmap service. Use this option to turn off portmap registration for Gluster NFS. On On/Off nfs.port \\<PORT- NUMBER> Use this option on systems that need Gluster NFS to be associated with a non-default port number. NA 38465-38467 nfs.disable Turn-off volume being exported by NFS Off On/Off Server server.allow-insecure Allow client connections from unprivileged ports. By default only privileged ports are allowed. This is a global setting in case insecure ports are to be enabled for all exports using a single option. On On/Off server.statedump-path Location of the state dump file. tmp directory of the brick New directory path server.allow-insecure Allows FUSE-based client connections from unprivileged ports.By default, this is enabled, meaning that ports can accept and reject messages from insecure ports. When disabled, only privileged ports are allowed. on on/off server.anongid Value of the GID used for the anonymous user when root-squash is enabled. When root-squash is enabled, all the requests received from the root GID (that is 0) are changed to have the GID of the anonymous user. 65534 (this UID is also known as nfsnobody) 0 - 4294967295 server.anonuid Value of the UID used for the anonymous user when root-squash is enabled. When root-squash is enabled, all the requests received from the root UID (that is 0) are changed to have the UID of the anonymous user. 65534 (this UID is also known as nfsnobody) 0 - 4294967295 server.event-threads Specifies the number of event threads to execute in parallel. Larger values would help process responses faster, depending on available processing power. 2 1-1024 server.gid-timeout The time period in seconds which controls when cached groups has to expire. This is the cache that contains the groups (GIDs) where a specified user (UID) belongs to. This option is used only when server.manage-gids is enabled. 2 0-4294967295 seconds server.manage-gids Resolve groups on the server-side. By enabling this option, the groups (GIDs) a user (UID) belongs to gets resolved on the server, instead of using the groups that were send in the RPC Call by the client. This option makes it possible to apply permission checks for users that belong to bigger group lists than the protocol supports (approximately 93). off on/off server.root-squash Prevents root users from having root privileges, and instead assigns them the privileges of nfsnobody. This squashes the power of the root users, preventing unauthorized modification of files on the Red Hat Gluster Storage servers. This option is used only for glusterFS NFS protocol. off on/off server.statedump-path Specifies the directory in which the statedumpfiles must be stored. path to directory /var/run/gluster (for a default installation) Storage storage.health-check-interval Number of seconds between health-checks done on the filesystem that is used for the brick(s). Defaults to 30 seconds, set to 0 to disable. tmp directory of the brick New directory path storage.linux-io_uring Enable/Disable io_uring based I/O at the posix xlator on the bricks. Off On/Off storage.fips-mode-rchecksum If enabled, posix_rchecksum uses the FIPS compliant SHA256 checksum, else it uses MD5. on on/ off storage.create-mask Maximum set (upper limit) of permission for the files that will be created. 0777 0000 - 0777 storage.create-directory-mask Maximum set (upper limit) of permission for the directories that will be created. 0777 0000 - 0777 storage.force-create-mode Minimum set (lower limit) of permission for the files that will be created. 0000 0000 - 0777 storage.force-create-directory Minimum set (lower limit) of permission for the directories that will be created. 0000 0000 - 0777 storage.health-check-interval Sets the time interval in seconds for a filesystem health check. You can set it to 0 to disable. 30 seconds 0-4294967295 seconds storage.reserve To reserve storage space at the brick. This option accepts size in form of MB and also in form of percentage. If user has configured the storage.reserve option using size in MB earlier, and then wants to give the size in percentage, it can be done using the same option. Also, the newest set value is considered, if it was in MB before and then if it sent in percentage, the percentage value becomes new value and the older one is over-written 1 (1% of the brick size) 0-100 Note We've found few performance xlators, options marked with * in above table have been causing more performance regression than improving. These xlators should be turned off for volumes.","title":"Tuning Volume Options"},{"location":"Administrator-Guide/arbiter-volumes-and-quorum/","text":"Arbiter volumes and quorum options in gluster The arbiter volume is a special subset of replica volumes that is aimed at preventing split-brains and providing the same consistency guarantees as a normal replica 3 volume without consuming 3x space. Arbiter volumes and quorum options in gluster Arbiter configuration Arbiter brick(s) sizing Why Arbiter? Split-brains in replica volumes Server-quorum and some pitfalls Client Quorum Replica 2 and Replica 3 volumes How Arbiter works Arbiter configuration The syntax for creating the volume is: # gluster volume create <VOLNAME> replica 2 arbiter 1 <NEW-BRICK> ... Note : The earlier syntax used to be replica 3 arbiter 1 but that was leading to confusions among users about the total no. of data bricks. For the sake of backward compatibility, the old syntax also works. In any case, the implied meaning is that there are 2 data bricks and 1 arbiter brick in a nx(2+1) arbiter volume. For example: # gluster volume create testvol replica 2 arbiter 1 server{1..6}:/bricks/brick volume create: testvol: success: please start the volume to access data This means that for every 3 bricks listed, 1 of them is an arbiter. We have created 6 bricks. With a replica count of three, each 3rd brick in the series will be a replica subvolume. Since we have two sets of 3, this created a distribute subvolume made of up two replica subvolumes. Each replica subvolume is defined to have 1 arbiter out of the 3 bricks. The arbiter bricks are taken from the end of each replica subvolume. # gluster volume info Volume Name: testvol Type: Distributed-Replicate Volume ID: ae6c4162-38c2-4368-ae5d-6bad141a4119 Status: Created Number of Bricks: 2 x (2 + 1) = 6 Transport-type: tcp Bricks: Brick1: server1:/bricks/brick Brick2: server2:/bricks/brick Brick3: server3:/bricks/brick (arbiter) Brick4: server4:/bricks/brick Brick5: server5:/bricks/brick Brick6: server6:/bricks/brick (arbiter) Options Reconfigured : transport.address-family: inet performance.readdir-ahead: on ` The arbiter brick will store only the file/directory names (i.e. the tree structure) and extended attributes (metadata) but not any data. i.e. the file size (as shown by ls -l ) will be zero bytes. It will also store other gluster metadata like the .glusterfs folder and its contents. Note: Enabling the arbiter feature automatically configures client-quorum to 'auto'. This setting is not to be changed. Arbiter brick(s) sizing Since the arbiter brick does not store file data, its disk usage will be considerably less than the other bricks of the replica. The sizing of the brick will depend on how many files you plan to store in the volume. A good estimate will be 4KB times the number of files in the replica. Note that the estimate also depends on the inode space alloted by the underlying filesystem for a given disk size. The maxpct value in XFS for volumes of size 1TB to 50TB is only 5%. If you want to store say 300 million files, 4KB x 300M gives us 1.2TB. 5% of this is around 60GB. Assuming the recommended inode size of 512 bytes, that gives us the ability to store only 60GB/512 ~= 120 million files. So it is better to choose a higher maxpct value (say 25%) while formatting an XFS disk of size greater than 1TB. Refer the man page of mkfs.xfs for details. Why Arbiter? Split-brains in replica volumes When a file is in split-brain, there is an inconsistency in either data or metadata (permissions, uid/gid, extended attributes etc.) of the file amongst the bricks of a replica and we do not have enough information to authoritatively pick a copy as being pristine and heal to the bad copies, despite all bricks being up and online. For directories, there is also an entry-split brain where a file inside it has different gfids/ file-type (say one is a file and another is a directory of the same name) across the bricks of a replica. This document describes how to resolve files that are in split-brain using gluster cli or the mount point. Almost always, split-brains occur due to network disconnects (where a client temporarily loses connection to the bricks) and very rarely due to the gluster brick processes going down or returning an error. Server-quorum and some pitfalls This document provides a detailed description of this feature. The volume options for server-quorum are: Option:cluster.server-quorum-ratio Value Description: 0 to 100 Option:cluster.server-quorum-type Value Description: none | server If set to server, this option enables the specified volume to participate in the server-side quorum. If set to none, that volume alone is not considered for volume checks. The cluster.server-quorum-ratio is a percentage figure and is cluster wide- i.e. you cannot have different ratio for different volumes in the same trusted pool. For a two-node trusted storage pool, it is important to set this value greater than 50%, so that two nodes separated from each other do not believe they have quorum simultaneously. For a two-node plain replica volume, this would mean both nodes need to be up and running. So there is no notion of HA/failover. There are users who create a replica 2 volume from 2 nodes and peer-probe a 'dummy' node without bricks and enable server quorum with a ratio of 51%. This does not prevent files from getting into split-brain. For example, if B1 and B2 are the bricks/nodes of the replica and B3 is the dummy node, we can still end up in split-brain like so: B1 goes down, B2 and B3 are up. Server-quorum is still. File is modified by the client. B2 goes down, B1 comes back up. Server-quorum is met. Same file is modified by the client. We now have different contents for the file in B1 and B2 ==>split-brain. In author\u2019s opinion, server-quorum is useful if you want to avoid split-brains to the volume(s) configuration across the nodes and not in the I/O path. Unlike in client-quorum where the volume becomes read-only when quorum is lost, loss of server-quorum in a particular node makes glusterd kill the brick processes on that node (for the participating volumes) making even reads impossible. Client Quorum Client-quorum is a feature implemented in AFR to prevent split-brains in the I/O path for replicate/distributed-replicate volumes. By default, if the client-quorum is not met for a particular replica subvol, it becomes unavailable. The other subvols (in a dist-rep volume) will still have R/W access. The following volume set options are used to configure it: Option: cluster.quorum-type Default Value: none Value Description: none|auto|fixed If set to \"fixed\", this option allows writes to a file only if the number of active bricks in that replica set (to which the file belongs) is greater than or equal to the count specified in the 'quorum-count' option. If set to \"auto\", this option allows write to the file only if number of bricks that are up >= ceil (of the total number of bricks that constitute that replica/2). If the number of replicas is even, then there is a further check: If the number of up bricks is exactly equal to n/2, then the first brick must be one of the bricks that are up. If it is more than n/2 then it is not necessary that the first brick is one of the up bricks. Option: cluster.quorum-count Value Description: The number of bricks that must be active in a replica-set to allow writes. This option is used in conjunction with cluster.quorum-type =fixed option to specify the number of bricks to be active to participate in quorum. If the quorum-type is auto then this option has no significance. Earlier, when quorm was not met, the replica subvolume turned read-only. But since glusterfs-3.13 and upwards, the subvolume becomes unavailable, i.e. all the file operations fail with ENOTCONN error instead of becoming EROFS. This means the cluster.quorum-reads volume option is also not supported. Replica 2 and Replica 3 volumes From the above descriptions, it is clear that client-quorum cannot really be applied to a replica 2 volume:(without costing HA). If the quorum-type is set to auto, then by the description given earlier, the first brick must always be up, irrespective of the status of the second brick. IOW, if only the second brick is up, the subvol returns ENOTCONN, i.e. no HA. If quorum-type is set to fixed, then the quorum-count has to be two to prevent split-brains (otherwise a write can succeed in brick1, another in brick2 =>split-brain). So for all practical purposes, if you want high availability in a replica 2 volume, it is recommended not to enable client-quorum. In a replica 3 volume, client-quorum is enabled by default and set to 'auto'. This means 2 bricks need to be up for the write to succeed. Here is how this configuration prevents files from ending up in split-brain: Say B1, B2 and B3 are the bricks: B3 is down, quorum is met, write happens on file B1 and B2. B3 comes up, B2 is down, quorum is again met, write happens on B1 and B3. B2 comes up, B1 goes down, quorum is met. Now when a write is issued, AFR sees that B2 and B3's pending xattrs blame each other and therefore the write is not allowed and is failed with ENOTCONN. How Arbiter works There are 2 components to the arbiter volume. One is the arbiter xlator that is loaded in the brick process of every 3rd (i.e. the arbiter) brick. The other is the arbitration logic itself that is present in AFR (the replicate xlator) loaded on the clients. The former acts as a sort of 'filter' translator for the FOPS- i.e. it allows entry operations to hit POSIX, blocks certain inode operations like read (unwinds the call with ENOTCONN) and unwinds other inode operations like write, truncate etc. with success without winding it down to POSIX. The latter i.e. the arbitration logic present in AFR takes full file locks when writing to a file, just like in normal replica volumes. The behavior of arbiter volumes in allowing/failing write FOPS in conjunction with client-quorum can be summarized in the below steps: If all 3 bricks are up (happy case), then there is no issue and the FOPs are allowed. If 2 bricks are up and if one of them is the arbiter (i.e. the 3rd brick) and it blames the other up brick for a given file, then all write FOPS will fail with ENOTCONN. This is because, in this scenario, the only true copy is on the brick that is down. Hence we cannot allow writes until that brick is also up. If the arbiter doesn't blame the other brick, FOPS will be allowed to proceed. 'Blaming' here is w.r.t the values of AFR changelog extended attributes. If 2 bricks are up and the arbiter is down, then FOPS will be allowed. When the arbiter comes up, the entry/metadata heals to it happen. Of course data heals are not needed. If only one brick is up, then client-quorum is not met and the volume returns ENOTCONN. In all cases, if there is only one source before the FOP is initiated (even if all bricks are up) and if the FOP fails on that source, the application will receive ENOTCONN. For example, assume that a write failed on B2 and B3, i.e. B1 is the only source. Now if for some reason, the second write failed on B1 (before there was a chance for selfheal to complete despite all brick being up), the application would receive failure (ENOTCONN) for that write. The bricks being up or down described above does not necessarily mean the brick process is offline. It can also mean the mount lost the connection to the brick due to network disconnects etc.","title":"Arbiter volumes and quorum options"},{"location":"Administrator-Guide/arbiter-volumes-and-quorum/#arbiter-volumes-and-quorum-options-in-gluster","text":"The arbiter volume is a special subset of replica volumes that is aimed at preventing split-brains and providing the same consistency guarantees as a normal replica 3 volume without consuming 3x space. Arbiter volumes and quorum options in gluster Arbiter configuration Arbiter brick(s) sizing Why Arbiter? Split-brains in replica volumes Server-quorum and some pitfalls Client Quorum Replica 2 and Replica 3 volumes How Arbiter works","title":"Arbiter volumes and quorum options in gluster"},{"location":"Administrator-Guide/arbiter-volumes-and-quorum/#arbiter-configuration","text":"The syntax for creating the volume is: # gluster volume create <VOLNAME> replica 2 arbiter 1 <NEW-BRICK> ... Note : The earlier syntax used to be replica 3 arbiter 1 but that was leading to confusions among users about the total no. of data bricks. For the sake of backward compatibility, the old syntax also works. In any case, the implied meaning is that there are 2 data bricks and 1 arbiter brick in a nx(2+1) arbiter volume. For example: # gluster volume create testvol replica 2 arbiter 1 server{1..6}:/bricks/brick volume create: testvol: success: please start the volume to access data This means that for every 3 bricks listed, 1 of them is an arbiter. We have created 6 bricks. With a replica count of three, each 3rd brick in the series will be a replica subvolume. Since we have two sets of 3, this created a distribute subvolume made of up two replica subvolumes. Each replica subvolume is defined to have 1 arbiter out of the 3 bricks. The arbiter bricks are taken from the end of each replica subvolume. # gluster volume info Volume Name: testvol Type: Distributed-Replicate Volume ID: ae6c4162-38c2-4368-ae5d-6bad141a4119 Status: Created Number of Bricks: 2 x (2 + 1) = 6 Transport-type: tcp Bricks: Brick1: server1:/bricks/brick Brick2: server2:/bricks/brick Brick3: server3:/bricks/brick (arbiter) Brick4: server4:/bricks/brick Brick5: server5:/bricks/brick Brick6: server6:/bricks/brick (arbiter) Options Reconfigured : transport.address-family: inet performance.readdir-ahead: on ` The arbiter brick will store only the file/directory names (i.e. the tree structure) and extended attributes (metadata) but not any data. i.e. the file size (as shown by ls -l ) will be zero bytes. It will also store other gluster metadata like the .glusterfs folder and its contents. Note: Enabling the arbiter feature automatically configures client-quorum to 'auto'. This setting is not to be changed.","title":"Arbiter configuration"},{"location":"Administrator-Guide/arbiter-volumes-and-quorum/#arbiter-bricks-sizing","text":"Since the arbiter brick does not store file data, its disk usage will be considerably less than the other bricks of the replica. The sizing of the brick will depend on how many files you plan to store in the volume. A good estimate will be 4KB times the number of files in the replica. Note that the estimate also depends on the inode space alloted by the underlying filesystem for a given disk size. The maxpct value in XFS for volumes of size 1TB to 50TB is only 5%. If you want to store say 300 million files, 4KB x 300M gives us 1.2TB. 5% of this is around 60GB. Assuming the recommended inode size of 512 bytes, that gives us the ability to store only 60GB/512 ~= 120 million files. So it is better to choose a higher maxpct value (say 25%) while formatting an XFS disk of size greater than 1TB. Refer the man page of mkfs.xfs for details.","title":"Arbiter brick(s) sizing"},{"location":"Administrator-Guide/arbiter-volumes-and-quorum/#why-arbiter","text":"","title":"Why Arbiter?"},{"location":"Administrator-Guide/arbiter-volumes-and-quorum/#split-brains-in-replica-volumes","text":"When a file is in split-brain, there is an inconsistency in either data or metadata (permissions, uid/gid, extended attributes etc.) of the file amongst the bricks of a replica and we do not have enough information to authoritatively pick a copy as being pristine and heal to the bad copies, despite all bricks being up and online. For directories, there is also an entry-split brain where a file inside it has different gfids/ file-type (say one is a file and another is a directory of the same name) across the bricks of a replica. This document describes how to resolve files that are in split-brain using gluster cli or the mount point. Almost always, split-brains occur due to network disconnects (where a client temporarily loses connection to the bricks) and very rarely due to the gluster brick processes going down or returning an error.","title":"Split-brains in replica volumes"},{"location":"Administrator-Guide/arbiter-volumes-and-quorum/#server-quorum-and-some-pitfalls","text":"This document provides a detailed description of this feature. The volume options for server-quorum are: Option:cluster.server-quorum-ratio Value Description: 0 to 100 Option:cluster.server-quorum-type Value Description: none | server If set to server, this option enables the specified volume to participate in the server-side quorum. If set to none, that volume alone is not considered for volume checks. The cluster.server-quorum-ratio is a percentage figure and is cluster wide- i.e. you cannot have different ratio for different volumes in the same trusted pool. For a two-node trusted storage pool, it is important to set this value greater than 50%, so that two nodes separated from each other do not believe they have quorum simultaneously. For a two-node plain replica volume, this would mean both nodes need to be up and running. So there is no notion of HA/failover. There are users who create a replica 2 volume from 2 nodes and peer-probe a 'dummy' node without bricks and enable server quorum with a ratio of 51%. This does not prevent files from getting into split-brain. For example, if B1 and B2 are the bricks/nodes of the replica and B3 is the dummy node, we can still end up in split-brain like so: B1 goes down, B2 and B3 are up. Server-quorum is still. File is modified by the client. B2 goes down, B1 comes back up. Server-quorum is met. Same file is modified by the client. We now have different contents for the file in B1 and B2 ==>split-brain. In author\u2019s opinion, server-quorum is useful if you want to avoid split-brains to the volume(s) configuration across the nodes and not in the I/O path. Unlike in client-quorum where the volume becomes read-only when quorum is lost, loss of server-quorum in a particular node makes glusterd kill the brick processes on that node (for the participating volumes) making even reads impossible.","title":"Server-quorum and some pitfalls"},{"location":"Administrator-Guide/arbiter-volumes-and-quorum/#client-quorum","text":"Client-quorum is a feature implemented in AFR to prevent split-brains in the I/O path for replicate/distributed-replicate volumes. By default, if the client-quorum is not met for a particular replica subvol, it becomes unavailable. The other subvols (in a dist-rep volume) will still have R/W access. The following volume set options are used to configure it: Option: cluster.quorum-type Default Value: none Value Description: none|auto|fixed If set to \"fixed\", this option allows writes to a file only if the number of active bricks in that replica set (to which the file belongs) is greater than or equal to the count specified in the 'quorum-count' option. If set to \"auto\", this option allows write to the file only if number of bricks that are up >= ceil (of the total number of bricks that constitute that replica/2). If the number of replicas is even, then there is a further check: If the number of up bricks is exactly equal to n/2, then the first brick must be one of the bricks that are up. If it is more than n/2 then it is not necessary that the first brick is one of the up bricks. Option: cluster.quorum-count Value Description: The number of bricks that must be active in a replica-set to allow writes. This option is used in conjunction with cluster.quorum-type =fixed option to specify the number of bricks to be active to participate in quorum. If the quorum-type is auto then this option has no significance. Earlier, when quorm was not met, the replica subvolume turned read-only. But since glusterfs-3.13 and upwards, the subvolume becomes unavailable, i.e. all the file operations fail with ENOTCONN error instead of becoming EROFS. This means the cluster.quorum-reads volume option is also not supported.","title":"Client Quorum"},{"location":"Administrator-Guide/arbiter-volumes-and-quorum/#replica-2-and-replica-3-volumes","text":"From the above descriptions, it is clear that client-quorum cannot really be applied to a replica 2 volume:(without costing HA). If the quorum-type is set to auto, then by the description given earlier, the first brick must always be up, irrespective of the status of the second brick. IOW, if only the second brick is up, the subvol returns ENOTCONN, i.e. no HA. If quorum-type is set to fixed, then the quorum-count has to be two to prevent split-brains (otherwise a write can succeed in brick1, another in brick2 =>split-brain). So for all practical purposes, if you want high availability in a replica 2 volume, it is recommended not to enable client-quorum. In a replica 3 volume, client-quorum is enabled by default and set to 'auto'. This means 2 bricks need to be up for the write to succeed. Here is how this configuration prevents files from ending up in split-brain: Say B1, B2 and B3 are the bricks: B3 is down, quorum is met, write happens on file B1 and B2. B3 comes up, B2 is down, quorum is again met, write happens on B1 and B3. B2 comes up, B1 goes down, quorum is met. Now when a write is issued, AFR sees that B2 and B3's pending xattrs blame each other and therefore the write is not allowed and is failed with ENOTCONN.","title":"Replica 2 and Replica 3 volumes"},{"location":"Administrator-Guide/arbiter-volumes-and-quorum/#how-arbiter-works","text":"There are 2 components to the arbiter volume. One is the arbiter xlator that is loaded in the brick process of every 3rd (i.e. the arbiter) brick. The other is the arbitration logic itself that is present in AFR (the replicate xlator) loaded on the clients. The former acts as a sort of 'filter' translator for the FOPS- i.e. it allows entry operations to hit POSIX, blocks certain inode operations like read (unwinds the call with ENOTCONN) and unwinds other inode operations like write, truncate etc. with success without winding it down to POSIX. The latter i.e. the arbitration logic present in AFR takes full file locks when writing to a file, just like in normal replica volumes. The behavior of arbiter volumes in allowing/failing write FOPS in conjunction with client-quorum can be summarized in the below steps: If all 3 bricks are up (happy case), then there is no issue and the FOPs are allowed. If 2 bricks are up and if one of them is the arbiter (i.e. the 3rd brick) and it blames the other up brick for a given file, then all write FOPS will fail with ENOTCONN. This is because, in this scenario, the only true copy is on the brick that is down. Hence we cannot allow writes until that brick is also up. If the arbiter doesn't blame the other brick, FOPS will be allowed to proceed. 'Blaming' here is w.r.t the values of AFR changelog extended attributes. If 2 bricks are up and the arbiter is down, then FOPS will be allowed. When the arbiter comes up, the entry/metadata heals to it happen. Of course data heals are not needed. If only one brick is up, then client-quorum is not met and the volume returns ENOTCONN. In all cases, if there is only one source before the FOP is initiated (even if all bricks are up) and if the FOP fails on that source, the application will receive ENOTCONN. For example, assume that a write failed on B2 and B3, i.e. B1 is the only source. Now if for some reason, the second write failed on B1 (before there was a chance for selfheal to complete despite all brick being up), the application would receive failure (ENOTCONN) for that write. The bricks being up or down described above does not necessarily mean the brick process is offline. It can also mean the mount lost the connection to the brick due to network disconnects etc.","title":"How Arbiter works"},{"location":"Administrator-Guide/formatting-and-mounting-bricks/","text":"Formatting and Mounting Bricks Creating a Thinly Provisioned Logical Volume To create a thinly provisioned logical volume, proceed with the following steps: Create a physical volume(PV) by using the pvcreate command. For example: # pvcreate --dataalignment 128K /dev/sdb Here, /dev/sdb is a storage device. Use the correct dataalignment option based on your device. Note The device name and the alignment value will vary based on the device you are using. Create a Volume Group (VG) from the PV using the vgcreate command: For example: # vgcreate --physicalextentsize 128K gfs_vg /dev/sdb It is recommended that only one VG must be created from one storage device. Create a thin-pool using the following commands: Create an LV to serve as the metadata device using the following command: # lvcreate -L metadev_sz --name metadata_device_name VOLGROUP For example: # lvcreate -L 16776960K --name gfs_pool_meta gfs_vg Create an LV to serve as the data device using the following command: # lvcreate -L datadev_sz --name thin_pool VOLGROUP` For example: # lvcreate -L 536870400K --name gfs_pool gfs_vg Create a thin pool from the data LV and the metadata LV using the following command: # lvconvert --chunksize STRIPE_WIDTH --thinpool VOLGROUP/thin_pool --poolmetadata VOLGROUP/metadata_device_name For example: # lvconvert --chunksize 1280K --thinpool gfs_vg/gfs_pool --poolmetadata gfs_vg/gfs_pool_meta Note By default, the newly provisioned chunks in a thin pool are zeroed to prevent data leaking between different block devices. # lvchange --zero n VOLGROUP/thin_pool For example: # lvchange --zero n gfs_vg/gfs_pool Create a thinly provisioned volume from the previously created pool using the lvcreate command: For example: # lvcreate -V 1G -T gfs_vg/gfs_pool -n gfs_lv It is recommended that only one LV should be created in a thin pool. Format bricks using the supported XFS configuration, mount the bricks, and verify the bricks are mounted correctly. Run mkfs.xfs -f -i size=512 -n size=8192 -d su=128k,sw=10 DEVICE to format the bricks to the supported XFS file system format. Here, DEVICE is the thin LV(here /dev/gfs_vg/gfs_lv ). The inode size is set to 512 bytes to accommodate for the extended attributes used by GlusterFS. Run mkdir /mountpoint to create a directory to link the brick to. Add an entry in /etc/fstab: /dev/gfs_vg/gfs_lv /mountpoint xfs rw,inode64,noatime,nouuid 1 2 Run mount /mountpoint to mount the brick. Run the df -h command to verify the brick is successfully mounted: # df -h /dev/gfs_vg/gfs_lv 16G 1.2G 15G 7% /exp1","title":"Formatting and Mounting Bricks"},{"location":"Administrator-Guide/formatting-and-mounting-bricks/#formatting-and-mounting-bricks","text":"","title":"Formatting and Mounting Bricks"},{"location":"Administrator-Guide/formatting-and-mounting-bricks/#creating-a-thinly-provisioned-logical-volume","text":"To create a thinly provisioned logical volume, proceed with the following steps: Create a physical volume(PV) by using the pvcreate command. For example: # pvcreate --dataalignment 128K /dev/sdb Here, /dev/sdb is a storage device. Use the correct dataalignment option based on your device. Note The device name and the alignment value will vary based on the device you are using. Create a Volume Group (VG) from the PV using the vgcreate command: For example: # vgcreate --physicalextentsize 128K gfs_vg /dev/sdb It is recommended that only one VG must be created from one storage device. Create a thin-pool using the following commands: Create an LV to serve as the metadata device using the following command: # lvcreate -L metadev_sz --name metadata_device_name VOLGROUP For example: # lvcreate -L 16776960K --name gfs_pool_meta gfs_vg Create an LV to serve as the data device using the following command: # lvcreate -L datadev_sz --name thin_pool VOLGROUP` For example: # lvcreate -L 536870400K --name gfs_pool gfs_vg Create a thin pool from the data LV and the metadata LV using the following command: # lvconvert --chunksize STRIPE_WIDTH --thinpool VOLGROUP/thin_pool --poolmetadata VOLGROUP/metadata_device_name For example: # lvconvert --chunksize 1280K --thinpool gfs_vg/gfs_pool --poolmetadata gfs_vg/gfs_pool_meta Note By default, the newly provisioned chunks in a thin pool are zeroed to prevent data leaking between different block devices. # lvchange --zero n VOLGROUP/thin_pool For example: # lvchange --zero n gfs_vg/gfs_pool Create a thinly provisioned volume from the previously created pool using the lvcreate command: For example: # lvcreate -V 1G -T gfs_vg/gfs_pool -n gfs_lv It is recommended that only one LV should be created in a thin pool. Format bricks using the supported XFS configuration, mount the bricks, and verify the bricks are mounted correctly. Run mkfs.xfs -f -i size=512 -n size=8192 -d su=128k,sw=10 DEVICE to format the bricks to the supported XFS file system format. Here, DEVICE is the thin LV(here /dev/gfs_vg/gfs_lv ). The inode size is set to 512 bytes to accommodate for the extended attributes used by GlusterFS. Run mkdir /mountpoint to create a directory to link the brick to. Add an entry in /etc/fstab: /dev/gfs_vg/gfs_lv /mountpoint xfs rw,inode64,noatime,nouuid 1 2 Run mount /mountpoint to mount the brick. Run the df -h command to verify the brick is successfully mounted: # df -h /dev/gfs_vg/gfs_lv 16G 1.2G 15G 7% /exp1","title":"Creating a Thinly Provisioned Logical Volume"},{"location":"Administrator-Guide/io_uring/","text":"io_uring support in gluster io_uring is an asynchronous I/O interface similar to linux-aio, but aims to be more performant. Refer https://kernel.dk/io_uring.pdf and https://kernel-recipes.org/en/2019/talks/faster-io-through-io_uring/ for more details. Incorporating io_uring in various layers of gluster is an ongoing activity but beginning with glusterfs-9.0, support has been added to the posix translator via the storage.linux-io_uring volume option. When this option is enabled, the posix translator in the glusterfs brick process (at the server side) will use io_uring calls for reads, writes and fsyncs as opposed to the normal pread/pwrite based syscalls. Example: [server~]# gluster volume set testvol storage.linux-io_uring on volume set: success [server~]# [server~]# gluster volume set testvol storage.linux-io_uring off volume set: success This option can be enabled/disabled only when the volume is not running. i.e. you can toggle the option when the volume is Created or is Stopped as indicated in gluster volume status $VOLNAME","title":"io_uring"},{"location":"Administrator-Guide/io_uring/#io_uring-support-in-gluster","text":"io_uring is an asynchronous I/O interface similar to linux-aio, but aims to be more performant. Refer https://kernel.dk/io_uring.pdf and https://kernel-recipes.org/en/2019/talks/faster-io-through-io_uring/ for more details. Incorporating io_uring in various layers of gluster is an ongoing activity but beginning with glusterfs-9.0, support has been added to the posix translator via the storage.linux-io_uring volume option. When this option is enabled, the posix translator in the glusterfs brick process (at the server side) will use io_uring calls for reads, writes and fsyncs as opposed to the normal pread/pwrite based syscalls.","title":"io_uring support in gluster"},{"location":"Administrator-Guide/io_uring/#example","text":"[server~]# gluster volume set testvol storage.linux-io_uring on volume set: success [server~]# [server~]# gluster volume set testvol storage.linux-io_uring off volume set: success This option can be enabled/disabled only when the volume is not running. i.e. you can toggle the option when the volume is Created or is Stopped as indicated in gluster volume status $VOLNAME","title":"Example:"},{"location":"Administrator-Guide/overview/","text":"Overview The Administration guide covers day to day management tasks as well as advanced configuration methods for your Gluster setup. You can manage your Gluster cluster using the Gluster CLI See the glossary for an explanation of the various terms used in this document.","title":"Overview"},{"location":"Administrator-Guide/overview/#overview","text":"The Administration guide covers day to day management tasks as well as advanced configuration methods for your Gluster setup. You can manage your Gluster cluster using the Gluster CLI See the glossary for an explanation of the various terms used in this document.","title":"Overview"},{"location":"Administrator-Guide/setting-up-storage/","text":"Setting Up Storage A volume is a logical collection of bricks where each brick is an export directory on a server in the trusted storage pool. Before creating a volume, you need to set up the bricks that will form the volume. Brick Naming Conventions Formatting and Mounting Bricks Posix ACLS","title":"Setting Up Storage"},{"location":"Administrator-Guide/setting-up-storage/#setting-up-storage","text":"A volume is a logical collection of bricks where each brick is an export directory on a server in the trusted storage pool. Before creating a volume, you need to set up the bricks that will form the volume. Brick Naming Conventions Formatting and Mounting Bricks Posix ACLS","title":"Setting Up Storage"},{"location":"CLI-Reference/cli-main/","text":"Gluster Command Line Interface Overview Use the Gluster CLI to setup and manage your Gluster cluster from a terminal. You can run the Gluster CLI on any Gluster server either by invoking the commands or by running the Gluster CLI in interactive mode. You can also use the gluster command remotely using SSH. The gluster CLI syntax is gluster <command> . To run a command directly: # gluster <command> For example, to view the status of all peers: # gluster peer status To run a command in interactive mode, start a gluster shell by typing: # gluster This will open a gluster command prompt. You now run the command at the prompt. gluster> <command> For example, to view the status of all peers, gluster> peer status Peer Commands The peer commands are used to manage the Trusted Server Pool (TSP). Command Syntax Description peer probe peer probe server Add server to the TSP peer detach peer detach server Remove server from the TSP peer status peer status Display the status of all nodes in the TSP pool list pool list List all nodes in the TSP Volume Commands The volume commands are used to setup and manage Gluster volumes. Command Syntax Description volume create volume create volname [options] bricks Create a volume called volname using the specified bricks with the configuration specified by options volume start volume start volname [force] Start volume volname volume stop volume stop volname Stop volume volname volume info volume info [ volname ] Display volume info for volname if provided, else for all volumes on the TSP volume status volumes status[ volname ] Display volume status for volname if provided, else for all volumes on the TSP volume list volume list List all volumes in the TSP volume set volume set volname option value Set option to value for volname volume get volume get volname < option |all> Display the value of option (if specified)for volname , or all options otherwise volume add-brick volume add-brick brick-1 ... brick-n Expand volname to include the bricks brick-1 to brick-n volume remove-brick volume remove-brick brick-1 ... brick-n \\<start|stop|status|commit|force> Shrink volname by removing the bricks brick-1 to brick-n . start will trigger a rebalance to migrate data from the removed bricks. stop will stop an ongoing remove-brick operation. force will remove the bricks immediately and any data on them will no longer be accessible from Gluster clients. volume replace-brick volume replace-brick volname old-brick new-brick Replace old-brick of volname with new-brick volume delete volume delete volname Delete volname For additional detail of all the available CLI commands, please refer to man gluster output.","title":"Overview"},{"location":"CLI-Reference/cli-main/#gluster-command-line-interface","text":"","title":"Gluster Command Line Interface"},{"location":"CLI-Reference/cli-main/#overview","text":"Use the Gluster CLI to setup and manage your Gluster cluster from a terminal. You can run the Gluster CLI on any Gluster server either by invoking the commands or by running the Gluster CLI in interactive mode. You can also use the gluster command remotely using SSH. The gluster CLI syntax is gluster <command> . To run a command directly: # gluster <command> For example, to view the status of all peers: # gluster peer status To run a command in interactive mode, start a gluster shell by typing: # gluster This will open a gluster command prompt. You now run the command at the prompt. gluster> <command> For example, to view the status of all peers, gluster> peer status","title":"Overview"},{"location":"CLI-Reference/cli-main/#peer-commands","text":"The peer commands are used to manage the Trusted Server Pool (TSP). Command Syntax Description peer probe peer probe server Add server to the TSP peer detach peer detach server Remove server from the TSP peer status peer status Display the status of all nodes in the TSP pool list pool list List all nodes in the TSP","title":"Peer Commands"},{"location":"CLI-Reference/cli-main/#volume-commands","text":"The volume commands are used to setup and manage Gluster volumes. Command Syntax Description volume create volume create volname [options] bricks Create a volume called volname using the specified bricks with the configuration specified by options volume start volume start volname [force] Start volume volname volume stop volume stop volname Stop volume volname volume info volume info [ volname ] Display volume info for volname if provided, else for all volumes on the TSP volume status volumes status[ volname ] Display volume status for volname if provided, else for all volumes on the TSP volume list volume list List all volumes in the TSP volume set volume set volname option value Set option to value for volname volume get volume get volname < option |all> Display the value of option (if specified)for volname , or all options otherwise volume add-brick volume add-brick brick-1 ... brick-n Expand volname to include the bricks brick-1 to brick-n volume remove-brick volume remove-brick brick-1 ... brick-n \\<start|stop|status|commit|force> Shrink volname by removing the bricks brick-1 to brick-n . start will trigger a rebalance to migrate data from the removed bricks. stop will stop an ongoing remove-brick operation. force will remove the bricks immediately and any data on them will no longer be accessible from Gluster clients. volume replace-brick volume replace-brick volname old-brick new-brick Replace old-brick of volname with new-brick volume delete volume delete volname Delete volname For additional detail of all the available CLI commands, please refer to man gluster output.","title":"Volume Commands"},{"location":"Contributors-Guide/Adding-your-blog/","text":"Adding your blog As a developer/user, you have blogged about gluster and want to share the post to Gluster community. OK, you can do that by editing planet-gluster feeds on Github. Please find instructions mentioned in the file and send a pull request. Once approved, all your gluster related posts will appear in planet.gluster.org website.","title":"Adding your gluster blog"},{"location":"Contributors-Guide/Adding-your-blog/#adding-your-blog","text":"As a developer/user, you have blogged about gluster and want to share the post to Gluster community. OK, you can do that by editing planet-gluster feeds on Github. Please find instructions mentioned in the file and send a pull request. Once approved, all your gluster related posts will appear in planet.gluster.org website.","title":"Adding your blog"},{"location":"Contributors-Guide/Bug-Reporting-Guidelines/","text":"Before filing an issue If you are finding any issues, these preliminary checks as useful: Is SELinux enabled? (you can use getenforce to check) Are iptables rules blocking any data traffic? ( iptables -L can help check) Are all the nodes reachable from each other? [ Network problem ] Please search issues to see if the bug has already been reported If an issue has been already filed for a particular release and you found the issue in another release, add a comment in issue. Anyone can search in github issues, you don't need an account. Searching requires some effort, but helps avoid duplicates, and you may find that your problem has already been solved. Reporting An Issue You should have an account with github.com Here is the link to file an issue: Github Note: Please go through all below sections to understand what information we need to put in a bug. So it will help the developer to root cause and fix it Required Information You should gather the information below before creating the bug report. Package Information Location from which the packages are used Package Info - version of glusterfs package installed Cluster Information Number of nodes in the cluster Hostnames and IPs of the gluster Node [if it is not a security issue] Hostname / IP will help developers in understanding & correlating with the logs Output of gluster peer status Node IP, from which the \"x\" operation is done \"x\" here means any operation that causes the issue Volume Information Number of volumes Volume Names Volume on which the particular issue is seen [ if applicable ] Type of volumes Volume options if available Output of gluster volume info Output of gluster volume status Get the statedump of the volume with the problem $ gluster volume statedump <vol-name> This dumps statedump per brick process in /var/run/gluster NOTE: Collect statedumps from one gluster Node in a directory. Repeat it in all Nodes containing the bricks of the volume. All the so collected directories could be archived, compressed and attached to bug Brick Information xfs options when a brick partition was done This could be obtained with this command : $ xfs_info /dev/mapper/vg1-brick Extended attributes on the bricks This could be obtained with this command: $ getfattr -d -m. -ehex /rhs/brick1/b1 Client Information OS Type ( Ubuntu, Fedora, RHEL ) OS Version: In case of Linux distro get the following : uname -r cat /etc/issue Fuse or NFS Mount point on the client with output of mount commands Output of df -Th command Tool Information If any tools are used for testing, provide the info/version about it if any IO is simulated using a script, provide the script Logs Information You can check logs for issues/warnings/errors. Self-heal logs Rebalance logs Glusterd logs Brick logs NFS logs (if applicable) Samba logs (if applicable) Client mount log Add the entire logs as attachment, if its very large to paste as a comment SOS report for CentOS/Fedora Get the sosreport from the involved gluster Node and Client [ in case of CentOS /Fedora ] Add a meaningful name/IP to the sosreport, by renaming/adding hostname/ip to the sosreport name","title":"Bug reporting guidelines"},{"location":"Contributors-Guide/Bug-Reporting-Guidelines/#before-filing-an-issue","text":"If you are finding any issues, these preliminary checks as useful: Is SELinux enabled? (you can use getenforce to check) Are iptables rules blocking any data traffic? ( iptables -L can help check) Are all the nodes reachable from each other? [ Network problem ] Please search issues to see if the bug has already been reported If an issue has been already filed for a particular release and you found the issue in another release, add a comment in issue. Anyone can search in github issues, you don't need an account. Searching requires some effort, but helps avoid duplicates, and you may find that your problem has already been solved.","title":"Before filing an issue"},{"location":"Contributors-Guide/Bug-Reporting-Guidelines/#reporting-an-issue","text":"You should have an account with github.com Here is the link to file an issue: Github Note: Please go through all below sections to understand what information we need to put in a bug. So it will help the developer to root cause and fix it","title":"Reporting An Issue"},{"location":"Contributors-Guide/Bug-Reporting-Guidelines/#required-information","text":"You should gather the information below before creating the bug report.","title":"Required Information"},{"location":"Contributors-Guide/Bug-Reporting-Guidelines/#package-information","text":"Location from which the packages are used Package Info - version of glusterfs package installed","title":"Package Information"},{"location":"Contributors-Guide/Bug-Reporting-Guidelines/#cluster-information","text":"Number of nodes in the cluster Hostnames and IPs of the gluster Node [if it is not a security issue] Hostname / IP will help developers in understanding & correlating with the logs Output of gluster peer status Node IP, from which the \"x\" operation is done \"x\" here means any operation that causes the issue","title":"Cluster Information"},{"location":"Contributors-Guide/Bug-Reporting-Guidelines/#volume-information","text":"Number of volumes Volume Names Volume on which the particular issue is seen [ if applicable ] Type of volumes Volume options if available Output of gluster volume info Output of gluster volume status Get the statedump of the volume with the problem $ gluster volume statedump <vol-name> This dumps statedump per brick process in /var/run/gluster NOTE: Collect statedumps from one gluster Node in a directory. Repeat it in all Nodes containing the bricks of the volume. All the so collected directories could be archived, compressed and attached to bug","title":"Volume Information"},{"location":"Contributors-Guide/Bug-Reporting-Guidelines/#brick-information","text":"xfs options when a brick partition was done This could be obtained with this command : $ xfs_info /dev/mapper/vg1-brick Extended attributes on the bricks This could be obtained with this command: $ getfattr -d -m. -ehex /rhs/brick1/b1","title":"Brick Information"},{"location":"Contributors-Guide/Bug-Reporting-Guidelines/#client-information","text":"OS Type ( Ubuntu, Fedora, RHEL ) OS Version: In case of Linux distro get the following : uname -r cat /etc/issue Fuse or NFS Mount point on the client with output of mount commands Output of df -Th command","title":"Client Information"},{"location":"Contributors-Guide/Bug-Reporting-Guidelines/#tool-information","text":"If any tools are used for testing, provide the info/version about it if any IO is simulated using a script, provide the script","title":"Tool Information"},{"location":"Contributors-Guide/Bug-Reporting-Guidelines/#logs-information","text":"You can check logs for issues/warnings/errors. Self-heal logs Rebalance logs Glusterd logs Brick logs NFS logs (if applicable) Samba logs (if applicable) Client mount log Add the entire logs as attachment, if its very large to paste as a comment","title":"Logs Information"},{"location":"Contributors-Guide/Bug-Reporting-Guidelines/#sos-report-for-centosfedora","text":"Get the sosreport from the involved gluster Node and Client [ in case of CentOS /Fedora ] Add a meaningful name/IP to the sosreport, by renaming/adding hostname/ip to the sosreport name","title":"SOS report for CentOS/Fedora"},{"location":"Contributors-Guide/Bug-Triage/","text":"Issues Triage Guidelines Triaging of issues is an important task; when done correctly, it can reduce the time between reporting an issue and the availability of a fix enormously. Triager should focus on new issues, and try to define the problem easily understandable and as accurate as possible. The goal of the triagers is to reduce the time that developers need to solve the bug report. A triager is like an assistant that helps with the information gathering and possibly the debugging of a new bug report. Because a triager helps preparing a bug before a developer gets involved, it can be a very nice role for new community members that are interested in technical aspects of the software. Triagers will stumble upon many different kind of issues, ranging from reports about spelling mistakes, or unclear log messages to memory leaks causing crashes or performance issues in environments with several hundred storage servers. Nobody expects that triagers can prepare all bug reports. Therefore most developers will be able to assist the triagers, answer questions and suggest approaches to debug and data to gather. Over time, triagers get more experienced and will rely less on developers. Issue triage can be summarized as below points: Is the issue a bug? an enhancement request? or a question? Assign the relevant label. Is there enough information in the issue description? Is it a duplicate issue? Is it assigned to correct component of GlusterFS? Is the bug summary is correct? Assigning issue or Adding people's github handle in the comment, so they get notified. The detailed discussion about the above points are below. Is there enough information? It's hard to generalize what makes a good report. For \"average\" reporters is definitely often helpful to have good steps to reproduce, GlusterFS software version , and information about the test/production environment, Linux/GNU distribution. If the reporter is a developer, steps to reproduce can sometimes be omitted as context is obvious. However, this can create a problem for contributors that need to find their way, hence it is strongly advised to list the steps to reproduce an issue. Other tips: There should be only one issue per report. Try not to mix related or similar looking bugs per report. It should be possible to call the described problem fixed at some point. \"Improve the documentation\" or \"It runs slow\" could never be called fixed, while \"Documentation should cover the topic Embedding\" or \"The page at http://en.wikipedia.org/wiki/Example should load in less than five seconds\" would have a criterion. A good summary of the bug will also help others in finding existing bugs and prevent filing of duplicates. If the bug is a graphical problem, you may want to ask for a screenshot to attach to the bug report. Make sure to ask that the screenshot should not contain any confidential information. Is it a duplicate? If you think that you have found a duplicate but you are not totally sure, just add a comment like \"This issue looks related to issue #NNN\" (and replace NNN by issue-id) so somebody else can take a look and help judging. Is it assigned with correct label? Go through the labels and assign the appropriate label Are the fields correct? Description Sometimes the description does not summarize the bug itself well. You may want to update the bug summary to make the report distinguishable. A good title may contain: A brief explanation of the root cause (if it was found) Some of the symptoms people are experiencing Assigning issue or Adding people's github handle in the comment Normally, developers and potential assignees of an area are already watching all the issues by default, but sometimes reports describe general issues. Only if you know developers who work in the area covered by the issue, and if you know that these developers accept getting CCed or assigned to certain reports, you can mention in comment or even assign the bug report to her/him. To get an idea who works in which area, check to know component owners, you can check the \"MAINTAINERS\" file in root of glusterfs code directory (see Simplified dev workflow ) Bugs present in multiple Versions During triaging you might come across a particular bug which is present across multiple version of GlusterFS. Add that in comment.","title":"Bug Triage"},{"location":"Contributors-Guide/Bug-Triage/#issues-triage-guidelines","text":"Triaging of issues is an important task; when done correctly, it can reduce the time between reporting an issue and the availability of a fix enormously. Triager should focus on new issues, and try to define the problem easily understandable and as accurate as possible. The goal of the triagers is to reduce the time that developers need to solve the bug report. A triager is like an assistant that helps with the information gathering and possibly the debugging of a new bug report. Because a triager helps preparing a bug before a developer gets involved, it can be a very nice role for new community members that are interested in technical aspects of the software. Triagers will stumble upon many different kind of issues, ranging from reports about spelling mistakes, or unclear log messages to memory leaks causing crashes or performance issues in environments with several hundred storage servers. Nobody expects that triagers can prepare all bug reports. Therefore most developers will be able to assist the triagers, answer questions and suggest approaches to debug and data to gather. Over time, triagers get more experienced and will rely less on developers. Issue triage can be summarized as below points: Is the issue a bug? an enhancement request? or a question? Assign the relevant label. Is there enough information in the issue description? Is it a duplicate issue? Is it assigned to correct component of GlusterFS? Is the bug summary is correct? Assigning issue or Adding people's github handle in the comment, so they get notified. The detailed discussion about the above points are below.","title":"Issues Triage Guidelines"},{"location":"Contributors-Guide/Bug-Triage/#is-there-enough-information","text":"It's hard to generalize what makes a good report. For \"average\" reporters is definitely often helpful to have good steps to reproduce, GlusterFS software version , and information about the test/production environment, Linux/GNU distribution. If the reporter is a developer, steps to reproduce can sometimes be omitted as context is obvious. However, this can create a problem for contributors that need to find their way, hence it is strongly advised to list the steps to reproduce an issue. Other tips: There should be only one issue per report. Try not to mix related or similar looking bugs per report. It should be possible to call the described problem fixed at some point. \"Improve the documentation\" or \"It runs slow\" could never be called fixed, while \"Documentation should cover the topic Embedding\" or \"The page at http://en.wikipedia.org/wiki/Example should load in less than five seconds\" would have a criterion. A good summary of the bug will also help others in finding existing bugs and prevent filing of duplicates. If the bug is a graphical problem, you may want to ask for a screenshot to attach to the bug report. Make sure to ask that the screenshot should not contain any confidential information.","title":"Is there enough information?"},{"location":"Contributors-Guide/Bug-Triage/#is-it-a-duplicate","text":"If you think that you have found a duplicate but you are not totally sure, just add a comment like \"This issue looks related to issue #NNN\" (and replace NNN by issue-id) so somebody else can take a look and help judging.","title":"Is it a duplicate?"},{"location":"Contributors-Guide/Bug-Triage/#is-it-assigned-with-correct-label","text":"Go through the labels and assign the appropriate label","title":"Is it assigned with correct label?"},{"location":"Contributors-Guide/Bug-Triage/#are-the-fields-correct","text":"","title":"Are the fields correct?"},{"location":"Contributors-Guide/Bug-Triage/#description","text":"Sometimes the description does not summarize the bug itself well. You may want to update the bug summary to make the report distinguishable. A good title may contain: A brief explanation of the root cause (if it was found) Some of the symptoms people are experiencing","title":"Description"},{"location":"Contributors-Guide/Bug-Triage/#assigning-issue-or-adding-peoples-github-handle-in-the-comment","text":"Normally, developers and potential assignees of an area are already watching all the issues by default, but sometimes reports describe general issues. Only if you know developers who work in the area covered by the issue, and if you know that these developers accept getting CCed or assigned to certain reports, you can mention in comment or even assign the bug report to her/him. To get an idea who works in which area, check to know component owners, you can check the \"MAINTAINERS\" file in root of glusterfs code directory (see Simplified dev workflow )","title":"Assigning issue or Adding people's github handle in the comment"},{"location":"Contributors-Guide/Bug-Triage/#bugs-present-in-multiple-versions","text":"During triaging you might come across a particular bug which is present across multiple version of GlusterFS. Add that in comment.","title":"Bugs present in multiple Versions"},{"location":"Contributors-Guide/GlusterFS-Release-process/","text":"Release Process for GlusterFS The GlusterFS release process aims to provide regular, stable releases, with the ability to also ship new features quickly, while also attempting to reduce the complexity for release maintainers. GlusterFS releases GlusterFS Major releases happen once every 4-6 months. Check Release Schedule for more information on the schedule for major releases. Minor releases happen every month for corresponding branch of major release. Each major release is supported till we have N+2 version is made available. Major releases don't guarantee complete backwards compatability with the previous major release. Minor releases will have guaranteed backwards compatibilty with earlier minor releases of the same branch. GlusterFS major release Each GlusterFS major release has a 4-6 month release window, in which changes get merged. This window is split into two phases. A Open phase, where all changes get merged A Stability phase, where only changes that stabilize the release get merged. The first 2-4 months of a release window will be the Open phase, and the last month will be the stability phase. The release engineer (or team doing the release) is responsible for messaging. Open phase Any changes are accepted during this phase. New features that are introduced in this phase, need to be capable of being selectively built. All changes in the master branch are automatically incuded in the next release. All changes will be accepted during the Open phase. The changes have a few requirements, a change fixing a bug SHOULD have public test case a change introducing a new feature MUST have a disable switch that can disable the feature during a build Stability phase This phase is used to stabilize any new features introduced in the open phase, or general bug fixes for already existing features. A new release-<version> branch is created at the beginning of this phase. All changes need to be sent to the master branch before getting backported to the new release branch. No new features will be merged in this phase. At the end of this phase, any new feature introduced that hasn't been declared stable will be disabled, if possible removed, to prevent confusion and set clear expectations towards users and developers. Patches accepted in the Stability phase have the following requirements: a change MUST fix an issue that users have reported or are very likely to hit each change SHOULD have a public test-case (.t or DiSTAF) a change MUST NOT add a new FOP a change MUST NOT add a new xlator a change SHOULD NOT add a new volume option, unless a public discussion was kept and several maintainers agree that this is the only right approach a change MAY add new values for existing volume options, these need to be documented in the release notes and be marked as a 'minor feature enhancement' or similar it is NOT RECOMMENDED to modify the contents of existing log messages, automation and log parsers can depend on the phrasing a change SHOULD NOT have more than approx. 100 lines changed, additional public discussion and agreement among maintainers is required to get big changes approved a change SHOULD NOT modify existing structures or parameters that get sent over the network, unless a public discussion was kept and several maintainers agree that this is the only right approach existing structures or parameters MAY get extended with additional values (i.e. new flags in a bitmap/mask) if the extensions are optional and do not affect older/newer client/server combinations Patches that do not satisfy the above requirements can still be submitted for review, but cannot be merged. Release procedure This procedure is followed by a release maintainer/manager, to perform the actual release. The release procedure for both major releases and minor releases is nearly the same. The procedure for the major releases starts at the beginning of the Stability phase, and for the minor release at the start of the release window. TODO: Add the release verification procedure Release steps The release-manager needs to follow the following steps, to actually perform the release once ready. Create tarball Add the release-notes to the docs/release-notes/ directory in the sources after merging the release-notes, create a tag like v3.6.2 push the tag to git.gluster.org create the tarball with the release job in Jenkins Notify packagers Notify the packagers that we need packages created. Provide the link to the source tarball from the Jenkins release job to the packagers mailinglist . A list of the people involved in the package maintenance for the different distributions is in the MAINTAINERS file in the sources, all of them should be subscribed to the packagers mailinglist. Create a new Tracker Bug for the next release The tracker bugs are used as guidance for blocker bugs and should get created when a release is made. To create one Create a new milestone base the contents on open issues, like the one for glusterfs-8 issues that were not fixed in previous release, but in milestone should be moved to the new milestone. Create Release Announcement (Major releases) The Release Announcement is based off the release notes. This needs to indicate: * What this release's overall focus is * Which versions will stop receiving updates as of this release * Links to the direct download folder * Feature set Best practice as of version-8 is to create a collaborative version of the release notes that both the release manager and community lead work on together, and the release manager posts to the mailing lists (gluster-users@, gluster-devel@, announce@). Create Upgrade Guide (Major releases) If required, as in the case of a major release, an upgrade guide needs to be available at the same time as the release. This document should go under the Upgrade Guide section of the glusterdocs repository. Send Release Announcement Once the Fedora/EL RPMs are ready (and any others that are ready by then), send the release announcement: Gluster Mailing lists gluster-announce gluster-devel gluster-users Gluster Blog The blog will automatically post to both Facebook and Twitter. Be careful with this! Gluster Twitter account Gluster Facebook page Gluster LinkedIn group","title":"GlusterFS Release process"},{"location":"Contributors-Guide/GlusterFS-Release-process/#release-process-for-glusterfs","text":"The GlusterFS release process aims to provide regular, stable releases, with the ability to also ship new features quickly, while also attempting to reduce the complexity for release maintainers.","title":"Release Process for GlusterFS"},{"location":"Contributors-Guide/GlusterFS-Release-process/#glusterfs-releases","text":"GlusterFS Major releases happen once every 4-6 months. Check Release Schedule for more information on the schedule for major releases. Minor releases happen every month for corresponding branch of major release. Each major release is supported till we have N+2 version is made available. Major releases don't guarantee complete backwards compatability with the previous major release. Minor releases will have guaranteed backwards compatibilty with earlier minor releases of the same branch.","title":"GlusterFS releases"},{"location":"Contributors-Guide/GlusterFS-Release-process/#glusterfs-major-release","text":"Each GlusterFS major release has a 4-6 month release window, in which changes get merged. This window is split into two phases. A Open phase, where all changes get merged A Stability phase, where only changes that stabilize the release get merged. The first 2-4 months of a release window will be the Open phase, and the last month will be the stability phase. The release engineer (or team doing the release) is responsible for messaging.","title":"GlusterFS major release"},{"location":"Contributors-Guide/GlusterFS-Release-process/#open-phase","text":"Any changes are accepted during this phase. New features that are introduced in this phase, need to be capable of being selectively built. All changes in the master branch are automatically incuded in the next release. All changes will be accepted during the Open phase. The changes have a few requirements, a change fixing a bug SHOULD have public test case a change introducing a new feature MUST have a disable switch that can disable the feature during a build","title":"Open phase"},{"location":"Contributors-Guide/GlusterFS-Release-process/#stability-phase","text":"This phase is used to stabilize any new features introduced in the open phase, or general bug fixes for already existing features. A new release-<version> branch is created at the beginning of this phase. All changes need to be sent to the master branch before getting backported to the new release branch. No new features will be merged in this phase. At the end of this phase, any new feature introduced that hasn't been declared stable will be disabled, if possible removed, to prevent confusion and set clear expectations towards users and developers. Patches accepted in the Stability phase have the following requirements: a change MUST fix an issue that users have reported or are very likely to hit each change SHOULD have a public test-case (.t or DiSTAF) a change MUST NOT add a new FOP a change MUST NOT add a new xlator a change SHOULD NOT add a new volume option, unless a public discussion was kept and several maintainers agree that this is the only right approach a change MAY add new values for existing volume options, these need to be documented in the release notes and be marked as a 'minor feature enhancement' or similar it is NOT RECOMMENDED to modify the contents of existing log messages, automation and log parsers can depend on the phrasing a change SHOULD NOT have more than approx. 100 lines changed, additional public discussion and agreement among maintainers is required to get big changes approved a change SHOULD NOT modify existing structures or parameters that get sent over the network, unless a public discussion was kept and several maintainers agree that this is the only right approach existing structures or parameters MAY get extended with additional values (i.e. new flags in a bitmap/mask) if the extensions are optional and do not affect older/newer client/server combinations Patches that do not satisfy the above requirements can still be submitted for review, but cannot be merged.","title":"Stability phase"},{"location":"Contributors-Guide/GlusterFS-Release-process/#release-procedure","text":"This procedure is followed by a release maintainer/manager, to perform the actual release. The release procedure for both major releases and minor releases is nearly the same. The procedure for the major releases starts at the beginning of the Stability phase, and for the minor release at the start of the release window. TODO: Add the release verification procedure","title":"Release procedure"},{"location":"Contributors-Guide/GlusterFS-Release-process/#release-steps","text":"The release-manager needs to follow the following steps, to actually perform the release once ready.","title":"Release steps"},{"location":"Contributors-Guide/GlusterFS-Release-process/#create-tarball","text":"Add the release-notes to the docs/release-notes/ directory in the sources after merging the release-notes, create a tag like v3.6.2 push the tag to git.gluster.org create the tarball with the release job in Jenkins","title":"Create tarball"},{"location":"Contributors-Guide/GlusterFS-Release-process/#notify-packagers","text":"Notify the packagers that we need packages created. Provide the link to the source tarball from the Jenkins release job to the packagers mailinglist . A list of the people involved in the package maintenance for the different distributions is in the MAINTAINERS file in the sources, all of them should be subscribed to the packagers mailinglist.","title":"Notify packagers"},{"location":"Contributors-Guide/GlusterFS-Release-process/#create-a-new-tracker-bug-for-the-next-release","text":"The tracker bugs are used as guidance for blocker bugs and should get created when a release is made. To create one Create a new milestone base the contents on open issues, like the one for glusterfs-8 issues that were not fixed in previous release, but in milestone should be moved to the new milestone.","title":"Create a new Tracker Bug for the next release"},{"location":"Contributors-Guide/GlusterFS-Release-process/#create-release-announcement","text":"(Major releases) The Release Announcement is based off the release notes. This needs to indicate: * What this release's overall focus is * Which versions will stop receiving updates as of this release * Links to the direct download folder * Feature set Best practice as of version-8 is to create a collaborative version of the release notes that both the release manager and community lead work on together, and the release manager posts to the mailing lists (gluster-users@, gluster-devel@, announce@).","title":"Create Release Announcement"},{"location":"Contributors-Guide/GlusterFS-Release-process/#create-upgrade-guide","text":"(Major releases) If required, as in the case of a major release, an upgrade guide needs to be available at the same time as the release. This document should go under the Upgrade Guide section of the glusterdocs repository.","title":"Create Upgrade Guide"},{"location":"Contributors-Guide/GlusterFS-Release-process/#send-release-announcement","text":"Once the Fedora/EL RPMs are ready (and any others that are ready by then), send the release announcement: Gluster Mailing lists gluster-announce gluster-devel gluster-users Gluster Blog The blog will automatically post to both Facebook and Twitter. Be careful with this! Gluster Twitter account Gluster Facebook page Gluster LinkedIn group","title":"Send Release Announcement"},{"location":"Contributors-Guide/Guidelines-For-Maintainers/","text":"Guidelines For Maintainers GlusterFS has maintainers, sub-maintainers and release maintainers to manage the project's codebase. Sub-maintainers are the owners for specific areas/components of the source tree. Maintainers operate across all components in the source tree.Release maintainers are the owners for various release branches (release-x.y) present in the GlusterFS repository. In the guidelines below, release maintainers and sub-maintainers are also implied when there is a reference to maintainers unless it is explicitly called out. Guidelines that Maintainers are expected to adhere to Ensure qualitative and timely management of patches sent for review. For merging patches into the repository, it is expected of maintainers to: Merge patches of owned components only. Seek approvals from all maintainers before merging a patchset spanning multiple components. Ensure that regression tests pass for all patches before merging. Ensure that regression tests accompany all patch submissions. Ensure the related Bug or GitHub Issue has sufficient details about the cause of the problem, or description of the introduction for the change. Ensure that documentation is updated for a noticeable change in user perceivable behavior or design. Encourage code unit tests from patch submitters to improve the overall quality of the codebase. Not merge patches written by themselves until there is a +2 Code Review vote by other reviewers. The responsibility of merging a patch into a release branch in normal circumstances will be that of the release maintainer's. Only in exceptional situations, maintainers & sub-maintainers will merge patches into a release branch. Release maintainers will ensure approval from appropriate maintainers before merging a patch into a release branch. Maintainers have a responsibility to the community, it is expected of maintainers to: Facilitate the community in all aspects. Be very active and visible in the community. Be objective and consider the larger interests of the community ahead of individual interests. Be receptive to user feedback. Address concerns & issues affecting users. Lead by example. Queries on Guidelines Any questions or comments regarding these guidelines can be routed to gluster-devel or slack channel . Patches in Github Github can be used to list patches that need reviews and/or can get merged from Pull Requests","title":"Guidelines For Maintainers"},{"location":"Contributors-Guide/Guidelines-For-Maintainers/#guidelines-for-maintainers","text":"GlusterFS has maintainers, sub-maintainers and release maintainers to manage the project's codebase. Sub-maintainers are the owners for specific areas/components of the source tree. Maintainers operate across all components in the source tree.Release maintainers are the owners for various release branches (release-x.y) present in the GlusterFS repository. In the guidelines below, release maintainers and sub-maintainers are also implied when there is a reference to maintainers unless it is explicitly called out.","title":"Guidelines For Maintainers"},{"location":"Contributors-Guide/Guidelines-For-Maintainers/#guidelines-that-maintainers-are-expected-to-adhere-to","text":"Ensure qualitative and timely management of patches sent for review. For merging patches into the repository, it is expected of maintainers to: Merge patches of owned components only. Seek approvals from all maintainers before merging a patchset spanning multiple components. Ensure that regression tests pass for all patches before merging. Ensure that regression tests accompany all patch submissions. Ensure the related Bug or GitHub Issue has sufficient details about the cause of the problem, or description of the introduction for the change. Ensure that documentation is updated for a noticeable change in user perceivable behavior or design. Encourage code unit tests from patch submitters to improve the overall quality of the codebase. Not merge patches written by themselves until there is a +2 Code Review vote by other reviewers. The responsibility of merging a patch into a release branch in normal circumstances will be that of the release maintainer's. Only in exceptional situations, maintainers & sub-maintainers will merge patches into a release branch. Release maintainers will ensure approval from appropriate maintainers before merging a patch into a release branch. Maintainers have a responsibility to the community, it is expected of maintainers to: Facilitate the community in all aspects. Be very active and visible in the community. Be objective and consider the larger interests of the community ahead of individual interests. Be receptive to user feedback. Address concerns & issues affecting users. Lead by example.","title":"Guidelines that Maintainers are expected to adhere to"},{"location":"Contributors-Guide/Guidelines-For-Maintainers/#queries-on-guidelines","text":"Any questions or comments regarding these guidelines can be routed to gluster-devel or slack channel .","title":"Queries on Guidelines"},{"location":"Contributors-Guide/Guidelines-For-Maintainers/#patches-in-github","text":"Github can be used to list patches that need reviews and/or can get merged from Pull Requests","title":"Patches in Github"},{"location":"Contributors-Guide/Index/","text":"Workflow Guide Bug Handling Bug reporting guidelines - Guideline for reporting a bug in GlusterFS Bug triage guidelines - Guideline on how to triage bugs for GlusterFS Release Process GlusterFS Release process - Our release process / checklist Patch Acceptance The Guidelines For Maintainers explains when maintainers can merge patches. Blogging about gluster The Adding your gluster blog explains how to add your gluster blog to Community blogger.","title":"Index"},{"location":"Contributors-Guide/Index/#workflow-guide","text":"","title":"Workflow Guide"},{"location":"Contributors-Guide/Index/#bug-handling","text":"Bug reporting guidelines - Guideline for reporting a bug in GlusterFS Bug triage guidelines - Guideline on how to triage bugs for GlusterFS","title":"Bug Handling"},{"location":"Contributors-Guide/Index/#release-process","text":"GlusterFS Release process - Our release process / checklist","title":"Release Process"},{"location":"Contributors-Guide/Index/#patch-acceptance","text":"The Guidelines For Maintainers explains when maintainers can merge patches.","title":"Patch Acceptance"},{"location":"Contributors-Guide/Index/#blogging-about-gluster","text":"The Adding your gluster blog explains how to add your gluster blog to Community blogger.","title":"Blogging about gluster"},{"location":"Developer-guide/Backport-Guidelines/","text":"Backport Guidelines In GlusterFS project, as a policy, any new change, bug fix, etc., are to be fixed in 'devel' branch before release branches. When a bug is fixed in the devel branch, it might be desirable or necessary in release branch. This page describes the policy GlusterFS has regarding the backports. As a user, or contributor, being aware of this policy would help you to understand how to request for backport from community. Policy No feature from devel would be backported to the release branch CVE ie., security vulnerability (listed on the CVE database) reported in the existing releases would be backported, after getting fixed in devel branch. Only topics which bring about data loss or, unavailability would be backported to the release. For any other issues, the project recommends that the installation be upgraded to a newer release where the specific bug has been addressed. Gluster provides 'rolling' upgrade support, i.e., one can upgrade their server version without stopping the application I/O, so we recommend migrating to higher version. Things to pay attention to while backporting a patch. If your patch meets the criteria above, or you are a user, who prefer to have a fix backported, because your current setup is facing issues, below are the steps you need to take care to submit a patch on release branch. The patch should have same 'Change-Id'. How to contact release owners? All release owners are part of 'gluster-devel@gluster.org' mailing list. Please write your expectation from next release there, so we can take that to consideration while making the release.","title":"Backport Guidelines"},{"location":"Developer-guide/Backport-Guidelines/#backport-guidelines","text":"In GlusterFS project, as a policy, any new change, bug fix, etc., are to be fixed in 'devel' branch before release branches. When a bug is fixed in the devel branch, it might be desirable or necessary in release branch. This page describes the policy GlusterFS has regarding the backports. As a user, or contributor, being aware of this policy would help you to understand how to request for backport from community.","title":"Backport Guidelines"},{"location":"Developer-guide/Backport-Guidelines/#policy","text":"No feature from devel would be backported to the release branch CVE ie., security vulnerability (listed on the CVE database) reported in the existing releases would be backported, after getting fixed in devel branch. Only topics which bring about data loss or, unavailability would be backported to the release. For any other issues, the project recommends that the installation be upgraded to a newer release where the specific bug has been addressed. Gluster provides 'rolling' upgrade support, i.e., one can upgrade their server version without stopping the application I/O, so we recommend migrating to higher version.","title":"Policy"},{"location":"Developer-guide/Backport-Guidelines/#things-to-pay-attention-to-while-backporting-a-patch","text":"If your patch meets the criteria above, or you are a user, who prefer to have a fix backported, because your current setup is facing issues, below are the steps you need to take care to submit a patch on release branch. The patch should have same 'Change-Id'.","title":"Things to pay attention to while backporting a patch."},{"location":"Developer-guide/Backport-Guidelines/#how-to-contact-release-owners","text":"All release owners are part of 'gluster-devel@gluster.org' mailing list. Please write your expectation from next release there, so we can take that to consideration while making the release.","title":"How to contact release owners?"},{"location":"Developer-guide/Building-GlusterFS/","text":"Building GlusterFS This page describes how to build and install GlusterFS. Build Requirements The following packages are required for building GlusterFS, GNU Autotools Automake Autoconf Libtool lex (generally flex) GNU Bison OpenSSL libxml2 Python 2.x libaio libibverbs librdmacm readline lvm2 glib2 liburcu cmocka libacl sqlite fuse-devel liburing-devel Fedora The following dnf command installs all the build requirements for Fedora, # dnf install automake autoconf libtool flex bison openssl-devel \\ libxml2-devel python-devel libaio-devel libibverbs-devel \\ librdmacm-devel readline-devel lvm2-devel glib2-devel \\ userspace-rcu-devel libcmocka-devel libacl-devel sqlite-devel \\ fuse-devel redhat-rpm-config rpcgen libtirpc-devel make \\ libuuid-devel liburing-devel Ubuntu The following apt-get command will install all the build requirements on Ubuntu, # sudo apt-get install make automake autoconf libtool flex bison \\ pkg-config libssl-dev libxml2-dev python-dev libaio-dev \\ libibverbs-dev librdmacm-dev libreadline-dev liblvm2-dev \\ libglib2.0-dev liburcu-dev libcmocka-dev libsqlite3-dev \\ libacl1-dev liburing-dev CentOS / Enterprise Linux v7 The following yum command installs the build requirements for CentOS / Enterprise Linux 7, # yum install autoconf automake bison cmockery2-devel dos2unix flex \\ fuse-devel glib2-devel libacl-devel libaio-devel libattr-devel \\ libcurl-devel libibverbs-devel librdmacm-devel libtirpc-devel \\ libtool libxml2-devel lvm2-devel make openssl-devel pkgconfig \\ pyliblzma python-devel python-eventlet python-netifaces \\ python-paste-deploy python-simplejson python-sphinx python-webob \\ pyxattr readline-devel rpm-build sqlite-devel systemtap-sdt-devel \\ tar userspace-rcu-devel Note: You will need to enable the CentOS SIG repos in order to install userspace-rcu-devel package For details check https://wiki.centos.org/SpecialInterestGroup/Storage Enable repositories for CentOS 8 The following yum command enables needed repositories providing the build requirements for CentOS 8, # yum-config-manager --enable powertools --enable devel CentOS / Enterprise Linux v8 The following yum command installs the build requirements for CentOS / Enterprise Linux 8, # yum install autoconf automake bison dos2unix flex fuse-devel glib2-devel \\ libacl-devel libaio-devel libattr-devel libcurl-devel libibverbs-devel \\ librdmacm-devel libtirpc-devel libuuid-devel libtool libxml2-devel \\ lvm2-devel make openssl-devel pkgconfig xz-devel python3-devel \\ python3-netifaces python3-paste-deploy python3-simplejson python3-sphinx \\ python3-webob python3-pyxattr readline-devel rpm-build sqlite-devel \\ systemtap-sdt-devel tar userspace-rcu-devel rpcgen liburing-devel Building from Source This section describes how to build GlusterFS from source. It is assumed you have a copy of the GlusterFS source (either from a released tarball or a git clone). All the commands below are to be run with the source directory as the working directory. Configuring for building Run the below commands once for configuring and setting up the build process. Run autogen to generate the configure script. # ./autogen.sh Once autogen completes successfully a configure script is generated. Run the configure script to generate the makefiles. # ./configure For CentOS 7, use: # ./configure --without-libtirpc If the above build requirements have been installed, running the configure script should give the below configure summary, GlusterFS configure summary =========================== FUSE client : yes Infiniband verbs : yes epoll IO multiplex : yes argp-standalone : no fusermount : yes readline : yes georeplication : yes Linux-AIO : yes Enable Debug : no Block Device xlator : yes glupy : yes Use syslog : yes XML output : yes Encryption xlator : yes Unit Tests : no Track priv ports : yes POSIX ACLs : yes Data Classification : yes SELinux features : yes firewalld-config : no Experimental xlators : yes Events : yes EC dynamic support : x64 sse avx Use memory pools : yes Nanosecond m/atimes : yes Legacy gNFS server : no During development it is good to enable a debug build. To do this run configure with a '--enable-debug' flag. # ./configure --enable-debug Further configuration flags can be found by running configure with a '--help' flag, # ./configure --help Please note to enable gNFS use the following flag # ./configure --enable-gnfs If you are looking at contributing by fixing some of the memory issues, use --enable-asan option # ./configure --enable-asan The above option will build with -fsanitize=address -fno-omit-frame-pointer options and uses the libasan.so shared library, so that needs to be available. io_uring is introduced on Linux kernel version 5.1. GlusterFS also needs the user space liburing helper library. If these are not available for your machine or if you wish to build GlusterFS without io_uring support, use --disable-linux-io_uring option # ./configure --disable-linux-io_uring Building Once configured, GlusterFS can be built with a simple make command. # make To speed up the build process on a multicore machine, add a '-jN' flag, where N is the number of parallel jobs. Installing Run 'make install' to install GlusterFS. By default, GlusterFS will be installed into '/usr/local' prefix. To change the install prefix, give the appropriate option to configure. If installing into the default prefix, you might need to use 'sudo' or 'su -c' to install. # sudo make install NOTE: glusterfs can be installed on any target path. However, the mount.glusterfs script has to be in /sbin/mount.glusterfs for mounting via command mount -t glusterfs to work. See -t section in man 8 mount for more details. Running GlusterFS GlusterFS can be only run as root, so the following commands will need to be run as root. If you've installed into the default '/usr/local' prefix, add '/usr/local/sbin' and '/usr/local/bin' to your PATH before running the below commands. A source install will generally not install any init scripts. So you will need to start glusterd manually. To manually start glusterd just run, # systemctl daemon-reload # systemctl start glusterd This will start glusterd and fork it into the background as a daemon process. You now run 'gluster' commands and make use of GlusterFS. Building packages Building RPMs Building RPMs is really simple. On a RPM based system, for eg. Fedora, get the source and do the configuration steps as shown in the 'Building from Source' section. After the configuration step, run the following steps to build RPMs, # cd extras/LinuxRPM # make glusterrpms This will create rpms from the source in 'extras/LinuxRPM'. (Note: You will need to install the rpmbuild requirements including rpmbuild and mock) For CentOS / Enterprise Linux 8 the dependencies can be installed via: # yum install mock rpm-build selinux-policy-devel","title":"Build and Install GlusterFS"},{"location":"Developer-guide/Building-GlusterFS/#building-glusterfs","text":"This page describes how to build and install GlusterFS.","title":"Building GlusterFS"},{"location":"Developer-guide/Building-GlusterFS/#build-requirements","text":"The following packages are required for building GlusterFS, GNU Autotools Automake Autoconf Libtool lex (generally flex) GNU Bison OpenSSL libxml2 Python 2.x libaio libibverbs librdmacm readline lvm2 glib2 liburcu cmocka libacl sqlite fuse-devel liburing-devel","title":"Build Requirements"},{"location":"Developer-guide/Building-GlusterFS/#fedora","text":"The following dnf command installs all the build requirements for Fedora, # dnf install automake autoconf libtool flex bison openssl-devel \\ libxml2-devel python-devel libaio-devel libibverbs-devel \\ librdmacm-devel readline-devel lvm2-devel glib2-devel \\ userspace-rcu-devel libcmocka-devel libacl-devel sqlite-devel \\ fuse-devel redhat-rpm-config rpcgen libtirpc-devel make \\ libuuid-devel liburing-devel","title":"Fedora"},{"location":"Developer-guide/Building-GlusterFS/#ubuntu","text":"The following apt-get command will install all the build requirements on Ubuntu, # sudo apt-get install make automake autoconf libtool flex bison \\ pkg-config libssl-dev libxml2-dev python-dev libaio-dev \\ libibverbs-dev librdmacm-dev libreadline-dev liblvm2-dev \\ libglib2.0-dev liburcu-dev libcmocka-dev libsqlite3-dev \\ libacl1-dev liburing-dev","title":"Ubuntu"},{"location":"Developer-guide/Building-GlusterFS/#centos-enterprise-linux-v7","text":"The following yum command installs the build requirements for CentOS / Enterprise Linux 7, # yum install autoconf automake bison cmockery2-devel dos2unix flex \\ fuse-devel glib2-devel libacl-devel libaio-devel libattr-devel \\ libcurl-devel libibverbs-devel librdmacm-devel libtirpc-devel \\ libtool libxml2-devel lvm2-devel make openssl-devel pkgconfig \\ pyliblzma python-devel python-eventlet python-netifaces \\ python-paste-deploy python-simplejson python-sphinx python-webob \\ pyxattr readline-devel rpm-build sqlite-devel systemtap-sdt-devel \\ tar userspace-rcu-devel Note: You will need to enable the CentOS SIG repos in order to install userspace-rcu-devel package For details check https://wiki.centos.org/SpecialInterestGroup/Storage","title":"CentOS / Enterprise Linux v7"},{"location":"Developer-guide/Building-GlusterFS/#enable-repositories-for-centos-8","text":"The following yum command enables needed repositories providing the build requirements for CentOS 8, # yum-config-manager --enable powertools --enable devel","title":"Enable repositories for CentOS 8"},{"location":"Developer-guide/Building-GlusterFS/#centos-enterprise-linux-v8","text":"The following yum command installs the build requirements for CentOS / Enterprise Linux 8, # yum install autoconf automake bison dos2unix flex fuse-devel glib2-devel \\ libacl-devel libaio-devel libattr-devel libcurl-devel libibverbs-devel \\ librdmacm-devel libtirpc-devel libuuid-devel libtool libxml2-devel \\ lvm2-devel make openssl-devel pkgconfig xz-devel python3-devel \\ python3-netifaces python3-paste-deploy python3-simplejson python3-sphinx \\ python3-webob python3-pyxattr readline-devel rpm-build sqlite-devel \\ systemtap-sdt-devel tar userspace-rcu-devel rpcgen liburing-devel","title":"CentOS / Enterprise Linux v8"},{"location":"Developer-guide/Building-GlusterFS/#building-from-source","text":"This section describes how to build GlusterFS from source. It is assumed you have a copy of the GlusterFS source (either from a released tarball or a git clone). All the commands below are to be run with the source directory as the working directory.","title":"Building from Source"},{"location":"Developer-guide/Building-GlusterFS/#configuring-for-building","text":"Run the below commands once for configuring and setting up the build process. Run autogen to generate the configure script. # ./autogen.sh Once autogen completes successfully a configure script is generated. Run the configure script to generate the makefiles. # ./configure For CentOS 7, use: # ./configure --without-libtirpc If the above build requirements have been installed, running the configure script should give the below configure summary, GlusterFS configure summary =========================== FUSE client : yes Infiniband verbs : yes epoll IO multiplex : yes argp-standalone : no fusermount : yes readline : yes georeplication : yes Linux-AIO : yes Enable Debug : no Block Device xlator : yes glupy : yes Use syslog : yes XML output : yes Encryption xlator : yes Unit Tests : no Track priv ports : yes POSIX ACLs : yes Data Classification : yes SELinux features : yes firewalld-config : no Experimental xlators : yes Events : yes EC dynamic support : x64 sse avx Use memory pools : yes Nanosecond m/atimes : yes Legacy gNFS server : no During development it is good to enable a debug build. To do this run configure with a '--enable-debug' flag. # ./configure --enable-debug Further configuration flags can be found by running configure with a '--help' flag, # ./configure --help Please note to enable gNFS use the following flag # ./configure --enable-gnfs If you are looking at contributing by fixing some of the memory issues, use --enable-asan option # ./configure --enable-asan The above option will build with -fsanitize=address -fno-omit-frame-pointer options and uses the libasan.so shared library, so that needs to be available. io_uring is introduced on Linux kernel version 5.1. GlusterFS also needs the user space liburing helper library. If these are not available for your machine or if you wish to build GlusterFS without io_uring support, use --disable-linux-io_uring option # ./configure --disable-linux-io_uring","title":"Configuring for building"},{"location":"Developer-guide/Building-GlusterFS/#building","text":"Once configured, GlusterFS can be built with a simple make command. # make To speed up the build process on a multicore machine, add a '-jN' flag, where N is the number of parallel jobs.","title":"Building"},{"location":"Developer-guide/Building-GlusterFS/#installing","text":"Run 'make install' to install GlusterFS. By default, GlusterFS will be installed into '/usr/local' prefix. To change the install prefix, give the appropriate option to configure. If installing into the default prefix, you might need to use 'sudo' or 'su -c' to install. # sudo make install NOTE: glusterfs can be installed on any target path. However, the mount.glusterfs script has to be in /sbin/mount.glusterfs for mounting via command mount -t glusterfs to work. See -t section in man 8 mount for more details.","title":"Installing"},{"location":"Developer-guide/Building-GlusterFS/#running-glusterfs","text":"GlusterFS can be only run as root, so the following commands will need to be run as root. If you've installed into the default '/usr/local' prefix, add '/usr/local/sbin' and '/usr/local/bin' to your PATH before running the below commands. A source install will generally not install any init scripts. So you will need to start glusterd manually. To manually start glusterd just run, # systemctl daemon-reload # systemctl start glusterd This will start glusterd and fork it into the background as a daemon process. You now run 'gluster' commands and make use of GlusterFS.","title":"Running GlusterFS"},{"location":"Developer-guide/Building-GlusterFS/#building-packages","text":"","title":"Building packages"},{"location":"Developer-guide/Building-GlusterFS/#building-rpms","text":"Building RPMs is really simple. On a RPM based system, for eg. Fedora, get the source and do the configuration steps as shown in the 'Building from Source' section. After the configuration step, run the following steps to build RPMs, # cd extras/LinuxRPM # make glusterrpms This will create rpms from the source in 'extras/LinuxRPM'. (Note: You will need to install the rpmbuild requirements including rpmbuild and mock) For CentOS / Enterprise Linux 8 the dependencies can be installed via: # yum install mock rpm-build selinux-policy-devel","title":"Building RPMs"},{"location":"Developer-guide/Developers-Index/","text":"Developers Contributing to the Gluster community Are you itching to send in patches and participate as a developer in the Gluster community? Here are a number of starting points for getting involved. All you need is your 'github' account to be handy. Remember that, Gluster community has multiple projects, each of which has its own way of handling PRs and patches. Decide on which project you want to contribute. Below documents are mostly about 'GlusterFS' project, which is the core of Gluster Community. Workflow Simplified Developer Workflow A simpler and faster intro to developing with GlusterFS, than the document below Developer Workflow Covers detail about requirements from a patch; tools and toolkits used by developers. This is recommended reading in order to begin contributions to the project. GD2 Developer Workflow Helps in on-boarding developers to contribute in GlusterD2 project. Compiling Gluster Building GlusterFS - How to compile Gluster from source code. Developing Projects - Ideas for projects you could create Fixing issues reported by tools for static code analysis This is a good starting point for developers to fix bugs in GlusterFS project. Releases and Backports Backport Guidelines describe the steps that branches too. Some more GlusterFS Developer documentation can be found in glusterfs documentation directory","title":"Developers Home"},{"location":"Developer-guide/Developers-Index/#developers","text":"","title":"Developers"},{"location":"Developer-guide/Developers-Index/#contributing-to-the-gluster-community","text":"Are you itching to send in patches and participate as a developer in the Gluster community? Here are a number of starting points for getting involved. All you need is your 'github' account to be handy. Remember that, Gluster community has multiple projects, each of which has its own way of handling PRs and patches. Decide on which project you want to contribute. Below documents are mostly about 'GlusterFS' project, which is the core of Gluster Community.","title":"Contributing to the Gluster community"},{"location":"Developer-guide/Developers-Index/#workflow","text":"Simplified Developer Workflow A simpler and faster intro to developing with GlusterFS, than the document below Developer Workflow Covers detail about requirements from a patch; tools and toolkits used by developers. This is recommended reading in order to begin contributions to the project. GD2 Developer Workflow Helps in on-boarding developers to contribute in GlusterD2 project.","title":"Workflow"},{"location":"Developer-guide/Developers-Index/#compiling-gluster","text":"Building GlusterFS - How to compile Gluster from source code.","title":"Compiling Gluster"},{"location":"Developer-guide/Developers-Index/#developing","text":"Projects - Ideas for projects you could create Fixing issues reported by tools for static code analysis This is a good starting point for developers to fix bugs in GlusterFS project.","title":"Developing"},{"location":"Developer-guide/Developers-Index/#releases-and-backports","text":"Backport Guidelines describe the steps that branches too. Some more GlusterFS Developer documentation can be found in glusterfs documentation directory","title":"Releases and Backports"},{"location":"Developer-guide/Development-Workflow/","text":"Development workflow of Gluster This document provides a detailed overview of the development model followed by the GlusterFS project. For a simpler overview visit Simplified development workflow . Basics The GlusterFS development model largely revolves around the features and functionality provided by Git version control system, Github and Jenkins continuous integration system. It is a primer for a contributor to the project. Git and Github Git is an extremely flexible, distributed version control system. GlusterFS's main repository is at Git and at GitHub . A good introduction to Git can be found at http://www-cs-students.stanford.edu/~blynn/gitmagic/ . Jenkins Jenkins is a Continuous Integration build system. Jenkins is hosted at http://build.gluster.org . Jenkins is configured to work with Github by setting up hooks. Every \"Change\" which is pushed to Github is automatically picked up by Jenkins, built and smoke tested. The output of all builds and tests can be viewed at http://build.gluster.org/job/smoke/ . Jenkins is also set up with a 'regression' job which is designed to execute test scripts provided as part of the code change. Preparatory Setup Here is a list of initial one-time steps before you can start hacking on code. Fork Repository Fork GlusterFS repository Clone a working tree Get yourself a working tree by cloning the development repository from # git clone git@github.com:${username}/glusterfs.git # cd glusterfs/ # git remote add upstream git@github.com:gluster/glusterfs.git Preferred email and set username On the first login, add your git/work email to your identity. You will have to click on the URL which is sent to your email and set up a proper Full Name. Select yourself a username. Make sure you set your git/work email as your preferred email. This should be the email address from which all your code commits are associated. Watch glusterfs In Github, watch the 'glusterfs' repository. Tick on suitable (All activity, Ignore, participating, or custom) type of notifications to get alerts. Email filters Set up a filter rule in your mail client to tag or classify emails with the header list: <glusterfs.gluster.github.com> as mails originating from the github system. Development & Other flows Issue Make sure there is an issue filed for the task you are working on. If it is not filed, open the issue with all the description. If it is a bug fix, add label \"Type:Bug\". If it is an RFC, provide all the documentation, and request for \"DocApproved\", and \"SpecApproved\" label. Code Start coding Make sure clang-format is installed and is run on the patch. Keep up-to-date GlusterFS is a large project with many developers, so there would be one or the other patch everyday. It is critical for developer to be up-to-date with devel repo to be Conflict-Free when PR is opened. Git provides many options to keep up-to-date, below is one of them # git fetch upstream # git rebase upstream/devel Branching policy This section describes both, the branching policies on the public repo as well as the suggested best-practice for local branching Devel/release branches In glusterfs, the 'devel' branch is the forward development branch. This is where new features come in first. In fact this is where almost every change (commit) comes in first. The devel branch is always kept in a buildable state and smoke tests pass. Release trains (3.1.z, 3.2.z,..., 8.y, 9.y) each have a branch originating from devel. Code freeze of each new release train is marked by the creation of the release-x.y branch. At this point, no new features are added to the release-x.y branch. All fixes and commits first get into devel. From there, only bug fixes get backported to the relevant release branches. From the release-x.y branch, actual release code snapshots (e.g. glusterfs-3.2.2 etc.) are tagged (git annotated tag with 'git tag -a') shipped as a tarball. Personal per-task branches As a best practice, it is recommended you perform all code changes for a task in a local branch in your working tree. The local branch should be created from the upstream branch to which you intend to submit the change. The name of the branch on your personal fork can start with issueNNNN, followed by anything of your choice. If you are submitting changes to the devel branch, first create a local task branch like this - # git checkout -b issueNNNN upstream/main ... <hack, commit> Building Environment Setup For details about the required packages for the build environment refer : Building GlusterFS Creating build environment Once the required packages are installed for your appropiate system, generate the build configuration: # ./autogen.sh # ./configure --enable-fusermount Build and install # make && make install Commit policy / PR description Typically you would have a local branch per task. You will need to sign-off your commit (git commit -s) before sending the patch for review. By signing off your patch, you agree to the terms listed under the \"Developer's Certificate of Origin\" section in the CONTRIBUTING file available in the repository root. Provide a meaningful commit message. Your commit message should be in the following format A short one-line title of format 'component: title', describing what the patch accomplishes An empty line following the subject Situation necessitating the patch Description of the code changes Reason for doing it this way (compared to others) Description of test cases When you open a PR, having a reference Issue for the commit is mandatory in GlusterFS. Commit message can have, either Fixes: #NNNN or Updates: #NNNN in a separate line in the commit message. Here, NNNN is the Issue ID in glusterfs repository. Each commit needs the author to have the 'Signed-off-by: Name ' line. Can do this by -s option for git commit. If the PR is not ready for review, apply the label work-in-progress. Check the availability of \"Draft PR\" is present for you, if yes, use that instead. Push the change After doing the local commit, it is time to submit the code for review. There is a script available inside glusterfs.git called rfc.sh. It is recommended you keep pushing to your repo every day, so you don't loose any work. You can submit your changes for review by simply executing # ./rfc.sh or # git push origin HEAD:issueNNN This script rfc.sh does the following: The first time it is executed, it downloads a git hook from http://review.gluster.org/tools/hooks/commit-msg and sets it up locally to generate a Change-Id: tag in your commit message (if it was not already generated.) Rebase your commit against the latest upstream HEAD. This rebase also causes your commits to undergo massaging from the just downloaded commit-msg hook. Prompt for a Reference Id for each commit (if it was not already provided) and include it as a \"fixes: #n\" tag in the commit log. You can just hit at this prompt if your submission is purely for review purposes. Push the changes for review. On a successful push, you will see a URL pointing to the change in Pull requests section. Test cases and Verification Auto-triggered tests The integration between Jenkins and Github triggers an event in Jenkins on every push of changes, to pick up the change and run build and smoke test on it. Part of the workflow is to aggregate and execute pre-commit test cases that accompany patches, cumulatively for every new patch. This guarantees that tests that are working till the present are not broken with the new patch. This is so that code changes and accompanying test cases are reviewed together. Once you upload the patch - All the required smoke tests would be auto-triggered. You can retrigger the smoke tests using \"/recheck smoke\" as comment. Passing the automated smoke test is a necessary condition but not sufficient. The regression tests would be triggered by a comment \"/run regression\" from developers in the @gluster organization once smoke test is passed. If smoke/regression fails, it is a good reason to skip code review till a fixed change is pushed later. You can click on the build URL automatically to inspect the reason for auto verification failure. In the Jenkins job page, you can click on the 'Console Output' link to see the exact point of failure. All code changes which are not trivial (typo fixes, code comment changes) must be accompanied with either a new test case script or extend/modify an existing test case script. It is important to review the test case in conjunction with the code change to analyze whether the code change is actually verified by the test case. Regression tests (i.e, execution of all test cases accumulated with every commit) is not automatically triggered as the test cases can be extensive and is quite expensive to execute for every change submission in the review/resubmit cycle. Passing the regression test is a necessary condition for merge along with code review points. To check and run all regression tests locally, run the below script from glusterfs root directory. # ./run-tests.sh To run a single regression test locally, run the below command. # prove -vf <path_to_the_file> NOTE: The testing framework needs perl-Test-Harness package to be installed. Ask for help as comment in PR if you have any questions about the process! It is important to note that Jenkins verification is only a generic verification of high-level tests. More concentrated testing effort for the patch is necessary with manual verification. Glusto test framework For any new feature that is posted for review, there should be accompanying set of tests in glusto-tests . These tests will be run nightly and/or before release to determine the health of the feature. Please go through glusto-tests project to understand more information on how to write and execute the tests in glusto. Extend/Modify old test cases in existing scripts - This is typically when present behavior (default values etc.) of code is changed. No test cases - This is typically when a code change is trivial (e.g. fixing typos in output strings, code comments). Only test case and no code change - This is typically when we are adding test cases to old code (already existing before this regression test policy was enforced). More details on how to work with test case scripts can be found in tests/README. Reviewing / Commenting Code review with Github is relatively easy compared to other available tools. Each change is presented as multiple files and each file can be reviewed in Side-by-Side mode. While reviewing it is possible to comment on each line by clicking on '+' icon and writing in your comments in the text box. Such in-line comments are saved as drafts, till you finally publish them by Starting a Review. Incorporate, rfc.sh, Reverify Code review comments are notified via email. After incorporating the changes in code, you can mark each of the inline comments as 'done' (optional). After all the changes to your local files, create new commits in the same branch with - # git commit -a -s Push the commit by executing rfc.sh. If your previous push was an \"rfc\" push (i.e, without a Issue Id) you will be prompted for a Issue Id again. You can re-push an rfc change without any other code change too by giving a Issue Id. On the new push, Jenkins will re-verify the new change (independent of what the verification result was for the previous push). It is the Change-Id line in the commit log (which does not change) that associates the new push as an update for the old push (even though they had different commit ids) under the same Change. If further changes are found necessary, changes can be requested or comments can be made on the new patch as well, and the same cycle repeats. If no further changes are necessary, the reviewer can approve the patch. Submission Qualifiers GlusterFS project follows 'Squash and Merge' method. This is mainly to preserve the historic Gerrit method of one patch in git log for one URL link. This also makes every merge a complete patch, which has passed all tests. For a change to get merged, there are two qualifiers that are enforced by the Github system. They are - A change should have at approver flag from Reviewers A change should have passed smoke and regression tests. The project maintainer will merge the changes once a patch meets these qualifiers. If you feel there is delay, feel free to add a comment, discuss the same in Slack channel, or send email. Submission Disqualifiers +2 : is equivalent to \"Approve\" from the people in the maintainer's group. +1 : can be given by a maintainer/reviewer by explicitly stating that in the comment. -1 : provide details on required changes and pick \"Request Changes\" while submitting your review. -2 : done by adding the DO-NOT-MERGE label. Any further discussions can happen as comments in the PR.","title":"Development-Workflow"},{"location":"Developer-guide/Development-Workflow/#development-workflow-of-gluster","text":"This document provides a detailed overview of the development model followed by the GlusterFS project. For a simpler overview visit Simplified development workflow .","title":"Development workflow of Gluster"},{"location":"Developer-guide/Development-Workflow/#basics","text":"The GlusterFS development model largely revolves around the features and functionality provided by Git version control system, Github and Jenkins continuous integration system. It is a primer for a contributor to the project.","title":"Basics"},{"location":"Developer-guide/Development-Workflow/#git-and-github","text":"Git is an extremely flexible, distributed version control system. GlusterFS's main repository is at Git and at GitHub . A good introduction to Git can be found at http://www-cs-students.stanford.edu/~blynn/gitmagic/ .","title":"Git and Github"},{"location":"Developer-guide/Development-Workflow/#jenkins","text":"Jenkins is a Continuous Integration build system. Jenkins is hosted at http://build.gluster.org . Jenkins is configured to work with Github by setting up hooks. Every \"Change\" which is pushed to Github is automatically picked up by Jenkins, built and smoke tested. The output of all builds and tests can be viewed at http://build.gluster.org/job/smoke/ . Jenkins is also set up with a 'regression' job which is designed to execute test scripts provided as part of the code change.","title":"Jenkins"},{"location":"Developer-guide/Development-Workflow/#preparatory-setup","text":"Here is a list of initial one-time steps before you can start hacking on code.","title":"Preparatory Setup"},{"location":"Developer-guide/Development-Workflow/#fork-repository","text":"Fork GlusterFS repository","title":"Fork Repository"},{"location":"Developer-guide/Development-Workflow/#clone-a-working-tree","text":"Get yourself a working tree by cloning the development repository from # git clone git@github.com:${username}/glusterfs.git # cd glusterfs/ # git remote add upstream git@github.com:gluster/glusterfs.git","title":"Clone a working tree"},{"location":"Developer-guide/Development-Workflow/#preferred-email-and-set-username","text":"On the first login, add your git/work email to your identity. You will have to click on the URL which is sent to your email and set up a proper Full Name. Select yourself a username. Make sure you set your git/work email as your preferred email. This should be the email address from which all your code commits are associated.","title":"Preferred email and set username"},{"location":"Developer-guide/Development-Workflow/#watch-glusterfs","text":"In Github, watch the 'glusterfs' repository. Tick on suitable (All activity, Ignore, participating, or custom) type of notifications to get alerts.","title":"Watch glusterfs"},{"location":"Developer-guide/Development-Workflow/#email-filters","text":"Set up a filter rule in your mail client to tag or classify emails with the header list: <glusterfs.gluster.github.com> as mails originating from the github system.","title":"Email filters"},{"location":"Developer-guide/Development-Workflow/#development-other-flows","text":"","title":"Development &amp; Other flows"},{"location":"Developer-guide/Development-Workflow/#issue","text":"Make sure there is an issue filed for the task you are working on. If it is not filed, open the issue with all the description. If it is a bug fix, add label \"Type:Bug\". If it is an RFC, provide all the documentation, and request for \"DocApproved\", and \"SpecApproved\" label.","title":"Issue"},{"location":"Developer-guide/Development-Workflow/#code","text":"Start coding Make sure clang-format is installed and is run on the patch.","title":"Code"},{"location":"Developer-guide/Development-Workflow/#keep-up-to-date","text":"GlusterFS is a large project with many developers, so there would be one or the other patch everyday. It is critical for developer to be up-to-date with devel repo to be Conflict-Free when PR is opened. Git provides many options to keep up-to-date, below is one of them # git fetch upstream # git rebase upstream/devel","title":"Keep up-to-date"},{"location":"Developer-guide/Development-Workflow/#branching-policy","text":"This section describes both, the branching policies on the public repo as well as the suggested best-practice for local branching","title":"Branching policy"},{"location":"Developer-guide/Development-Workflow/#develrelease-branches","text":"In glusterfs, the 'devel' branch is the forward development branch. This is where new features come in first. In fact this is where almost every change (commit) comes in first. The devel branch is always kept in a buildable state and smoke tests pass. Release trains (3.1.z, 3.2.z,..., 8.y, 9.y) each have a branch originating from devel. Code freeze of each new release train is marked by the creation of the release-x.y branch. At this point, no new features are added to the release-x.y branch. All fixes and commits first get into devel. From there, only bug fixes get backported to the relevant release branches. From the release-x.y branch, actual release code snapshots (e.g. glusterfs-3.2.2 etc.) are tagged (git annotated tag with 'git tag -a') shipped as a tarball.","title":"Devel/release branches"},{"location":"Developer-guide/Development-Workflow/#personal-per-task-branches","text":"As a best practice, it is recommended you perform all code changes for a task in a local branch in your working tree. The local branch should be created from the upstream branch to which you intend to submit the change. The name of the branch on your personal fork can start with issueNNNN, followed by anything of your choice. If you are submitting changes to the devel branch, first create a local task branch like this - # git checkout -b issueNNNN upstream/main ... <hack, commit>","title":"Personal per-task branches"},{"location":"Developer-guide/Development-Workflow/#building","text":"","title":"Building"},{"location":"Developer-guide/Development-Workflow/#environment-setup","text":"For details about the required packages for the build environment refer : Building GlusterFS","title":"Environment Setup"},{"location":"Developer-guide/Development-Workflow/#creating-build-environment","text":"Once the required packages are installed for your appropiate system, generate the build configuration: # ./autogen.sh # ./configure --enable-fusermount","title":"Creating build environment"},{"location":"Developer-guide/Development-Workflow/#build-and-install","text":"# make && make install","title":"Build and install"},{"location":"Developer-guide/Development-Workflow/#commit-policy-pr-description","text":"Typically you would have a local branch per task. You will need to sign-off your commit (git commit -s) before sending the patch for review. By signing off your patch, you agree to the terms listed under the \"Developer's Certificate of Origin\" section in the CONTRIBUTING file available in the repository root. Provide a meaningful commit message. Your commit message should be in the following format A short one-line title of format 'component: title', describing what the patch accomplishes An empty line following the subject Situation necessitating the patch Description of the code changes Reason for doing it this way (compared to others) Description of test cases When you open a PR, having a reference Issue for the commit is mandatory in GlusterFS. Commit message can have, either Fixes: #NNNN or Updates: #NNNN in a separate line in the commit message. Here, NNNN is the Issue ID in glusterfs repository. Each commit needs the author to have the 'Signed-off-by: Name ' line. Can do this by -s option for git commit. If the PR is not ready for review, apply the label work-in-progress. Check the availability of \"Draft PR\" is present for you, if yes, use that instead.","title":"Commit policy / PR description"},{"location":"Developer-guide/Development-Workflow/#push-the-change","text":"After doing the local commit, it is time to submit the code for review. There is a script available inside glusterfs.git called rfc.sh. It is recommended you keep pushing to your repo every day, so you don't loose any work. You can submit your changes for review by simply executing # ./rfc.sh or # git push origin HEAD:issueNNN This script rfc.sh does the following: The first time it is executed, it downloads a git hook from http://review.gluster.org/tools/hooks/commit-msg and sets it up locally to generate a Change-Id: tag in your commit message (if it was not already generated.) Rebase your commit against the latest upstream HEAD. This rebase also causes your commits to undergo massaging from the just downloaded commit-msg hook. Prompt for a Reference Id for each commit (if it was not already provided) and include it as a \"fixes: #n\" tag in the commit log. You can just hit at this prompt if your submission is purely for review purposes. Push the changes for review. On a successful push, you will see a URL pointing to the change in Pull requests section.","title":"Push the change"},{"location":"Developer-guide/Development-Workflow/#test-cases-and-verification","text":"","title":"Test cases and Verification"},{"location":"Developer-guide/Development-Workflow/#auto-triggered-tests","text":"The integration between Jenkins and Github triggers an event in Jenkins on every push of changes, to pick up the change and run build and smoke test on it. Part of the workflow is to aggregate and execute pre-commit test cases that accompany patches, cumulatively for every new patch. This guarantees that tests that are working till the present are not broken with the new patch. This is so that code changes and accompanying test cases are reviewed together. Once you upload the patch - All the required smoke tests would be auto-triggered. You can retrigger the smoke tests using \"/recheck smoke\" as comment. Passing the automated smoke test is a necessary condition but not sufficient. The regression tests would be triggered by a comment \"/run regression\" from developers in the @gluster organization once smoke test is passed. If smoke/regression fails, it is a good reason to skip code review till a fixed change is pushed later. You can click on the build URL automatically to inspect the reason for auto verification failure. In the Jenkins job page, you can click on the 'Console Output' link to see the exact point of failure. All code changes which are not trivial (typo fixes, code comment changes) must be accompanied with either a new test case script or extend/modify an existing test case script. It is important to review the test case in conjunction with the code change to analyze whether the code change is actually verified by the test case. Regression tests (i.e, execution of all test cases accumulated with every commit) is not automatically triggered as the test cases can be extensive and is quite expensive to execute for every change submission in the review/resubmit cycle. Passing the regression test is a necessary condition for merge along with code review points. To check and run all regression tests locally, run the below script from glusterfs root directory. # ./run-tests.sh To run a single regression test locally, run the below command. # prove -vf <path_to_the_file> NOTE: The testing framework needs perl-Test-Harness package to be installed. Ask for help as comment in PR if you have any questions about the process! It is important to note that Jenkins verification is only a generic verification of high-level tests. More concentrated testing effort for the patch is necessary with manual verification.","title":"Auto-triggered tests"},{"location":"Developer-guide/Development-Workflow/#glusto-test-framework","text":"For any new feature that is posted for review, there should be accompanying set of tests in glusto-tests . These tests will be run nightly and/or before release to determine the health of the feature. Please go through glusto-tests project to understand more information on how to write and execute the tests in glusto. Extend/Modify old test cases in existing scripts - This is typically when present behavior (default values etc.) of code is changed. No test cases - This is typically when a code change is trivial (e.g. fixing typos in output strings, code comments). Only test case and no code change - This is typically when we are adding test cases to old code (already existing before this regression test policy was enforced). More details on how to work with test case scripts can be found in tests/README.","title":"Glusto test framework"},{"location":"Developer-guide/Development-Workflow/#reviewing-commenting","text":"Code review with Github is relatively easy compared to other available tools. Each change is presented as multiple files and each file can be reviewed in Side-by-Side mode. While reviewing it is possible to comment on each line by clicking on '+' icon and writing in your comments in the text box. Such in-line comments are saved as drafts, till you finally publish them by Starting a Review.","title":"Reviewing / Commenting"},{"location":"Developer-guide/Development-Workflow/#incorporate-rfcsh-reverify","text":"Code review comments are notified via email. After incorporating the changes in code, you can mark each of the inline comments as 'done' (optional). After all the changes to your local files, create new commits in the same branch with - # git commit -a -s Push the commit by executing rfc.sh. If your previous push was an \"rfc\" push (i.e, without a Issue Id) you will be prompted for a Issue Id again. You can re-push an rfc change without any other code change too by giving a Issue Id. On the new push, Jenkins will re-verify the new change (independent of what the verification result was for the previous push). It is the Change-Id line in the commit log (which does not change) that associates the new push as an update for the old push (even though they had different commit ids) under the same Change. If further changes are found necessary, changes can be requested or comments can be made on the new patch as well, and the same cycle repeats. If no further changes are necessary, the reviewer can approve the patch.","title":"Incorporate, rfc.sh, Reverify"},{"location":"Developer-guide/Development-Workflow/#submission-qualifiers","text":"GlusterFS project follows 'Squash and Merge' method. This is mainly to preserve the historic Gerrit method of one patch in git log for one URL link. This also makes every merge a complete patch, which has passed all tests. For a change to get merged, there are two qualifiers that are enforced by the Github system. They are - A change should have at approver flag from Reviewers A change should have passed smoke and regression tests. The project maintainer will merge the changes once a patch meets these qualifiers. If you feel there is delay, feel free to add a comment, discuss the same in Slack channel, or send email.","title":"Submission Qualifiers"},{"location":"Developer-guide/Development-Workflow/#submission-disqualifiers","text":"+2 : is equivalent to \"Approve\" from the people in the maintainer's group. +1 : can be given by a maintainer/reviewer by explicitly stating that in the comment. -1 : provide details on required changes and pick \"Request Changes\" while submitting your review. -2 : done by adding the DO-NOT-MERGE label. Any further discussions can happen as comments in the PR.","title":"Submission Disqualifiers"},{"location":"Developer-guide/Easy-Fix-Bugs/","text":"Easy Fix Bugs Fixing easy issues is an excellent method to start contributing patches to Gluster. Sometimes an Easy Fix issue has a patch attached. In those cases, the Patch keyword has been added to the bug. These bugs can be used by new contributors that would like to verify their workflow. Bug 1099645 is one example of those. All such issues can be found here Guidelines for new comers While trying to write a patch, do not hesitate to ask questions. If something in the documentation is unclear, we do need to know so that we can improve it. There are no stupid questions, and it's more stupid to not ask questions that others can easily answer. Always assume that if you have a question, someone else would like to hear the answer too. Reach out to the developers in #gluster on Gluster Slack channel, or on one of the mailing lists, try to keep the discussions public so that anyone can learn from it.","title":"EasyFix bugs"},{"location":"Developer-guide/Easy-Fix-Bugs/#easy-fix-bugs","text":"Fixing easy issues is an excellent method to start contributing patches to Gluster. Sometimes an Easy Fix issue has a patch attached. In those cases, the Patch keyword has been added to the bug. These bugs can be used by new contributors that would like to verify their workflow. Bug 1099645 is one example of those. All such issues can be found here","title":"Easy Fix Bugs"},{"location":"Developer-guide/Easy-Fix-Bugs/#guidelines-for-new-comers","text":"While trying to write a patch, do not hesitate to ask questions. If something in the documentation is unclear, we do need to know so that we can improve it. There are no stupid questions, and it's more stupid to not ask questions that others can easily answer. Always assume that if you have a question, someone else would like to hear the answer too. Reach out to the developers in #gluster on Gluster Slack channel, or on one of the mailing lists, try to keep the discussions public so that anyone can learn from it.","title":"Guidelines for new comers"},{"location":"Developer-guide/Fixing-issues-reported-by-tools-for-static-code-analysis/","text":"Static Code Analysis Tools Bug fixes for issues reported by Static Code Analysis Tools should follow Development Work Flow Coverity GlusterFS is part of Coverity's scan program. To see Coverity issues you have to be a member of the GlusterFS project in Coverity scan website. Here is the link to Coverity scan website Go to above link and subscribe to GlusterFS project (as contributor). It will send a request to Admin for including you in the Project. Once admins for the GlusterFS Coverity scan approve your request, you will be able to see the defects raised by Coverity. Issue #1060 can be used as a umbrella bug for Coverity issues in master branch unless you are trying to fix a specific issue. When you decide to work on some issue, please assign it to your name in the same Coverity website. So that we don't step on each others work. When marking a bug intentional in Coverity scan website, please put an explanation for the same. So that it will help others to understand the reasoning behind it. If you have more questions please send it to gluster-devel mailing list CPP Check Cppcheck is available in Fedora and EL's EPEL repo Install Cppcheck # dnf install cppcheck Clone GlusterFS code # git clone https://github.com/gluster/glusterfs Run Cpp check # cppcheck glusterfs/ 2>cppcheck.log Clang-Scan Daily Runs We have daily runs of static source code analysis tool clang-scan on the glusterfs sources. There are daily analyses of the master and on currently supported branches. Results are posted at https://build.gluster.org/job/clang-scan/lastBuild/clangScanBuildBugs/ Issue #1000 can be used as a umbrella bug for Clang issues in master branch unless you are trying to fix a specific issue.","title":"Fixing issues reported by tools for static code analysis"},{"location":"Developer-guide/Fixing-issues-reported-by-tools-for-static-code-analysis/#static-code-analysis-tools","text":"Bug fixes for issues reported by Static Code Analysis Tools should follow Development Work Flow","title":"Static Code Analysis Tools"},{"location":"Developer-guide/Fixing-issues-reported-by-tools-for-static-code-analysis/#coverity","text":"GlusterFS is part of Coverity's scan program. To see Coverity issues you have to be a member of the GlusterFS project in Coverity scan website. Here is the link to Coverity scan website Go to above link and subscribe to GlusterFS project (as contributor). It will send a request to Admin for including you in the Project. Once admins for the GlusterFS Coverity scan approve your request, you will be able to see the defects raised by Coverity. Issue #1060 can be used as a umbrella bug for Coverity issues in master branch unless you are trying to fix a specific issue. When you decide to work on some issue, please assign it to your name in the same Coverity website. So that we don't step on each others work. When marking a bug intentional in Coverity scan website, please put an explanation for the same. So that it will help others to understand the reasoning behind it. If you have more questions please send it to gluster-devel mailing list","title":"Coverity"},{"location":"Developer-guide/Fixing-issues-reported-by-tools-for-static-code-analysis/#cpp-check","text":"Cppcheck is available in Fedora and EL's EPEL repo Install Cppcheck # dnf install cppcheck Clone GlusterFS code # git clone https://github.com/gluster/glusterfs Run Cpp check # cppcheck glusterfs/ 2>cppcheck.log","title":"CPP Check"},{"location":"Developer-guide/Fixing-issues-reported-by-tools-for-static-code-analysis/#clang-scan-daily-runs","text":"We have daily runs of static source code analysis tool clang-scan on the glusterfs sources. There are daily analyses of the master and on currently supported branches. Results are posted at https://build.gluster.org/job/clang-scan/lastBuild/clangScanBuildBugs/ Issue #1000 can be used as a umbrella bug for Clang issues in master branch unless you are trying to fix a specific issue.","title":"Clang-Scan Daily Runs"},{"location":"Developer-guide/Projects/","text":"Projects This page contains a list of project ideas which will be suitable for students (for GSOC, internship etc.) Projects/Features which needs contributors RIO Issue: https://github.com/gluster/glusterfs/issues/243 This is a new distribution logic, which can scale Gluster to 1000s of nodes. Composition xlator for small files Merge small files into a designated large file using our own custom semantics. This can improve our small file performance. Path based geo-replication Issue: https://github.com/gluster/glusterfs/issues/460 This would allow remote volume to be of different type (NFS/S3 etc etc) too. Project Quota support Issue: https://github.com/gluster/glusterfs/issues/184 This will make Gluster's Quota faster, and also provide desired behavior. Cluster testing framework based on gluster-tester Repo: https://github.com/aravindavk/gluster-tester Build a cluster using docker images (or VMs). Write a tool which would extend current gluster testing's .t format to take NODE as an addition parameter to run command. This would make upgrade and downgrade testing very easy and feasible. Network layer changes Issue: https://github.com/gluster/glusterfs/issues/391 There is many improvements we can do in this area","title":"Project Ideas"},{"location":"Developer-guide/Projects/#projects","text":"This page contains a list of project ideas which will be suitable for students (for GSOC, internship etc.)","title":"Projects"},{"location":"Developer-guide/Projects/#projectsfeatures-which-needs-contributors","text":"","title":"Projects/Features which needs contributors"},{"location":"Developer-guide/Projects/#rio","text":"Issue: https://github.com/gluster/glusterfs/issues/243 This is a new distribution logic, which can scale Gluster to 1000s of nodes.","title":"RIO"},{"location":"Developer-guide/Projects/#composition-xlator-for-small-files","text":"Merge small files into a designated large file using our own custom semantics. This can improve our small file performance.","title":"Composition xlator for small files"},{"location":"Developer-guide/Projects/#path-based-geo-replication","text":"Issue: https://github.com/gluster/glusterfs/issues/460 This would allow remote volume to be of different type (NFS/S3 etc etc) too.","title":"Path based geo-replication"},{"location":"Developer-guide/Projects/#project-quota-support","text":"Issue: https://github.com/gluster/glusterfs/issues/184 This will make Gluster's Quota faster, and also provide desired behavior.","title":"Project Quota support"},{"location":"Developer-guide/Projects/#cluster-testing-framework-based-on-gluster-tester","text":"Repo: https://github.com/aravindavk/gluster-tester Build a cluster using docker images (or VMs). Write a tool which would extend current gluster testing's .t format to take NODE as an addition parameter to run command. This would make upgrade and downgrade testing very easy and feasible.","title":"Cluster testing framework based on gluster-tester"},{"location":"Developer-guide/Projects/#network-layer-changes","text":"Issue: https://github.com/gluster/glusterfs/issues/391 There is many improvements we can do in this area","title":"Network layer changes"},{"location":"Developer-guide/Simplified-Development-Workflow/","text":"Simplified development workflow for GlusterFS This page gives a simplified model of the development workflow used by the GlusterFS project. This will give the steps required to get a patch accepted into the GlusterFS source. Visit Development Work Flow a more detailed description of the workflow. Initial preparation The GlusterFS development workflow revolves around GitHub and Jenkins . Using these both tools requires some initial preparation. Get the source Git clone the GlusterFS source using git clone git@github.com:${username}/glusterfs.git cd glusterfs/ git remote add upstream git@github.com:gluster/glusterfs.git This will clone the GlusterFS source into a subdirectory named glusterfs with the devel branch checked out. Dev system setup You should install and setup Git on your development system. Use your distribution specific package manger to install git. After installation configure git. At the minimum, set a git user email. To set the email do, git config --global user.name <name> git config --global user.email <email address> Next, install the build requirements for GlusterFS. Refer Building GlusterFS - Build Requirements for the actual requirements. Actual development The commands in this section are to be run inside the glusterfs source directory. Create a development branch It is recommended to use separate local development branches for each change you want to contribute to GlusterFS. To create a development branch, first checkout the upstream branch you want to work on and update it. More details on the upstream branching model for GlusterFS can be found at Development Work Flow - Branching_policy . For example if you want to develop on the devel branch, # git checkout devel # git pull Now, create a new branch from devel and switch to the new branch. It is recommended to have descriptive branch names. Do, git branch issueNNNN git checkout issueNNNN or, git checkout -b issueNNNN upstream/main to do both in one command. Here, NNNN is the Issue ID in glusterfs repository. Hack Once you've switched to the development branch, you can perform the actual code changes. Build and test to see if your changes work. Tests Unless your changes are very minor and trivial, you should also add a test for your change. Tests are used to ensure that the changes you did are not broken inadvertently. More details on tests can be found at Development Workflow - Test cases and Development Workflow - Regression tests and test cases. Regression test Once your change is working, locally you can run the regression test suite to make sure you haven't broken anything. The regression test suite requires a working GlusterFS installation and needs to be run as root. To run the regression test suite, do # make install # ./run-tests.sh or, After uploading the patch The regression tests would be triggered by a comment \"/run regression\" from developers in the @gluster organization. Commit your changes If you haven't broken anything, you can now commit your changes. First identify the files that you modified/added/deleted using git-status and stage these files. git status git add <list of modified files> Now, commit these changes using # git commit -s Provide a meaningful commit message. The commit message policy is described at Development Work Flow - Commit policy . It is essential that you commit with the '-s' option, which will sign-off the commit with your configured email. Submit for review To submit your change for review, run the rfc.sh script, # ./rfc.sh or git push origin HEAD:issueNNN More details on the rfc.sh script are available at Development Work Flow - rfc.sh . Review process Your change will now be reviewed by the GlusterFS maintainers and component owners. You can follow and take part in the review process on the change at the review url. The review process involves several steps. To know component owners , you can check the \"MAINTAINERS\" file in root of glusterfs code directory Automated verification Every change submitted to github triggers an initial automated verification on jenkins known as smoke tests. The automated verification ensures that your change doesn't break the build and has an associated bug-id. Developers get a chance to retrigger the smoke tests using \"/recheck smoke\" as comment. More details can be found at Development Work Flow - Auto verification . Formal review Once the auto verification is successful, the component owners will perform a formal review. If they are okay with your change, they will give a positive review. If not they will give a negative review and add comments on the reasons. More information regarding the review qualifiers and disqualifiers is available at Development Work Flow - Submission Qualifiers and Development Work Flow - Submission Disqualifiers . If your change gets a negative review, you will need to address the comments and resubmit your change. Resubmission Switch to your development branch and make new changes to address the review comments. Build and test to see if the new changes are working. Stage your changes and commit your new changes in new commits using, # git commit -a -s Now you can resubmit the commit for review using the rfc.sh script or git push. The formal review process could take a long time. To increase chances for a speedy review, you can add the component owners as reviewers on the pull request. This will ensure they notice the change. The list of component owners can be found in the MAINTAINERS file present in the GlusterFS source Verification After a component owner has given a positive review, a developer will run the regression test suite on your change to verify that your change works and hasn't broken anything. This verification is done with the help of jenkins. If the verification fails, you will need to make necessary changes and resubmit an updated commit for review. Acceptance After successful verification, a maintainer will Squash and merge your change into the upstream GlusterFS source. Your change will now be available in the upstream git repo for everyone to use.","title":"Simplified Development Workflow"},{"location":"Developer-guide/Simplified-Development-Workflow/#simplified-development-workflow-for-glusterfs","text":"This page gives a simplified model of the development workflow used by the GlusterFS project. This will give the steps required to get a patch accepted into the GlusterFS source. Visit Development Work Flow a more detailed description of the workflow.","title":"Simplified development workflow for GlusterFS"},{"location":"Developer-guide/Simplified-Development-Workflow/#initial-preparation","text":"The GlusterFS development workflow revolves around GitHub and Jenkins . Using these both tools requires some initial preparation.","title":"Initial preparation"},{"location":"Developer-guide/Simplified-Development-Workflow/#get-the-source","text":"Git clone the GlusterFS source using git clone git@github.com:${username}/glusterfs.git cd glusterfs/ git remote add upstream git@github.com:gluster/glusterfs.git This will clone the GlusterFS source into a subdirectory named glusterfs with the devel branch checked out.","title":"Get the source"},{"location":"Developer-guide/Simplified-Development-Workflow/#dev-system-setup","text":"You should install and setup Git on your development system. Use your distribution specific package manger to install git. After installation configure git. At the minimum, set a git user email. To set the email do, git config --global user.name <name> git config --global user.email <email address> Next, install the build requirements for GlusterFS. Refer Building GlusterFS - Build Requirements for the actual requirements.","title":"Dev system setup"},{"location":"Developer-guide/Simplified-Development-Workflow/#actual-development","text":"The commands in this section are to be run inside the glusterfs source directory.","title":"Actual development"},{"location":"Developer-guide/Simplified-Development-Workflow/#create-a-development-branch","text":"It is recommended to use separate local development branches for each change you want to contribute to GlusterFS. To create a development branch, first checkout the upstream branch you want to work on and update it. More details on the upstream branching model for GlusterFS can be found at Development Work Flow - Branching_policy . For example if you want to develop on the devel branch, # git checkout devel # git pull Now, create a new branch from devel and switch to the new branch. It is recommended to have descriptive branch names. Do, git branch issueNNNN git checkout issueNNNN or, git checkout -b issueNNNN upstream/main to do both in one command. Here, NNNN is the Issue ID in glusterfs repository.","title":"Create a development branch"},{"location":"Developer-guide/Simplified-Development-Workflow/#hack","text":"Once you've switched to the development branch, you can perform the actual code changes. Build and test to see if your changes work.","title":"Hack"},{"location":"Developer-guide/Simplified-Development-Workflow/#tests","text":"Unless your changes are very minor and trivial, you should also add a test for your change. Tests are used to ensure that the changes you did are not broken inadvertently. More details on tests can be found at Development Workflow - Test cases and Development Workflow - Regression tests and test cases.","title":"Tests"},{"location":"Developer-guide/Simplified-Development-Workflow/#regression-test","text":"Once your change is working, locally you can run the regression test suite to make sure you haven't broken anything. The regression test suite requires a working GlusterFS installation and needs to be run as root. To run the regression test suite, do # make install # ./run-tests.sh or, After uploading the patch The regression tests would be triggered by a comment \"/run regression\" from developers in the @gluster organization.","title":"Regression test"},{"location":"Developer-guide/Simplified-Development-Workflow/#commit-your-changes","text":"If you haven't broken anything, you can now commit your changes. First identify the files that you modified/added/deleted using git-status and stage these files. git status git add <list of modified files> Now, commit these changes using # git commit -s Provide a meaningful commit message. The commit message policy is described at Development Work Flow - Commit policy . It is essential that you commit with the '-s' option, which will sign-off the commit with your configured email.","title":"Commit your changes"},{"location":"Developer-guide/Simplified-Development-Workflow/#submit-for-review","text":"To submit your change for review, run the rfc.sh script, # ./rfc.sh or git push origin HEAD:issueNNN More details on the rfc.sh script are available at Development Work Flow - rfc.sh .","title":"Submit for review"},{"location":"Developer-guide/Simplified-Development-Workflow/#review-process","text":"Your change will now be reviewed by the GlusterFS maintainers and component owners. You can follow and take part in the review process on the change at the review url. The review process involves several steps. To know component owners , you can check the \"MAINTAINERS\" file in root of glusterfs code directory","title":"Review process"},{"location":"Developer-guide/Simplified-Development-Workflow/#automated-verification","text":"Every change submitted to github triggers an initial automated verification on jenkins known as smoke tests. The automated verification ensures that your change doesn't break the build and has an associated bug-id. Developers get a chance to retrigger the smoke tests using \"/recheck smoke\" as comment. More details can be found at Development Work Flow - Auto verification .","title":"Automated verification"},{"location":"Developer-guide/Simplified-Development-Workflow/#formal-review","text":"Once the auto verification is successful, the component owners will perform a formal review. If they are okay with your change, they will give a positive review. If not they will give a negative review and add comments on the reasons. More information regarding the review qualifiers and disqualifiers is available at Development Work Flow - Submission Qualifiers and Development Work Flow - Submission Disqualifiers . If your change gets a negative review, you will need to address the comments and resubmit your change.","title":"Formal review"},{"location":"Developer-guide/Simplified-Development-Workflow/#resubmission","text":"Switch to your development branch and make new changes to address the review comments. Build and test to see if the new changes are working. Stage your changes and commit your new changes in new commits using, # git commit -a -s Now you can resubmit the commit for review using the rfc.sh script or git push. The formal review process could take a long time. To increase chances for a speedy review, you can add the component owners as reviewers on the pull request. This will ensure they notice the change. The list of component owners can be found in the MAINTAINERS file present in the GlusterFS source","title":"Resubmission"},{"location":"Developer-guide/Simplified-Development-Workflow/#verification","text":"After a component owner has given a positive review, a developer will run the regression test suite on your change to verify that your change works and hasn't broken anything. This verification is done with the help of jenkins. If the verification fails, you will need to make necessary changes and resubmit an updated commit for review.","title":"Verification"},{"location":"Developer-guide/Simplified-Development-Workflow/#acceptance","text":"After successful verification, a maintainer will Squash and merge your change into the upstream GlusterFS source. Your change will now be available in the upstream git repo for everyone to use.","title":"Acceptance"},{"location":"Developer-guide/compiling-rpms/","text":"How to compile GlusterFS RPMs from git source, for RHEL/CentOS, and Fedora Creating rpm's of GlusterFS from git source is fairly easy, once you know the steps. RPMs can be compiled on at least the following OS's: Red Hat Enterprise Linux 5, 6 (& 7 when available) CentOS 5, 6, 7 and 8 Fedora 16-20 Specific instructions for compiling are below. If you're using: Fedora 16-20 - Follow the Fedora steps, then do all of the Common steps. CentOS 5.x - Follow the CentOS 5.x steps, then do all of the Common steps CentOS 6.x - Follow the CentOS 6.x steps, then do all of the Common steps. CentOS 8.x - Follow the CentOS 8.x steps, then follow from step 2 in the Common steps. RHEL 6.x - Follow the RHEL 6.x steps, then do all of the Common steps. Note - these instructions have been explicitly tested on all of CentOS 5.10, RHEL 6.4, CentOS 6.4+, CentOS 8.4, and Fedora 16-20. Other releases of RHEL/CentOS and Fedora may work too but haven't been tested. Please update this page appropriately if you do so. :) Preparation steps for Fedora 16-20 (only) Install gcc, the python development headers, and python setuptools: # sudo yum -y install gcc python-devel python-setuptools If you're compiling GlusterFS version 3.4, then install python-swiftclient. Other GlusterFS versions don't need it: # sudo easy_install simplejson python-swiftclient Now follow through with the Common Steps part below. Preparation steps for CentOS 5.x (only) You'll need EPEL installed first and some CentOS-specific packages. The commands below will get that done for you. After that, follow through the \"Common steps\" section. Install EPEL first: # curl -OL `[`http://download.fedoraproject.org/pub/epel/5/x86_64/epel-release-5-4.noarch.rpm`](http://download.fedoraproject.org/pub/epel/5/x86_64/epel-release-5-4.noarch.rpm) # sudo yum -y install epel-release-5-4.noarch.rpm --nogpgcheck Install the packages required only on CentOS 5.x: # sudo yum -y install buildsys-macros gcc ncurses-devel \\ python-ctypes python-sphinx10 redhat-rpm-config Now follow through with the Common Steps part below. Preparation steps for CentOS 6.x (only) You'll need EPEL installed first and some CentOS-specific packages. The commands below will get that done for you. After that, follow through the \"Common steps\" section. Install EPEL first: # sudo yum -y install `[`http://download.fedoraproject.org/pub/epel/6/x86_64/epel-release-6-8.noarch.rpm`](http://download.fedoraproject.org/pub/epel/6/x86_64/epel-release-6-8.noarch.rpm) Install the packages required only on CentOS: # sudo yum -y install python-webob1.0 python-paste-deploy1.5 python-sphinx10 redhat-rpm-config Now follow through with the Common Steps part below. Preparation steps for CentOS 8.x (only) You'll need EPEL installed and then the powertools package enabled. Install EPEL first: # sudo rpm -ivh https://dl.fedoraproject.org/pub/epel/epel-release-latest-8.noarch.rpm Enable the PowerTools repo and install CentOS 8.x specific packages for building the rpms. # sudo yum --enablerepo=PowerTools install automake autoconf libtool flex bison openssl-devel \\ libxml2-devel libaio-devel libibverbs-devel librdmacm-devel readline-devel lvm2-devel \\ glib2-devel userspace-rcu-devel libcmocka-devel libacl-devel sqlite-devel fuse-devel \\ redhat-rpm-config rpcgen libtirpc-devel make python3-devel rsync libuuid-devel \\ rpm-build dbench perl-Test-Harness attr libcurl-devel selinux-policy-devel -y Now follow through from Point 2 in the Common Steps part below. Preparation steps for RHEL 6.x (only) You'll need EPEL installed first and some RHEL specific packages. The 2 commands below will get that done for you. After that, follow through the \"Common steps\" section. Install EPEL first: # sudo yum -y install `[`http://download.fedoraproject.org/pub/epel/6/x86_64/epel-release-6-8.noarch.rpm`](http://download.fedoraproject.org/pub/epel/6/x86_64/epel-release-6-8.noarch.rpm) Install the packages required only on RHEL: # sudo yum -y --enablerepo=rhel-6-server-optional-rpms install python-webob1.0 \\ python-paste-deploy1.5 python-sphinx10 redhat-rpm-config Now follow through with the Common Steps part below. Common Steps These steps are for both Fedora and RHEL/CentOS. At the end you'll have the complete set of GlusterFS RPMs for your platform, ready to be installed. NOTES for step 1 below: If you're on RHEL/CentOS 5.x and get a message about lvm2-devel not being available, it's ok. You can ignore it. :) If you're on RHEL/CentOS 6.x and get any messages about python-eventlet, python-netifaces, python-sphinx and/or pyxattr not being available, it's ok. You can ignore them. :) If you're on CentOS 8.x, you can skip step 1 and start from step 2. Also, for CentOS 8.x, the steps have been tested for the master branch. It is unknown if it would work for older branches. Install the needed packages # sudo yum -y --disablerepo=rhs* --enablerepo=*optional-rpms install git autoconf \\ automake bison dos2unix flex fuse-devel glib2-devel libaio-devel \\ libattr-devel libibverbs-devel librdmacm-devel libtool libxml2-devel lvm2-devel make \\ openssl-devel pkgconfig pyliblzma python-devel python-eventlet python-netifaces \\ python-paste-deploy python-simplejson python-sphinx python-webob pyxattr readline-devel \\ rpm-build systemtap-sdt-devel tar libcmocka-devel Clone the GlusterFS git repository # git clone `[`git://git.gluster.org/glusterfs`](git://git.gluster.org/glusterfs) # cd glusterfs Choose which branch to compile If you want to compile the latest development code, you can skip this step and go on to the next one. :) If instead, you want to compile the code for a specific release of GlusterFS (such as v3.4), get the list of release names here: # git branch -a | grep release remotes/origin/release-2.0 remotes/origin/release-3.0 remotes/origin/release-3.1 remotes/origin/release-3.2 remotes/origin/release-3.3 remotes/origin/release-3.4 remotes/origin/release-3.5 Then switch to the correct release using the git \"checkout\" command, and the name of the release after the \"remotes/origin/\" bit from the list above: # git checkout release-3.4 NOTE - The CentOS 5.x instructions have only been tested for the master branch in GlusterFS git. It is unknown (yet) if they work for branches older than release-3.5. If you are compiling the latest development code you can skip steps 4 and 5 . Instead, you can run the below command and you will get the RPMs. # extras/LinuxRPM/make_glusterrpms Configure and compile GlusterFS Now you're ready to compile Gluster: # ./autogen.sh # ./configure --enable-fusermount # make dist Create the GlusterFS RPMs # cd extras/LinuxRPM # make glusterrpms That should complete with no errors, leaving you with a directory containing the RPMs. # ls -l *rpm -rw-rw-r-- 1 jc jc 3966111 Mar 2 12:15 glusterfs-3git-1.el5.centos.src.rpm -rw-rw-r-- 1 jc jc 1548890 Mar 2 12:17 glusterfs-3git-1.el5.centos.x86_64.rpm -rw-rw-r-- 1 jc jc 66680 Mar 2 12:17 glusterfs-api-3git-1.el5.centos.x86_64.rpm -rw-rw-r-- 1 jc jc 20399 Mar 2 12:17 glusterfs-api-devel-3git-1.el5.centos.x86_64.rpm -rw-rw-r-- 1 jc jc 123806 Mar 2 12:17 glusterfs-cli-3git-1.el5.centos.x86_64.rpm -rw-rw-r-- 1 jc jc 7850357 Mar 2 12:17 glusterfs-debuginfo-3git-1.el5.centos.x86_64.rpm -rw-rw-r-- 1 jc jc 112677 Mar 2 12:17 glusterfs-devel-3git-1.el5.centos.x86_64.rpm -rw-rw-r-- 1 jc jc 100410 Mar 2 12:17 glusterfs-fuse-3git-1.el5.centos.x86_64.rpm -rw-rw-r-- 1 jc jc 187221 Mar 2 12:17 glusterfs-geo-replication-3git-1.el5.centos.x86_64.rpm -rw-rw-r-- 1 jc jc 299171 Mar 2 12:17 glusterfs-libs-3git-1.el5.centos.x86_64.rpm -rw-rw-r-- 1 jc jc 44943 Mar 2 12:17 glusterfs-rdma-3git-1.el5.centos.x86_64.rpm -rw-rw-r-- 1 jc jc 123065 Mar 2 12:17 glusterfs-regression-tests-3git-1.el5.centos.x86_64.rpm -rw-rw-r-- 1 jc jc 16224 Mar 2 12:17 glusterfs-resource-agents-3git-1.el5.centos.x86_64.rpm -rw-rw-r-- 1 jc jc 654043 Mar 2 12:17 glusterfs-server-3git-1.el5.centos.x86_64.rpm","title":"Compiling RPMS"},{"location":"Developer-guide/compiling-rpms/#how-to-compile-glusterfs-rpms-from-git-source-for-rhelcentos-and-fedora","text":"Creating rpm's of GlusterFS from git source is fairly easy, once you know the steps. RPMs can be compiled on at least the following OS's: Red Hat Enterprise Linux 5, 6 (& 7 when available) CentOS 5, 6, 7 and 8 Fedora 16-20 Specific instructions for compiling are below. If you're using: Fedora 16-20 - Follow the Fedora steps, then do all of the Common steps. CentOS 5.x - Follow the CentOS 5.x steps, then do all of the Common steps CentOS 6.x - Follow the CentOS 6.x steps, then do all of the Common steps. CentOS 8.x - Follow the CentOS 8.x steps, then follow from step 2 in the Common steps. RHEL 6.x - Follow the RHEL 6.x steps, then do all of the Common steps. Note - these instructions have been explicitly tested on all of CentOS 5.10, RHEL 6.4, CentOS 6.4+, CentOS 8.4, and Fedora 16-20. Other releases of RHEL/CentOS and Fedora may work too but haven't been tested. Please update this page appropriately if you do so. :)","title":"How to compile GlusterFS RPMs from git source, for RHEL/CentOS, and Fedora"},{"location":"Developer-guide/compiling-rpms/#preparation-steps-for-fedora-16-20-only","text":"Install gcc, the python development headers, and python setuptools: # sudo yum -y install gcc python-devel python-setuptools If you're compiling GlusterFS version 3.4, then install python-swiftclient. Other GlusterFS versions don't need it: # sudo easy_install simplejson python-swiftclient Now follow through with the Common Steps part below.","title":"Preparation steps for Fedora 16-20 (only)"},{"location":"Developer-guide/compiling-rpms/#preparation-steps-for-centos-5x-only","text":"You'll need EPEL installed first and some CentOS-specific packages. The commands below will get that done for you. After that, follow through the \"Common steps\" section. Install EPEL first: # curl -OL `[`http://download.fedoraproject.org/pub/epel/5/x86_64/epel-release-5-4.noarch.rpm`](http://download.fedoraproject.org/pub/epel/5/x86_64/epel-release-5-4.noarch.rpm) # sudo yum -y install epel-release-5-4.noarch.rpm --nogpgcheck Install the packages required only on CentOS 5.x: # sudo yum -y install buildsys-macros gcc ncurses-devel \\ python-ctypes python-sphinx10 redhat-rpm-config Now follow through with the Common Steps part below.","title":"Preparation steps for CentOS 5.x (only)"},{"location":"Developer-guide/compiling-rpms/#preparation-steps-for-centos-6x-only","text":"You'll need EPEL installed first and some CentOS-specific packages. The commands below will get that done for you. After that, follow through the \"Common steps\" section. Install EPEL first: # sudo yum -y install `[`http://download.fedoraproject.org/pub/epel/6/x86_64/epel-release-6-8.noarch.rpm`](http://download.fedoraproject.org/pub/epel/6/x86_64/epel-release-6-8.noarch.rpm) Install the packages required only on CentOS: # sudo yum -y install python-webob1.0 python-paste-deploy1.5 python-sphinx10 redhat-rpm-config Now follow through with the Common Steps part below.","title":"Preparation steps for CentOS 6.x (only)"},{"location":"Developer-guide/compiling-rpms/#preparation-steps-for-centos-8x-only","text":"You'll need EPEL installed and then the powertools package enabled. Install EPEL first: # sudo rpm -ivh https://dl.fedoraproject.org/pub/epel/epel-release-latest-8.noarch.rpm Enable the PowerTools repo and install CentOS 8.x specific packages for building the rpms. # sudo yum --enablerepo=PowerTools install automake autoconf libtool flex bison openssl-devel \\ libxml2-devel libaio-devel libibverbs-devel librdmacm-devel readline-devel lvm2-devel \\ glib2-devel userspace-rcu-devel libcmocka-devel libacl-devel sqlite-devel fuse-devel \\ redhat-rpm-config rpcgen libtirpc-devel make python3-devel rsync libuuid-devel \\ rpm-build dbench perl-Test-Harness attr libcurl-devel selinux-policy-devel -y Now follow through from Point 2 in the Common Steps part below.","title":"Preparation steps for CentOS 8.x (only)"},{"location":"Developer-guide/compiling-rpms/#preparation-steps-for-rhel-6x-only","text":"You'll need EPEL installed first and some RHEL specific packages. The 2 commands below will get that done for you. After that, follow through the \"Common steps\" section. Install EPEL first: # sudo yum -y install `[`http://download.fedoraproject.org/pub/epel/6/x86_64/epel-release-6-8.noarch.rpm`](http://download.fedoraproject.org/pub/epel/6/x86_64/epel-release-6-8.noarch.rpm) Install the packages required only on RHEL: # sudo yum -y --enablerepo=rhel-6-server-optional-rpms install python-webob1.0 \\ python-paste-deploy1.5 python-sphinx10 redhat-rpm-config Now follow through with the Common Steps part below.","title":"Preparation steps for RHEL 6.x (only)"},{"location":"Developer-guide/compiling-rpms/#common-steps","text":"These steps are for both Fedora and RHEL/CentOS. At the end you'll have the complete set of GlusterFS RPMs for your platform, ready to be installed. NOTES for step 1 below: If you're on RHEL/CentOS 5.x and get a message about lvm2-devel not being available, it's ok. You can ignore it. :) If you're on RHEL/CentOS 6.x and get any messages about python-eventlet, python-netifaces, python-sphinx and/or pyxattr not being available, it's ok. You can ignore them. :) If you're on CentOS 8.x, you can skip step 1 and start from step 2. Also, for CentOS 8.x, the steps have been tested for the master branch. It is unknown if it would work for older branches. Install the needed packages # sudo yum -y --disablerepo=rhs* --enablerepo=*optional-rpms install git autoconf \\ automake bison dos2unix flex fuse-devel glib2-devel libaio-devel \\ libattr-devel libibverbs-devel librdmacm-devel libtool libxml2-devel lvm2-devel make \\ openssl-devel pkgconfig pyliblzma python-devel python-eventlet python-netifaces \\ python-paste-deploy python-simplejson python-sphinx python-webob pyxattr readline-devel \\ rpm-build systemtap-sdt-devel tar libcmocka-devel Clone the GlusterFS git repository # git clone `[`git://git.gluster.org/glusterfs`](git://git.gluster.org/glusterfs) # cd glusterfs Choose which branch to compile If you want to compile the latest development code, you can skip this step and go on to the next one. :) If instead, you want to compile the code for a specific release of GlusterFS (such as v3.4), get the list of release names here: # git branch -a | grep release remotes/origin/release-2.0 remotes/origin/release-3.0 remotes/origin/release-3.1 remotes/origin/release-3.2 remotes/origin/release-3.3 remotes/origin/release-3.4 remotes/origin/release-3.5 Then switch to the correct release using the git \"checkout\" command, and the name of the release after the \"remotes/origin/\" bit from the list above: # git checkout release-3.4 NOTE - The CentOS 5.x instructions have only been tested for the master branch in GlusterFS git. It is unknown (yet) if they work for branches older than release-3.5. If you are compiling the latest development code you can skip steps 4 and 5 . Instead, you can run the below command and you will get the RPMs. # extras/LinuxRPM/make_glusterrpms Configure and compile GlusterFS Now you're ready to compile Gluster: # ./autogen.sh # ./configure --enable-fusermount # make dist Create the GlusterFS RPMs # cd extras/LinuxRPM # make glusterrpms That should complete with no errors, leaving you with a directory containing the RPMs. # ls -l *rpm -rw-rw-r-- 1 jc jc 3966111 Mar 2 12:15 glusterfs-3git-1.el5.centos.src.rpm -rw-rw-r-- 1 jc jc 1548890 Mar 2 12:17 glusterfs-3git-1.el5.centos.x86_64.rpm -rw-rw-r-- 1 jc jc 66680 Mar 2 12:17 glusterfs-api-3git-1.el5.centos.x86_64.rpm -rw-rw-r-- 1 jc jc 20399 Mar 2 12:17 glusterfs-api-devel-3git-1.el5.centos.x86_64.rpm -rw-rw-r-- 1 jc jc 123806 Mar 2 12:17 glusterfs-cli-3git-1.el5.centos.x86_64.rpm -rw-rw-r-- 1 jc jc 7850357 Mar 2 12:17 glusterfs-debuginfo-3git-1.el5.centos.x86_64.rpm -rw-rw-r-- 1 jc jc 112677 Mar 2 12:17 glusterfs-devel-3git-1.el5.centos.x86_64.rpm -rw-rw-r-- 1 jc jc 100410 Mar 2 12:17 glusterfs-fuse-3git-1.el5.centos.x86_64.rpm -rw-rw-r-- 1 jc jc 187221 Mar 2 12:17 glusterfs-geo-replication-3git-1.el5.centos.x86_64.rpm -rw-rw-r-- 1 jc jc 299171 Mar 2 12:17 glusterfs-libs-3git-1.el5.centos.x86_64.rpm -rw-rw-r-- 1 jc jc 44943 Mar 2 12:17 glusterfs-rdma-3git-1.el5.centos.x86_64.rpm -rw-rw-r-- 1 jc jc 123065 Mar 2 12:17 glusterfs-regression-tests-3git-1.el5.centos.x86_64.rpm -rw-rw-r-- 1 jc jc 16224 Mar 2 12:17 glusterfs-resource-agents-3git-1.el5.centos.x86_64.rpm -rw-rw-r-- 1 jc jc 654043 Mar 2 12:17 glusterfs-server-3git-1.el5.centos.x86_64.rpm","title":"Common Steps"},{"location":"Developer-guide/coredump-on-customer-setup/","text":"Get core dump on a customer set up without killing the process Why do we need this? Finding the root cause of an issue that occurred in the customer/production setup is a challenging task. Most of the time we cannot replicate/setup the environment and scenario which is leading to the issue on our test setup. In such cases, we got to grab most of the information from the system where the problem has occurred. What information we look for and also useful? The information like a core dump is very helpful to catch the root cause of an issue by adding ASSERT() in the code at the places where we feel something is wrong and install the custom build on the affected setup. But the issue is ASSERT() would kill the process and produce the core dump. Is it a good idea to do ASSERT() on customer setup? Remember we are seeking help from customer setup, they unlikely agree to kill the process and produce the core dump for us to root cause it. It affects the customer\u2019s business and nobody agrees with this proposal. What if we have a way to produce a core dump without a kill? Yes, Glusterfs provides a way to do this. Gluster has customized ASSERT() i.e GF_ASSERT() in place which helps in producing the core dump without killing the associated process and also provides a script which can be run on the customer set up that produces the core dump without harming the running process (This presumes we already have GF_ASSERT() at the expected place in the current build running on customer setup. If not, we need to install custom build on that setup by adding GF_ASSERT()). Is GF_ASSERT() newly introduced in Gluster code? No. GF_ASSERT() is already there in the codebase before this improvement. In the debug build, GF_ASSERT() kills the process and produces the core dump but in the production build, it just logs the error and moves on. What we have done is we just changed the implementation of the code and now in production build also we get the core dump but the process won\u2019t be killed. The code places where GF_ASSERT() is not covered, please add it as per the requirement. Here are the steps to achieve the goal: Add GF_ASSERT() in the Gluster code path where you expect something wrong is happening. Build the Gluster code, install and mount the Gluster volume (For detailed steps refer: Gluster quick start guide). Now, in the other terminal, run the gfcore.py script # ./extras/debug/gfcore.py $PID 1 /tmp/ (PID of the gluster process you are interested in, got it by ps -ef | grep gluster in the previous step. For more details, check # ./extras/debug/gfcore.py --help ) Hit the code path where you have introduced GF_ASSERT(). If GF_ASSERT() is in fuse_write() path, you can hit the code path by writing on to a file present under Gluster moun. Ex: # dd if=/dev/zero of=/mnt/glustrefs/abcd bs=1M count=1 where /mnt/glusterfs is the gluster mount Go to the terminal where the gdb is running (step 3) and observe that the gdb process is terminated Go to the directory where the core-dump is produced. Default would be present working directory. Access the core dump using gdb Ex: # gdb -ex \"core-file $GFCORE_FILE\" $GLUSTER_BINARY (1st arg would be core file name and 2nd arg is o/p of file command in the previous step) Observe that the Gluster process is unaffected by checking its process state. Check pid status using ps -ef | grep gluster Thanks, Xavi Hernandez(jahernan@redhat.com) for the idea. This will ease many Gluster developer's/maintainer\u2019s life.","title":"Get core dump on a customer set up without killing the process"},{"location":"Developer-guide/coredump-on-customer-setup/#get-core-dump-on-a-customer-set-up-without-killing-the-process","text":"","title":"Get core dump on a customer set up without killing the process"},{"location":"Developer-guide/coredump-on-customer-setup/#why-do-we-need-this","text":"Finding the root cause of an issue that occurred in the customer/production setup is a challenging task. Most of the time we cannot replicate/setup the environment and scenario which is leading to the issue on our test setup. In such cases, we got to grab most of the information from the system where the problem has occurred.","title":"Why do we need this?"},{"location":"Developer-guide/coredump-on-customer-setup/#what-information-we-look-for-and-also-useful","text":"The information like a core dump is very helpful to catch the root cause of an issue by adding ASSERT() in the code at the places where we feel something is wrong and install the custom build on the affected setup. But the issue is ASSERT() would kill the process and produce the core dump.","title":"What information we look for and also useful?"},{"location":"Developer-guide/coredump-on-customer-setup/#is-it-a-good-idea-to-do-assert-on-customer-setup","text":"Remember we are seeking help from customer setup, they unlikely agree to kill the process and produce the core dump for us to root cause it. It affects the customer\u2019s business and nobody agrees with this proposal.","title":"Is it a good idea to do ASSERT() on customer setup?"},{"location":"Developer-guide/coredump-on-customer-setup/#what-if-we-have-a-way-to-produce-a-core-dump-without-a-kill","text":"Yes, Glusterfs provides a way to do this. Gluster has customized ASSERT() i.e GF_ASSERT() in place which helps in producing the core dump without killing the associated process and also provides a script which can be run on the customer set up that produces the core dump without harming the running process (This presumes we already have GF_ASSERT() at the expected place in the current build running on customer setup. If not, we need to install custom build on that setup by adding GF_ASSERT()).","title":"What if we have a way to produce a core dump without a kill?"},{"location":"Developer-guide/coredump-on-customer-setup/#is-gf_assert-newly-introduced-in-gluster-code","text":"No. GF_ASSERT() is already there in the codebase before this improvement. In the debug build, GF_ASSERT() kills the process and produces the core dump but in the production build, it just logs the error and moves on. What we have done is we just changed the implementation of the code and now in production build also we get the core dump but the process won\u2019t be killed. The code places where GF_ASSERT() is not covered, please add it as per the requirement.","title":"Is GF_ASSERT() newly introduced in Gluster code?"},{"location":"Developer-guide/coredump-on-customer-setup/#here-are-the-steps-to-achieve-the-goal","text":"Add GF_ASSERT() in the Gluster code path where you expect something wrong is happening. Build the Gluster code, install and mount the Gluster volume (For detailed steps refer: Gluster quick start guide). Now, in the other terminal, run the gfcore.py script # ./extras/debug/gfcore.py $PID 1 /tmp/ (PID of the gluster process you are interested in, got it by ps -ef | grep gluster in the previous step. For more details, check # ./extras/debug/gfcore.py --help ) Hit the code path where you have introduced GF_ASSERT(). If GF_ASSERT() is in fuse_write() path, you can hit the code path by writing on to a file present under Gluster moun. Ex: # dd if=/dev/zero of=/mnt/glustrefs/abcd bs=1M count=1 where /mnt/glusterfs is the gluster mount Go to the terminal where the gdb is running (step 3) and observe that the gdb process is terminated Go to the directory where the core-dump is produced. Default would be present working directory. Access the core dump using gdb Ex: # gdb -ex \"core-file $GFCORE_FILE\" $GLUSTER_BINARY (1st arg would be core file name and 2nd arg is o/p of file command in the previous step) Observe that the Gluster process is unaffected by checking its process state. Check pid status using ps -ef | grep gluster Thanks, Xavi Hernandez(jahernan@redhat.com) for the idea. This will ease many Gluster developer's/maintainer\u2019s life.","title":"Here are the steps to achieve the goal:"},{"location":"GlusterFS-Tools/","text":"GlusterFS Tools glusterfind gfind missing files","title":"GlusterFS Tools List"},{"location":"GlusterFS-Tools/#glusterfs-tools","text":"glusterfind gfind missing files","title":"GlusterFS Tools"},{"location":"GlusterFS-Tools/gfind-missing-files/","text":"Introduction The tool gfind_missing_files.sh can be used to find the missing files in a GlusterFS geo-replicated secondary volume. The tool uses a multi-threaded crawler operating on the backend .glusterfs of a brickpath which is passed as one of the parameters to the tool. It does a stat on each entry in the secondary volume mount to check for the presence of a file. The tool uses the aux-gfid-mount thereby avoiding path conversions and potentially saving time. This tool should be run on every node and each brickpath in a geo-replicated primary volume to find the missing files on the secondary volume. The script gfind_missing_files.sh is a wrapper script that in turn uses the gcrawler binary to do the backend crawling. The script detects the gfids of the missing files and runs the gfid-to-path conversion script to list out the missing files with their full pathnames. Usage bash gfind_missing_files.sh <BRICK_PATH> <SECONDARY_HOST> <SECONDARY_VOL> <OUTFILE> BRICK_PATH - Full path of the brick SECONDARY_HOST - Hostname of gluster volume SECONDARY_VOL - Gluster volume name OUTFILE - Output file which contains gfids of the missing files The gfid-to-path conversion uses a quicker algorithm for converting gfids to paths and it is possible that in some cases all missing gfids may not be converted to their respective paths. Example output(126733 missing files) # ionice -c 2 -n 7 ./gfind_missing_files.sh /bricks/m3 acdc secondary-vol ~/test_results/m3-4.txt Calling crawler... Crawl Complete. gfids of skipped files are available in the file /root/test_results/m3-4.txt Starting gfid to path conversion Path names of skipped files are available in the file /root/test_results/m3-4.txt_pathnames WARNING: Unable to convert some GFIDs to Paths, GFIDs logged to /root/test_results/m3-4.txt_gfids Use bash gfid_to_path.sh <brick-path> /root/test_results/m3-4.txt_gfids to convert those GFIDs to Path Total Missing File Count : 126733 In such cases, an additional step is needed to convert those gfids to paths. This can be used as shown below: bash gfid_to_path.sh <BRICK_PATH> <GFID_FILE> BRICK_PATH - Full path of the brick. GFID_FILE - OUTFILE_gfids got from gfind_missing_files.sh Things to keep in mind when running the tool Running this tool can result in a crawl of the backend filesystem at each brick which can be intensive. To ensure there is no impact on ongoing I/O on RHS volumes, we recommend that this tool be run at a low I/O scheduling class (best-effort) and priority. ionice -c 2 -p <pid of gfind_missing_files.sh> We do not recommend interrupting the tool when it is running (e.g. by doing CTRL^C). It is better to wait for the tool to finish execution. In case it is interrupted, manually unmount the Slave Volume. umount <MOUNT_POINT>","title":"gfind missing files"},{"location":"GlusterFS-Tools/gfind-missing-files/#introduction","text":"The tool gfind_missing_files.sh can be used to find the missing files in a GlusterFS geo-replicated secondary volume. The tool uses a multi-threaded crawler operating on the backend .glusterfs of a brickpath which is passed as one of the parameters to the tool. It does a stat on each entry in the secondary volume mount to check for the presence of a file. The tool uses the aux-gfid-mount thereby avoiding path conversions and potentially saving time. This tool should be run on every node and each brickpath in a geo-replicated primary volume to find the missing files on the secondary volume. The script gfind_missing_files.sh is a wrapper script that in turn uses the gcrawler binary to do the backend crawling. The script detects the gfids of the missing files and runs the gfid-to-path conversion script to list out the missing files with their full pathnames.","title":"Introduction"},{"location":"GlusterFS-Tools/gfind-missing-files/#usage","text":"bash gfind_missing_files.sh <BRICK_PATH> <SECONDARY_HOST> <SECONDARY_VOL> <OUTFILE> BRICK_PATH - Full path of the brick SECONDARY_HOST - Hostname of gluster volume SECONDARY_VOL - Gluster volume name OUTFILE - Output file which contains gfids of the missing files The gfid-to-path conversion uses a quicker algorithm for converting gfids to paths and it is possible that in some cases all missing gfids may not be converted to their respective paths.","title":"Usage"},{"location":"GlusterFS-Tools/gfind-missing-files/#example-output126733-missing-files","text":"# ionice -c 2 -n 7 ./gfind_missing_files.sh /bricks/m3 acdc secondary-vol ~/test_results/m3-4.txt Calling crawler... Crawl Complete. gfids of skipped files are available in the file /root/test_results/m3-4.txt Starting gfid to path conversion Path names of skipped files are available in the file /root/test_results/m3-4.txt_pathnames WARNING: Unable to convert some GFIDs to Paths, GFIDs logged to /root/test_results/m3-4.txt_gfids Use bash gfid_to_path.sh <brick-path> /root/test_results/m3-4.txt_gfids to convert those GFIDs to Path Total Missing File Count : 126733 In such cases, an additional step is needed to convert those gfids to paths. This can be used as shown below: bash gfid_to_path.sh <BRICK_PATH> <GFID_FILE> BRICK_PATH - Full path of the brick. GFID_FILE - OUTFILE_gfids got from gfind_missing_files.sh","title":"Example output(126733 missing files)"},{"location":"GlusterFS-Tools/gfind-missing-files/#things-to-keep-in-mind-when-running-the-tool","text":"Running this tool can result in a crawl of the backend filesystem at each brick which can be intensive. To ensure there is no impact on ongoing I/O on RHS volumes, we recommend that this tool be run at a low I/O scheduling class (best-effort) and priority. ionice -c 2 -p <pid of gfind_missing_files.sh> We do not recommend interrupting the tool when it is running (e.g. by doing CTRL^C). It is better to wait for the tool to finish execution. In case it is interrupted, manually unmount the Slave Volume. umount <MOUNT_POINT>","title":"Things to keep in mind when running the tool"},{"location":"GlusterFS-Tools/glusterfind/","text":"glusterfind - A tool to find Modified files/dirs A tool which helps to get full/incremental list of files/dirs from GlusterFS Volume using Changelog/Find. In Gluster volumes, detecting the modified files is challenging. Readdir on a directory leads to multiple network calls since files in a directory are distributed across nodes. This tool should be run in one of the node, which will get Volume info and gets the list of nodes and brick paths. For each brick, it spawns the process and runs crawler command in respective node. Crawler will be run in brick FS(xfs, ext4 etc) and not in Gluster Mount. Crawler generates output file with the list of files modified after last run or after the session creation. Session Management Create a glusterfind session to remember the time when last sync or processing complete. For example, your backup application runs every day and gets incremental results on each run. The tool maintains session in $GLUSTERD_WORKDIR/glusterfind/ , for each session it creates and directory and creates a sub directory with Volume name. (Default working directory is /var/lib/glusterd, in some systems this location may change. To find Working dir location run grep working-directory /etc/glusterfs/glusterd.vol or grep working-directory /usr/local/etc/glusterfs/glusterd.vol if source install) For example, if the session name is \"backup\" and volume name is \"datavol\", then the tool creates $GLUSTERD_WORKDIR/glusterfind/backup/datavol . Now onwards we refer this directory as $SESSION_DIR . create => pre => post => [delete] Once the session is created, we can run the tool with two steps Pre and Post. To collect the list of modified files after the create time or last run time, we need to call pre command. Pre command finds the modified files and generates output file. Consumer can check the exit code of pre command and start processing those files. As a post processing step run the post command to update the session time as per latest run. For example, backup utility runs Pre command and gets the list of files/directories changed. Sync those files to backup target and inform to glusterfind by calling Post command. At the end of Pre command, $SESSION_DIR/status.pre status file will get created. Pre status file stores the time when current crawl is started, and get all the files/dirs modified till that time. Once Post is called, $SESSION_DIR/status.pre will be renamed to $SESSION_DIR/status . content of this file will be used as start time for the next crawl. During Pre, we can force the tool to do full find instead of incremental find. Tool uses find command in brick backend to get list of files/dirs. When glusterfind create , in that node it generates ssh key($GLUSTERD_WORKDIR/glusterfind.secret.pem) and distributes to all Peers via Glusterd. Once ssh key is distributed in Trusted pool, tool can run ssh commands and copy files from other Volume nodes. When glusterfind pre is run, it internally runs gluster volume info to get list of nodes and respective brick paths. For each brick, it calls respective node agents via ssh to find the modified files/dirs which are local them. Once each node agents generates output file, glusterfind collects all the files via scp and merges it into given output file. When glusterfind post is run, it renames $SESSION_DIR/status.pre file to $SESSION_DIR/status . Changelog Mode and GFID to Path conversion Incremental find uses Changelogs to get the list of GFIDs modified/created. Any application expects file path instead of GFID. Their is no standard/easy way to convert from GFID to Path. If we set build-pgfid option in Volume GlusterFS starts recording each files parent directory GFID as xattr in file on any ENTRY fop. trusted.pgfid.<GFID>=NUM_LINKS To convert from GFID to path, we can mount Volume with aux-gfid-mount option, and get Path information by a getfattr query. getfattr -n glusterfs.ancestry.path -e text /mnt/datavol/.gfid/<GFID> This approach is slow, for a requested file gets parent GFID via xattr and reads that directory to gets the file which is having same inode number as of GFID file. To improve the performance, glusterfind uses build-pgfid option, but instead of using getfattr on mount it gets the details from brick backend. glusterfind collects all parent GFIDs at once and starts crawling each directory. Instead of processing one GFID to Path conversion, it gets inode numbers of all input GFIDs and filter while reading parent directory. Above method is fast compared to find -samefile since it crawls only required directories to find files with same inode number as GFID file. But pgfid information only available when a lookup is made or any ENTRY fop to a file after enabling build-pgfid. The files created before build-pgfid enable will not get converted to path from GFID with this approach. Tool collects the list of GFIDs failed to convert with above method and does a full crawl to convert it to path. Find command is used to crawl entire namespace. Instead of calling find command for every GFID, glusterfind uses an efficient way to convert all GFID to path with single call to find command. Usage Create the session glusterfind create SESSION_NAME VOLNAME [--force] glusterfind create --help Where, SESSION_NAME is any name without space to identify when run second time. When a node is added to Volume then the tool expects ssh keys to be copied to new node(s) also. Run Create command with --force to distribute keys again. Examples, # glusterfind create --help # glusterfind create backup datavol # glusterfind create antivirus_scanner datavol # glusterfind create backup datavol --force Pre Command glusterfind pre SESSION_NAME VOLUME_NAME OUTFILE glusterfind pre --help We need not specify Volume name since session already has the details. List of files will be populated in OUTFILE. To trigger the full find, call the pre command with --full argument. Multiple crawlers are available for incremental find, we can choose crawl type with --crawl argument. Examples, # glusterfind pre backup datavol /root/backup.txt # glusterfind pre backup datavol /root/backup.txt --full # # Changelog based crawler, works only for incremental # glusterfind pre backup datavol /root/backup.txt --crawler=changelog # # Find based crawler, works for both full and incremental # glusterfind pre backup datavol /root/backup.txt --crawler=brickfind Output file contains list of files/dirs relative to the Volume mount, if we need to prefix with any path to have absolute path then, # glusterfind pre backup datavol /root/backup.txt --file-prefix=/mnt/datavol/ List Command To get the list of sessions and respective session time, glusterfind list [--session SESSION_NAME] [--volume VOLUME_NAME] Examples, # glusterfind list # glusterfind list --session backup Example output, SESSION VOLUME SESSION TIME --------------------------------------------------------------------------- backup datavol 2015-03-04 17:35:34 Post Command glusterfind post SESSION_NAME VOLUME_NAME Examples, # glusterfind post backup datavol Delete Command glusterfind delete SESSION_NAME VOLUME_NAME Examples, # glusterfind delete backup datavol Adding more Crawlers Adding more crawlers is very simple, Add an entry in $GLUSTERD_WORKDIR/glusterfind.conf . glusterfind can choose your crawler using --crawl argument. [crawlers] changelog=/usr/libexec/glusterfs/glusterfind/changelog.py brickfind=/usr/libexec/glusterfs/glusterfind/brickfind.py For example, if you have a multithreaded brick crawler, say parallelbrickcrawl add it to the conf file. [crawlers] changelog=/usr/libexec/glusterfs/glusterfind/changelog.py brickfind=/usr/libexec/glusterfs/glusterfind/brickfind.py parallelbrickcrawl=/root/parallelbrickcrawl Custom crawler can be executable script/binary which accepts volume name, brick path, output_file and start time(and optional debug flag) For example, /root/parallelbrickcrawl SESSION_NAME VOLUME BRICK_PATH OUTFILE START_TIME [--debug] Where START_TIME is in unix epoch format, START_TIME will be zero for full find. Known Issues Deleted files will not get listed, since we can't convert GFID to Path if file/dir is deleted. Only new name will get listed if Renamed. All hardlinks will get listed.","title":"glusterfind"},{"location":"GlusterFS-Tools/glusterfind/#glusterfind-a-tool-to-find-modified-filesdirs","text":"A tool which helps to get full/incremental list of files/dirs from GlusterFS Volume using Changelog/Find. In Gluster volumes, detecting the modified files is challenging. Readdir on a directory leads to multiple network calls since files in a directory are distributed across nodes. This tool should be run in one of the node, which will get Volume info and gets the list of nodes and brick paths. For each brick, it spawns the process and runs crawler command in respective node. Crawler will be run in brick FS(xfs, ext4 etc) and not in Gluster Mount. Crawler generates output file with the list of files modified after last run or after the session creation.","title":"glusterfind - A tool to find Modified files/dirs"},{"location":"GlusterFS-Tools/glusterfind/#session-management","text":"Create a glusterfind session to remember the time when last sync or processing complete. For example, your backup application runs every day and gets incremental results on each run. The tool maintains session in $GLUSTERD_WORKDIR/glusterfind/ , for each session it creates and directory and creates a sub directory with Volume name. (Default working directory is /var/lib/glusterd, in some systems this location may change. To find Working dir location run grep working-directory /etc/glusterfs/glusterd.vol or grep working-directory /usr/local/etc/glusterfs/glusterd.vol if source install) For example, if the session name is \"backup\" and volume name is \"datavol\", then the tool creates $GLUSTERD_WORKDIR/glusterfind/backup/datavol . Now onwards we refer this directory as $SESSION_DIR . create => pre => post => [delete] Once the session is created, we can run the tool with two steps Pre and Post. To collect the list of modified files after the create time or last run time, we need to call pre command. Pre command finds the modified files and generates output file. Consumer can check the exit code of pre command and start processing those files. As a post processing step run the post command to update the session time as per latest run. For example, backup utility runs Pre command and gets the list of files/directories changed. Sync those files to backup target and inform to glusterfind by calling Post command. At the end of Pre command, $SESSION_DIR/status.pre status file will get created. Pre status file stores the time when current crawl is started, and get all the files/dirs modified till that time. Once Post is called, $SESSION_DIR/status.pre will be renamed to $SESSION_DIR/status . content of this file will be used as start time for the next crawl. During Pre, we can force the tool to do full find instead of incremental find. Tool uses find command in brick backend to get list of files/dirs. When glusterfind create , in that node it generates ssh key($GLUSTERD_WORKDIR/glusterfind.secret.pem) and distributes to all Peers via Glusterd. Once ssh key is distributed in Trusted pool, tool can run ssh commands and copy files from other Volume nodes. When glusterfind pre is run, it internally runs gluster volume info to get list of nodes and respective brick paths. For each brick, it calls respective node agents via ssh to find the modified files/dirs which are local them. Once each node agents generates output file, glusterfind collects all the files via scp and merges it into given output file. When glusterfind post is run, it renames $SESSION_DIR/status.pre file to $SESSION_DIR/status .","title":"Session Management"},{"location":"GlusterFS-Tools/glusterfind/#changelog-mode-and-gfid-to-path-conversion","text":"Incremental find uses Changelogs to get the list of GFIDs modified/created. Any application expects file path instead of GFID. Their is no standard/easy way to convert from GFID to Path. If we set build-pgfid option in Volume GlusterFS starts recording each files parent directory GFID as xattr in file on any ENTRY fop. trusted.pgfid.<GFID>=NUM_LINKS To convert from GFID to path, we can mount Volume with aux-gfid-mount option, and get Path information by a getfattr query. getfattr -n glusterfs.ancestry.path -e text /mnt/datavol/.gfid/<GFID> This approach is slow, for a requested file gets parent GFID via xattr and reads that directory to gets the file which is having same inode number as of GFID file. To improve the performance, glusterfind uses build-pgfid option, but instead of using getfattr on mount it gets the details from brick backend. glusterfind collects all parent GFIDs at once and starts crawling each directory. Instead of processing one GFID to Path conversion, it gets inode numbers of all input GFIDs and filter while reading parent directory. Above method is fast compared to find -samefile since it crawls only required directories to find files with same inode number as GFID file. But pgfid information only available when a lookup is made or any ENTRY fop to a file after enabling build-pgfid. The files created before build-pgfid enable will not get converted to path from GFID with this approach. Tool collects the list of GFIDs failed to convert with above method and does a full crawl to convert it to path. Find command is used to crawl entire namespace. Instead of calling find command for every GFID, glusterfind uses an efficient way to convert all GFID to path with single call to find command.","title":"Changelog Mode and GFID to Path conversion"},{"location":"GlusterFS-Tools/glusterfind/#usage","text":"","title":"Usage"},{"location":"GlusterFS-Tools/glusterfind/#create-the-session","text":"glusterfind create SESSION_NAME VOLNAME [--force] glusterfind create --help Where, SESSION_NAME is any name without space to identify when run second time. When a node is added to Volume then the tool expects ssh keys to be copied to new node(s) also. Run Create command with --force to distribute keys again. Examples, # glusterfind create --help # glusterfind create backup datavol # glusterfind create antivirus_scanner datavol # glusterfind create backup datavol --force","title":"Create the session"},{"location":"GlusterFS-Tools/glusterfind/#pre-command","text":"glusterfind pre SESSION_NAME VOLUME_NAME OUTFILE glusterfind pre --help We need not specify Volume name since session already has the details. List of files will be populated in OUTFILE. To trigger the full find, call the pre command with --full argument. Multiple crawlers are available for incremental find, we can choose crawl type with --crawl argument. Examples, # glusterfind pre backup datavol /root/backup.txt # glusterfind pre backup datavol /root/backup.txt --full # # Changelog based crawler, works only for incremental # glusterfind pre backup datavol /root/backup.txt --crawler=changelog # # Find based crawler, works for both full and incremental # glusterfind pre backup datavol /root/backup.txt --crawler=brickfind Output file contains list of files/dirs relative to the Volume mount, if we need to prefix with any path to have absolute path then, # glusterfind pre backup datavol /root/backup.txt --file-prefix=/mnt/datavol/","title":"Pre Command"},{"location":"GlusterFS-Tools/glusterfind/#list-command","text":"To get the list of sessions and respective session time, glusterfind list [--session SESSION_NAME] [--volume VOLUME_NAME] Examples, # glusterfind list # glusterfind list --session backup Example output, SESSION VOLUME SESSION TIME --------------------------------------------------------------------------- backup datavol 2015-03-04 17:35:34","title":"List Command"},{"location":"GlusterFS-Tools/glusterfind/#post-command","text":"glusterfind post SESSION_NAME VOLUME_NAME Examples, # glusterfind post backup datavol","title":"Post Command"},{"location":"GlusterFS-Tools/glusterfind/#delete-command","text":"glusterfind delete SESSION_NAME VOLUME_NAME Examples, # glusterfind delete backup datavol","title":"Delete Command"},{"location":"GlusterFS-Tools/glusterfind/#adding-more-crawlers","text":"Adding more crawlers is very simple, Add an entry in $GLUSTERD_WORKDIR/glusterfind.conf . glusterfind can choose your crawler using --crawl argument. [crawlers] changelog=/usr/libexec/glusterfs/glusterfind/changelog.py brickfind=/usr/libexec/glusterfs/glusterfind/brickfind.py For example, if you have a multithreaded brick crawler, say parallelbrickcrawl add it to the conf file. [crawlers] changelog=/usr/libexec/glusterfs/glusterfind/changelog.py brickfind=/usr/libexec/glusterfs/glusterfind/brickfind.py parallelbrickcrawl=/root/parallelbrickcrawl Custom crawler can be executable script/binary which accepts volume name, brick path, output_file and start time(and optional debug flag) For example, /root/parallelbrickcrawl SESSION_NAME VOLUME BRICK_PATH OUTFILE START_TIME [--debug] Where START_TIME is in unix epoch format, START_TIME will be zero for full find.","title":"Adding more Crawlers"},{"location":"GlusterFS-Tools/glusterfind/#known-issues","text":"Deleted files will not get listed, since we can't convert GFID to Path if file/dir is deleted. Only new name will get listed if Renamed. All hardlinks will get listed.","title":"Known Issues"},{"location":"Install-Guide/Common-criteria/","text":"Getting Started This tutorial will cover different options for getting a Gluster cluster up and running. Here is a rundown of the steps we need to do. To start, we will go over some common things you will need to know for setting up Gluster. Next, choose the method you want to use to set up your first cluster: Within a virtual machine To bare metal servers To EC2 instances in Amazon Finally, we will install Gluster, create a few volumes, and test using them. General Setup Principles No matter where you will be installing Gluster, it helps to understand a few key concepts on what the moving parts are. First, it is important to understand that GlusterFS isn\u2019t really a filesystem in and of itself. It concatenates existing filesystems into one (or more) big chunks so that data being written into or read out of Gluster gets distributed across multiple hosts simultaneously. This means that you can use space from any host that you have available. Typically, XFS is recommended but it can be used with other filesystems as well. Most commonly EXT4 is used when XFS isn\u2019t, but you can (and many, many people do) use another filesystem that suits you. Now that we understand that, we can define a few of the common terms used in Gluster. A trusted pool refers collectively to the hosts in a given Gluster Cluster. A node or \u201cserver\u201d refers to any server that is part of a trusted pool. In general, this assumes all nodes are in the same trusted pool. A brick is used to refer to any device (really this means filesystem) that is being used for Gluster storage. An export refers to the mount path of the brick(s) on a given server, for example, /export/brick1 The term Global Namespace is a fancy way of saying a Gluster volume A Gluster volume is a collection of one or more bricks (of course, typically this is two or more). This is analogous to /etc/exports entries for NFS. GNFS and kNFS . GNFS is how we refer to our inline NFS server. kNFS stands for kernel NFS, or, as most people would say, just plain NFS. Most often, you will want kNFS services disabled on the Gluster nodes. Gluster NFS doesn't take any additional configuration and works just like you would expect with NFSv3. It is possible to configure Gluster and NFS to live in harmony if you want to. Other notes: For this test, if you do not have DNS set up, you can get away with using /etc/hosts entries for the two nodes. However, when you move from this basic setup to using Gluster in production, correct DNS entries (forward and reverse) and NTP are essential. When you install the Operating System, do not format the Gluster storage disks! We will use specific settings with the mkfs command later on when we set up Gluster. If you are testing with a single disk (not recommended), make sure to carve out a free partition or two to be used by Gluster later, so that you can format or reformat at will during your testing. Firewalls are great, except when they aren\u2019t. For storage servers, being able to operate in a trusted environment without firewalls can mean huge gains in performance, and is recommended. In case you absolutely need to set up a firewall, have a look at Setting up clients for information on the ports used. Click here to get started","title":"Common Criteria"},{"location":"Install-Guide/Common-criteria/#getting-started","text":"This tutorial will cover different options for getting a Gluster cluster up and running. Here is a rundown of the steps we need to do. To start, we will go over some common things you will need to know for setting up Gluster. Next, choose the method you want to use to set up your first cluster: Within a virtual machine To bare metal servers To EC2 instances in Amazon Finally, we will install Gluster, create a few volumes, and test using them.","title":"Getting Started"},{"location":"Install-Guide/Common-criteria/#general-setup-principles","text":"No matter where you will be installing Gluster, it helps to understand a few key concepts on what the moving parts are. First, it is important to understand that GlusterFS isn\u2019t really a filesystem in and of itself. It concatenates existing filesystems into one (or more) big chunks so that data being written into or read out of Gluster gets distributed across multiple hosts simultaneously. This means that you can use space from any host that you have available. Typically, XFS is recommended but it can be used with other filesystems as well. Most commonly EXT4 is used when XFS isn\u2019t, but you can (and many, many people do) use another filesystem that suits you. Now that we understand that, we can define a few of the common terms used in Gluster. A trusted pool refers collectively to the hosts in a given Gluster Cluster. A node or \u201cserver\u201d refers to any server that is part of a trusted pool. In general, this assumes all nodes are in the same trusted pool. A brick is used to refer to any device (really this means filesystem) that is being used for Gluster storage. An export refers to the mount path of the brick(s) on a given server, for example, /export/brick1 The term Global Namespace is a fancy way of saying a Gluster volume A Gluster volume is a collection of one or more bricks (of course, typically this is two or more). This is analogous to /etc/exports entries for NFS. GNFS and kNFS . GNFS is how we refer to our inline NFS server. kNFS stands for kernel NFS, or, as most people would say, just plain NFS. Most often, you will want kNFS services disabled on the Gluster nodes. Gluster NFS doesn't take any additional configuration and works just like you would expect with NFSv3. It is possible to configure Gluster and NFS to live in harmony if you want to. Other notes: For this test, if you do not have DNS set up, you can get away with using /etc/hosts entries for the two nodes. However, when you move from this basic setup to using Gluster in production, correct DNS entries (forward and reverse) and NTP are essential. When you install the Operating System, do not format the Gluster storage disks! We will use specific settings with the mkfs command later on when we set up Gluster. If you are testing with a single disk (not recommended), make sure to carve out a free partition or two to be used by Gluster later, so that you can format or reformat at will during your testing. Firewalls are great, except when they aren\u2019t. For storage servers, being able to operate in a trusted environment without firewalls can mean huge gains in performance, and is recommended. In case you absolutely need to set up a firewall, have a look at Setting up clients for information on the ports used. Click here to get started","title":"General Setup Principles"},{"location":"Install-Guide/Community-Packages/","text":"Community Packages GlusterFS Tentative plans for community convenience packages. A yes means packages are (or will be) provided in the respective repository. A no means no plans to build new updates. Existing packages will remain in the repos. The following GlusterFS versions have reached EOL[1]: 8, 7, 6 and earlier. 10 9 CentOS Storage SIG[2] 7 no yes 8 yes yes Stream 8 yes yes Stream 9 yes yes Fedora[3] F34 yes yes\u00b9 F35 yes yes\u00b9 F36(rawhide) yes yes\u00b9 Debian[3] Stretch/9 no yes Buster/10 yes yes Bullseye/11 yes yes Bookworm/12(sid) yes yes Ubuntu Launchpad[4] Xenial/16.04 no yes Bionic/18.04 yes yes Focal/20.04 yes yes Impish/21.10 yes yes Jammy/22.04 yes yes OpenSUSE Build Service[5] Leap15.2 no yes Leap15.3 yes yes Leap15.4 yes yes SLES12SP5 no yes SLES15SP2 no yes SLES15SP3 yes yes SLES15SP4 yes yes Tumbleweed yes yes NOTE - We are not building Debian arm packages due to resource constraints for a while now. There will be only amd64 packages present on download.gluster.org Related Packages glusterfs-selinux gdeploy gluster-block glusterfs-coreutils nfs-ganesha Samba CentOS Storage SIG[2] 7 yes yes yes yes yes yes 8 yes tbd yes yes yes yes Stream 8 yes tbd yes yes yes yes Stream 9 yes tbd yes yes yes yes Fedora[3] F34 yes yes yes yes yes ? F35 yes yes yes yes yes ? F36(rawhide) yes yes yes yes yes ? Debian[3] Stretch/9 n/a no no yes yes ? Buster/10 n/a no no yes yes ? Bullseye/11 n/a no no yes yes ? Bookworm/12(sid) n/a no no yes yes ? Ubuntu Launchpad[4] Xenial/16.04 n/a/ no no yes yes ? Bionic/18.04 n/a no no yes yes ? Focal/20.04 n/a no no yes yes ? Impish/21.10 n/a no no yes yes ? Jammy/22.04 n/a no no yes yes ? OpenSUSE Build Service[5] Leap15.2 n/a yes yes yes yes ? Leap15.3 n/a yes yes yes yes ? Leap15.4 n/a yes yes yes yes ? SLES12SP5 n/a yes yes yes yes ? SLES15SP2 n/a yes yes yes yes ? SLES15SP3 n/a yes yes yes yes ? SLES15SP4 n/a yes yes yes yes ? Tumbleweed n/a yes yes yes yes ? [1] https://www.gluster.org/release-schedule/ [2] https://wiki.centos.org/SpecialInterestGroup/Storage [3] https://download.gluster.org/pub/gluster/glusterfs [4] https://launchpad.net/~gluster [5] http://download.opensuse.org/repositories/home:/glusterfs:/ \u00b9 Fedora Updates, UpdatesTesting, or Rawhide repository. Use dnf to install.","title":"Community Packages"},{"location":"Install-Guide/Community-Packages/#community-packages","text":"","title":"Community Packages"},{"location":"Install-Guide/Community-Packages/#glusterfs","text":"Tentative plans for community convenience packages. A yes means packages are (or will be) provided in the respective repository. A no means no plans to build new updates. Existing packages will remain in the repos. The following GlusterFS versions have reached EOL[1]: 8, 7, 6 and earlier. 10 9 CentOS Storage SIG[2] 7 no yes 8 yes yes Stream 8 yes yes Stream 9 yes yes Fedora[3] F34 yes yes\u00b9 F35 yes yes\u00b9 F36(rawhide) yes yes\u00b9 Debian[3] Stretch/9 no yes Buster/10 yes yes Bullseye/11 yes yes Bookworm/12(sid) yes yes Ubuntu Launchpad[4] Xenial/16.04 no yes Bionic/18.04 yes yes Focal/20.04 yes yes Impish/21.10 yes yes Jammy/22.04 yes yes OpenSUSE Build Service[5] Leap15.2 no yes Leap15.3 yes yes Leap15.4 yes yes SLES12SP5 no yes SLES15SP2 no yes SLES15SP3 yes yes SLES15SP4 yes yes Tumbleweed yes yes NOTE - We are not building Debian arm packages due to resource constraints for a while now. There will be only amd64 packages present on download.gluster.org","title":"GlusterFS"},{"location":"Install-Guide/Community-Packages/#related-packages","text":"glusterfs-selinux gdeploy gluster-block glusterfs-coreutils nfs-ganesha Samba CentOS Storage SIG[2] 7 yes yes yes yes yes yes 8 yes tbd yes yes yes yes Stream 8 yes tbd yes yes yes yes Stream 9 yes tbd yes yes yes yes Fedora[3] F34 yes yes yes yes yes ? F35 yes yes yes yes yes ? F36(rawhide) yes yes yes yes yes ? Debian[3] Stretch/9 n/a no no yes yes ? Buster/10 n/a no no yes yes ? Bullseye/11 n/a no no yes yes ? Bookworm/12(sid) n/a no no yes yes ? Ubuntu Launchpad[4] Xenial/16.04 n/a/ no no yes yes ? Bionic/18.04 n/a no no yes yes ? Focal/20.04 n/a no no yes yes ? Impish/21.10 n/a no no yes yes ? Jammy/22.04 n/a no no yes yes ? OpenSUSE Build Service[5] Leap15.2 n/a yes yes yes yes ? Leap15.3 n/a yes yes yes yes ? Leap15.4 n/a yes yes yes yes ? SLES12SP5 n/a yes yes yes yes ? SLES15SP2 n/a yes yes yes yes ? SLES15SP3 n/a yes yes yes yes ? SLES15SP4 n/a yes yes yes yes ? Tumbleweed n/a yes yes yes yes ? [1] https://www.gluster.org/release-schedule/ [2] https://wiki.centos.org/SpecialInterestGroup/Storage [3] https://download.gluster.org/pub/gluster/glusterfs [4] https://launchpad.net/~gluster [5] http://download.opensuse.org/repositories/home:/glusterfs:/ \u00b9 Fedora Updates, UpdatesTesting, or Rawhide repository. Use dnf to install.","title":"Related Packages"},{"location":"Install-Guide/Configure/","text":"Configure Firewall For the Gluster to communicate within a cluster either the firewalls have to be turned off or enable communication for each server. # iptables -I INPUT -p all -s `<ip-address>` -j ACCEPT Configure the trusted pool Remember that the trusted pool is the term used to define a cluster of nodes in Gluster. Choose a server to be your \u201cprimary\u201d server. This is just to keep things simple, you will generally want to run all commands from this tutorial. Keep in mind, running many Gluster specific commands (like gluster volume create ) on one server in the cluster will execute the same command on all other servers. Replace nodename with hostname of the other server in the cluster, or IP address if you don\u2019t have DNS or /etc/hosts entries. Let say we want to connect to node02 : # gluster peer probe node02 Notice that running gluster peer status from the second node shows that the first node has already been added. Partition the disk Assuming you have an empty disk at /dev/sdb : (You can check the partitions on your system using fdisk -l ) # fdisk /dev/sdb And then create a single XFS partition using fdisk Format the partition # mkfs.xfs -i size=512 /dev/sdb1 Add an entry to /etc/fstab # echo \"/dev/sdb1 /export/sdb1 xfs defaults 0 0\" >> /etc/fstab Mount the partition as a Gluster \"brick\" # mkdir -p /export/sdb1 && mount -a && mkdir -p /export/sdb1/brick Set up a Gluster volume The most basic Gluster volume type is a \u201cDistribute only\u201d volume (also referred to as a \u201cpure DHT\u201d volume if you want to impress the folks at the water cooler). This type of volume simply distributes the data evenly across the available bricks in a volume. So, if I write 100 files, on average, fifty will end up on one server, and fifty will end up on another. This is faster than a \u201creplicated\u201d volume, but isn\u2019t as popular since it doesn\u2019t give you two of the most sought after features of Gluster \u2014 multiple copies of the data, and automatic failover if something goes wrong. To set up a replicated volume: # gluster volume create gv0 replica 3 node01.mydomain.net:/export/sdb1/brick \\ node02.mydomain.net:/export/sdb1/brick \\ node03.mydomain.net:/export/sdb1/brick Breaking this down into pieces: the first part says to create a gluster volume named gv0 (the name is arbitrary, gv0 was chosen simply because it\u2019s less typing than gluster_volume_0 ). make the volume a replica volume keep a copy of the data on at least 3 bricks at any given time. Since we only have three bricks total, this means each server will house a copy of the data. we specify which nodes to use, and which bricks on those nodes. The order here is important when you have more bricks. It is possible (as of the most current release as of this writing, Gluster 3.3) to specify the bricks in such a way that you would make both copies of the data reside on a single node. This would make for an embarrassing explanation to your boss when your bulletproof, completely redundant, always on super cluster comes to a grinding halt when a single point of failure occurs. Now, we can check to make sure things are working as expected: # gluster volume info And you should see results similar to the following: Volume Name: gv0 Type: Replicate Volume ID: 8bc3e96b-a1b6-457d-8f7a-a91d1d4dc019 Status: Created Number of Bricks: 1 x 3 = 3 Transport-type: tcp Bricks: Brick1: node01.yourdomain.net:/export/sdb1/brick Brick2: node02.yourdomain.net:/export/sdb1/brick Brick3: node03.yourdomain.net:/export/sdb1/brick This shows us essentially what we just specified during the volume creation. The one this to mention is the Status . A status of Created means that the volume has been created, but hasn\u2019t yet been started, which would cause any attempt to mount the volume fail. Now, we should start the volume. # gluster volume start gv0 Find all documentation here","title":"Configure"},{"location":"Install-Guide/Configure/#configure-firewall","text":"For the Gluster to communicate within a cluster either the firewalls have to be turned off or enable communication for each server. # iptables -I INPUT -p all -s `<ip-address>` -j ACCEPT","title":"Configure Firewall"},{"location":"Install-Guide/Configure/#configure-the-trusted-pool","text":"Remember that the trusted pool is the term used to define a cluster of nodes in Gluster. Choose a server to be your \u201cprimary\u201d server. This is just to keep things simple, you will generally want to run all commands from this tutorial. Keep in mind, running many Gluster specific commands (like gluster volume create ) on one server in the cluster will execute the same command on all other servers. Replace nodename with hostname of the other server in the cluster, or IP address if you don\u2019t have DNS or /etc/hosts entries. Let say we want to connect to node02 : # gluster peer probe node02 Notice that running gluster peer status from the second node shows that the first node has already been added.","title":"Configure the trusted pool"},{"location":"Install-Guide/Configure/#partition-the-disk","text":"Assuming you have an empty disk at /dev/sdb : (You can check the partitions on your system using fdisk -l ) # fdisk /dev/sdb And then create a single XFS partition using fdisk","title":"Partition the disk"},{"location":"Install-Guide/Configure/#format-the-partition","text":"# mkfs.xfs -i size=512 /dev/sdb1","title":"Format the partition"},{"location":"Install-Guide/Configure/#add-an-entry-to-etcfstab","text":"# echo \"/dev/sdb1 /export/sdb1 xfs defaults 0 0\" >> /etc/fstab","title":"Add an entry to /etc/fstab"},{"location":"Install-Guide/Configure/#mount-the-partition-as-a-gluster-brick","text":"# mkdir -p /export/sdb1 && mount -a && mkdir -p /export/sdb1/brick","title":"Mount the partition as a Gluster \"brick\""},{"location":"Install-Guide/Configure/#set-up-a-gluster-volume","text":"The most basic Gluster volume type is a \u201cDistribute only\u201d volume (also referred to as a \u201cpure DHT\u201d volume if you want to impress the folks at the water cooler). This type of volume simply distributes the data evenly across the available bricks in a volume. So, if I write 100 files, on average, fifty will end up on one server, and fifty will end up on another. This is faster than a \u201creplicated\u201d volume, but isn\u2019t as popular since it doesn\u2019t give you two of the most sought after features of Gluster \u2014 multiple copies of the data, and automatic failover if something goes wrong. To set up a replicated volume: # gluster volume create gv0 replica 3 node01.mydomain.net:/export/sdb1/brick \\ node02.mydomain.net:/export/sdb1/brick \\ node03.mydomain.net:/export/sdb1/brick Breaking this down into pieces: the first part says to create a gluster volume named gv0 (the name is arbitrary, gv0 was chosen simply because it\u2019s less typing than gluster_volume_0 ). make the volume a replica volume keep a copy of the data on at least 3 bricks at any given time. Since we only have three bricks total, this means each server will house a copy of the data. we specify which nodes to use, and which bricks on those nodes. The order here is important when you have more bricks. It is possible (as of the most current release as of this writing, Gluster 3.3) to specify the bricks in such a way that you would make both copies of the data reside on a single node. This would make for an embarrassing explanation to your boss when your bulletproof, completely redundant, always on super cluster comes to a grinding halt when a single point of failure occurs. Now, we can check to make sure things are working as expected: # gluster volume info And you should see results similar to the following: Volume Name: gv0 Type: Replicate Volume ID: 8bc3e96b-a1b6-457d-8f7a-a91d1d4dc019 Status: Created Number of Bricks: 1 x 3 = 3 Transport-type: tcp Bricks: Brick1: node01.yourdomain.net:/export/sdb1/brick Brick2: node02.yourdomain.net:/export/sdb1/brick Brick3: node03.yourdomain.net:/export/sdb1/brick This shows us essentially what we just specified during the volume creation. The one this to mention is the Status . A status of Created means that the volume has been created, but hasn\u2019t yet been started, which would cause any attempt to mount the volume fail. Now, we should start the volume. # gluster volume start gv0 Find all documentation here","title":"Set up a Gluster volume"},{"location":"Install-Guide/Install/","text":"Installing Gluster For RPM based distributions, if you will be using InfiniBand, add the glusterfs RDMA package to the installations. For RPM based systems, yum/dnf is used as the install method in order to satisfy external depencies such as compat-readline5 Community Packages Packages are provided according to this table . For Debian Add the GPG key to apt: wget -O - https://download.gluster.org/pub/gluster/glusterfs/9/rsa.pub | apt-key add - If the rsa.pub is not available at the above location, please look here https://download.gluster.org/pub/gluster/glusterfs/7/rsa.pub and add the GPG key to apt: wget -O - https://download.gluster.org/pub/gluster/glusterfs/7/rsa.pub | apt-key add - Add the source: DEBID=$(grep 'VERSION_ID=' /etc/os-release | cut -d '=' -f 2 | tr -d '\"') DEBVER=$(grep 'VERSION=' /etc/os-release | grep -Eo '[a-z]+') DEBARCH=$(dpkg --print-architecture) echo deb https://download.gluster.org/pub/gluster/glusterfs/LATEST/Debian/${DEBID}/${DEBARCH}/apt ${DEBVER} main > /etc/apt/sources.list.d/gluster.list Update package list: apt update Install: apt install glusterfs-server For Ubuntu Install software-properties-common: apt install software-properties-common Then add the community GlusterFS PPA: add-apt-repository ppa:gluster/glusterfs-7 apt update Finally, install the packages: apt install glusterfs-server Note: Packages exist for Ubuntu 16.04 LTS, 18.04 LTS, 20.04 LTS, 20.10, 21.04 For Red Hat/CentOS RPMs for CentOS and other RHEL clones are available from the CentOS Storage SIG mirrors. For more installation details refer Gluster Quick start guide from CentOS Storage SIG. For Fedora Install the Gluster packages: dnf install glusterfs-server Once you are finished installing, you can move on to configuration section. For Arch Linux Install the Gluster package: pacman -S glusterfs","title":"Install"},{"location":"Install-Guide/Install/#installing-gluster","text":"For RPM based distributions, if you will be using InfiniBand, add the glusterfs RDMA package to the installations. For RPM based systems, yum/dnf is used as the install method in order to satisfy external depencies such as compat-readline5","title":"Installing Gluster"},{"location":"Install-Guide/Install/#community-packages","text":"Packages are provided according to this table .","title":"Community Packages"},{"location":"Install-Guide/Install/#for-debian","text":"Add the GPG key to apt: wget -O - https://download.gluster.org/pub/gluster/glusterfs/9/rsa.pub | apt-key add - If the rsa.pub is not available at the above location, please look here https://download.gluster.org/pub/gluster/glusterfs/7/rsa.pub and add the GPG key to apt: wget -O - https://download.gluster.org/pub/gluster/glusterfs/7/rsa.pub | apt-key add - Add the source: DEBID=$(grep 'VERSION_ID=' /etc/os-release | cut -d '=' -f 2 | tr -d '\"') DEBVER=$(grep 'VERSION=' /etc/os-release | grep -Eo '[a-z]+') DEBARCH=$(dpkg --print-architecture) echo deb https://download.gluster.org/pub/gluster/glusterfs/LATEST/Debian/${DEBID}/${DEBARCH}/apt ${DEBVER} main > /etc/apt/sources.list.d/gluster.list Update package list: apt update Install: apt install glusterfs-server","title":"For Debian"},{"location":"Install-Guide/Install/#for-ubuntu","text":"Install software-properties-common: apt install software-properties-common Then add the community GlusterFS PPA: add-apt-repository ppa:gluster/glusterfs-7 apt update Finally, install the packages: apt install glusterfs-server Note: Packages exist for Ubuntu 16.04 LTS, 18.04 LTS, 20.04 LTS, 20.10, 21.04","title":"For Ubuntu"},{"location":"Install-Guide/Install/#for-red-hatcentos","text":"RPMs for CentOS and other RHEL clones are available from the CentOS Storage SIG mirrors. For more installation details refer Gluster Quick start guide from CentOS Storage SIG.","title":"For Red Hat/CentOS"},{"location":"Install-Guide/Install/#for-fedora","text":"Install the Gluster packages: dnf install glusterfs-server Once you are finished installing, you can move on to configuration section.","title":"For Fedora"},{"location":"Install-Guide/Install/#for-arch-linux","text":"Install the Gluster package: pacman -S glusterfs","title":"For Arch Linux"},{"location":"Install-Guide/Overview/","text":"Overview Purpose The Install Guide (IG) is aimed at providing the sequence of steps needed for setting up Gluster. It contains a reasonable degree of detail which helps an administrator to understand the terminology, the choices and how to configure the deployment to the storage needs of their application workload. The Quick Start Guide (QSG) is designed to get a deployment with default choices and is aimed at those who want to spend less time to get to a deployment. After you deploy Gluster by following these steps, we recommend that you read the Gluster Admin Guide to learn how to administer Gluster and how to select a volume type that fits your needs. Also, be sure to enlist the help of the Gluster community via the IRC or, Slack channels (see https://www.gluster.org/community/) or Q&A section. Overview Before we begin, let\u2019s talk about what Gluster is, address a few myths and misconceptions, and define a few terms. This will help you to avoid some of the common issues that others encounter as they start their journey with Gluster. What is Gluster Gluster is a distributed scale-out filesystem that allows rapid provisioning of additional storage based on your storage consumption needs. It incorporates automatic failover as a primary feature. All of this is accomplished without a centralized metadata server. What is Gluster without making me learn an extra glossary of terminology? Gluster is an easy way to provision your own storage backend NAS using almost any hardware you choose. You can add as much as you want to start with, and if you need more later, adding more takes just a few steps. You can configure failover automatically, so that if a server goes down, you don\u2019t lose access to the data. No manual steps are required for failover. When you fix the server that failed and bring it back online, you don\u2019t have to do anything to get the data back except wait. In the meantime, the most current copy of your data keeps getting served from the node that was still running. You can build a clustered filesystem in a matter of minutes\u2026 it is trivially easy for basic setups It takes advantage of what we refer to as \u201ccommodity hardware\u201d, which means, we run on just about any hardware you can think of, from that stack of decomm\u2019s and gigabit switches in the corner no one can figure out what to do with (how many license servers do you really need, after all?), to that dream array you were speccing out online. Don\u2019t worry, I won\u2019t tell your boss. It takes advantage of commodity software too. No need to mess with kernels or fine tune the OS to a tee. We run on top of most unix filesystems, with XFS and ext4 being the most popular choices. We do have some recommendations for more heavily utilized arrays, but these are simple to implement and you probably have some of these configured already anyway. Gluster data can be accessed from just about anywhere \u2013 You can use traditional NFS, SMB/CIFS for Windows clients, or our own native GlusterFS (a few additional packages are needed on the client machines for this, but as you will see, they are quite small). There are even more advanced features than this, but for now we will focus on the basics. It\u2019s not just a toy. Gluster is enterprise-ready, and commercial support is available if you need it. It is used in some of the most taxing environments like media serving, natural resource exploration, medical imaging, and even as a filesystem for Big Data. Is Gluster going to work for me and what I need it to do? Most likely, yes. People use Gluster for storage needs of a variety of application workloads. You are encouraged to ask around in our IRC or, Slack channels or Q&A forums to see if anyone has tried something similar. That being said, there are a few places where Gluster is going to need more consideration than others. Accessing Gluster from SMB/CIFS is often going to be slow by most people\u2019s standards. If you only moderate access by users, then it most likely won\u2019t be an issue for you. On the other hand, adding enough Gluster servers into the mix, some people have seen better performance with us than other solutions due to the scale out nature of the technology Gluster is traditionally better when using file sizes of at least 16KB (with a sweet spot around 128KB or so). What is the cost and complexity required to set up cluster? Question: How many billions of dollars is it going to cost to setup a cluster? Don\u2019t I need redundant networking, super fast SSD\u2019s, technology from Alpha Centauri delivered by men in black, etc\u2026? I have never seen anyone spend even close to a billion, unless they got the rust proof coating on the servers. You don\u2019t seem like the type that would get bamboozled like that, so have no fear. For the purpose of this tutorial, if your laptop can run two VM\u2019s with 1GB of memory each, you can get started testing and the only thing you are going to pay for is coffee (assuming the coffee shop doesn\u2019t make you pay them back for the electricity to power your laptop). If you want to test on bare metal, since Gluster is built with commodity hardware in mind, and because there is no centralized meta-data server, a very simple cluster can be deployed with two basic servers (2 CPU\u2019s, 4GB of RAM each, 1 Gigabit network). This is sufficient to have a nice file share or a place to put some nightly backups. Gluster is deployed successfully on all kinds of disks, from the lowliest 5200 RPM SATA to mightiest 1.21 gigawatt SSD\u2019s. The more performance you need, the more consideration you will want to put into how much hardware to buy, but the great thing about Gluster is that you can start small, and add on as your needs grow. OK, but if I add servers on later, don\u2019t they have to be exactly the same? In a perfect world, sure. Having the hardware be the same means less troubleshooting when the fires start popping up. But plenty of people deploy Gluster on mix and match hardware, and successfully. Get started by checking some Common Criteria","title":"Overview"},{"location":"Install-Guide/Overview/#overview","text":"","title":"Overview"},{"location":"Install-Guide/Overview/#purpose","text":"The Install Guide (IG) is aimed at providing the sequence of steps needed for setting up Gluster. It contains a reasonable degree of detail which helps an administrator to understand the terminology, the choices and how to configure the deployment to the storage needs of their application workload. The Quick Start Guide (QSG) is designed to get a deployment with default choices and is aimed at those who want to spend less time to get to a deployment. After you deploy Gluster by following these steps, we recommend that you read the Gluster Admin Guide to learn how to administer Gluster and how to select a volume type that fits your needs. Also, be sure to enlist the help of the Gluster community via the IRC or, Slack channels (see https://www.gluster.org/community/) or Q&A section.","title":"Purpose"},{"location":"Install-Guide/Overview/#overview_1","text":"Before we begin, let\u2019s talk about what Gluster is, address a few myths and misconceptions, and define a few terms. This will help you to avoid some of the common issues that others encounter as they start their journey with Gluster.","title":"Overview"},{"location":"Install-Guide/Overview/#what-is-gluster","text":"Gluster is a distributed scale-out filesystem that allows rapid provisioning of additional storage based on your storage consumption needs. It incorporates automatic failover as a primary feature. All of this is accomplished without a centralized metadata server.","title":"What is Gluster"},{"location":"Install-Guide/Overview/#what-is-gluster-without-making-me-learn-an-extra-glossary-of-terminology","text":"Gluster is an easy way to provision your own storage backend NAS using almost any hardware you choose. You can add as much as you want to start with, and if you need more later, adding more takes just a few steps. You can configure failover automatically, so that if a server goes down, you don\u2019t lose access to the data. No manual steps are required for failover. When you fix the server that failed and bring it back online, you don\u2019t have to do anything to get the data back except wait. In the meantime, the most current copy of your data keeps getting served from the node that was still running. You can build a clustered filesystem in a matter of minutes\u2026 it is trivially easy for basic setups It takes advantage of what we refer to as \u201ccommodity hardware\u201d, which means, we run on just about any hardware you can think of, from that stack of decomm\u2019s and gigabit switches in the corner no one can figure out what to do with (how many license servers do you really need, after all?), to that dream array you were speccing out online. Don\u2019t worry, I won\u2019t tell your boss. It takes advantage of commodity software too. No need to mess with kernels or fine tune the OS to a tee. We run on top of most unix filesystems, with XFS and ext4 being the most popular choices. We do have some recommendations for more heavily utilized arrays, but these are simple to implement and you probably have some of these configured already anyway. Gluster data can be accessed from just about anywhere \u2013 You can use traditional NFS, SMB/CIFS for Windows clients, or our own native GlusterFS (a few additional packages are needed on the client machines for this, but as you will see, they are quite small). There are even more advanced features than this, but for now we will focus on the basics. It\u2019s not just a toy. Gluster is enterprise-ready, and commercial support is available if you need it. It is used in some of the most taxing environments like media serving, natural resource exploration, medical imaging, and even as a filesystem for Big Data.","title":"What is Gluster without making me learn an extra glossary of terminology?"},{"location":"Install-Guide/Overview/#is-gluster-going-to-work-for-me-and-what-i-need-it-to-do","text":"Most likely, yes. People use Gluster for storage needs of a variety of application workloads. You are encouraged to ask around in our IRC or, Slack channels or Q&A forums to see if anyone has tried something similar. That being said, there are a few places where Gluster is going to need more consideration than others. Accessing Gluster from SMB/CIFS is often going to be slow by most people\u2019s standards. If you only moderate access by users, then it most likely won\u2019t be an issue for you. On the other hand, adding enough Gluster servers into the mix, some people have seen better performance with us than other solutions due to the scale out nature of the technology Gluster is traditionally better when using file sizes of at least 16KB (with a sweet spot around 128KB or so).","title":"Is Gluster going to work for me and what I need it to do?"},{"location":"Install-Guide/Overview/#what-is-the-cost-and-complexity-required-to-set-up-cluster","text":"Question: How many billions of dollars is it going to cost to setup a cluster? Don\u2019t I need redundant networking, super fast SSD\u2019s, technology from Alpha Centauri delivered by men in black, etc\u2026? I have never seen anyone spend even close to a billion, unless they got the rust proof coating on the servers. You don\u2019t seem like the type that would get bamboozled like that, so have no fear. For the purpose of this tutorial, if your laptop can run two VM\u2019s with 1GB of memory each, you can get started testing and the only thing you are going to pay for is coffee (assuming the coffee shop doesn\u2019t make you pay them back for the electricity to power your laptop). If you want to test on bare metal, since Gluster is built with commodity hardware in mind, and because there is no centralized meta-data server, a very simple cluster can be deployed with two basic servers (2 CPU\u2019s, 4GB of RAM each, 1 Gigabit network). This is sufficient to have a nice file share or a place to put some nightly backups. Gluster is deployed successfully on all kinds of disks, from the lowliest 5200 RPM SATA to mightiest 1.21 gigawatt SSD\u2019s. The more performance you need, the more consideration you will want to put into how much hardware to buy, but the great thing about Gluster is that you can start small, and add on as your needs grow.","title":"What is the cost and complexity required to set up cluster?"},{"location":"Install-Guide/Overview/#ok-but-if-i-add-servers-on-later-dont-they-have-to-be-exactly-the-same","text":"In a perfect world, sure. Having the hardware be the same means less troubleshooting when the fires start popping up. But plenty of people deploy Gluster on mix and match hardware, and successfully. Get started by checking some Common Criteria","title":"OK, but if I add servers on later, don\u2019t they have to be exactly the same?"},{"location":"Install-Guide/Setup-Bare-metal/","text":"Setup Bare Metal Note: You only need one of the three setup methods! Setup, Method 2 \u2013 Setting up on physical servers To set up Gluster on physical servers, we recommend two servers of very modest specifications (2 CPUs, 2GB of RAM, 1GBE). Since we are dealing with physical hardware here, keep in mind, what we are showing here is for testing purposes. In the end, remember that forces beyond your control (aka, your bosses\u2019 boss...) can force you to take that the \u201cjust for a quick test\u201d environment right into production, despite your kicking and screaming against it. To prevent this, it can be a good idea to deploy your test environment as much as possible the same way you would to a production environment (in case it becomes one, as mentioned above). That being said, here is a reminder of some of the best practices we mentioned before: Make sure DNS and NTP are setup, correct, and working If you have access to a backend storage network, use it! 10GBE or InfiniBand are great if you have access to them, but even a 1GBE backbone can help you get the most out of your deployment. Make sure that the interfaces you are going to use are also in DNS since we will be using the hostnames when we deploy Gluster When it comes to disks, the more the merrier. Although you could technically fake things out with a single disk, there would be performance issues as soon as you tried to do any real work on the servers With the explosion of commodity hardware, you don\u2019t need to be a hardware expert these days to deploy a server. Although this is generally a good thing, it also means that paying attention to some important, performance-impacting BIOS settings is commonly ignored. Several points that might cause issues when if you're unaware of them: Most manufacturers enable power saving mode by default. This is a great idea for servers that do not have high-performance requirements. For the average storage server, the performance-impact of the power savings is not a reasonable tradeoff Newer motherboards and processors have lots of nifty features! Enhancements in virtualization, newer ways of doing predictive algorithms and NUMA are just a few to mention. To be safe, many manufactures ship hardware with settings meant to work with as massive a variety of workloads and configurations as they have customers. One issue you could face is when you set up that blazing-fast 10GBE card you were so thrilled about installing? In many cases, it would end up being crippled by a default 1x speed put in place on the PCI-E bus by the motherboard. Thankfully, most manufacturers show all the BIOS settings, including the defaults, right in the manual. It only takes a few minutes to download, and you don\u2019t even have to power off the server unless you need to make changes. More and more boards include the functionality to make changes in the BIOS on the fly without even powering the box off. One word of caution of course, is don\u2019t go too crazy. Fretting over each tiny little detail and setting is usually not worth the time, and the more changes you make, the more you need to document and implement later. Try to find the happy balance between time spent managing the hardware (which ideally should be as close to zero after you setup initially) and the expected gains you get back from it. Finally, remember that some hardware really is better than others. Without pointing fingers anywhere specifically, it is often true that onboard components are not as robust as add-ons. As a general rule, you can safely delegate the onboard hardware to things like management network for the NIC\u2019s, and for installing the OS onto a SATA drive. At least twice a year you should check the manufacturer's website for bulletins about your hardware. Critical performance issues are often resolved with a simple driver or firmware update. As often as not, these updates affect the two most critical pieces of hardware on a machine you want to use for networked storage - the RAID controller and the NIC's. Once you have set up the servers and installed the OS, you are ready to move on to the install section.","title":"Setting up on physical servers"},{"location":"Install-Guide/Setup-Bare-metal/#setup-bare-metal","text":"Note: You only need one of the three setup methods!","title":"Setup Bare Metal"},{"location":"Install-Guide/Setup-Bare-metal/#setup-method-2-setting-up-on-physical-servers","text":"To set up Gluster on physical servers, we recommend two servers of very modest specifications (2 CPUs, 2GB of RAM, 1GBE). Since we are dealing with physical hardware here, keep in mind, what we are showing here is for testing purposes. In the end, remember that forces beyond your control (aka, your bosses\u2019 boss...) can force you to take that the \u201cjust for a quick test\u201d environment right into production, despite your kicking and screaming against it. To prevent this, it can be a good idea to deploy your test environment as much as possible the same way you would to a production environment (in case it becomes one, as mentioned above). That being said, here is a reminder of some of the best practices we mentioned before: Make sure DNS and NTP are setup, correct, and working If you have access to a backend storage network, use it! 10GBE or InfiniBand are great if you have access to them, but even a 1GBE backbone can help you get the most out of your deployment. Make sure that the interfaces you are going to use are also in DNS since we will be using the hostnames when we deploy Gluster When it comes to disks, the more the merrier. Although you could technically fake things out with a single disk, there would be performance issues as soon as you tried to do any real work on the servers With the explosion of commodity hardware, you don\u2019t need to be a hardware expert these days to deploy a server. Although this is generally a good thing, it also means that paying attention to some important, performance-impacting BIOS settings is commonly ignored. Several points that might cause issues when if you're unaware of them: Most manufacturers enable power saving mode by default. This is a great idea for servers that do not have high-performance requirements. For the average storage server, the performance-impact of the power savings is not a reasonable tradeoff Newer motherboards and processors have lots of nifty features! Enhancements in virtualization, newer ways of doing predictive algorithms and NUMA are just a few to mention. To be safe, many manufactures ship hardware with settings meant to work with as massive a variety of workloads and configurations as they have customers. One issue you could face is when you set up that blazing-fast 10GBE card you were so thrilled about installing? In many cases, it would end up being crippled by a default 1x speed put in place on the PCI-E bus by the motherboard. Thankfully, most manufacturers show all the BIOS settings, including the defaults, right in the manual. It only takes a few minutes to download, and you don\u2019t even have to power off the server unless you need to make changes. More and more boards include the functionality to make changes in the BIOS on the fly without even powering the box off. One word of caution of course, is don\u2019t go too crazy. Fretting over each tiny little detail and setting is usually not worth the time, and the more changes you make, the more you need to document and implement later. Try to find the happy balance between time spent managing the hardware (which ideally should be as close to zero after you setup initially) and the expected gains you get back from it. Finally, remember that some hardware really is better than others. Without pointing fingers anywhere specifically, it is often true that onboard components are not as robust as add-ons. As a general rule, you can safely delegate the onboard hardware to things like management network for the NIC\u2019s, and for installing the OS onto a SATA drive. At least twice a year you should check the manufacturer's website for bulletins about your hardware. Critical performance issues are often resolved with a simple driver or firmware update. As often as not, these updates affect the two most critical pieces of hardware on a machine you want to use for networked storage - the RAID controller and the NIC's. Once you have set up the servers and installed the OS, you are ready to move on to the install section.","title":"Setup, Method 2 \u2013 Setting up on physical servers"},{"location":"Install-Guide/Setup-aws/","text":"Setup AWS Note: You only need one of the three setup methods! Setup, Method 3 \u2013 Deploying in AWS Deploying in Amazon can be one of the fastest ways to get up and running with Gluster. Of course, most of what we cover here will work with other cloud platforms. Deploy at least two instances. For testing, you can use micro instances (I even go as far as using spot instances in most cases). Debates rage on what size instance to use in production, and there is really no correct answer. As with most things, the real answer is \u201cwhatever works for you\u201d, where the trade-offs between cost and performance are balanced in a continual dance of trying to make your project successful while making sure there is enough money left over in the budget for you to get that sweet new ping pong table in the break room. For cloud platforms, your data is wide open right from the start. As such, you shouldn\u2019t allow open access to all ports in your security groups if you plan to put a single piece of even the least valuable information on the test instances. By least valuable, I mean \u201cCash value of this coupon is 1/100th of 1 cent\u201d kind of least valuable. Don\u2019t be the next one to end up as a breaking news flash on the latest inconsiderate company to allow their data to fall into the hands of the baddies. See Step 2 for the minimum ports you will need open to use Gluster You can use the free \u201cephemeral\u201d storage for the Gluster bricks during testing, but make sure to use some form of protection against data loss when you move to production. Typically this means EBS backed volumes or using S3 to periodically back up your data bricks. Other notes: In production, it is recommended to replicate your VM\u2019s across multiple zones. For purpose of this tutorial, it is overkill, but if anyone is interested in this please let us know since we are always looking to write articles on the most requested features and questions. Using EBS volumes and Elastic IPs are also recommended in production. For testing, you can safely ignore these as long as you are aware that the data could be lost at any moment, so make sure your test deployment is just that, testing only. Performance can fluctuate wildly in a cloud environment. If performance issues are seen, there are several possible strategies, but keep in mind that this is the perfect place to take advantage of the scale-out capability of Gluster. While it is not true in all cases that deploying more instances will necessarily result in a \u201cfaster\u201d cluster, in general, you will see that adding more nodes means more performance for the cluster overall. If a node reboots, you will typically need to do some extra work to get Gluster running again using the default EC2 configuration. If a node is shut down, it can mean absolute loss of the node (depending on how you set things up). This is well beyond the scope of this document but is discussed in any number of AWS-related forums and posts. Since I found out the hard way myself (oh, so you read the manual every time?!), I thought it worth at least mentioning. Once you have both instances up, you can proceed to the install page.","title":"Deploying in AWS"},{"location":"Install-Guide/Setup-aws/#setup-aws","text":"Note: You only need one of the three setup methods!","title":"Setup AWS"},{"location":"Install-Guide/Setup-aws/#setup-method-3-deploying-in-aws","text":"Deploying in Amazon can be one of the fastest ways to get up and running with Gluster. Of course, most of what we cover here will work with other cloud platforms. Deploy at least two instances. For testing, you can use micro instances (I even go as far as using spot instances in most cases). Debates rage on what size instance to use in production, and there is really no correct answer. As with most things, the real answer is \u201cwhatever works for you\u201d, where the trade-offs between cost and performance are balanced in a continual dance of trying to make your project successful while making sure there is enough money left over in the budget for you to get that sweet new ping pong table in the break room. For cloud platforms, your data is wide open right from the start. As such, you shouldn\u2019t allow open access to all ports in your security groups if you plan to put a single piece of even the least valuable information on the test instances. By least valuable, I mean \u201cCash value of this coupon is 1/100th of 1 cent\u201d kind of least valuable. Don\u2019t be the next one to end up as a breaking news flash on the latest inconsiderate company to allow their data to fall into the hands of the baddies. See Step 2 for the minimum ports you will need open to use Gluster You can use the free \u201cephemeral\u201d storage for the Gluster bricks during testing, but make sure to use some form of protection against data loss when you move to production. Typically this means EBS backed volumes or using S3 to periodically back up your data bricks. Other notes: In production, it is recommended to replicate your VM\u2019s across multiple zones. For purpose of this tutorial, it is overkill, but if anyone is interested in this please let us know since we are always looking to write articles on the most requested features and questions. Using EBS volumes and Elastic IPs are also recommended in production. For testing, you can safely ignore these as long as you are aware that the data could be lost at any moment, so make sure your test deployment is just that, testing only. Performance can fluctuate wildly in a cloud environment. If performance issues are seen, there are several possible strategies, but keep in mind that this is the perfect place to take advantage of the scale-out capability of Gluster. While it is not true in all cases that deploying more instances will necessarily result in a \u201cfaster\u201d cluster, in general, you will see that adding more nodes means more performance for the cluster overall. If a node reboots, you will typically need to do some extra work to get Gluster running again using the default EC2 configuration. If a node is shut down, it can mean absolute loss of the node (depending on how you set things up). This is well beyond the scope of this document but is discussed in any number of AWS-related forums and posts. Since I found out the hard way myself (oh, so you read the manual every time?!), I thought it worth at least mentioning. Once you have both instances up, you can proceed to the install page.","title":"Setup, Method 3 \u2013 Deploying in AWS"},{"location":"Install-Guide/Setup-virt/","text":"Setup on Virtual Machine Note: You only need one of the three setup methods! Setup, Method 1 \u2013 Setting up in virtual machines As we just mentioned, to set up Gluster using virtual machines, you will need at least two virtual machines with at least 1GB of RAM each. You may be able to test with less but most users will find it too slow for their tastes. The particular virtualization product you use is a matter of choice. Common platforms include Xen, VMware ESX and Workstation, VirtualBox, and KVM. For purpose of this article, all steps assume KVM but the concepts are expected to be simple to translate to other platforms as well. The article assumes you know the particulars of how to create a virtual machine and have installed a 64 bit linux distribution already. Create or clone two VM\u2019s, with the following setup on each: 2 disks using the VirtIO driver, one for the base OS and one that we will use as a Gluster \u201cbrick\u201d. You can add more later to try testing some more advanced configurations, but for now let\u2019s keep it simple. Note: If you have ample space available, consider allocating all the disk space at once. 2 NIC\u2019s using VirtIO driver. The second NIC is not strictly required, but can be used to demonstrate setting up a separate network for storage and management traffic. Note: Attach each NIC to a separate network. Other notes: Make sure that if you clone the VM, that Gluster has not already been installed. Gluster generates a UUID to \u201cfingerprint\u201d each system, so cloning a previously deployed system will result in errors later on. Once these are prepared, you are ready to move on to the install section.","title":"Setting up in virtual machines"},{"location":"Install-Guide/Setup-virt/#setup-on-virtual-machine","text":"Note: You only need one of the three setup methods!","title":"Setup on Virtual Machine"},{"location":"Install-Guide/Setup-virt/#setup-method-1-setting-up-in-virtual-machines","text":"As we just mentioned, to set up Gluster using virtual machines, you will need at least two virtual machines with at least 1GB of RAM each. You may be able to test with less but most users will find it too slow for their tastes. The particular virtualization product you use is a matter of choice. Common platforms include Xen, VMware ESX and Workstation, VirtualBox, and KVM. For purpose of this article, all steps assume KVM but the concepts are expected to be simple to translate to other platforms as well. The article assumes you know the particulars of how to create a virtual machine and have installed a 64 bit linux distribution already. Create or clone two VM\u2019s, with the following setup on each: 2 disks using the VirtIO driver, one for the base OS and one that we will use as a Gluster \u201cbrick\u201d. You can add more later to try testing some more advanced configurations, but for now let\u2019s keep it simple. Note: If you have ample space available, consider allocating all the disk space at once. 2 NIC\u2019s using VirtIO driver. The second NIC is not strictly required, but can be used to demonstrate setting up a separate network for storage and management traffic. Note: Attach each NIC to a separate network. Other notes: Make sure that if you clone the VM, that Gluster has not already been installed. Gluster generates a UUID to \u201cfingerprint\u201d each system, so cloning a previously deployed system will result in errors later on. Once these are prepared, you are ready to move on to the install section.","title":"Setup, Method 1 \u2013 Setting up in virtual machines"},{"location":"Ops-Guide/Overview/","text":"Overview Over the years the infrastructure and services consumed by the Gluster.org community have grown organically. There have been instances of design and planning but the growth has mostly been ad-hoc and need-based. Central to the plan of revitalizing the Gluster.org community is the ability to provide well-maintained infrastructure services with predictable uptimes and resilience. We're migrating the existing services into the Community Cage. The implied objective is that the transition would open up ways and means of the formation of a loose coalition among Infrastructure Administrators who provide expertise and guidance to the community projects within the OSAS team. A small group of Gluster.org community members was asked to assess the current utilization and propose a planned growth. The ad-hoc nature of the existing infrastructure impedes the development of a proposal based on standardized methods of extrapolation. A part of the projection is based on a combination of patterns and heuristics - problems that have been observed and how mitigation strategies have enabled the community to continue to consume the services available. The guiding principle for the assessment has been the need to migrate services to \"Software-as-a-Service\" models and providers wherever applicable and deemed fit. To illustrate this specific directive - the documentation/docs aspect of Gluster.org has been continuously migrating artifacts to readthedocs.org while focusing on simple integration with the website. The website itself has been put within the Gluster.org Github.com account to enable ease of maintenance and sustainability. For more details look at the full Tools List .","title":"Index"},{"location":"Ops-Guide/Overview/#overview","text":"Over the years the infrastructure and services consumed by the Gluster.org community have grown organically. There have been instances of design and planning but the growth has mostly been ad-hoc and need-based. Central to the plan of revitalizing the Gluster.org community is the ability to provide well-maintained infrastructure services with predictable uptimes and resilience. We're migrating the existing services into the Community Cage. The implied objective is that the transition would open up ways and means of the formation of a loose coalition among Infrastructure Administrators who provide expertise and guidance to the community projects within the OSAS team. A small group of Gluster.org community members was asked to assess the current utilization and propose a planned growth. The ad-hoc nature of the existing infrastructure impedes the development of a proposal based on standardized methods of extrapolation. A part of the projection is based on a combination of patterns and heuristics - problems that have been observed and how mitigation strategies have enabled the community to continue to consume the services available. The guiding principle for the assessment has been the need to migrate services to \"Software-as-a-Service\" models and providers wherever applicable and deemed fit. To illustrate this specific directive - the documentation/docs aspect of Gluster.org has been continuously migrating artifacts to readthedocs.org while focusing on simple integration with the website. The website itself has been put within the Gluster.org Github.com account to enable ease of maintenance and sustainability. For more details look at the full Tools List .","title":"Overview"},{"location":"Ops-Guide/Tools/","text":"Tools We Use Service/Tool Purpose Hosted At Github Code Review Github Jenkins CI, build-verification-test Temporary Racks Backups Website, Gerrit and Jenkins backup Rackspace Docs Documentation content mkdocs.org download.gluster.org Official download site of the binaries Rackspace Mailman Lists mailman Rackspace www.gluster.org Web asset Rackspace Notes download.gluster.org: Resiliency is important for availability and metrics. Since it's official download, access need to restricted as much as possible. Few developers building the community packages have access. If anyone requires access can raise an issue at gluster/project-infrastructure with valid reason Mailman: Should be migrated to a separate host. Should be made more redundant (ie, more than 1 MX). www.gluster.org: Framework, Artifacts now exist under gluster.github.com. Has various legacy installation of software (mediawiki, etc ), being cleaned as we find them.","title":"Tools"},{"location":"Ops-Guide/Tools/#tools-we-use","text":"Service/Tool Purpose Hosted At Github Code Review Github Jenkins CI, build-verification-test Temporary Racks Backups Website, Gerrit and Jenkins backup Rackspace Docs Documentation content mkdocs.org download.gluster.org Official download site of the binaries Rackspace Mailman Lists mailman Rackspace www.gluster.org Web asset Rackspace","title":"Tools We Use"},{"location":"Ops-Guide/Tools/#notes","text":"download.gluster.org: Resiliency is important for availability and metrics. Since it's official download, access need to restricted as much as possible. Few developers building the community packages have access. If anyone requires access can raise an issue at gluster/project-infrastructure with valid reason Mailman: Should be migrated to a separate host. Should be made more redundant (ie, more than 1 MX). www.gluster.org: Framework, Artifacts now exist under gluster.github.com. Has various legacy installation of software (mediawiki, etc ), being cleaned as we find them.","title":"Notes"},{"location":"Quick-Start-Guide/Architecture/","text":"Architecture A gluster volume is a collection of servers belonging to a Trusted Storage Pool. A management daemon (glusterd) runs on each server and manages a brick process (glusterfsd) which in turn exports the underlying on disk storage (XFS filesystem). The client process mounts the volume and exposes the storage from all the bricks as a single unified storage namespace to the applications accessing it. The client and brick processes' stacks have various translators loaded in them. I/O from the application is routed to different bricks via these translators. Types of Volumes Gluster file system supports different types of volumes based on the requirements. Some volumes are good for scaling storage size, some for improving performance and some for both. 1. Distributed Glusterfs Volume - This is the type of volume which is created by default if no volume type is specified. Here, files are distributed across various bricks in the volume. So file1 may be stored only in brick1 or brick2 but not on both. Hence there is no data redundancy . The purpose for such a storage volume is to easily & cheaply scale the volume size. However this also means that a brick failure will lead to complete loss of data and one must rely on the underlying hardware for data loss protection. Create a Distributed Volume gluster volume create NEW-VOLNAME [transport [tcp | rdma | tcp,rdma]] NEW-BRICK... For example to create a distributed volume with four storage servers using TCP. # gluster volume create test-volume server1:/exp1 server2:/exp2 server3:/exp3 server4:/exp4 volume create: test-volume: success: please start the volume to access data To display the volume info # gluster volume info Volume Name: test-volume Type: Distribute Status: Created Number of Bricks: 4 Transport-type: tcp Bricks: Brick1: server1:/exp1 Brick2: server2:/exp2 Brick3: server3:/exp3 Brick4: server4:/exp4 2. Replicated Glusterfs Volume - In this volume we overcome the risk of data loss which is present in the distributed volume. Here exact copies of the data are maintained on all bricks. The number of replicas in the volume can be decided by client while creating the volume. So we need to have at least two bricks to create a volume with 2 replicas or a minimum of three bricks to create a volume of 3 replicas. One major advantage of such a volume is that even if one brick fails the data can still be accessed from its replicated bricks. Such a volume is used for better reliability and data redundancy. Create a Replicated Volume gluster volume create NEW-VOLNAME [replica COUNT] [transport [tcp |rdma | tcp,rdma]] NEW-BRICK... For example , to create a replicated volume with three storage servers: # gluster volume create test-volume replica 3 transport tcp \\ server1:/exp1 server2:/exp2 server3:/exp3 volume create: test-volume: success: please start the volume to access data 3. Distributed Replicated Glusterfs Volume - In this volume files are distributed across replicated sets of bricks. The number of bricks must be a multiple of the replica count. Also the order in which we specify the bricks is important since adjacent bricks become replicas of each other. This type of volume is used when high availability of data due to redundancy and scaling storage is required. So if there were eight bricks and replica count 2 then the first two bricks become replicas of each other then the next two and so on. This volume is denoted as 4x2. Similarly if there were eight bricks and replica count 4 then four bricks become replica of each other and we denote this volume as 2x4 volume. Create the distributed replicated volume: gluster volume create NEW-VOLNAME [replica COUNT] [transport [tcp | rdma | tcp,rdma]] NEW-BRICK... For example , six node distributed replicated volume with a three-way mirror: # gluster volume create test-volume replica 3 transport tcp server1:/exp1 server2:/exp2 server3:/exp3 server4:/exp4 server5:/exp5 server6:/exp6 volume create: test-volume: success: please start the volume to access data 4. Dispersed Glusterfs Volume - Dispersed volumes are based on erasure codes. It stripes the encoded data of files, with some redundancy added, across multiple bricks in the volume. You can use dispersed volumes to have a configurable level of reliability with minimum space waste. The number of redundant bricks in the volume can be decided by clients while creating the volume. Redundant bricks determines how many bricks can be lost without interrupting the operation of the volume. Create a dispersed volume: # gluster volume create test-volume [disperse [<COUNT>]] [disperse-data <COUNT>] [redundancy <COUNT>] [transport tcp | rdma | tcp,rdma] <NEW-BRICK> For example , three node dispersed volume with level of redundancy 1, (2 + 1): # gluster volume create test-volume disperse 3 redundancy 1 server1:/exp1 server2:/exp2 server3:/exp3 volume create: test-volume: success: please start the volume to access data 5. Distributed Dispersed Glusterfs Volume - Distributed dispersed volumes are the equivalent to distributed replicated volumes, but using dispersed subvolumes instead of replicated ones. The number of bricks must be a multiple of the 1st subvol. The purpose for such a volume is to easily scale the volume size and distribute the load across various bricks. Create a distributed dispersed volume: # gluster volume create [disperse [<COUNT>]] [disperse-data <COUNT>] [redundancy <COUNT>] [transport tcp | rdma | tcp,rdma] <NEW-BRICK> For example , six node distributed dispersed volume with level of redundancy 1, 2 x (2 + 1) = 6: # gluster volume create test-volume disperse 3 redundancy 1 server1:/exp1 server2:/exp2 server3:/exp3 server4:/exp4 server5:/exp5 server6:/exp6 volume create: test-volume: success: please start the volume to access data Note : A dispersed volume can be created by specifying the number of bricks in a disperse set, by specifying the number of redundancy bricks, or both. If disperse is not specified, or the <COUNT> is missing, the entire volume will be treated as a single disperse set composed by all bricks enumerated in the command line. If redundancy is not specified, it is computed automatically to be the optimal value. If this value does not exist, it's assumed to be '1' and a warning message is shown: # gluster volume create test-volume disperse 4 server{1..4}:/bricks/test-volume There isn't an optimal redundancy value for this configuration. Do you want to create the volume with redundancy 1 ? (y/n) In all cases where redundancy is automatically computed and it's not equal to '1', a warning message is displayed: # gluster volume create test-volume disperse 6 server{1..6}:/bricks/test-volume The optimal redundancy for this configuration is 2. Do you want to create the volume with this value ? (y/n) redundancy must be greater than 0, and the total number of bricks must be greater than 2 * redundancy . This means that a dispersed volume must have a minimum of 3 bricks. FUSE GlusterFS is a userspace filesystem. The GluserFS developers opted for this approach in order to avoid the need to have modules in the Linux kernel. As it is a userspace filesystem, to interact with kernel VFS, GlusterFS makes use of FUSE (File System in Userspace). For a long time, implementation of a userspace filesystem was considered impossible. FUSE was developed as a solution for this. FUSE is a kernel module that supports interaction between kernel VFS and non-privileged user applications and it has an API that can be accessed from userspace. Using this API, any type of filesystem can be written using almost any language you prefer as there are many bindings between FUSE and other languages. Structural diagram of FUSE. This shows a filesystem \"hello world\" that is compiled to create a binary \"hello\". It is executed with a filesystem mount point /tmp/fuse. Then the user issues a command ls -l on the mount point /tmp/fuse. This command reaches VFS via glibc and since the mount /tmp/fuse corresponds to a FUSE based filesystem, VFS passes it over to FUSE module. The FUSE kernel module contacts the actual filesystem binary \"hello\" after passing through glibc and FUSE library in userspace(libfuse). The result is returned by the \"hello\" through the same path and reaches the ls -l command. The communication between FUSE kernel module and the FUSE library(libfuse) is via a special file descriptor which is obtained by opening /dev/fuse. This file can be opened multiple times, and the obtained file descriptor is passed to the mount syscall, to match up the descriptor with the mounted filesystem. More about userspace filesystems FUSE reference Translators Translating \u201ctranslators\u201d : A translator converts requests from users into requests for storage. *One to one, one to many, one to zero (e.g. caching) A translator can modify requests on the way through : convert one request type to another ( during the request transfer amongst the translators) modify paths, flags, even data (e.g. encryption) Translators can intercept or block the requests. (e.g. access control) Or spawn new requests (e.g. pre-fetch) How Do Translators Work? Shared Objects Dynamically loaded according to 'volfile' dlopen/dlsync setup pointers to parents / children call init (constructor) call IO functions through fops. Conventions for validating/ passing options, etc. The configuration of translators (since GlusterFS 3.1) is managed through the gluster command line interface (cli), so you don't need to know in what order to graph the translators together. Types of Translators List of known translators with their current status. Translator Type Functional Purpose Storage Lowest level translator, stores and accesses data from local file system. Debug Provide interface and statistics for errors and debugging. Cluster Handle distribution and replication of data as it relates to writing to and reading from bricks & nodes. Encryption Extension translators for on-the-fly encryption/decryption of stored data. Protocol Extension translators for client/server communication protocols. Performance Tuning translators to adjust for workload and I/O profiles. Bindings Add extensibility, e.g. The Python interface written by Jeff Darcy to extend API interaction with GlusterFS. System System access translators, e.g. Interfacing with file system access control. Scheduler I/O schedulers that determine how to distribute new write operations across clustered systems. Features Add additional features such as Quotas, Filters, Locks, etc. The default / general hierarchy of translators in vol files : All the translators hooked together to perform a function is called a graph. The left-set of translators comprises of Client-stack .The right-set of translators comprises of Server-stack . The glusterfs translators can be sub-divided into many categories, but two important categories are - Cluster and Performance translators : One of the most important and the first translator the data/request has to go through is fuse translator which falls under the category of Mount Translators . Cluster Translators : DHT(Distributed Hash Table) AFR(Automatic File Replication) Performance Translators : io-cache io-threads md-cache O-B (open behind) QR (quick read) r-a (read-ahead) w-b (write-behind) Other Feature Translators include: changelog locks - GlusterFS has locks translator which provides the following internal locking operations called inodelk , entrylk , which are used by afr to achieve synchronization of operations on files or directories that conflict with each other. marker quota Debug Translators trace - To trace the error logs generated during the communication amongst the translators. io-stats DHT(Distributed Hash Table) Translator What is DHT? DHT is the real core of how GlusterFS aggregates capacity and performance across multiple servers. Its responsibility is to place each file on exactly one of its subvolumes \u2013 unlike either replication (which places copies on all of its subvolumes) or striping (which places pieces onto all of its subvolumes). It\u2019s a routing function, not splitting or copying. How DHT works ? The basic method used in DHT is consistent hashing. Each subvolume (brick) is assigned a range within a 32-bit hash space, covering the entire range with no holes or overlaps. Then each file is also assigned a value in that same space, by hashing its name. Exactly one brick will have an assigned range including the file\u2019s hash value, and so the file \u201cshould\u201d be on that brick. However, there are many cases where that won\u2019t be the case, such as when the set of bricks (and therefore the range assignment of ranges) has changed since the file was created, or when a brick is nearly full. Much of the complexity in DHT involves these special cases, which we\u2019ll discuss in a moment. When you open() a file, the distribute translator is giving one piece of information to find your file, the file-name. To determine where that file is, the translator runs the file-name through a hashing algorithm in order to turn that file-name into a number. A few Observations of DHT hash-values assignment : The assignment of hash ranges to bricks is determined by extended attributes stored on directories, hence distribution is directory-specific. Consistent hashing is usually thought of as hashing around a circle, but in GlusterFS it\u2019s more linear. There\u2019s no need to \u201cwrap around\u201d at zero, because there\u2019s always a break (between one brick\u2019s range and another\u2019s) at zero. If a brick is missing, there will be a hole in the hash space. Even worse, if hash ranges are reassigned while a brick is offline, some of the new ranges might overlap with the (now out of date) range stored on that brick, creating a bit of confusion about where files should be. AFR(Automatic File Replication) Translator The Automatic File Replication (AFR) translator in GlusterFS makes use of the extended attributes to keep track of the file operations.It is responsible for replicating the data across the bricks. Responsibilities of AFR Its responsibilities include the following: Maintain replication consistency (i.e. Data on both the bricks should be same, even in the cases where there are operations happening on same file/directory in parallel from multiple applications/mount points as long as all the bricks in replica set are up). Provide a way of recovering data in case of failures as long as there is at least one brick which has the correct data. Serve fresh data for read/stat/readdir etc. Geo-Replication Geo-replication provides asynchronous replication of data across geographically distinct locations and was introduced in Glusterfs 3.2. It mainly works across WAN and is used to replicate the entire volume unlike AFR which is intra-cluster replication. This is mainly useful for backup of entire data for disaster recovery. Geo-replication uses a primary-secondary model, whereby replication occurs between a Primary and a Secondary , both of which should be GlusterFS volumes. Geo-replication provides an incremental replication service over Local Area Networks (LANs), Wide Area Network (WANs), and across the Internet. Geo-replication over LAN You can configure Geo-replication to mirror data over a Local Area Network. Geo-replication over WAN You can configure Geo-replication to replicate data over a Wide Area Network. Geo-replication over Internet You can configure Geo-replication to mirror data over the Internet. Multi-site cascading Geo-replication You can configure Geo-replication to mirror data in a cascading fashion across multiple sites. There are mainly two aspects while asynchronously replicating data: 1. Change detection - These include file-operation necessary details. There are two methods to sync the detected changes: i. Changelogs - Changelog is a translator which records necessary details for the fops that occur. The changes can be written in binary format or ASCII. There are three category with each category represented by a specific changelog format. All three types of categories are recorded in a single changelog file. Entry - create(), mkdir(), mknod(), symlink(), link(), rename(), unlink(), rmdir() Data - write(), writev(), truncate(), ftruncate() Meta - setattr(), fsetattr(), setxattr(), fsetxattr(), removexattr(), fremovexattr() In order to record the type of operation and entity underwent, a type identifier is used. Normally, the entity on which the operation is performed would be identified by the pathname, but we choose to use GlusterFS internal file identifier (GFID) instead (as GlusterFS supports GFID based backend and the pathname field may not always be valid and other reasons which are out of scope of this document). Therefore, the format of the record for the three types of operation can be summarized as follows: Entry - GFID + FOP + MODE + UID + GID + PARGFID/BNAME [PARGFID/BNAME] Meta - GFID of the file Data - GFID of the file GFID's are analogous to inodes. Data and Meta fops record the GFID of the entity on which the operation was performed, thereby recording that there was a data/metadata change on the inode. Entry fops record at the minimum a set of six or seven records (depending on the type of operation), that is sufficient to identify what type of operation the entity underwent. Normally this record includes the GFID of the entity, the type of file operation (which is an integer [an enumerated value which is used in Glusterfs]) and the parent GFID and the basename (analogous to parent inode and basename). Changelog file is rolled over after a specific time interval. We then perform processing operations on the file like converting it to understandable/human readable format, keeping private copy of the changelog etc. The library then consumes these logs and serves application requests. ii. Xsync - Marker translator maintains an extended attribute \u201cxtime\u201d for each file and directory. Whenever any update happens it would update the xtime attribute of that file and all its ancestors. So the change is propagated from the node (where the change has occurred) all the way to the root. Consider the above directory tree structure. At time T1 the primary and secondary were in sync each other. At time T2 a new file File2 was created. This will trigger the xtime marking (where xtime is the current timestamp) from File2 upto to the root, i.e, the xtime of File2, Dir3, Dir1 and finally Dir0 all will be updated. Geo-replication daemon crawls the file system based on the condition that xtime(primary) > xtime(secondary). Hence in our example it would crawl only the left part of the directory structure since the right part of the directory structure still has equal timestamp. Although the crawling algorithm is fast we still need to crawl a good part of the directory structure. 2. Replication - We use rsync for data replication. Rsync is an external utility which will calculate the diff of the two files and sends this difference from source to sync. Overall working of GlusterFS As soon as GlusterFS is installed in a server node, a gluster management daemon(glusterd) binary will be created. This daemon should be running in all participating nodes in the cluster. After starting glusterd, a trusted server pool(TSP) can be created consisting of all storage server nodes (TSP can contain even a single node). Now bricks which are the basic units of storage can be created as export directories in these servers. Any number of bricks from this TSP can be clubbed together to form a volume. Once a volume is created, a glusterfsd process starts running in each of the participating brick. Along with this, configuration files known as vol files will be generated inside /var/lib/glusterd/vols/. There will be configuration files corresponding to each brick in the volume. This will contain all the details about that particular brick. Configuration file required by a client process will also be created. Now our filesystem is ready to use. We can mount this volume on a client machine very easily as follows and use it like we use a local storage: mount.glusterfs <IP or hostname> : <volume_name> <mount_point> IP or hostname can be that of any node in the trusted server pool in which the required volume is created. When we mount the volume in the client, the client glusterfs process communicates with the servers\u2019 glusterd process. Server glusterd process sends a configuration file (vol file) containing the list of client translators and another containing the information of each brick in the volume with the help of which the client glusterfs process can now directly communicate with each brick\u2019s glusterfsd process. The setup is now complete and the volume is now ready for client's service. When a system call (File operation or Fop) is issued by client in the mounted filesystem, the VFS (identifying the type of filesystem to be glusterfs) will send the request to the FUSE kernel module. The FUSE kernel module will in turn send it to the GlusterFS in the userspace of the client node via /dev/fuse (this has been described in FUSE section). The GlusterFS process on the client consists of a stack of translators called the client translators which are defined in the configuration file(vol file) sent by the storage server glusterd process. The first among these translators being the FUSE translator which consists of the FUSE library(libfuse). Each translator has got functions corresponding to each file operation or fop supported by glusterfs. The request will hit the corresponding function in each of the translators. Main client translators include: FUSE translator DHT translator- DHT translator maps the request to the correct brick that contains the file or directory required. AFR translator- It receives the request from the previous translator and if the volume type is replicate, it duplicates the request and passes it on to the Protocol client translators of the replicas. Protocol Client translator- Protocol Client translator is the last in the client translator stack. This translator is divided into multiple threads, one for each brick in the volume. This will directly communicate with the glusterfsd of each brick. In the storage server node that contains the brick in need, the request again goes through a series of translators known as server translators, main ones being: Protocol server translator POSIX translator The request will finally reach VFS and then will communicate with the underlying native filesystem. The response will retrace the same path.","title":"Architecture"},{"location":"Quick-Start-Guide/Architecture/#architecture","text":"A gluster volume is a collection of servers belonging to a Trusted Storage Pool. A management daemon (glusterd) runs on each server and manages a brick process (glusterfsd) which in turn exports the underlying on disk storage (XFS filesystem). The client process mounts the volume and exposes the storage from all the bricks as a single unified storage namespace to the applications accessing it. The client and brick processes' stacks have various translators loaded in them. I/O from the application is routed to different bricks via these translators.","title":"Architecture"},{"location":"Quick-Start-Guide/Architecture/#types-of-volumes","text":"Gluster file system supports different types of volumes based on the requirements. Some volumes are good for scaling storage size, some for improving performance and some for both. 1. Distributed Glusterfs Volume - This is the type of volume which is created by default if no volume type is specified. Here, files are distributed across various bricks in the volume. So file1 may be stored only in brick1 or brick2 but not on both. Hence there is no data redundancy . The purpose for such a storage volume is to easily & cheaply scale the volume size. However this also means that a brick failure will lead to complete loss of data and one must rely on the underlying hardware for data loss protection. Create a Distributed Volume gluster volume create NEW-VOLNAME [transport [tcp | rdma | tcp,rdma]] NEW-BRICK... For example to create a distributed volume with four storage servers using TCP. # gluster volume create test-volume server1:/exp1 server2:/exp2 server3:/exp3 server4:/exp4 volume create: test-volume: success: please start the volume to access data To display the volume info # gluster volume info Volume Name: test-volume Type: Distribute Status: Created Number of Bricks: 4 Transport-type: tcp Bricks: Brick1: server1:/exp1 Brick2: server2:/exp2 Brick3: server3:/exp3 Brick4: server4:/exp4 2. Replicated Glusterfs Volume - In this volume we overcome the risk of data loss which is present in the distributed volume. Here exact copies of the data are maintained on all bricks. The number of replicas in the volume can be decided by client while creating the volume. So we need to have at least two bricks to create a volume with 2 replicas or a minimum of three bricks to create a volume of 3 replicas. One major advantage of such a volume is that even if one brick fails the data can still be accessed from its replicated bricks. Such a volume is used for better reliability and data redundancy. Create a Replicated Volume gluster volume create NEW-VOLNAME [replica COUNT] [transport [tcp |rdma | tcp,rdma]] NEW-BRICK... For example , to create a replicated volume with three storage servers: # gluster volume create test-volume replica 3 transport tcp \\ server1:/exp1 server2:/exp2 server3:/exp3 volume create: test-volume: success: please start the volume to access data 3. Distributed Replicated Glusterfs Volume - In this volume files are distributed across replicated sets of bricks. The number of bricks must be a multiple of the replica count. Also the order in which we specify the bricks is important since adjacent bricks become replicas of each other. This type of volume is used when high availability of data due to redundancy and scaling storage is required. So if there were eight bricks and replica count 2 then the first two bricks become replicas of each other then the next two and so on. This volume is denoted as 4x2. Similarly if there were eight bricks and replica count 4 then four bricks become replica of each other and we denote this volume as 2x4 volume. Create the distributed replicated volume: gluster volume create NEW-VOLNAME [replica COUNT] [transport [tcp | rdma | tcp,rdma]] NEW-BRICK... For example , six node distributed replicated volume with a three-way mirror: # gluster volume create test-volume replica 3 transport tcp server1:/exp1 server2:/exp2 server3:/exp3 server4:/exp4 server5:/exp5 server6:/exp6 volume create: test-volume: success: please start the volume to access data 4. Dispersed Glusterfs Volume - Dispersed volumes are based on erasure codes. It stripes the encoded data of files, with some redundancy added, across multiple bricks in the volume. You can use dispersed volumes to have a configurable level of reliability with minimum space waste. The number of redundant bricks in the volume can be decided by clients while creating the volume. Redundant bricks determines how many bricks can be lost without interrupting the operation of the volume. Create a dispersed volume: # gluster volume create test-volume [disperse [<COUNT>]] [disperse-data <COUNT>] [redundancy <COUNT>] [transport tcp | rdma | tcp,rdma] <NEW-BRICK> For example , three node dispersed volume with level of redundancy 1, (2 + 1): # gluster volume create test-volume disperse 3 redundancy 1 server1:/exp1 server2:/exp2 server3:/exp3 volume create: test-volume: success: please start the volume to access data 5. Distributed Dispersed Glusterfs Volume - Distributed dispersed volumes are the equivalent to distributed replicated volumes, but using dispersed subvolumes instead of replicated ones. The number of bricks must be a multiple of the 1st subvol. The purpose for such a volume is to easily scale the volume size and distribute the load across various bricks. Create a distributed dispersed volume: # gluster volume create [disperse [<COUNT>]] [disperse-data <COUNT>] [redundancy <COUNT>] [transport tcp | rdma | tcp,rdma] <NEW-BRICK> For example , six node distributed dispersed volume with level of redundancy 1, 2 x (2 + 1) = 6: # gluster volume create test-volume disperse 3 redundancy 1 server1:/exp1 server2:/exp2 server3:/exp3 server4:/exp4 server5:/exp5 server6:/exp6 volume create: test-volume: success: please start the volume to access data Note : A dispersed volume can be created by specifying the number of bricks in a disperse set, by specifying the number of redundancy bricks, or both. If disperse is not specified, or the <COUNT> is missing, the entire volume will be treated as a single disperse set composed by all bricks enumerated in the command line. If redundancy is not specified, it is computed automatically to be the optimal value. If this value does not exist, it's assumed to be '1' and a warning message is shown: # gluster volume create test-volume disperse 4 server{1..4}:/bricks/test-volume There isn't an optimal redundancy value for this configuration. Do you want to create the volume with redundancy 1 ? (y/n) In all cases where redundancy is automatically computed and it's not equal to '1', a warning message is displayed: # gluster volume create test-volume disperse 6 server{1..6}:/bricks/test-volume The optimal redundancy for this configuration is 2. Do you want to create the volume with this value ? (y/n) redundancy must be greater than 0, and the total number of bricks must be greater than 2 * redundancy . This means that a dispersed volume must have a minimum of 3 bricks.","title":"Types of Volumes"},{"location":"Quick-Start-Guide/Architecture/#fuse","text":"GlusterFS is a userspace filesystem. The GluserFS developers opted for this approach in order to avoid the need to have modules in the Linux kernel. As it is a userspace filesystem, to interact with kernel VFS, GlusterFS makes use of FUSE (File System in Userspace). For a long time, implementation of a userspace filesystem was considered impossible. FUSE was developed as a solution for this. FUSE is a kernel module that supports interaction between kernel VFS and non-privileged user applications and it has an API that can be accessed from userspace. Using this API, any type of filesystem can be written using almost any language you prefer as there are many bindings between FUSE and other languages. Structural diagram of FUSE. This shows a filesystem \"hello world\" that is compiled to create a binary \"hello\". It is executed with a filesystem mount point /tmp/fuse. Then the user issues a command ls -l on the mount point /tmp/fuse. This command reaches VFS via glibc and since the mount /tmp/fuse corresponds to a FUSE based filesystem, VFS passes it over to FUSE module. The FUSE kernel module contacts the actual filesystem binary \"hello\" after passing through glibc and FUSE library in userspace(libfuse). The result is returned by the \"hello\" through the same path and reaches the ls -l command. The communication between FUSE kernel module and the FUSE library(libfuse) is via a special file descriptor which is obtained by opening /dev/fuse. This file can be opened multiple times, and the obtained file descriptor is passed to the mount syscall, to match up the descriptor with the mounted filesystem. More about userspace filesystems FUSE reference","title":"FUSE"},{"location":"Quick-Start-Guide/Architecture/#translators","text":"Translating \u201ctranslators\u201d : A translator converts requests from users into requests for storage. *One to one, one to many, one to zero (e.g. caching) A translator can modify requests on the way through : convert one request type to another ( during the request transfer amongst the translators) modify paths, flags, even data (e.g. encryption) Translators can intercept or block the requests. (e.g. access control) Or spawn new requests (e.g. pre-fetch) How Do Translators Work? Shared Objects Dynamically loaded according to 'volfile' dlopen/dlsync setup pointers to parents / children call init (constructor) call IO functions through fops. Conventions for validating/ passing options, etc. The configuration of translators (since GlusterFS 3.1) is managed through the gluster command line interface (cli), so you don't need to know in what order to graph the translators together.","title":"Translators"},{"location":"Quick-Start-Guide/Architecture/#types-of-translators","text":"List of known translators with their current status. Translator Type Functional Purpose Storage Lowest level translator, stores and accesses data from local file system. Debug Provide interface and statistics for errors and debugging. Cluster Handle distribution and replication of data as it relates to writing to and reading from bricks & nodes. Encryption Extension translators for on-the-fly encryption/decryption of stored data. Protocol Extension translators for client/server communication protocols. Performance Tuning translators to adjust for workload and I/O profiles. Bindings Add extensibility, e.g. The Python interface written by Jeff Darcy to extend API interaction with GlusterFS. System System access translators, e.g. Interfacing with file system access control. Scheduler I/O schedulers that determine how to distribute new write operations across clustered systems. Features Add additional features such as Quotas, Filters, Locks, etc. The default / general hierarchy of translators in vol files : All the translators hooked together to perform a function is called a graph. The left-set of translators comprises of Client-stack .The right-set of translators comprises of Server-stack . The glusterfs translators can be sub-divided into many categories, but two important categories are - Cluster and Performance translators : One of the most important and the first translator the data/request has to go through is fuse translator which falls under the category of Mount Translators . Cluster Translators : DHT(Distributed Hash Table) AFR(Automatic File Replication) Performance Translators : io-cache io-threads md-cache O-B (open behind) QR (quick read) r-a (read-ahead) w-b (write-behind) Other Feature Translators include: changelog locks - GlusterFS has locks translator which provides the following internal locking operations called inodelk , entrylk , which are used by afr to achieve synchronization of operations on files or directories that conflict with each other. marker quota Debug Translators trace - To trace the error logs generated during the communication amongst the translators. io-stats","title":"Types of Translators"},{"location":"Quick-Start-Guide/Architecture/#dhtdistributed-hash-table-translator","text":"What is DHT? DHT is the real core of how GlusterFS aggregates capacity and performance across multiple servers. Its responsibility is to place each file on exactly one of its subvolumes \u2013 unlike either replication (which places copies on all of its subvolumes) or striping (which places pieces onto all of its subvolumes). It\u2019s a routing function, not splitting or copying. How DHT works ? The basic method used in DHT is consistent hashing. Each subvolume (brick) is assigned a range within a 32-bit hash space, covering the entire range with no holes or overlaps. Then each file is also assigned a value in that same space, by hashing its name. Exactly one brick will have an assigned range including the file\u2019s hash value, and so the file \u201cshould\u201d be on that brick. However, there are many cases where that won\u2019t be the case, such as when the set of bricks (and therefore the range assignment of ranges) has changed since the file was created, or when a brick is nearly full. Much of the complexity in DHT involves these special cases, which we\u2019ll discuss in a moment. When you open() a file, the distribute translator is giving one piece of information to find your file, the file-name. To determine where that file is, the translator runs the file-name through a hashing algorithm in order to turn that file-name into a number. A few Observations of DHT hash-values assignment : The assignment of hash ranges to bricks is determined by extended attributes stored on directories, hence distribution is directory-specific. Consistent hashing is usually thought of as hashing around a circle, but in GlusterFS it\u2019s more linear. There\u2019s no need to \u201cwrap around\u201d at zero, because there\u2019s always a break (between one brick\u2019s range and another\u2019s) at zero. If a brick is missing, there will be a hole in the hash space. Even worse, if hash ranges are reassigned while a brick is offline, some of the new ranges might overlap with the (now out of date) range stored on that brick, creating a bit of confusion about where files should be.","title":"DHT(Distributed Hash Table) Translator"},{"location":"Quick-Start-Guide/Architecture/#afrautomatic-file-replication-translator","text":"The Automatic File Replication (AFR) translator in GlusterFS makes use of the extended attributes to keep track of the file operations.It is responsible for replicating the data across the bricks.","title":"AFR(Automatic File Replication) Translator"},{"location":"Quick-Start-Guide/Architecture/#responsibilities-of-afr","text":"Its responsibilities include the following: Maintain replication consistency (i.e. Data on both the bricks should be same, even in the cases where there are operations happening on same file/directory in parallel from multiple applications/mount points as long as all the bricks in replica set are up). Provide a way of recovering data in case of failures as long as there is at least one brick which has the correct data. Serve fresh data for read/stat/readdir etc.","title":"Responsibilities of AFR"},{"location":"Quick-Start-Guide/Architecture/#geo-replication","text":"Geo-replication provides asynchronous replication of data across geographically distinct locations and was introduced in Glusterfs 3.2. It mainly works across WAN and is used to replicate the entire volume unlike AFR which is intra-cluster replication. This is mainly useful for backup of entire data for disaster recovery. Geo-replication uses a primary-secondary model, whereby replication occurs between a Primary and a Secondary , both of which should be GlusterFS volumes. Geo-replication provides an incremental replication service over Local Area Networks (LANs), Wide Area Network (WANs), and across the Internet. Geo-replication over LAN You can configure Geo-replication to mirror data over a Local Area Network. Geo-replication over WAN You can configure Geo-replication to replicate data over a Wide Area Network. Geo-replication over Internet You can configure Geo-replication to mirror data over the Internet. Multi-site cascading Geo-replication You can configure Geo-replication to mirror data in a cascading fashion across multiple sites. There are mainly two aspects while asynchronously replicating data: 1. Change detection - These include file-operation necessary details. There are two methods to sync the detected changes: i. Changelogs - Changelog is a translator which records necessary details for the fops that occur. The changes can be written in binary format or ASCII. There are three category with each category represented by a specific changelog format. All three types of categories are recorded in a single changelog file. Entry - create(), mkdir(), mknod(), symlink(), link(), rename(), unlink(), rmdir() Data - write(), writev(), truncate(), ftruncate() Meta - setattr(), fsetattr(), setxattr(), fsetxattr(), removexattr(), fremovexattr() In order to record the type of operation and entity underwent, a type identifier is used. Normally, the entity on which the operation is performed would be identified by the pathname, but we choose to use GlusterFS internal file identifier (GFID) instead (as GlusterFS supports GFID based backend and the pathname field may not always be valid and other reasons which are out of scope of this document). Therefore, the format of the record for the three types of operation can be summarized as follows: Entry - GFID + FOP + MODE + UID + GID + PARGFID/BNAME [PARGFID/BNAME] Meta - GFID of the file Data - GFID of the file GFID's are analogous to inodes. Data and Meta fops record the GFID of the entity on which the operation was performed, thereby recording that there was a data/metadata change on the inode. Entry fops record at the minimum a set of six or seven records (depending on the type of operation), that is sufficient to identify what type of operation the entity underwent. Normally this record includes the GFID of the entity, the type of file operation (which is an integer [an enumerated value which is used in Glusterfs]) and the parent GFID and the basename (analogous to parent inode and basename). Changelog file is rolled over after a specific time interval. We then perform processing operations on the file like converting it to understandable/human readable format, keeping private copy of the changelog etc. The library then consumes these logs and serves application requests. ii. Xsync - Marker translator maintains an extended attribute \u201cxtime\u201d for each file and directory. Whenever any update happens it would update the xtime attribute of that file and all its ancestors. So the change is propagated from the node (where the change has occurred) all the way to the root. Consider the above directory tree structure. At time T1 the primary and secondary were in sync each other. At time T2 a new file File2 was created. This will trigger the xtime marking (where xtime is the current timestamp) from File2 upto to the root, i.e, the xtime of File2, Dir3, Dir1 and finally Dir0 all will be updated. Geo-replication daemon crawls the file system based on the condition that xtime(primary) > xtime(secondary). Hence in our example it would crawl only the left part of the directory structure since the right part of the directory structure still has equal timestamp. Although the crawling algorithm is fast we still need to crawl a good part of the directory structure. 2. Replication - We use rsync for data replication. Rsync is an external utility which will calculate the diff of the two files and sends this difference from source to sync.","title":"Geo-Replication"},{"location":"Quick-Start-Guide/Architecture/#overall-working-of-glusterfs","text":"As soon as GlusterFS is installed in a server node, a gluster management daemon(glusterd) binary will be created. This daemon should be running in all participating nodes in the cluster. After starting glusterd, a trusted server pool(TSP) can be created consisting of all storage server nodes (TSP can contain even a single node). Now bricks which are the basic units of storage can be created as export directories in these servers. Any number of bricks from this TSP can be clubbed together to form a volume. Once a volume is created, a glusterfsd process starts running in each of the participating brick. Along with this, configuration files known as vol files will be generated inside /var/lib/glusterd/vols/. There will be configuration files corresponding to each brick in the volume. This will contain all the details about that particular brick. Configuration file required by a client process will also be created. Now our filesystem is ready to use. We can mount this volume on a client machine very easily as follows and use it like we use a local storage: mount.glusterfs <IP or hostname> : <volume_name> <mount_point> IP or hostname can be that of any node in the trusted server pool in which the required volume is created. When we mount the volume in the client, the client glusterfs process communicates with the servers\u2019 glusterd process. Server glusterd process sends a configuration file (vol file) containing the list of client translators and another containing the information of each brick in the volume with the help of which the client glusterfs process can now directly communicate with each brick\u2019s glusterfsd process. The setup is now complete and the volume is now ready for client's service. When a system call (File operation or Fop) is issued by client in the mounted filesystem, the VFS (identifying the type of filesystem to be glusterfs) will send the request to the FUSE kernel module. The FUSE kernel module will in turn send it to the GlusterFS in the userspace of the client node via /dev/fuse (this has been described in FUSE section). The GlusterFS process on the client consists of a stack of translators called the client translators which are defined in the configuration file(vol file) sent by the storage server glusterd process. The first among these translators being the FUSE translator which consists of the FUSE library(libfuse). Each translator has got functions corresponding to each file operation or fop supported by glusterfs. The request will hit the corresponding function in each of the translators. Main client translators include: FUSE translator DHT translator- DHT translator maps the request to the correct brick that contains the file or directory required. AFR translator- It receives the request from the previous translator and if the volume type is replicate, it duplicates the request and passes it on to the Protocol client translators of the replicas. Protocol Client translator- Protocol Client translator is the last in the client translator stack. This translator is divided into multiple threads, one for each brick in the volume. This will directly communicate with the glusterfsd of each brick. In the storage server node that contains the brick in need, the request again goes through a series of translators known as server translators, main ones being: Protocol server translator POSIX translator The request will finally reach VFS and then will communicate with the underlying native filesystem. The response will retrace the same path.","title":"Overall working of GlusterFS"},{"location":"Quick-Start-Guide/Quickstart/","text":"Installing GlusterFS - a Quick Start Guide Purpose of this document This document is intended to provide a step-by-step guide to setting up GlusterFS for the first time with minimum degree of complexity. For the purposes of this guide, it is required to use Fedora 30 (or, higher, see https://fedoraproject.org/wiki/End_of_life) virtual machine instances. After you deploy GlusterFS by following these steps, we recommend that you read the GlusterFS Admin Guide to how to select a volume type that fits your needs and administer GlusterFS. The GlusterFS Install Guide provides a more detailed explanation of the steps we show in this Quick Start Guide. If you would like a more detailed walkthrough with instructions for installing using different methods (in local virtual machines, EC2 and baremetal) and different distributions, then have a look at the Install guide. Using Ansible to deploy and manage GlusterFS If you are already an Ansible user, and are more comfortable with setting up distributed systems with Ansible, we recommend you to skip all these and move over to gluster-ansible repository, which gives most of the details to get the systems running faster. Automatically deploying GlusterFS with Puppet-Gluster+Vagrant To deploy GlusterFS using scripted methods, please read this article . Step 1 \u2013 Have at least three nodes Fedora 30 (or later) on 3 nodes named \"server1\", \"server2\" and \"server3\" A working network connection At least two virtual disks, one for the OS installation, and one to be used to serve GlusterFS storage (sdb), on each of these VMs. This will emulate a real-world deployment, where you would want to separate GlusterFS storage from the OS install. Setup NTP on each of these servers to get the proper functioning of many applications on top of filesystem. This is an important requirement Note : GlusterFS stores its dynamically generated configuration files at /var/lib/glusterd . If at any point in time GlusterFS is unable to write to these files (for example, when the backing filesystem is full), it will at minimum cause erratic behavior for your system; or worse, take your system offline completely. It is recommended to create separate partitions for directories such as /var/log to reduce the chances of this happening. Step 2 - Format and mount the bricks Perform this step on all the nodes, \"server{1,2,3}\" Note : We are going to use the XFS filesystem for the backend bricks. But Gluster is designed to work on top of any filesystem, which supports extended attributes. The following examples assume that the brick will be residing on /dev/sdb1. # mkfs.xfs -i size=512 /dev/sdb1 # mkdir -p /data/brick1 # echo '/dev/sdb1 /data/brick1 xfs defaults 1 2' >> /etc/fstab # mount -a && mount You should now see sdb1 mounted at /data/brick1 Step 3 - Installing GlusterFS Install the software # yum install glusterfs-server Start the GlusterFS management daemon: # service glusterd start # service glusterd status glusterd.service - LSB: glusterfs server Loaded: loaded (/etc/rc.d/init.d/glusterd) Active: active (running) since Mon, 13 Aug 2012 13:02:11 -0700; 2s ago Process: 19254 ExecStart=/etc/rc.d/init.d/glusterd start (code=exited, status=0/SUCCESS) CGroup: name=systemd:/system/glusterd.service \u251c 19260 /usr/sbin/glusterd -p /run/glusterd.pid \u251c 19304 /usr/sbin/glusterfsd --xlator-option georep-server.listen-port=24009 -s localhost... \u2514 19309 /usr/sbin/glusterfs -f /var/lib/glusterd/nfs/nfs-server.vol -p /var/lib/glusterd/... Step 4 - Configure the firewall The gluster processes on the nodes need to be able to communicate with each other. To simplify this setup, configure the firewall on each node to accept all traffic from the other node. # iptables -I INPUT -p all -s <ip-address> -j ACCEPT where ip-address is the address of the other node. Step 5 - Configure the trusted pool From \"server1\" # gluster peer probe server2 # gluster peer probe server3 Note: When using hostnames, the first server needs to be probed from one other server to set its hostname. From \"server2\" # gluster peer probe server1 Note: Once this pool has been established, only trusted members may probe new servers into the pool. A new server cannot probe the pool, it must be probed from the pool. Check the peer status on server1 # gluster peer status You should see something like this (the UUID will differ) Number of Peers: 2 Hostname: server2 Uuid: f0e7b138-4874-4bc0-ab91-54f20c7068b4 State: Peer in Cluster (Connected) Hostname: server3 Uuid: f0e7b138-4532-4bc0-ab91-54f20c701241 State: Peer in Cluster (Connected) Step 6 - Set up a GlusterFS volume On all servers: # mkdir -p /data/brick1/gv0 From any single server: # gluster volume create gv0 replica 3 server1:/data/brick1/gv0 server2:/data/brick1/gv0 server3:/data/brick1/gv0 volume create: gv0: success: please start the volume to access data # gluster volume start gv0 volume start: gv0: success Confirm that the volume shows \"Started\": # gluster volume info You should see something like this (the Volume ID will differ): Volume Name: gv0 Type: Replicate Volume ID: f25cc3d8-631f-41bd-96e1-3e22a4c6f71f Status: Started Snapshot Count: 0 Number of Bricks: 1 x 3 = 3 Transport-type: tcp Bricks: Brick1: server1:/data/brick1/gv0 Brick2: server2:/data/brick1/gv0 Brick3: server3:/data/brick1/gv0 Options Reconfigured: transport.address-family: inet Note: If the volume does not show \"Started\", the files under /var/log/glusterfs/glusterd.log should be checked in order to debug and diagnose the situation. These logs can be looked at on one or, all the servers configured. Step 7 - Testing the GlusterFS volume For this step, we will use one of the servers to mount the volume. Typically, you would do this from an external machine, known as a \"client\". Since using this method would require additional packages to be installed on the client machine, we will use one of the servers as a simple place to test first , as if it were that \"client\". # mount -t glusterfs server1:/gv0 /mnt # for i in `seq -w 1 100`; do cp -rp /var/log/messages /mnt/copy-test-$i; done First, check the client mount point: # ls -lA /mnt/copy* | wc -l You should see 100 files returned. Next, check the GlusterFS brick mount points on each server: # ls -lA /data/brick1/gv0/copy* You should see 100 files on each server using the method we listed here. Without replication, in a distribute only volume (not detailed here), you should see about 33 files on each one.","title":"Quick Start Guide"},{"location":"Quick-Start-Guide/Quickstart/#installing-glusterfs-a-quick-start-guide","text":"","title":"Installing GlusterFS - a Quick Start Guide"},{"location":"Quick-Start-Guide/Quickstart/#purpose-of-this-document","text":"This document is intended to provide a step-by-step guide to setting up GlusterFS for the first time with minimum degree of complexity. For the purposes of this guide, it is required to use Fedora 30 (or, higher, see https://fedoraproject.org/wiki/End_of_life) virtual machine instances. After you deploy GlusterFS by following these steps, we recommend that you read the GlusterFS Admin Guide to how to select a volume type that fits your needs and administer GlusterFS. The GlusterFS Install Guide provides a more detailed explanation of the steps we show in this Quick Start Guide. If you would like a more detailed walkthrough with instructions for installing using different methods (in local virtual machines, EC2 and baremetal) and different distributions, then have a look at the Install guide.","title":"Purpose of this document"},{"location":"Quick-Start-Guide/Quickstart/#using-ansible-to-deploy-and-manage-glusterfs","text":"If you are already an Ansible user, and are more comfortable with setting up distributed systems with Ansible, we recommend you to skip all these and move over to gluster-ansible repository, which gives most of the details to get the systems running faster.","title":"Using Ansible to deploy and manage GlusterFS"},{"location":"Quick-Start-Guide/Quickstart/#automatically-deploying-glusterfs-with-puppet-glustervagrant","text":"To deploy GlusterFS using scripted methods, please read this article .","title":"Automatically deploying GlusterFS with Puppet-Gluster+Vagrant"},{"location":"Quick-Start-Guide/Quickstart/#step-1-have-at-least-three-nodes","text":"Fedora 30 (or later) on 3 nodes named \"server1\", \"server2\" and \"server3\" A working network connection At least two virtual disks, one for the OS installation, and one to be used to serve GlusterFS storage (sdb), on each of these VMs. This will emulate a real-world deployment, where you would want to separate GlusterFS storage from the OS install. Setup NTP on each of these servers to get the proper functioning of many applications on top of filesystem. This is an important requirement Note : GlusterFS stores its dynamically generated configuration files at /var/lib/glusterd . If at any point in time GlusterFS is unable to write to these files (for example, when the backing filesystem is full), it will at minimum cause erratic behavior for your system; or worse, take your system offline completely. It is recommended to create separate partitions for directories such as /var/log to reduce the chances of this happening.","title":"Step 1 \u2013 Have at least three nodes"},{"location":"Quick-Start-Guide/Quickstart/#step-2-format-and-mount-the-bricks","text":"Perform this step on all the nodes, \"server{1,2,3}\" Note : We are going to use the XFS filesystem for the backend bricks. But Gluster is designed to work on top of any filesystem, which supports extended attributes. The following examples assume that the brick will be residing on /dev/sdb1. # mkfs.xfs -i size=512 /dev/sdb1 # mkdir -p /data/brick1 # echo '/dev/sdb1 /data/brick1 xfs defaults 1 2' >> /etc/fstab # mount -a && mount You should now see sdb1 mounted at /data/brick1","title":"Step 2 - Format and mount the bricks"},{"location":"Quick-Start-Guide/Quickstart/#step-3-installing-glusterfs","text":"Install the software # yum install glusterfs-server Start the GlusterFS management daemon: # service glusterd start # service glusterd status glusterd.service - LSB: glusterfs server Loaded: loaded (/etc/rc.d/init.d/glusterd) Active: active (running) since Mon, 13 Aug 2012 13:02:11 -0700; 2s ago Process: 19254 ExecStart=/etc/rc.d/init.d/glusterd start (code=exited, status=0/SUCCESS) CGroup: name=systemd:/system/glusterd.service \u251c 19260 /usr/sbin/glusterd -p /run/glusterd.pid \u251c 19304 /usr/sbin/glusterfsd --xlator-option georep-server.listen-port=24009 -s localhost... \u2514 19309 /usr/sbin/glusterfs -f /var/lib/glusterd/nfs/nfs-server.vol -p /var/lib/glusterd/...","title":"Step 3 - Installing GlusterFS"},{"location":"Quick-Start-Guide/Quickstart/#step-4-configure-the-firewall","text":"The gluster processes on the nodes need to be able to communicate with each other. To simplify this setup, configure the firewall on each node to accept all traffic from the other node. # iptables -I INPUT -p all -s <ip-address> -j ACCEPT where ip-address is the address of the other node.","title":"Step 4 - Configure the firewall"},{"location":"Quick-Start-Guide/Quickstart/#step-5-configure-the-trusted-pool","text":"From \"server1\" # gluster peer probe server2 # gluster peer probe server3 Note: When using hostnames, the first server needs to be probed from one other server to set its hostname. From \"server2\" # gluster peer probe server1 Note: Once this pool has been established, only trusted members may probe new servers into the pool. A new server cannot probe the pool, it must be probed from the pool. Check the peer status on server1 # gluster peer status You should see something like this (the UUID will differ) Number of Peers: 2 Hostname: server2 Uuid: f0e7b138-4874-4bc0-ab91-54f20c7068b4 State: Peer in Cluster (Connected) Hostname: server3 Uuid: f0e7b138-4532-4bc0-ab91-54f20c701241 State: Peer in Cluster (Connected)","title":"Step 5 - Configure the trusted pool"},{"location":"Quick-Start-Guide/Quickstart/#step-6-set-up-a-glusterfs-volume","text":"On all servers: # mkdir -p /data/brick1/gv0 From any single server: # gluster volume create gv0 replica 3 server1:/data/brick1/gv0 server2:/data/brick1/gv0 server3:/data/brick1/gv0 volume create: gv0: success: please start the volume to access data # gluster volume start gv0 volume start: gv0: success Confirm that the volume shows \"Started\": # gluster volume info You should see something like this (the Volume ID will differ): Volume Name: gv0 Type: Replicate Volume ID: f25cc3d8-631f-41bd-96e1-3e22a4c6f71f Status: Started Snapshot Count: 0 Number of Bricks: 1 x 3 = 3 Transport-type: tcp Bricks: Brick1: server1:/data/brick1/gv0 Brick2: server2:/data/brick1/gv0 Brick3: server3:/data/brick1/gv0 Options Reconfigured: transport.address-family: inet Note: If the volume does not show \"Started\", the files under /var/log/glusterfs/glusterd.log should be checked in order to debug and diagnose the situation. These logs can be looked at on one or, all the servers configured.","title":"Step 6 - Set up a GlusterFS volume"},{"location":"Quick-Start-Guide/Quickstart/#step-7-testing-the-glusterfs-volume","text":"For this step, we will use one of the servers to mount the volume. Typically, you would do this from an external machine, known as a \"client\". Since using this method would require additional packages to be installed on the client machine, we will use one of the servers as a simple place to test first , as if it were that \"client\". # mount -t glusterfs server1:/gv0 /mnt # for i in `seq -w 1 100`; do cp -rp /var/log/messages /mnt/copy-test-$i; done First, check the client mount point: # ls -lA /mnt/copy* | wc -l You should see 100 files returned. Next, check the GlusterFS brick mount points on each server: # ls -lA /data/brick1/gv0/copy* You should see 100 files on each server using the method we listed here. Without replication, in a distribute only volume (not detailed here), you should see about 33 files on each one.","title":"Step 7 - Testing the GlusterFS volume"},{"location":"Troubleshooting/","text":"Troubleshooting Guide This guide describes some commonly seen issues and steps to recover from them. If that doesn\u2019t help, reach out to the Gluster community , in which case the guide also describes what information needs to be provided in order to debug the issue. At minimum, we need the version of gluster running and the output of gluster volume info . Where Do I Start? Is the issue already listed in the component specific troubleshooting sections? CLI and Glusterd Issues Heal related issues Resolving Split brains Geo-replication Issues Gluster NFS Issues File Locks If that didn't help, here is how to debug further. Identifying the problem and getting the necessary information to diagnose it is the first step in troubleshooting your Gluster setup. As Gluster operations involve interactions between multiple processes, this can involve multiple steps. What Happened? An operation failed High Memory Usage A Gluster process crashed","title":"Index"},{"location":"Troubleshooting/#troubleshooting-guide","text":"This guide describes some commonly seen issues and steps to recover from them. If that doesn\u2019t help, reach out to the Gluster community , in which case the guide also describes what information needs to be provided in order to debug the issue. At minimum, we need the version of gluster running and the output of gluster volume info .","title":"Troubleshooting Guide"},{"location":"Troubleshooting/#where-do-i-start","text":"Is the issue already listed in the component specific troubleshooting sections? CLI and Glusterd Issues Heal related issues Resolving Split brains Geo-replication Issues Gluster NFS Issues File Locks If that didn't help, here is how to debug further. Identifying the problem and getting the necessary information to diagnose it is the first step in troubleshooting your Gluster setup. As Gluster operations involve interactions between multiple processes, this can involve multiple steps.","title":"Where Do I Start?"},{"location":"Troubleshooting/#what-happened","text":"An operation failed High Memory Usage A Gluster process crashed","title":"What Happened?"},{"location":"Troubleshooting/gfid-to-path/","text":"Convert GFID to Path GlusterFS internal file identifier (GFID) is a uuid that is unique to each file across the entire cluster. This is analogous to inode number in a normal filesystem. The GFID of a file is stored in its xattr named trusted.gfid . Special mount using gfid-access translator: # mount -t glusterfs -o aux-gfid-mount vm1:test /mnt/testvol Assuming, you have GFID of a file from changelog (or somewhere else). For trying this out, you can get GFID of a file from mountpoint: # getfattr -n glusterfs.gfid.string /mnt/testvol/dir/file Get file path from GFID (Method 1): (Lists hardlinks delimited by : , returns path as seen from mountpoint) Turn on build-pgfid option # gluster volume set test build-pgfid on Read virtual xattr glusterfs.ancestry.path which contains the file path getfattr -n glusterfs.ancestry.path -e text /mnt/testvol/.gfid/<GFID> Example: [root@vm1 glusterfs]# ls -il /mnt/testvol/dir/ total 1 10610563327990022372 -rw-r--r--. 2 root root 3 Jul 17 18:05 file 10610563327990022372 -rw-r--r--. 2 root root 3 Jul 17 18:05 file3 [root@vm1 glusterfs]# getfattr -n glusterfs.gfid.string /mnt/testvol/dir/file getfattr: Removing leading '/' from absolute path names # file: mnt/testvol/dir/file glusterfs.gfid.string=\"11118443-1894-4273-9340-4b212fa1c0e4\" [root@vm1 glusterfs]# getfattr -n glusterfs.ancestry.path -e text /mnt/testvol/.gfid/11118443-1894-4273-9340-4b212fa1c0e4 getfattr: Removing leading '/' from absolute path names # file: mnt/testvol/.gfid/11118443-1894-4273-9340-4b212fa1c0e4 glusterfs.ancestry.path=\"/dir/file:/dir/file3\" Get file path from GFID (Method 2): (Does not list all hardlinks, returns backend brick path) getfattr -n trusted.glusterfs.pathinfo -e text /mnt/testvol/.gfid/<GFID> Example: [root@vm1 glusterfs]# getfattr -n trusted.glusterfs.pathinfo -e text /mnt/testvol/.gfid/11118443-1894-4273-9340-4b212fa1c0e4 getfattr: Removing leading '/' from absolute path names # file: mnt/testvol/.gfid/11118443-1894-4273-9340-4b212fa1c0e4 trusted.glusterfs.pathinfo=\"(<DISTRIBUTE:test-dht> <POSIX(/mnt/brick-test/b):vm1:/mnt/brick-test/b/dir//file3>)\" References and links: posix: placeholders for GFID to path conversion","title":"gfid to path"},{"location":"Troubleshooting/gfid-to-path/#convert-gfid-to-path","text":"GlusterFS internal file identifier (GFID) is a uuid that is unique to each file across the entire cluster. This is analogous to inode number in a normal filesystem. The GFID of a file is stored in its xattr named trusted.gfid .","title":"Convert GFID to Path"},{"location":"Troubleshooting/gfid-to-path/#special-mount-using-gfid-access-translator","text":"# mount -t glusterfs -o aux-gfid-mount vm1:test /mnt/testvol Assuming, you have GFID of a file from changelog (or somewhere else). For trying this out, you can get GFID of a file from mountpoint: # getfattr -n glusterfs.gfid.string /mnt/testvol/dir/file","title":"Special mount using gfid-access translator:"},{"location":"Troubleshooting/gfid-to-path/#get-file-path-from-gfid-method-1","text":"(Lists hardlinks delimited by : , returns path as seen from mountpoint)","title":"Get file path from GFID (Method 1):"},{"location":"Troubleshooting/gfid-to-path/#turn-on-build-pgfid-option","text":"# gluster volume set test build-pgfid on Read virtual xattr glusterfs.ancestry.path which contains the file path getfattr -n glusterfs.ancestry.path -e text /mnt/testvol/.gfid/<GFID> Example: [root@vm1 glusterfs]# ls -il /mnt/testvol/dir/ total 1 10610563327990022372 -rw-r--r--. 2 root root 3 Jul 17 18:05 file 10610563327990022372 -rw-r--r--. 2 root root 3 Jul 17 18:05 file3 [root@vm1 glusterfs]# getfattr -n glusterfs.gfid.string /mnt/testvol/dir/file getfattr: Removing leading '/' from absolute path names # file: mnt/testvol/dir/file glusterfs.gfid.string=\"11118443-1894-4273-9340-4b212fa1c0e4\" [root@vm1 glusterfs]# getfattr -n glusterfs.ancestry.path -e text /mnt/testvol/.gfid/11118443-1894-4273-9340-4b212fa1c0e4 getfattr: Removing leading '/' from absolute path names # file: mnt/testvol/.gfid/11118443-1894-4273-9340-4b212fa1c0e4 glusterfs.ancestry.path=\"/dir/file:/dir/file3\"","title":"Turn on build-pgfid option"},{"location":"Troubleshooting/gfid-to-path/#get-file-path-from-gfid-method-2","text":"(Does not list all hardlinks, returns backend brick path) getfattr -n trusted.glusterfs.pathinfo -e text /mnt/testvol/.gfid/<GFID> Example: [root@vm1 glusterfs]# getfattr -n trusted.glusterfs.pathinfo -e text /mnt/testvol/.gfid/11118443-1894-4273-9340-4b212fa1c0e4 getfattr: Removing leading '/' from absolute path names # file: mnt/testvol/.gfid/11118443-1894-4273-9340-4b212fa1c0e4 trusted.glusterfs.pathinfo=\"(<DISTRIBUTE:test-dht> <POSIX(/mnt/brick-test/b):vm1:/mnt/brick-test/b/dir//file3>)\"","title":"Get file path from GFID (Method 2):"},{"location":"Troubleshooting/gfid-to-path/#references-and-links","text":"posix: placeholders for GFID to path conversion","title":"References and links:"},{"location":"Troubleshooting/gluster-crash/","text":"Debugging a Crash To find out why a Gluster process terminated abruptly, we need the following: A coredump of the process that crashed The exact version of Gluster that is running The Gluster log files the output of gluster volume info Steps to reproduce the crash if available Contact the community with this information or open an issue","title":"Crashes"},{"location":"Troubleshooting/gluster-crash/#debugging-a-crash","text":"To find out why a Gluster process terminated abruptly, we need the following: A coredump of the process that crashed The exact version of Gluster that is running The Gluster log files the output of gluster volume info Steps to reproduce the crash if available Contact the community with this information or open an issue","title":"Debugging a Crash"},{"location":"Troubleshooting/resolving-splitbrain/","text":"Heal info and split-brain resolution This document explains the heal info command available in gluster for monitoring pending heals in replicate volumes and the methods available to resolve split-brains. Types of Split-Brains: A file is said to be in split-brain when Gluster AFR cannot determine which copy in the replica is the correct one. There are three types of split-brains: Data split-brain: The data in the file differs on the bricks in the replica set Metadata split-brain: The metadata differs on the bricks Entry split-brain: The GFID of the file is different on the bricks in the replica or the type of the file is different on the bricks in the replica. Type-mismatch cannot be healed using any of the split-brain resolution methods while gfid split-brains can be. 1) Volume heal info: Usage: gluster volume heal <VOLNAME> info This lists all the files that require healing (and will be processed by the self-heal daemon). It prints either their path or their GFID. Interpreting the output All the files listed in the output of this command need to be healed. The files listed may also be accompanied by the following tags: a) 'Is in split-brain' A file in data or metadata split-brain will be listed with \" - Is in split-brain\" appended after its path/GFID. E.g. \"/file4\" in the output provided below. However, for a file in GFID split-brain, the parent directory of the file is shown to be in split-brain and the file itself is shown to be needing healing, e.g. \"/dir\" in the output provided below is in split-brain because of GFID split-brain of file \"/dir/a\". Files in split-brain cannot be healed without resolving the split-brain. b) 'Is possibly undergoing heal' When the heal info command is run, it (or to be more specific, the 'glfsheal' binary that is executed when you run the command) takes locks on each file to find if it needs healing. However, if the self-heal daemon had already started healing the file, it would have taken locks which glfsheal wouldn't be able to acquire. In such a case, it could print this message. Another possible case could be multiple glfsheal processes running simultaneously (e.g. multiple users ran a heal info command at the same time) and competing for same lock. The following is an example of heal info command's output. Example Consider a replica volume \"test\" with two bricks b1 and b2; self-heal daemon off, mounted at /mnt. # gluster volume heal test info Brick \\<hostname:brickpath-b1> <gfid:aaca219f-0e25-4576-8689-3bfd93ca70c2> - Is in split-brain <gfid:39f301ae-4038-48c2-a889-7dac143e82dd> - Is in split-brain <gfid:c3c94de2-232d-4083-b534-5da17fc476ac> - Is in split-brain <gfid:6dc78b20-7eb6-49a3-8edb-087b90142246> Number of entries: 4 Brick <hostname:brickpath-b2> /dir/file2 /dir/file1 - Is in split-brain /dir - Is in split-brain /dir/file3 /file4 - Is in split-brain /dir/a Number of entries: 6 Analysis of the output It can be seen that A) from brick b1, four entries need healing: 1) file with gfid:6dc78b20-7eb6-49a3-8edb-087b90142246 needs healing 2) \"aaca219f-0e25-4576-8689-3bfd93ca70c2\", \"39f301ae-4038-48c2-a889-7dac143e82dd\" and \"c3c94de2-232d-4083-b534-5da17fc476ac\" are in split-brain B) from brick b2 six entries need healing- 1) \"a\", \"file2\" and \"file3\" need healing 2) \"file1\", \"file4\" & \"/dir\" are in split-brain 2. Volume heal info split-brain Usage: gluster volume heal <VOLNAME> info split-brain This command only shows the list of files that are in split-brain. The output is therefore a subset of gluster volume heal <VOLNAME> info Example # gluster volume heal test info split-brain Brick <hostname:brickpath-b1> <gfid:aaca219f-0e25-4576-8689-3bfd93ca70c2> <gfid:39f301ae-4038-48c2-a889-7dac143e82dd> <gfid:c3c94de2-232d-4083-b534-5da17fc476ac> Number of entries in split-brain: 3 Brick <hostname:brickpath-b2> /dir/file1 /dir /file4 Number of entries in split-brain: 3 Note that similar to the heal info command, for GFID split-brains (same filename but different GFID) their parent directories are listed to be in split-brain. 3. Resolution of split-brain using gluster CLI Once the files in split-brain are identified, their resolution can be done from the gluster command line using various policies. Type-mismatch cannot be healed using this methods. Split-brain resolution commands let the user resolve data, metadata, and GFID split-brains. 3.1 Resolution of data/metadata split-brain using gluster CLI Data and metadata split-brains can be resolved using the following policies: i) Select the bigger-file as source This command is useful for per file healing where it is known/decided that the file with bigger size is to be considered as source. gluster volume heal <VOLNAME> split-brain bigger-file <FILE> Here, <FILE> can be either the full file name as seen from the root of the volume (or) the GFID-string representation of the file, which sometimes gets displayed in the heal info command's output. Once this command is executed, the replica containing the <FILE> with a bigger size is found and healing is completed with that brick as a source. Example : Consider the earlier output of the heal info split-brain command. Before healing the file, notice file size and md5 checksums : On brick b1: [brick1]# stat b1/dir/file1 File: \u2018b1/dir/file1\u2019 Size: 17 Blocks: 16 IO Block: 4096 regular file Device: fd03h/64771d Inode: 919362 Links: 2 Access: (0644/-rw-r--r--) Uid: ( 0/ root) Gid: ( 0/ root) Access: 2015-03-06 13:55:40.149897333 +0530 Modify: 2015-03-06 13:55:37.206880347 +0530 Change: 2015-03-06 13:55:37.206880347 +0530 Birth: - [brick1]# [brick1]# md5sum b1/dir/file1 040751929ceabf77c3c0b3b662f341a8 b1/dir/file1 On brick b2: [brick2]# stat b2/dir/file1 File: \u2018b2/dir/file1\u2019 Size: 13 Blocks: 16 IO Block: 4096 regular file Device: fd03h/64771d Inode: 919365 Links: 2 Access: (0644/-rw-r--r--) Uid: ( 0/ root) Gid: ( 0/ root) Access: 2015-03-06 13:54:22.974451898 +0530 Modify: 2015-03-06 13:52:22.910758923 +0530 Change: 2015-03-06 13:52:22.910758923 +0530 Birth: - [brick2]# [brick2]# md5sum b2/dir/file1 cb11635a45d45668a403145059c2a0d5 b2/dir/file1 Healing file1 using the above command :- gluster volume heal test split-brain bigger-file /dir/file1 Healed /dir/file1. After healing is complete, the md5sum and file size on both bricks should be the same. On brick b1: [brick1]# stat b1/dir/file1 File: \u2018b1/dir/file1\u2019 Size: 17 Blocks: 16 IO Block: 4096 regular file Device: fd03h/64771d Inode: 919362 Links: 2 Access: (0644/-rw-r--r--) Uid: ( 0/ root) Gid: ( 0/ root) Access: 2015-03-06 14:17:27.752429505 +0530 Modify: 2015-03-06 13:55:37.206880347 +0530 Change: 2015-03-06 14:17:12.880343950 +0530 Birth: - [brick1]# [brick1]# md5sum b1/dir/file1 040751929ceabf77c3c0b3b662f341a8 b1/dir/file1 On brick b2: [brick2]# stat b2/dir/file1 File: \u2018b2/dir/file1\u2019 Size: 17 Blocks: 16 IO Block: 4096 regular file Device: fd03h/64771d Inode: 919365 Links: 2 Access: (0644/-rw-r--r--) Uid: ( 0/ root) Gid: ( 0/ root) Access: 2015-03-06 14:17:23.249403600 +0530 Modify: 2015-03-06 13:55:37.206880000 +0530 Change: 2015-03-06 14:17:12.881343955 +0530 Birth: - [brick2]# [brick2]# md5sum b2/dir/file1 040751929ceabf77c3c0b3b662f341a8 b2/dir/file1 ii) Select the file with the latest mtime as source gluster volume heal <VOLNAME> split-brain latest-mtime <FILE> As is perhaps self-explanatory, this command uses the brick having the latest modification time for <FILE> as the source for healing. iii) Select one of the bricks in the replica as the source for a particular file gluster volume heal <VOLNAME> split-brain source-brick <HOSTNAME:BRICKNAME> <FILE> Here, <HOSTNAME:BRICKNAME> is selected as source brick and <FILE> present in the source brick is taken as the source for healing. Example : Notice the md5 checksums and file size before and after healing. Before heal : On brick b1: [brick1]# stat b1/file4 File: \u2018b1/file4\u2019 Size: 4 Blocks: 16 IO Block: 4096 regular file Device: fd03h/64771d Inode: 919356 Links: 2 Access: (0644/-rw-r--r--) Uid: ( 0/ root) Gid: ( 0/ root) Access: 2015-03-06 13:53:19.417085062 +0530 Modify: 2015-03-06 13:53:19.426085114 +0530 Change: 2015-03-06 13:53:19.426085114 +0530 Birth: - [brick1]# [brick1]# md5sum b1/file4 b6273b589df2dfdbd8fe35b1011e3183 b1/file4 On brick b2: [brick2]# stat b2/file4 File: \u2018b2/file4\u2019 Size: 4 Blocks: 16 IO Block: 4096 regular file Device: fd03h/64771d Inode: 919358 Links: 2 Access: (0644/-rw-r--r--) Uid: ( 0/ root) Gid: ( 0/ root) Access: 2015-03-06 13:52:35.761833096 +0530 Modify: 2015-03-06 13:52:35.769833142 +0530 Change: 2015-03-06 13:52:35.769833142 +0530 Birth: - [brick2]# [brick2]# md5sum b2/file4 0bee89b07a248e27c83fc3d5951213c1 b2/file4 Healing the file with gfid c3c94de2-232d-4083-b534-5da17fc476ac using the above command : # gluster volume heal test split-brain source-brick test-host:/test/b1 gfid:c3c94de2-232d-4083-b534-5da17fc476ac Healed gfid:c3c94de2-232d-4083-b534-5da17fc476ac. After healing : On brick b1: # stat b1/file4 File: \u2018b1/file4\u2019 Size: 4 Blocks: 16 IO Block: 4096 regular file Device: fd03h/64771d Inode: 919356 Links: 2 Access: (0644/-rw-r--r--) Uid: ( 0/ root) Gid: ( 0/ root) Access: 2015-03-06 14:23:38.944609863 +0530 Modify: 2015-03-06 13:53:19.426085114 +0530 Change: 2015-03-06 14:27:15.058927962 +0530 Birth: - # md5sum b1/file4 b6273b589df2dfdbd8fe35b1011e3183 b1/file4 On brick b2: # stat b2/file4 File: \u2018b2/file4\u2019 Size: 4 Blocks: 16 IO Block: 4096 regular file Device: fd03h/64771d Inode: 919358 Links: 2 Access: (0644/-rw-r--r--) Uid: ( 0/ root) Gid: ( 0/ root) Access: 2015-03-06 14:23:38.944609000 +0530 Modify: 2015-03-06 13:53:19.426085000 +0530 Change: 2015-03-06 14:27:15.059927968 +0530 Birth: - # md5sum b2/file4 b6273b589df2dfdbd8fe35b1011e3183 b2/file4 iv) Select one brick of the replica as the source for all files gluster volume heal <VOLNAME> split-brain source-brick <HOSTNAME:BRICKNAME> Consider a scenario where many files are in split-brain such that one brick of replica pair is source. As the result of the above command all split-brained files in <HOSTNAME:BRICKNAME> are selected as source and healed to the sink. Example: Consider a volume having three entries \"a, b and c\" in split-brain. # gluster volume heal test split-brain source-brick test-host:/test/b1 Healed gfid:944b4764-c253-4f02-b35f-0d0ae2f86c0f. Healed gfid:3256d814-961c-4e6e-8df2-3a3143269ced. Healed gfid:b23dd8de-af03-4006-a803-96d8bc0df004. Number of healed entries: 3 3.2 Resolution of GFID split-brain using gluster CLI GFID split-brains can also be resolved by the gluster command line using the same policies that are used to resolve data and metadata split-brains. i) Selecting the bigger-file as source This method is useful for per file healing and where you can decided that the file with bigger size is to be considered as source. Run the following command to obtain the path of the file that is in split-brain: # gluster volume heal VOLNAME info split-brain From the output, identify the files for which file operations performed from the client failed with input/output error. Example : # gluster volume heal testvol info Brick 10.70.47.45:/bricks/brick2/b0 /f5 / - Is in split-brain Status: Connected Number of entries: 2 Brick 10.70.47.144:/bricks/brick2/b1 /f5 / - Is in split-brain Status: Connected Number of entries: 2 Note Entries which are in GFID split-brain may not be shown as in split-brain by the heal info or heal info split-brain commands always. For entry split-brains, it is the parent directory which is shown as being in split-brain. So one might need to run info split-brain to get the dir names and then heal info to get the list of files under that dir which might be in split-brain (it could just be needing heal without split-brain). In the above command, testvol is the volume name, b0 and b1 are the bricks. Execute the below getfattr command on the brick to fetch information if a file is in GFID split-brain or not. # getfattr -d -e hex -m. <path-to-file> Example : On brick /b0 # getfattr -d -m . -e hex /bricks/brick2/b0/f5 getfattr: Removing leading '/' from absolute path names file: bricks/brick2/b0/f5 security.selinux=0x73797374656d5f753a6f626a6563745f723a676c7573746572645f627269636b5f743a733000 trusted.afr.testvol-client-1=0x000000020000000100000000 trusted.afr.dirty=0x000000000000000000000000 trusted.gfid=0xce0a9956928e40afb78e95f78defd64f trusted.gfid2path.9cde09916eabc845=0x30303030303030302d303030302d303030302d303030302d3030303030303030303030312f6635 On brick /b1 # getfattr -d -m . -e hex /bricks/brick2/b1/f5 getfattr: Removing leading '/' from absolute path names file: bricks/brick2/b1/f5 security.selinux=0x73797374656d5f753a6f626a6563745f723a676c7573746572645f627269636b5f743a733000 trusted.afr.testvol-client-0=0x000000020000000100000000 trusted.afr.dirty=0x000000000000000000000000 trusted.gfid=0x9563544118653550e888ab38c232e0c trusted.gfid2path.9cde09916eabc845=0x30303030303030302d303030302d303030302d303030302d3030303030303030303030312f6635 You can notice the difference in GFID for the file f5 in both the bricks. You can find the differences in the file size by executing stat command on the file from the bricks. On brick /b0 # stat /bricks/brick2/b0/f5 File: \u2018/bricks/brick2/b0/f5\u2019 Size: 15 Blocks: 8 IO Block: 4096 regular file Device: fd15h/64789d Inode: 67113350 Links: 2 Access: (0644/-rw-r--r--) Uid: ( 0/ root) Gid: ( 0/ root) Context: system_u:object_r:glusterd_brick_t:s0 Access: 2018-08-29 20:46:26.353751073 +0530 Modify: 2018-08-29 20:46:26.361751203 +0530 Change: 2018-08-29 20:47:16.363751236 +0530 Birth: - On brick /b1 # stat /bricks/brick2/b1/f5 File: \u2018/bricks/brick2/b1/f5\u2019 Size: 2 Blocks: 8 IO Block: 4096 regular file Device: fd15h/64789d Inode: 67111750 Links: 2 Access: (0644/-rw-r--r--) Uid: ( 0/ root) Gid: ( 0/ root) Context: system_u:object_r:glusterd_brick_t:s0 Access: 2018-08-29 20:44:56.153301616 +0530 Modify: 2018-08-29 20:44:56.161301745 +0530 Change: 2018-08-29 20:44:56.162301761 +0530 Birth: - Execute the following command along with the full filename as seen from the root of the volume which is displayed in the heal info command's output: # gluster volume heal VOLNAME split-brain bigger-file FILE Example : # gluster volume heal testvol split-brain bigger-file /f5 GFID split-brain resolved for file /f5 After the healing is complete, the GFID of the file on both the bricks must be the same as that of the file which had the bigger size. The following is a sample output of the getfattr command after completion of healing the file. On brick /b0 # getfattr -d -m . -e hex /bricks/brick2/b0/f5 getfattr: Removing leading '/' from absolute path names file: bricks/brick2/b0/f5 security.selinux=0x73797374656d5f753a6f626a6563745f723a676c7573746572645f627269636b5f743a733000 trusted.gfid=0xce0a9956928e40afb78e95f78defd64f trusted.gfid2path.9cde09916eabc845=0x30303030303030302d303030302d303030302d303030302d3030303030303030303030312f6635 On brick /b1 # getfattr -d -m . -e hex /bricks/brick2/b1/f5 getfattr: Removing leading '/' from absolute path names file: bricks/brick2/b1/f5 security.selinux=0x73797374656d5f753a6f626a6563745f723a676c7573746572645f627269636b5f743a733000 trusted.gfid=0xce0a9956928e40afb78e95f78defd64f trusted.gfid2path.9cde09916eabc845=0x30303030303030302d303030302d303030302d303030302d3030303030303030303030312f6635 ii) Selecting the file with latest mtime as source This method is useful for per file healing and if you want the file with latest mtime has to be considered as source. Example : Lets take another file which is in GFID split-brain and try to heal that using the latest-mtime option. On brick /b0 # getfattr -d -m . -e hex /bricks/brick2/b0/f4 getfattr: Removing leading '/' from absolute path names file: bricks/brick2/b0/f4 security.selinux=0x73797374656d5f753a6f626a6563745f723a676c7573746572645f627269636b5f743a733000 trusted.afr.testvol-client-1=0x000000020000000100000000 trusted.afr.dirty=0x000000000000000000000000 trusted.gfid=0xb66b66d07b315f3c9cffac2fb6422a28 trusted.gfid2path.364f55367c7bd6f4=0x30303030303030302d303030302d303030302d303030302d3030303030303030303030312f6634 On brick /b1 # getfattr -d -m . -e hex /bricks/brick2/b1/f4 getfattr: Removing leading '/' from absolute path names file: bricks/brick2/b1/f4 security.selinux=0x73797374656d5f753a6f626a6563745f723a676c7573746572645f627269636b5f743a733000 trusted.afr.testvol-client-0=0x000000020000000100000000 trusted.afr.dirty=0x000000000000000000000000 trusted.gfid=0x87242f808c6e56a007ef7d49d197acff trusted.gfid2path.364f55367c7bd6f4=0x30303030303030302d303030302d303030302d303030302d3030303030303030303030312f6634 You can notice the difference in GFID for the file f4 in both the bricks. You can find the difference in the modification time by executing stat command on the file from the bricks. On brick /b0 # stat /bricks/brick2/b0/f4 File: \u2018/bricks/brick2/b0/f4\u2019 Size: 14 Blocks: 8 IO Block: 4096 regular file Device: fd15h/64789d Inode: 67113349 Links: 2 Access: (0644/-rw-r--r--) Uid: ( 0/ root) Gid: ( 0/ root) Context: system_u:object_r:glusterd_brick_t:s0 Access: 2018-08-29 20:57:38.913629991 +0530 Modify: 2018-08-29 20:57:38.921630122 +0530 Change: 2018-08-29 20:57:38.923630154 +0530 Birth: - On brick /b1 # stat /bricks/brick2/b1/f4 File: \u2018/bricks/brick2/b1/f4\u2019 Size: 2 Blocks: 8 IO Block: 4096 regular file Device: fd15h/64789d Inode: 67111749 Links: 2 Access: (0644/-rw-r--r--) Uid: ( 0/ root) Gid: ( 0/ root) Context: system_u:object_r:glusterd_brick_t:s0 Access: 2018-08-24 20:54:50.953217256 +0530 Modify: 2018-08-24 20:54:50.961217385 +0530 Change: 2018-08-24 20:54:50.962217402 +0530 Birth: - Execute the following command: # gluster volume heal VOLNAME split-brain latest-mtime FILE Example : # gluster volume heal testvol split-brain latest-mtime /f4 GFID split-brain resolved for file /f4 After the healing is complete, the GFID of the files on both bricks must be same. The following is a sample output of the getfattr command after completion of healing the file. You can notice that the file has been healed using the brick having the latest mtime as the source. On brick /b0 ```console# getfattr -d -m . -e hex /bricks/brick2/b0/f4 getfattr: Removing leading '/' from absolute path names file: bricks/brick2/b0/f4 security.selinux=0x73797374656d5f753a6f626a6563745f723a676c7573746572645f627269636b5f743a733000 trusted.gfid=0xb66b66d07b315f3c9cffac2fb6422a28 trusted.gfid2path.364f55367c7bd6f4=0x30303030303030302d303030302d303030302d303030302d3030303030303030303030312f6634 On brick /b1 ```console # getfattr -d -m . -e hex /bricks/brick2/b1/f4 getfattr: Removing leading '/' from absolute path names file: bricks/brick2/b1/f4 security.selinux=0x73797374656d5f753a6f626a6563745f723a676c7573746572645f627269636b5f743a733000 trusted.gfid=0xb66b66d07b315f3c9cffac2fb6422a28 trusted.gfid2path.364f55367c7bd6f4=0x30303030303030302d303030302d303030302d303030302d3030303030303030303030312f6634 iii) Select one of the bricks in the replica as source for a particular file This method is useful for per file healing and if you know which copy of the file is good. Example : Lets take another file which is in GFID split-brain and try to heal that using the source-brick option. On brick /b0 # getfattr -d -m . -e hex /bricks/brick2/b0/f3 getfattr: Removing leading '/' from absolute path names file: bricks/brick2/b0/f3 security.selinux=0x73797374656d5f753a6f626a6563745f723a676c7573746572645f627269636b5f743a733000 trusted.afr.testvol-client-1=0x000000020000000100000000 trusted.afr.dirty=0x000000000000000000000000 trusted.gfid=0x9d542fb1b3b15837a2f7f9dcdf5d6ee8 trusted.gfid2path.364f55367c7bd6f4=0x30303030303030302d303030302d303030302d303030302d3030303030303030303030312f6634 On brick /b1 # getfattr -d -m . -e hex /bricks/brick2/b1/f3 getfattr: Removing leading '/' from absolute path names file: bricks/brick2/b0/f3 security.selinux=0x73797374656d5f753a6f626a6563745f723a676c7573746572645f627269636b5f743a733000 trusted.afr.testvol-client-1=0x000000020000000100000000 trusted.afr.dirty=0x000000000000000000000000 trusted.gfid=0xc90d9b0f65f6530b95b9f3f8334033df trusted.gfid2path.364f55367c7bd6f4=0x30303030303030302d303030302d303030302d303030302d3030303030303030303030312f6634 You can notice the difference in GFID for the file f3 in both the bricks. Execute the following command: # gluster volume heal VOLNAME split-brain source-brick HOSTNAME:export-directory-absolute-path FILE In this command, FILE present in HOSTNAME : export-directory-absolute-path is taken as source for healing. Example : # gluster volume heal testvol split-brain source-brick 10.70.47.144:/bricks/brick2/b1 /f3 GFID split-brain resolved for file /f3 After the healing is complete, the GFID of the file on both the bricks should be same as that of the brick which was chosen as source for healing. The following is a sample output of the getfattr command after the file is healed. On brick /b0 # getfattr -d -m . -e hex /bricks/brick2/b0/f3 getfattr: Removing leading '/' from absolute path names file: bricks/brick2/b0/f3 security.selinux=0x73797374656d5f753a6f626a6563745f723a676c7573746572645f627269636b5f743a733000 trusted.gfid=0x90d9b0f65f6530b95b9f3f8334033df trusted.gfid2path.364f55367c7bd6f4=0x30303030303030302d303030302d303030302d303030302d3030303030303030303030312f6634 On brick /b1 # getfattr -d -m . -e hex /bricks/brick2/b1/f3 getfattr: Removing leading '/' from absolute path names file: bricks/brick2/b1/f3 security.selinux=0x73797374656d5f753a6f626a6563745f723a676c7573746572645f627269636b5f743a733000 trusted.gfid=0x90d9b0f65f6530b95b9f3f8334033df trusted.gfid2path.364f55367c7bd6f4=0x30303030303030302d303030302d303030302d303030302d3030303030303030303030312f6634 Note - One cannot use the GFID of the file as an argument with any of the CLI options to resolve GFID split-brain. It should be the absolute path as seen from the mount point to the file considered as source. With source-brick option there is no way to resolve all the GFID split-brain in one shot by not specifying any file path in the CLI as done while resolving data or metadata split-brain. For each file in GFID split-brain, run the CLI with the policy you want to use. Resolving directory GFID split-brain using CLI with the \"source-brick\" option in a \"distributed-replicated\" volume needs to be done on all the sub-volumes explicitly, which are in this state. Since directories get created on all the sub-volumes, using one particular brick as source for directory GFID split-brain heals the directory for that particular sub-volume. Source brick should be chosen in such a way that after heal all the bricks of all the sub-volumes have the same GFID. Note: As mentioned earlier, type-mismatch can not be resolved using CLI. Type-mismatch means different st_mode values (for example, the entry is a file in one brick while it is a directory on the other). Trying to heal such entry would fail. Example The entry named \"entry1\" is of different types on the bricks of the replica. Lets try to heal that using the split-brain CLI. # gluster volume heal test split-brain source-brick test-host:/test/b1 /entry1 Healing /entry1 failed:Operation not permitted. Volume heal failed. However, they can be fixed by deleting the file from all but one bricks. See Fixing Directory entry split-brain An overview of working of heal info commands When these commands are invoked, a \"glfsheal\" process is spawned which reads the entries from the various sub-directories under /<brick-path>/.glusterfs/indices/ of all the bricks that are up (that it can connect to) one after another. These entries are GFIDs of files that might need healing. Once GFID entries from a brick are obtained, based on the lookup response of this file on each participating brick of replica-pair & trusted.afr.* extended attributes it is found out if the file needs healing, is in split-brain etc based on the requirement of each command and displayed to the user. 4. Resolution of split-brain from the mount point A set of getfattr and setfattr commands have been provided to detect the data and metadata split-brain status of a file and resolve split-brain, if any, from mount point. Consider a volume \"test\", having bricks b0, b1, b2 and b3. # gluster volume info test Volume Name: test Type: Distributed-Replicate Volume ID: 00161935-de9e-4b80-a643-b36693183b61 Status: Started Number of Bricks: 2 x 2 = 4 Transport-type: tcp Bricks: Brick1: test-host:/test/b0 Brick2: test-host:/test/b1 Brick3: test-host:/test/b2 Brick4: test-host:/test/b3 Directory structure of the bricks is as follows: # tree -R /test/b? /test/b0 \u251c\u2500\u2500 dir \u2502 \u2514\u2500\u2500 a \u2514\u2500\u2500 file100 /test/b1 \u251c\u2500\u2500 dir \u2502 \u2514\u2500\u2500 a \u2514\u2500\u2500 file100 /test/b2 \u251c\u2500\u2500 dir \u251c\u2500\u2500 file1 \u251c\u2500\u2500 file2 \u2514\u2500\u2500 file99 /test/b3 \u251c\u2500\u2500 dir \u251c\u2500\u2500 file1 \u251c\u2500\u2500 file2 \u2514\u2500\u2500 file99 Some files in the volume are in split-brain. # gluster v heal test info split-brain Brick test-host:/test/b0/ /file100 /dir Number of entries in split-brain: 2 Brick test-host:/test/b1/ /file100 /dir Number of entries in split-brain: 2 Brick test-host:/test/b2/ /file99 <gfid:5399a8d1-aee9-4653-bb7f-606df02b3696> Number of entries in split-brain: 2 Brick test-host:/test/b3/ <gfid:05c4b283-af58-48ed-999e-4d706c7b97d5> <gfid:5399a8d1-aee9-4653-bb7f-606df02b3696> Number of entries in split-brain: 2 To know data/metadata split-brain status of a file: getfattr -n replica.split-brain-status <path-to-file> The above command executed from mount provides information if a file is in data/metadata split-brain. Also provides the list of afr children to analyze to get more information about the file. This command is not applicable to gfid/directory split-brain. Example: 1) \"file100\" is in metadata split-brain. Executing the above mentioned command for file100 gives : # getfattr -n replica.split-brain-status file100 file: file100 replica.split-brain-status=\"data-split-brain:no metadata-split-brain:yes Choices:test-client-0,test-client-1\" 2) \"file1\" is in data split-brain. # getfattr -n replica.split-brain-status file1 file: file1 replica.split-brain-status=\"data-split-brain:yes metadata-split-brain:no Choices:test-client-2,test-client-3\" 3) \"file99\" is in both data and metadata split-brain. # getfattr -n replica.split-brain-status file99 file: file99 replica.split-brain-status=\"data-split-brain:yes metadata-split-brain:yes Choices:test-client-2,test-client-3\" 4) \"dir\" is in directory split-brain but as mentioned earlier, the above command is not applicable to such split-brain. So it says that the file is not under data or metadata split-brain. # getfattr -n replica.split-brain-status dir file: dir replica.split-brain-status=\"The file is not under data or metadata split-brain\" 5) \"file2\" is not in any kind of split-brain. # getfattr -n replica.split-brain-status file2 file: file2 replica.split-brain-status=\"The file is not under data or metadata split-brain\" To analyze the files in data and metadata split-brain Trying to do operations (say cat, getfattr etc) from the mount on files in split-brain, gives an input/output error. To enable the users analyze such files, a setfattr command is provided. # setfattr -n replica.split-brain-choice -v \"choiceX\" <path-to-file> Using this command, a particular brick can be chosen to access the file in split-brain from. Example: 1) \"file1\" is in data-split-brain. Trying to read from the file gives input/output error. # cat file1 cat: file1: Input/output error Split-brain choices provided for file1 were test-client-2 and test-client-3. Setting test-client-2 as split-brain choice for file1 serves reads from b2 for the file. # setfattr -n replica.split-brain-choice -v test-client-2 file1 Now, read operations on the file can be done. # cat file1 xyz Similarly, to inspect the file from other choice, replica.split-brain-choice is to be set to test-client-3. Trying to inspect the file from a wrong choice errors out. To undo the split-brain-choice that has been set, the above mentioned setfattr command can be used with \"none\" as the value for extended attribute. Example: # setfattr -n replica.split-brain-choice -v none file1 Now performing cat operation on the file will again result in input/output error, as before. # cat file cat: file1: Input/output error Once the choice for resolving split-brain is made, source brick is supposed to be set for the healing to be done. This is done using the following command: # setfattr -n replica.split-brain-heal-finalize -v <heal-choice> <path-to-file> Example # setfattr -n replica.split-brain-heal-finalize -v test-client-2 file1 The above process can be used to resolve data and/or metadata split-brain on all the files. NOTE : 1) If \"fopen-keep-cache\" fuse mount option is disabled then inode needs to be invalidated each time before selecting a new replica.split-brain-choice to inspect a file. This can be done by using: # sefattr -n inode-invalidate -v 0 <path-to-file> 2) The above mentioned process for split-brain resolution from mount will not work on nfs mounts as it doesn't provide xattrs support. 5. Automagic unsplit-brain by [ctime|mtime|size|majority] The CLI and fuse mount based resolution methods require intervention in the sense that the admin/ user needs to run the commands manually. There is a cluster.favorite-child-policy volume option which when set to one of the various policies available, automatically resolve split-brains without user intervention. The default value is 'none', i.e. it is disabled. # gluster volume set help | grep -A3 cluster.favorite-child-policy Option: cluster.favorite-child-policy Default Value: none Description: This option can be used to automatically resolve split-brains using various policies without user intervention. \"size\" picks the file with the biggest size as the source. \"ctime\" and \"mtime\" pick the file with the latest ctime and mtime respectively as the source. \"majority\" picks a file with identical mtime and size in more than half the number of bricks in the replica. cluster.favorite-child-policy applies to all files of the volume. It is assumed that if this option is enabled with a particular policy, you don't care to examine the split-brain files on a per file basis but just want the split-brain to be resolved as and when it occurs based on the set policy. Manual Split-Brain Resolution: Quick Start: Get the path of the file that is in split-brain: It can be obtained either by a) The command gluster volume heal info split-brain . b) Identify the files for which file operations performed from the client keep failing with Input/Output error. Close the applications that opened this file from the mount point. In case of VMs, they need to be powered-off. Decide on the correct copy: This is done by observing the afr changelog extended attributes of the file on the bricks using the getfattr command; then identifying the type of split-brain (data split-brain, metadata split-brain, entry split-brain or split-brain due to gfid-mismatch); and finally determining which of the bricks contains the 'good copy' of the file. getfattr -d -m . -e hex <file-path-on-brick> . It is also possible that one brick might contain the correct data while the other might contain the correct metadata. Reset the relevant extended attribute on the brick(s) that contains the 'bad copy' of the file data/metadata using the setfattr command. setfattr -n <attribute-name> -v <attribute-value> <file-path-on-brick> Trigger self-heal on the file by performing lookup from the client: ls -l <file-path-on-gluster-mount> Detailed Instructions for steps 3 through 5: To understand how to resolve split-brain we need to know how to interpret the afr changelog extended attributes. Execute getfattr -d -m . -e hex <file-path-on-brick> Example: [root@store3 ~]# getfattr -d -e hex -m. brick-a/file.txt \\#file: brick-a/file.txt security.selinux=0x726f6f743a6f626a6563745f723a66696c655f743a733000 trusted.afr.vol-client-2=0x000000000000000000000000 trusted.afr.vol-client-3=0x000000000200000000000000 trusted.gfid=0x307a5c9efddd4e7c96e94fd4bcdcbd1b The extended attributes with trusted.afr.<volname>-client-<subvolume-index> are used by afr to maintain changelog of the file.The values of the trusted.afr.<volname>-client-<subvolume-index> are calculated by the glusterfs client (fuse or nfs-server) processes. When the glusterfs client modifies a file or directory, the client contacts each brick and updates the changelog extended attribute according to the response of the brick. 'subvolume-index' is nothing but (brick number - 1) in gluster volume info <volname> output. Example: [root@pranithk-laptop ~]# gluster volume info vol Volume Name: vol Type: Distributed-Replicate Volume ID: 4f2d7849-fbd6-40a2-b346-d13420978a01 Status: Created Number of Bricks: 4 x 2 = 8 Transport-type: tcp Bricks: brick-a: pranithk-laptop:/gfs/brick-a brick-b: pranithk-laptop:/gfs/brick-b brick-c: pranithk-laptop:/gfs/brick-c brick-d: pranithk-laptop:/gfs/brick-d brick-e: pranithk-laptop:/gfs/brick-e brick-f: pranithk-laptop:/gfs/brick-f brick-g: pranithk-laptop:/gfs/brick-g brick-h: pranithk-laptop:/gfs/brick-h In the example above: Brick | Replica set | Brick subvolume index ---------------------------------------------------------------------------- -/gfs/brick-a | 0 | 0 -/gfs/brick-b | 0 | 1 -/gfs/brick-c | 1 | 2 -/gfs/brick-d | 1 | 3 -/gfs/brick-e | 2 | 4 -/gfs/brick-f | 2 | 5 -/gfs/brick-g | 3 | 6 -/gfs/brick-h | 3 | 7 Each file in a brick maintains the changelog of itself and that of the files present in all the other bricks in its replica set as seen by that brick. In the example volume given above, all files in brick-a will have 2 entries, one for itself and the other for the file present in its replica pair, i.e.brick-b: trusted.afr.vol-client-0=0x000000000000000000000000 -->changelog for itself (brick-a) trusted.afr.vol-client-1=0x000000000000000000000000 -->changelog for brick-b as seen by brick-a Likewise, all files in brick-b will have: trusted.afr.vol-client-0=0x000000000000000000000000 -->changelog for brick-a as seen by brick-b trusted.afr.vol-client-1=0x000000000000000000000000 -->changelog for itself (brick-b) The same can be extended for other replica pairs. Interpreting Changelog (roughly pending operation count) Value: Each extended attribute has a value which is 24 hexa decimal digits. First 8 digits represent changelog of data. Second 8 digits represent changelog of metadata. Last 8 digits represent Changelog of directory entries. Pictorially representing the same, we have: 0x 000003d7 00000001 00000000 | | | | | \\_ changelog of directory entries | \\_ changelog of metadata \\ _ changelog of data For Directories metadata and entry changelogs are valid. For regular files data and metadata changelogs are valid. For special files like device files etc metadata changelog is valid. When a file split-brain happens it could be either data split-brain or meta-data split-brain or both. When a split-brain happens the changelog of the file would be something like this: Example:(Lets consider both data, metadata split-brain on same file). [root@pranithk-laptop vol]# getfattr -d -m . -e hex /gfs/brick-?/a getfattr: Removing leading '/' from absolute path names \\#file: gfs/brick-a/a trusted.afr.vol-client-0=0x000000000000000000000000 trusted.afr.vol-client-1=0x000003d70000000100000000 trusted.gfid=0x80acdbd886524f6fbefa21fc356fed57 \\#file: gfs/brick-b/a trusted.afr.vol-client-0=0x000003b00000000100000000 trusted.afr.vol-client-1=0x000000000000000000000000 trusted.gfid=0x80acdbd886524f6fbefa21fc356fed57 Observations: According to changelog extended attributes on file /gfs/brick-a/a: The first 8 digits of trusted.afr.vol-client-0 are all zeros (0x00000000................), and the first 8 digits of trusted.afr.vol-client-1 are not all zeros (0x000003d7................). So the changelog on /gfs/brick-a/a implies that some data operations succeeded on itself but failed on /gfs/brick-b/a. The second 8 digits of trusted.afr.vol-client-0 are all zeros (0x........00000000........), and the second 8 digits of trusted.afr.vol-client-1 are not all zeros (0x........00000001........). So the changelog on /gfs/brick-a/a implies that some metadata operations succeeded on itself but failed on /gfs/brick-b/a. According to Changelog extended attributes on file /gfs/brick-b/a: The first 8 digits of trusted.afr.vol-client-0 are not all zeros (0x000003b0................), and the first 8 digits of trusted.afr.vol-client-1 are all zeros (0x00000000................). So the changelog on /gfs/brick-b/a implies that some data operations succeeded on itself but failed on /gfs/brick-a/a. The second 8 digits of trusted.afr.vol-client-0 are not all zeros (0x........00000001........), and the second 8 digits of trusted.afr.vol-client-1 are all zeros (0x........00000000........). So the changelog on /gfs/brick-b/a implies that some metadata operations succeeded on itself but failed on /gfs/brick-a/a. Since both the copies have data, metadata changes that are not on the other file, it is in both data and metadata split-brain. Deciding on the correct copy: The user may have to inspect stat,getfattr output of the files to decide which metadata to retain and contents of the file to decide which data to retain. Continuing with the example above, lets say we want to retain the data of /gfs/brick-a/a and metadata of /gfs/brick-b/a. Resetting the relevant changelogs to resolve the split-brain: For resolving data-split-brain: We need to change the changelog extended attributes on the files as if some data operations succeeded on /gfs/brick-a/a but failed on /gfs/brick-b/a. But /gfs/brick-b/a should NOT have any changelog which says some data operations succeeded on /gfs/brick-b/a but failed on /gfs/brick-a/a. We need to reset the data part of the changelog on trusted.afr.vol-client-0 of /gfs/brick-b/a. For resolving metadata-split-brain: We need to change the changelog extended attributes on the files as if some metadata operations succeeded on /gfs/brick-b/a but failed on /gfs/brick-a/a. But /gfs/brick-a/a should NOT have any changelog which says some metadata operations succeeded on /gfs/brick-a/a but failed on /gfs/brick-b/a. We need to reset metadata part of the changelog on trusted.afr.vol-client-1 of /gfs/brick-a/a So, the intended changes are: On /gfs/brick-b/a: For trusted.afr.vol-client-0 0x000003b00000000100000000 to 0x000000000000000100000000 (Note that the metadata part is still not all zeros) Hence execute setfattr -n trusted.afr.vol-client-0 -v 0x000000000000000100000000 /gfs/brick-b/a On /gfs/brick-a/a: For trusted.afr.vol-client-1 0x000003d70000000100000000 to 0x000003d70000000000000000 (Note that the data part is still not all zeros) Hence execute setfattr -n trusted.afr.vol-client-1 -v 0x000003d70000000000000000 /gfs/brick-a/a Thus after the above operations are done, the changelogs look like this: [root@pranithk-laptop vol]# getfattr -d -m . -e hex /gfs/brick-?/a getfattr: Removing leading '/' from absolute path names #file: gfs/brick-a/a trusted.afr.vol-client-0=0x000000000000000000000000 trusted.afr.vol-client-1=0x000003d70000000000000000 trusted.gfid=0x80acdbd886524f6fbefa21fc356fed57 #file: gfs/brick-b/a trusted.afr.vol-client-0=0x000000000000000100000000 trusted.afr.vol-client-1=0x000000000000000000000000 trusted.gfid=0x80acdbd886524f6fbefa21fc356fed57 Triggering Self-heal: Perform ls -l <file-path-on-gluster-mount> to trigger healing. Fixing Directory entry split-brain: Afr has the ability to conservatively merge different entries in the directories when there is a split-brain on directory. If on one brick directory 'd' has entries '1', '2' and has entries '3', '4' on the other brick then afr will merge all of the entries in the directory to have '1', '2', '3', '4' entries in the same directory. (Note: this may result in deleted files to re-appear in case the split-brain happens because of deletion of files on the directory) Split-brain resolution needs human intervention when there is at least one entry which has same file name but different gfid in that directory. Example: On brick-a the directory has entries '1' (with gfid g1), '2' and on brick-b directory has entries '1' (with gfid g2) and '3'. These kinds of directory split-brains need human intervention to resolve. The user needs to remove either file '1' on brick-a or the file '1' on brick-b to resolve the split-brain. In addition, the corresponding gfid-link file also needs to be removed.The gfid-link files are present in the .glusterfs folder in the top-level directory of the brick. If the gfid of the file is 0x307a5c9efddd4e7c96e94fd4bcdcbd1b (the trusted.gfid extended attribute got from the getfattr command earlier),the gfid-link file can be found at /gfs/brick-a/.glusterfs/30/7a/307a5c9efddd4e7c96e94fd4bcdcbd1b Word of caution: Before deleting the gfid-link, we have to ensure that there are no hard links to the file present on that brick. If hard-links exist,they must be deleted as well.","title":"Troubleshooting Split-Brains"},{"location":"Troubleshooting/resolving-splitbrain/#heal-info-and-split-brain-resolution","text":"This document explains the heal info command available in gluster for monitoring pending heals in replicate volumes and the methods available to resolve split-brains.","title":"Heal info and split-brain resolution"},{"location":"Troubleshooting/resolving-splitbrain/#types-of-split-brains","text":"A file is said to be in split-brain when Gluster AFR cannot determine which copy in the replica is the correct one. There are three types of split-brains: Data split-brain: The data in the file differs on the bricks in the replica set Metadata split-brain: The metadata differs on the bricks Entry split-brain: The GFID of the file is different on the bricks in the replica or the type of the file is different on the bricks in the replica. Type-mismatch cannot be healed using any of the split-brain resolution methods while gfid split-brains can be.","title":"Types of Split-Brains:"},{"location":"Troubleshooting/resolving-splitbrain/#1-volume-heal-info","text":"Usage: gluster volume heal <VOLNAME> info This lists all the files that require healing (and will be processed by the self-heal daemon). It prints either their path or their GFID.","title":"1) Volume heal info:"},{"location":"Troubleshooting/resolving-splitbrain/#interpreting-the-output","text":"All the files listed in the output of this command need to be healed. The files listed may also be accompanied by the following tags: a) 'Is in split-brain' A file in data or metadata split-brain will be listed with \" - Is in split-brain\" appended after its path/GFID. E.g. \"/file4\" in the output provided below. However, for a file in GFID split-brain, the parent directory of the file is shown to be in split-brain and the file itself is shown to be needing healing, e.g. \"/dir\" in the output provided below is in split-brain because of GFID split-brain of file \"/dir/a\". Files in split-brain cannot be healed without resolving the split-brain. b) 'Is possibly undergoing heal' When the heal info command is run, it (or to be more specific, the 'glfsheal' binary that is executed when you run the command) takes locks on each file to find if it needs healing. However, if the self-heal daemon had already started healing the file, it would have taken locks which glfsheal wouldn't be able to acquire. In such a case, it could print this message. Another possible case could be multiple glfsheal processes running simultaneously (e.g. multiple users ran a heal info command at the same time) and competing for same lock. The following is an example of heal info command's output.","title":"Interpreting the output"},{"location":"Troubleshooting/resolving-splitbrain/#example","text":"Consider a replica volume \"test\" with two bricks b1 and b2; self-heal daemon off, mounted at /mnt. # gluster volume heal test info Brick \\<hostname:brickpath-b1> <gfid:aaca219f-0e25-4576-8689-3bfd93ca70c2> - Is in split-brain <gfid:39f301ae-4038-48c2-a889-7dac143e82dd> - Is in split-brain <gfid:c3c94de2-232d-4083-b534-5da17fc476ac> - Is in split-brain <gfid:6dc78b20-7eb6-49a3-8edb-087b90142246> Number of entries: 4 Brick <hostname:brickpath-b2> /dir/file2 /dir/file1 - Is in split-brain /dir - Is in split-brain /dir/file3 /file4 - Is in split-brain /dir/a Number of entries: 6","title":"Example"},{"location":"Troubleshooting/resolving-splitbrain/#analysis-of-the-output","text":"It can be seen that A) from brick b1, four entries need healing: 1) file with gfid:6dc78b20-7eb6-49a3-8edb-087b90142246 needs healing 2) \"aaca219f-0e25-4576-8689-3bfd93ca70c2\", \"39f301ae-4038-48c2-a889-7dac143e82dd\" and \"c3c94de2-232d-4083-b534-5da17fc476ac\" are in split-brain B) from brick b2 six entries need healing- 1) \"a\", \"file2\" and \"file3\" need healing 2) \"file1\", \"file4\" & \"/dir\" are in split-brain","title":"Analysis of the output"},{"location":"Troubleshooting/resolving-splitbrain/#2-volume-heal-info-split-brain","text":"Usage: gluster volume heal <VOLNAME> info split-brain This command only shows the list of files that are in split-brain. The output is therefore a subset of gluster volume heal <VOLNAME> info","title":"2. Volume heal info split-brain"},{"location":"Troubleshooting/resolving-splitbrain/#example_1","text":"# gluster volume heal test info split-brain Brick <hostname:brickpath-b1> <gfid:aaca219f-0e25-4576-8689-3bfd93ca70c2> <gfid:39f301ae-4038-48c2-a889-7dac143e82dd> <gfid:c3c94de2-232d-4083-b534-5da17fc476ac> Number of entries in split-brain: 3 Brick <hostname:brickpath-b2> /dir/file1 /dir /file4 Number of entries in split-brain: 3 Note that similar to the heal info command, for GFID split-brains (same filename but different GFID) their parent directories are listed to be in split-brain.","title":"Example"},{"location":"Troubleshooting/resolving-splitbrain/#3-resolution-of-split-brain-using-gluster-cli","text":"Once the files in split-brain are identified, their resolution can be done from the gluster command line using various policies. Type-mismatch cannot be healed using this methods. Split-brain resolution commands let the user resolve data, metadata, and GFID split-brains.","title":"3. Resolution of split-brain using gluster CLI"},{"location":"Troubleshooting/resolving-splitbrain/#31-resolution-of-datametadata-split-brain-using-gluster-cli","text":"Data and metadata split-brains can be resolved using the following policies:","title":"3.1 Resolution of data/metadata split-brain using gluster CLI"},{"location":"Troubleshooting/resolving-splitbrain/#i-select-the-bigger-file-as-source","text":"This command is useful for per file healing where it is known/decided that the file with bigger size is to be considered as source. gluster volume heal <VOLNAME> split-brain bigger-file <FILE> Here, <FILE> can be either the full file name as seen from the root of the volume (or) the GFID-string representation of the file, which sometimes gets displayed in the heal info command's output. Once this command is executed, the replica containing the <FILE> with a bigger size is found and healing is completed with that brick as a source.","title":"i) Select the bigger-file as source"},{"location":"Troubleshooting/resolving-splitbrain/#example_2","text":"Consider the earlier output of the heal info split-brain command. Before healing the file, notice file size and md5 checksums : On brick b1: [brick1]# stat b1/dir/file1 File: \u2018b1/dir/file1\u2019 Size: 17 Blocks: 16 IO Block: 4096 regular file Device: fd03h/64771d Inode: 919362 Links: 2 Access: (0644/-rw-r--r--) Uid: ( 0/ root) Gid: ( 0/ root) Access: 2015-03-06 13:55:40.149897333 +0530 Modify: 2015-03-06 13:55:37.206880347 +0530 Change: 2015-03-06 13:55:37.206880347 +0530 Birth: - [brick1]# [brick1]# md5sum b1/dir/file1 040751929ceabf77c3c0b3b662f341a8 b1/dir/file1 On brick b2: [brick2]# stat b2/dir/file1 File: \u2018b2/dir/file1\u2019 Size: 13 Blocks: 16 IO Block: 4096 regular file Device: fd03h/64771d Inode: 919365 Links: 2 Access: (0644/-rw-r--r--) Uid: ( 0/ root) Gid: ( 0/ root) Access: 2015-03-06 13:54:22.974451898 +0530 Modify: 2015-03-06 13:52:22.910758923 +0530 Change: 2015-03-06 13:52:22.910758923 +0530 Birth: - [brick2]# [brick2]# md5sum b2/dir/file1 cb11635a45d45668a403145059c2a0d5 b2/dir/file1 Healing file1 using the above command :- gluster volume heal test split-brain bigger-file /dir/file1 Healed /dir/file1. After healing is complete, the md5sum and file size on both bricks should be the same. On brick b1: [brick1]# stat b1/dir/file1 File: \u2018b1/dir/file1\u2019 Size: 17 Blocks: 16 IO Block: 4096 regular file Device: fd03h/64771d Inode: 919362 Links: 2 Access: (0644/-rw-r--r--) Uid: ( 0/ root) Gid: ( 0/ root) Access: 2015-03-06 14:17:27.752429505 +0530 Modify: 2015-03-06 13:55:37.206880347 +0530 Change: 2015-03-06 14:17:12.880343950 +0530 Birth: - [brick1]# [brick1]# md5sum b1/dir/file1 040751929ceabf77c3c0b3b662f341a8 b1/dir/file1 On brick b2: [brick2]# stat b2/dir/file1 File: \u2018b2/dir/file1\u2019 Size: 17 Blocks: 16 IO Block: 4096 regular file Device: fd03h/64771d Inode: 919365 Links: 2 Access: (0644/-rw-r--r--) Uid: ( 0/ root) Gid: ( 0/ root) Access: 2015-03-06 14:17:23.249403600 +0530 Modify: 2015-03-06 13:55:37.206880000 +0530 Change: 2015-03-06 14:17:12.881343955 +0530 Birth: - [brick2]# [brick2]# md5sum b2/dir/file1 040751929ceabf77c3c0b3b662f341a8 b2/dir/file1","title":"Example :"},{"location":"Troubleshooting/resolving-splitbrain/#ii-select-the-file-with-the-latest-mtime-as-source","text":"gluster volume heal <VOLNAME> split-brain latest-mtime <FILE> As is perhaps self-explanatory, this command uses the brick having the latest modification time for <FILE> as the source for healing.","title":"ii) Select the file with the latest mtime as source"},{"location":"Troubleshooting/resolving-splitbrain/#iii-select-one-of-the-bricks-in-the-replica-as-the-source-for-a-particular-file","text":"gluster volume heal <VOLNAME> split-brain source-brick <HOSTNAME:BRICKNAME> <FILE> Here, <HOSTNAME:BRICKNAME> is selected as source brick and <FILE> present in the source brick is taken as the source for healing.","title":"iii) Select one of the bricks in the replica as the source for a particular file"},{"location":"Troubleshooting/resolving-splitbrain/#example_3","text":"Notice the md5 checksums and file size before and after healing. Before heal : On brick b1: [brick1]# stat b1/file4 File: \u2018b1/file4\u2019 Size: 4 Blocks: 16 IO Block: 4096 regular file Device: fd03h/64771d Inode: 919356 Links: 2 Access: (0644/-rw-r--r--) Uid: ( 0/ root) Gid: ( 0/ root) Access: 2015-03-06 13:53:19.417085062 +0530 Modify: 2015-03-06 13:53:19.426085114 +0530 Change: 2015-03-06 13:53:19.426085114 +0530 Birth: - [brick1]# [brick1]# md5sum b1/file4 b6273b589df2dfdbd8fe35b1011e3183 b1/file4 On brick b2: [brick2]# stat b2/file4 File: \u2018b2/file4\u2019 Size: 4 Blocks: 16 IO Block: 4096 regular file Device: fd03h/64771d Inode: 919358 Links: 2 Access: (0644/-rw-r--r--) Uid: ( 0/ root) Gid: ( 0/ root) Access: 2015-03-06 13:52:35.761833096 +0530 Modify: 2015-03-06 13:52:35.769833142 +0530 Change: 2015-03-06 13:52:35.769833142 +0530 Birth: - [brick2]# [brick2]# md5sum b2/file4 0bee89b07a248e27c83fc3d5951213c1 b2/file4 Healing the file with gfid c3c94de2-232d-4083-b534-5da17fc476ac using the above command : # gluster volume heal test split-brain source-brick test-host:/test/b1 gfid:c3c94de2-232d-4083-b534-5da17fc476ac Healed gfid:c3c94de2-232d-4083-b534-5da17fc476ac. After healing : On brick b1: # stat b1/file4 File: \u2018b1/file4\u2019 Size: 4 Blocks: 16 IO Block: 4096 regular file Device: fd03h/64771d Inode: 919356 Links: 2 Access: (0644/-rw-r--r--) Uid: ( 0/ root) Gid: ( 0/ root) Access: 2015-03-06 14:23:38.944609863 +0530 Modify: 2015-03-06 13:53:19.426085114 +0530 Change: 2015-03-06 14:27:15.058927962 +0530 Birth: - # md5sum b1/file4 b6273b589df2dfdbd8fe35b1011e3183 b1/file4 On brick b2: # stat b2/file4 File: \u2018b2/file4\u2019 Size: 4 Blocks: 16 IO Block: 4096 regular file Device: fd03h/64771d Inode: 919358 Links: 2 Access: (0644/-rw-r--r--) Uid: ( 0/ root) Gid: ( 0/ root) Access: 2015-03-06 14:23:38.944609000 +0530 Modify: 2015-03-06 13:53:19.426085000 +0530 Change: 2015-03-06 14:27:15.059927968 +0530 Birth: - # md5sum b2/file4 b6273b589df2dfdbd8fe35b1011e3183 b2/file4","title":"Example :"},{"location":"Troubleshooting/resolving-splitbrain/#iv-select-one-brick-of-the-replica-as-the-source-for-all-files","text":"gluster volume heal <VOLNAME> split-brain source-brick <HOSTNAME:BRICKNAME> Consider a scenario where many files are in split-brain such that one brick of replica pair is source. As the result of the above command all split-brained files in <HOSTNAME:BRICKNAME> are selected as source and healed to the sink.","title":"iv) Select one brick of the replica as the source for all files"},{"location":"Troubleshooting/resolving-splitbrain/#example_4","text":"Consider a volume having three entries \"a, b and c\" in split-brain. # gluster volume heal test split-brain source-brick test-host:/test/b1 Healed gfid:944b4764-c253-4f02-b35f-0d0ae2f86c0f. Healed gfid:3256d814-961c-4e6e-8df2-3a3143269ced. Healed gfid:b23dd8de-af03-4006-a803-96d8bc0df004. Number of healed entries: 3","title":"Example:"},{"location":"Troubleshooting/resolving-splitbrain/#32-resolution-of-gfid-split-brain-using-gluster-cli","text":"GFID split-brains can also be resolved by the gluster command line using the same policies that are used to resolve data and metadata split-brains.","title":"3.2 Resolution of GFID split-brain using gluster CLI"},{"location":"Troubleshooting/resolving-splitbrain/#i-selecting-the-bigger-file-as-source","text":"This method is useful for per file healing and where you can decided that the file with bigger size is to be considered as source. Run the following command to obtain the path of the file that is in split-brain: # gluster volume heal VOLNAME info split-brain From the output, identify the files for which file operations performed from the client failed with input/output error.","title":"i) Selecting the bigger-file as source"},{"location":"Troubleshooting/resolving-splitbrain/#example_5","text":"# gluster volume heal testvol info Brick 10.70.47.45:/bricks/brick2/b0 /f5 / - Is in split-brain Status: Connected Number of entries: 2 Brick 10.70.47.144:/bricks/brick2/b1 /f5 / - Is in split-brain Status: Connected Number of entries: 2 Note Entries which are in GFID split-brain may not be shown as in split-brain by the heal info or heal info split-brain commands always. For entry split-brains, it is the parent directory which is shown as being in split-brain. So one might need to run info split-brain to get the dir names and then heal info to get the list of files under that dir which might be in split-brain (it could just be needing heal without split-brain). In the above command, testvol is the volume name, b0 and b1 are the bricks. Execute the below getfattr command on the brick to fetch information if a file is in GFID split-brain or not. # getfattr -d -e hex -m. <path-to-file>","title":"Example :"},{"location":"Troubleshooting/resolving-splitbrain/#example_6","text":"On brick /b0 # getfattr -d -m . -e hex /bricks/brick2/b0/f5 getfattr: Removing leading '/' from absolute path names file: bricks/brick2/b0/f5 security.selinux=0x73797374656d5f753a6f626a6563745f723a676c7573746572645f627269636b5f743a733000 trusted.afr.testvol-client-1=0x000000020000000100000000 trusted.afr.dirty=0x000000000000000000000000 trusted.gfid=0xce0a9956928e40afb78e95f78defd64f trusted.gfid2path.9cde09916eabc845=0x30303030303030302d303030302d303030302d303030302d3030303030303030303030312f6635 On brick /b1 # getfattr -d -m . -e hex /bricks/brick2/b1/f5 getfattr: Removing leading '/' from absolute path names file: bricks/brick2/b1/f5 security.selinux=0x73797374656d5f753a6f626a6563745f723a676c7573746572645f627269636b5f743a733000 trusted.afr.testvol-client-0=0x000000020000000100000000 trusted.afr.dirty=0x000000000000000000000000 trusted.gfid=0x9563544118653550e888ab38c232e0c trusted.gfid2path.9cde09916eabc845=0x30303030303030302d303030302d303030302d303030302d3030303030303030303030312f6635 You can notice the difference in GFID for the file f5 in both the bricks. You can find the differences in the file size by executing stat command on the file from the bricks. On brick /b0 # stat /bricks/brick2/b0/f5 File: \u2018/bricks/brick2/b0/f5\u2019 Size: 15 Blocks: 8 IO Block: 4096 regular file Device: fd15h/64789d Inode: 67113350 Links: 2 Access: (0644/-rw-r--r--) Uid: ( 0/ root) Gid: ( 0/ root) Context: system_u:object_r:glusterd_brick_t:s0 Access: 2018-08-29 20:46:26.353751073 +0530 Modify: 2018-08-29 20:46:26.361751203 +0530 Change: 2018-08-29 20:47:16.363751236 +0530 Birth: - On brick /b1 # stat /bricks/brick2/b1/f5 File: \u2018/bricks/brick2/b1/f5\u2019 Size: 2 Blocks: 8 IO Block: 4096 regular file Device: fd15h/64789d Inode: 67111750 Links: 2 Access: (0644/-rw-r--r--) Uid: ( 0/ root) Gid: ( 0/ root) Context: system_u:object_r:glusterd_brick_t:s0 Access: 2018-08-29 20:44:56.153301616 +0530 Modify: 2018-08-29 20:44:56.161301745 +0530 Change: 2018-08-29 20:44:56.162301761 +0530 Birth: - Execute the following command along with the full filename as seen from the root of the volume which is displayed in the heal info command's output: # gluster volume heal VOLNAME split-brain bigger-file FILE","title":"Example :"},{"location":"Troubleshooting/resolving-splitbrain/#example_7","text":"# gluster volume heal testvol split-brain bigger-file /f5 GFID split-brain resolved for file /f5 After the healing is complete, the GFID of the file on both the bricks must be the same as that of the file which had the bigger size. The following is a sample output of the getfattr command after completion of healing the file. On brick /b0 # getfattr -d -m . -e hex /bricks/brick2/b0/f5 getfattr: Removing leading '/' from absolute path names file: bricks/brick2/b0/f5 security.selinux=0x73797374656d5f753a6f626a6563745f723a676c7573746572645f627269636b5f743a733000 trusted.gfid=0xce0a9956928e40afb78e95f78defd64f trusted.gfid2path.9cde09916eabc845=0x30303030303030302d303030302d303030302d303030302d3030303030303030303030312f6635 On brick /b1 # getfattr -d -m . -e hex /bricks/brick2/b1/f5 getfattr: Removing leading '/' from absolute path names file: bricks/brick2/b1/f5 security.selinux=0x73797374656d5f753a6f626a6563745f723a676c7573746572645f627269636b5f743a733000 trusted.gfid=0xce0a9956928e40afb78e95f78defd64f trusted.gfid2path.9cde09916eabc845=0x30303030303030302d303030302d303030302d303030302d3030303030303030303030312f6635","title":"Example :"},{"location":"Troubleshooting/resolving-splitbrain/#ii-selecting-the-file-with-latest-mtime-as-source","text":"This method is useful for per file healing and if you want the file with latest mtime has to be considered as source.","title":"ii) Selecting the file with latest mtime as source"},{"location":"Troubleshooting/resolving-splitbrain/#example_8","text":"Lets take another file which is in GFID split-brain and try to heal that using the latest-mtime option. On brick /b0 # getfattr -d -m . -e hex /bricks/brick2/b0/f4 getfattr: Removing leading '/' from absolute path names file: bricks/brick2/b0/f4 security.selinux=0x73797374656d5f753a6f626a6563745f723a676c7573746572645f627269636b5f743a733000 trusted.afr.testvol-client-1=0x000000020000000100000000 trusted.afr.dirty=0x000000000000000000000000 trusted.gfid=0xb66b66d07b315f3c9cffac2fb6422a28 trusted.gfid2path.364f55367c7bd6f4=0x30303030303030302d303030302d303030302d303030302d3030303030303030303030312f6634 On brick /b1 # getfattr -d -m . -e hex /bricks/brick2/b1/f4 getfattr: Removing leading '/' from absolute path names file: bricks/brick2/b1/f4 security.selinux=0x73797374656d5f753a6f626a6563745f723a676c7573746572645f627269636b5f743a733000 trusted.afr.testvol-client-0=0x000000020000000100000000 trusted.afr.dirty=0x000000000000000000000000 trusted.gfid=0x87242f808c6e56a007ef7d49d197acff trusted.gfid2path.364f55367c7bd6f4=0x30303030303030302d303030302d303030302d303030302d3030303030303030303030312f6634 You can notice the difference in GFID for the file f4 in both the bricks. You can find the difference in the modification time by executing stat command on the file from the bricks. On brick /b0 # stat /bricks/brick2/b0/f4 File: \u2018/bricks/brick2/b0/f4\u2019 Size: 14 Blocks: 8 IO Block: 4096 regular file Device: fd15h/64789d Inode: 67113349 Links: 2 Access: (0644/-rw-r--r--) Uid: ( 0/ root) Gid: ( 0/ root) Context: system_u:object_r:glusterd_brick_t:s0 Access: 2018-08-29 20:57:38.913629991 +0530 Modify: 2018-08-29 20:57:38.921630122 +0530 Change: 2018-08-29 20:57:38.923630154 +0530 Birth: - On brick /b1 # stat /bricks/brick2/b1/f4 File: \u2018/bricks/brick2/b1/f4\u2019 Size: 2 Blocks: 8 IO Block: 4096 regular file Device: fd15h/64789d Inode: 67111749 Links: 2 Access: (0644/-rw-r--r--) Uid: ( 0/ root) Gid: ( 0/ root) Context: system_u:object_r:glusterd_brick_t:s0 Access: 2018-08-24 20:54:50.953217256 +0530 Modify: 2018-08-24 20:54:50.961217385 +0530 Change: 2018-08-24 20:54:50.962217402 +0530 Birth: - Execute the following command: # gluster volume heal VOLNAME split-brain latest-mtime FILE","title":"Example :"},{"location":"Troubleshooting/resolving-splitbrain/#example_9","text":"# gluster volume heal testvol split-brain latest-mtime /f4 GFID split-brain resolved for file /f4 After the healing is complete, the GFID of the files on both bricks must be same. The following is a sample output of the getfattr command after completion of healing the file. You can notice that the file has been healed using the brick having the latest mtime as the source. On brick /b0 ```console# getfattr -d -m . -e hex /bricks/brick2/b0/f4 getfattr: Removing leading '/' from absolute path names file: bricks/brick2/b0/f4 security.selinux=0x73797374656d5f753a6f626a6563745f723a676c7573746572645f627269636b5f743a733000 trusted.gfid=0xb66b66d07b315f3c9cffac2fb6422a28 trusted.gfid2path.364f55367c7bd6f4=0x30303030303030302d303030302d303030302d303030302d3030303030303030303030312f6634 On brick /b1 ```console # getfattr -d -m . -e hex /bricks/brick2/b1/f4 getfattr: Removing leading '/' from absolute path names file: bricks/brick2/b1/f4 security.selinux=0x73797374656d5f753a6f626a6563745f723a676c7573746572645f627269636b5f743a733000 trusted.gfid=0xb66b66d07b315f3c9cffac2fb6422a28 trusted.gfid2path.364f55367c7bd6f4=0x30303030303030302d303030302d303030302d303030302d3030303030303030303030312f6634","title":"Example :"},{"location":"Troubleshooting/resolving-splitbrain/#iii-select-one-of-the-bricks-in-the-replica-as-source-for-a-particular-file","text":"This method is useful for per file healing and if you know which copy of the file is good.","title":"iii) Select one of the bricks in the replica as source for a particular file"},{"location":"Troubleshooting/resolving-splitbrain/#example_10","text":"Lets take another file which is in GFID split-brain and try to heal that using the source-brick option. On brick /b0 # getfattr -d -m . -e hex /bricks/brick2/b0/f3 getfattr: Removing leading '/' from absolute path names file: bricks/brick2/b0/f3 security.selinux=0x73797374656d5f753a6f626a6563745f723a676c7573746572645f627269636b5f743a733000 trusted.afr.testvol-client-1=0x000000020000000100000000 trusted.afr.dirty=0x000000000000000000000000 trusted.gfid=0x9d542fb1b3b15837a2f7f9dcdf5d6ee8 trusted.gfid2path.364f55367c7bd6f4=0x30303030303030302d303030302d303030302d303030302d3030303030303030303030312f6634 On brick /b1 # getfattr -d -m . -e hex /bricks/brick2/b1/f3 getfattr: Removing leading '/' from absolute path names file: bricks/brick2/b0/f3 security.selinux=0x73797374656d5f753a6f626a6563745f723a676c7573746572645f627269636b5f743a733000 trusted.afr.testvol-client-1=0x000000020000000100000000 trusted.afr.dirty=0x000000000000000000000000 trusted.gfid=0xc90d9b0f65f6530b95b9f3f8334033df trusted.gfid2path.364f55367c7bd6f4=0x30303030303030302d303030302d303030302d303030302d3030303030303030303030312f6634 You can notice the difference in GFID for the file f3 in both the bricks. Execute the following command: # gluster volume heal VOLNAME split-brain source-brick HOSTNAME:export-directory-absolute-path FILE In this command, FILE present in HOSTNAME : export-directory-absolute-path is taken as source for healing.","title":"Example :"},{"location":"Troubleshooting/resolving-splitbrain/#example_11","text":"# gluster volume heal testvol split-brain source-brick 10.70.47.144:/bricks/brick2/b1 /f3 GFID split-brain resolved for file /f3 After the healing is complete, the GFID of the file on both the bricks should be same as that of the brick which was chosen as source for healing. The following is a sample output of the getfattr command after the file is healed. On brick /b0 # getfattr -d -m . -e hex /bricks/brick2/b0/f3 getfattr: Removing leading '/' from absolute path names file: bricks/brick2/b0/f3 security.selinux=0x73797374656d5f753a6f626a6563745f723a676c7573746572645f627269636b5f743a733000 trusted.gfid=0x90d9b0f65f6530b95b9f3f8334033df trusted.gfid2path.364f55367c7bd6f4=0x30303030303030302d303030302d303030302d303030302d3030303030303030303030312f6634 On brick /b1 # getfattr -d -m . -e hex /bricks/brick2/b1/f3 getfattr: Removing leading '/' from absolute path names file: bricks/brick2/b1/f3 security.selinux=0x73797374656d5f753a6f626a6563745f723a676c7573746572645f627269636b5f743a733000 trusted.gfid=0x90d9b0f65f6530b95b9f3f8334033df trusted.gfid2path.364f55367c7bd6f4=0x30303030303030302d303030302d303030302d303030302d3030303030303030303030312f6634 Note - One cannot use the GFID of the file as an argument with any of the CLI options to resolve GFID split-brain. It should be the absolute path as seen from the mount point to the file considered as source. With source-brick option there is no way to resolve all the GFID split-brain in one shot by not specifying any file path in the CLI as done while resolving data or metadata split-brain. For each file in GFID split-brain, run the CLI with the policy you want to use. Resolving directory GFID split-brain using CLI with the \"source-brick\" option in a \"distributed-replicated\" volume needs to be done on all the sub-volumes explicitly, which are in this state. Since directories get created on all the sub-volumes, using one particular brick as source for directory GFID split-brain heals the directory for that particular sub-volume. Source brick should be chosen in such a way that after heal all the bricks of all the sub-volumes have the same GFID.","title":"Example :"},{"location":"Troubleshooting/resolving-splitbrain/#note","text":"As mentioned earlier, type-mismatch can not be resolved using CLI. Type-mismatch means different st_mode values (for example, the entry is a file in one brick while it is a directory on the other). Trying to heal such entry would fail.","title":"Note:"},{"location":"Troubleshooting/resolving-splitbrain/#example_12","text":"The entry named \"entry1\" is of different types on the bricks of the replica. Lets try to heal that using the split-brain CLI. # gluster volume heal test split-brain source-brick test-host:/test/b1 /entry1 Healing /entry1 failed:Operation not permitted. Volume heal failed. However, they can be fixed by deleting the file from all but one bricks. See Fixing Directory entry split-brain","title":"Example"},{"location":"Troubleshooting/resolving-splitbrain/#an-overview-of-working-of-heal-info-commands","text":"When these commands are invoked, a \"glfsheal\" process is spawned which reads the entries from the various sub-directories under /<brick-path>/.glusterfs/indices/ of all the bricks that are up (that it can connect to) one after another. These entries are GFIDs of files that might need healing. Once GFID entries from a brick are obtained, based on the lookup response of this file on each participating brick of replica-pair & trusted.afr.* extended attributes it is found out if the file needs healing, is in split-brain etc based on the requirement of each command and displayed to the user.","title":"An overview of working of heal info commands"},{"location":"Troubleshooting/resolving-splitbrain/#4-resolution-of-split-brain-from-the-mount-point","text":"A set of getfattr and setfattr commands have been provided to detect the data and metadata split-brain status of a file and resolve split-brain, if any, from mount point. Consider a volume \"test\", having bricks b0, b1, b2 and b3. # gluster volume info test Volume Name: test Type: Distributed-Replicate Volume ID: 00161935-de9e-4b80-a643-b36693183b61 Status: Started Number of Bricks: 2 x 2 = 4 Transport-type: tcp Bricks: Brick1: test-host:/test/b0 Brick2: test-host:/test/b1 Brick3: test-host:/test/b2 Brick4: test-host:/test/b3 Directory structure of the bricks is as follows: # tree -R /test/b? /test/b0 \u251c\u2500\u2500 dir \u2502 \u2514\u2500\u2500 a \u2514\u2500\u2500 file100 /test/b1 \u251c\u2500\u2500 dir \u2502 \u2514\u2500\u2500 a \u2514\u2500\u2500 file100 /test/b2 \u251c\u2500\u2500 dir \u251c\u2500\u2500 file1 \u251c\u2500\u2500 file2 \u2514\u2500\u2500 file99 /test/b3 \u251c\u2500\u2500 dir \u251c\u2500\u2500 file1 \u251c\u2500\u2500 file2 \u2514\u2500\u2500 file99 Some files in the volume are in split-brain. # gluster v heal test info split-brain Brick test-host:/test/b0/ /file100 /dir Number of entries in split-brain: 2 Brick test-host:/test/b1/ /file100 /dir Number of entries in split-brain: 2 Brick test-host:/test/b2/ /file99 <gfid:5399a8d1-aee9-4653-bb7f-606df02b3696> Number of entries in split-brain: 2 Brick test-host:/test/b3/ <gfid:05c4b283-af58-48ed-999e-4d706c7b97d5> <gfid:5399a8d1-aee9-4653-bb7f-606df02b3696> Number of entries in split-brain: 2","title":"4. Resolution of split-brain from the mount point"},{"location":"Troubleshooting/resolving-splitbrain/#to-know-datametadata-split-brain-status-of-a-file","text":"getfattr -n replica.split-brain-status <path-to-file> The above command executed from mount provides information if a file is in data/metadata split-brain. Also provides the list of afr children to analyze to get more information about the file. This command is not applicable to gfid/directory split-brain.","title":"To know data/metadata split-brain status of a file:"},{"location":"Troubleshooting/resolving-splitbrain/#example_13","text":"1) \"file100\" is in metadata split-brain. Executing the above mentioned command for file100 gives : # getfattr -n replica.split-brain-status file100 file: file100 replica.split-brain-status=\"data-split-brain:no metadata-split-brain:yes Choices:test-client-0,test-client-1\" 2) \"file1\" is in data split-brain. # getfattr -n replica.split-brain-status file1 file: file1 replica.split-brain-status=\"data-split-brain:yes metadata-split-brain:no Choices:test-client-2,test-client-3\" 3) \"file99\" is in both data and metadata split-brain. # getfattr -n replica.split-brain-status file99 file: file99 replica.split-brain-status=\"data-split-brain:yes metadata-split-brain:yes Choices:test-client-2,test-client-3\" 4) \"dir\" is in directory split-brain but as mentioned earlier, the above command is not applicable to such split-brain. So it says that the file is not under data or metadata split-brain. # getfattr -n replica.split-brain-status dir file: dir replica.split-brain-status=\"The file is not under data or metadata split-brain\" 5) \"file2\" is not in any kind of split-brain. # getfattr -n replica.split-brain-status file2 file: file2 replica.split-brain-status=\"The file is not under data or metadata split-brain\"","title":"Example:"},{"location":"Troubleshooting/resolving-splitbrain/#to-analyze-the-files-in-data-and-metadata-split-brain","text":"Trying to do operations (say cat, getfattr etc) from the mount on files in split-brain, gives an input/output error. To enable the users analyze such files, a setfattr command is provided. # setfattr -n replica.split-brain-choice -v \"choiceX\" <path-to-file> Using this command, a particular brick can be chosen to access the file in split-brain from.","title":"To analyze the files in data and metadata split-brain"},{"location":"Troubleshooting/resolving-splitbrain/#example_14","text":"1) \"file1\" is in data-split-brain. Trying to read from the file gives input/output error. # cat file1 cat: file1: Input/output error Split-brain choices provided for file1 were test-client-2 and test-client-3. Setting test-client-2 as split-brain choice for file1 serves reads from b2 for the file. # setfattr -n replica.split-brain-choice -v test-client-2 file1 Now, read operations on the file can be done. # cat file1 xyz Similarly, to inspect the file from other choice, replica.split-brain-choice is to be set to test-client-3. Trying to inspect the file from a wrong choice errors out. To undo the split-brain-choice that has been set, the above mentioned setfattr command can be used with \"none\" as the value for extended attribute.","title":"Example:"},{"location":"Troubleshooting/resolving-splitbrain/#example_15","text":"# setfattr -n replica.split-brain-choice -v none file1 Now performing cat operation on the file will again result in input/output error, as before. # cat file cat: file1: Input/output error Once the choice for resolving split-brain is made, source brick is supposed to be set for the healing to be done. This is done using the following command: # setfattr -n replica.split-brain-heal-finalize -v <heal-choice> <path-to-file>","title":"Example:"},{"location":"Troubleshooting/resolving-splitbrain/#example_16","text":"# setfattr -n replica.split-brain-heal-finalize -v test-client-2 file1 The above process can be used to resolve data and/or metadata split-brain on all the files. NOTE : 1) If \"fopen-keep-cache\" fuse mount option is disabled then inode needs to be invalidated each time before selecting a new replica.split-brain-choice to inspect a file. This can be done by using: # sefattr -n inode-invalidate -v 0 <path-to-file> 2) The above mentioned process for split-brain resolution from mount will not work on nfs mounts as it doesn't provide xattrs support.","title":"Example"},{"location":"Troubleshooting/resolving-splitbrain/#5-automagic-unsplit-brain-by-ctimemtimesizemajority","text":"The CLI and fuse mount based resolution methods require intervention in the sense that the admin/ user needs to run the commands manually. There is a cluster.favorite-child-policy volume option which when set to one of the various policies available, automatically resolve split-brains without user intervention. The default value is 'none', i.e. it is disabled. # gluster volume set help | grep -A3 cluster.favorite-child-policy Option: cluster.favorite-child-policy Default Value: none Description: This option can be used to automatically resolve split-brains using various policies without user intervention. \"size\" picks the file with the biggest size as the source. \"ctime\" and \"mtime\" pick the file with the latest ctime and mtime respectively as the source. \"majority\" picks a file with identical mtime and size in more than half the number of bricks in the replica. cluster.favorite-child-policy applies to all files of the volume. It is assumed that if this option is enabled with a particular policy, you don't care to examine the split-brain files on a per file basis but just want the split-brain to be resolved as and when it occurs based on the set policy.","title":"5. Automagic unsplit-brain by [ctime|mtime|size|majority]"},{"location":"Troubleshooting/resolving-splitbrain/#manual-split-brain-resolution","text":"","title":"Manual Split-Brain Resolution:"},{"location":"Troubleshooting/resolving-splitbrain/#quick-start","text":"Get the path of the file that is in split-brain: It can be obtained either by a) The command gluster volume heal info split-brain . b) Identify the files for which file operations performed from the client keep failing with Input/Output error. Close the applications that opened this file from the mount point. In case of VMs, they need to be powered-off. Decide on the correct copy: This is done by observing the afr changelog extended attributes of the file on the bricks using the getfattr command; then identifying the type of split-brain (data split-brain, metadata split-brain, entry split-brain or split-brain due to gfid-mismatch); and finally determining which of the bricks contains the 'good copy' of the file. getfattr -d -m . -e hex <file-path-on-brick> . It is also possible that one brick might contain the correct data while the other might contain the correct metadata. Reset the relevant extended attribute on the brick(s) that contains the 'bad copy' of the file data/metadata using the setfattr command. setfattr -n <attribute-name> -v <attribute-value> <file-path-on-brick> Trigger self-heal on the file by performing lookup from the client: ls -l <file-path-on-gluster-mount>","title":"Quick Start:"},{"location":"Troubleshooting/resolving-splitbrain/#detailed-instructions-for-steps-3-through-5","text":"To understand how to resolve split-brain we need to know how to interpret the afr changelog extended attributes. Execute getfattr -d -m . -e hex <file-path-on-brick> Example: [root@store3 ~]# getfattr -d -e hex -m. brick-a/file.txt \\#file: brick-a/file.txt security.selinux=0x726f6f743a6f626a6563745f723a66696c655f743a733000 trusted.afr.vol-client-2=0x000000000000000000000000 trusted.afr.vol-client-3=0x000000000200000000000000 trusted.gfid=0x307a5c9efddd4e7c96e94fd4bcdcbd1b The extended attributes with trusted.afr.<volname>-client-<subvolume-index> are used by afr to maintain changelog of the file.The values of the trusted.afr.<volname>-client-<subvolume-index> are calculated by the glusterfs client (fuse or nfs-server) processes. When the glusterfs client modifies a file or directory, the client contacts each brick and updates the changelog extended attribute according to the response of the brick. 'subvolume-index' is nothing but (brick number - 1) in gluster volume info <volname> output. Example: [root@pranithk-laptop ~]# gluster volume info vol Volume Name: vol Type: Distributed-Replicate Volume ID: 4f2d7849-fbd6-40a2-b346-d13420978a01 Status: Created Number of Bricks: 4 x 2 = 8 Transport-type: tcp Bricks: brick-a: pranithk-laptop:/gfs/brick-a brick-b: pranithk-laptop:/gfs/brick-b brick-c: pranithk-laptop:/gfs/brick-c brick-d: pranithk-laptop:/gfs/brick-d brick-e: pranithk-laptop:/gfs/brick-e brick-f: pranithk-laptop:/gfs/brick-f brick-g: pranithk-laptop:/gfs/brick-g brick-h: pranithk-laptop:/gfs/brick-h In the example above: Brick | Replica set | Brick subvolume index ---------------------------------------------------------------------------- -/gfs/brick-a | 0 | 0 -/gfs/brick-b | 0 | 1 -/gfs/brick-c | 1 | 2 -/gfs/brick-d | 1 | 3 -/gfs/brick-e | 2 | 4 -/gfs/brick-f | 2 | 5 -/gfs/brick-g | 3 | 6 -/gfs/brick-h | 3 | 7 Each file in a brick maintains the changelog of itself and that of the files present in all the other bricks in its replica set as seen by that brick. In the example volume given above, all files in brick-a will have 2 entries, one for itself and the other for the file present in its replica pair, i.e.brick-b: trusted.afr.vol-client-0=0x000000000000000000000000 -->changelog for itself (brick-a) trusted.afr.vol-client-1=0x000000000000000000000000 -->changelog for brick-b as seen by brick-a Likewise, all files in brick-b will have: trusted.afr.vol-client-0=0x000000000000000000000000 -->changelog for brick-a as seen by brick-b trusted.afr.vol-client-1=0x000000000000000000000000 -->changelog for itself (brick-b) The same can be extended for other replica pairs. Interpreting Changelog (roughly pending operation count) Value: Each extended attribute has a value which is 24 hexa decimal digits. First 8 digits represent changelog of data. Second 8 digits represent changelog of metadata. Last 8 digits represent Changelog of directory entries. Pictorially representing the same, we have: 0x 000003d7 00000001 00000000 | | | | | \\_ changelog of directory entries | \\_ changelog of metadata \\ _ changelog of data For Directories metadata and entry changelogs are valid. For regular files data and metadata changelogs are valid. For special files like device files etc metadata changelog is valid. When a file split-brain happens it could be either data split-brain or meta-data split-brain or both. When a split-brain happens the changelog of the file would be something like this: Example:(Lets consider both data, metadata split-brain on same file). [root@pranithk-laptop vol]# getfattr -d -m . -e hex /gfs/brick-?/a getfattr: Removing leading '/' from absolute path names \\#file: gfs/brick-a/a trusted.afr.vol-client-0=0x000000000000000000000000 trusted.afr.vol-client-1=0x000003d70000000100000000 trusted.gfid=0x80acdbd886524f6fbefa21fc356fed57 \\#file: gfs/brick-b/a trusted.afr.vol-client-0=0x000003b00000000100000000 trusted.afr.vol-client-1=0x000000000000000000000000 trusted.gfid=0x80acdbd886524f6fbefa21fc356fed57","title":"Detailed Instructions for steps 3 through 5:"},{"location":"Troubleshooting/resolving-splitbrain/#observations","text":"","title":"Observations:"},{"location":"Troubleshooting/resolving-splitbrain/#according-to-changelog-extended-attributes-on-file-gfsbrick-aa","text":"The first 8 digits of trusted.afr.vol-client-0 are all zeros (0x00000000................), and the first 8 digits of trusted.afr.vol-client-1 are not all zeros (0x000003d7................). So the changelog on /gfs/brick-a/a implies that some data operations succeeded on itself but failed on /gfs/brick-b/a. The second 8 digits of trusted.afr.vol-client-0 are all zeros (0x........00000000........), and the second 8 digits of trusted.afr.vol-client-1 are not all zeros (0x........00000001........). So the changelog on /gfs/brick-a/a implies that some metadata operations succeeded on itself but failed on /gfs/brick-b/a.","title":"According to changelog extended attributes on file /gfs/brick-a/a:"},{"location":"Troubleshooting/resolving-splitbrain/#according-to-changelog-extended-attributes-on-file-gfsbrick-ba","text":"The first 8 digits of trusted.afr.vol-client-0 are not all zeros (0x000003b0................), and the first 8 digits of trusted.afr.vol-client-1 are all zeros (0x00000000................). So the changelog on /gfs/brick-b/a implies that some data operations succeeded on itself but failed on /gfs/brick-a/a. The second 8 digits of trusted.afr.vol-client-0 are not all zeros (0x........00000001........), and the second 8 digits of trusted.afr.vol-client-1 are all zeros (0x........00000000........). So the changelog on /gfs/brick-b/a implies that some metadata operations succeeded on itself but failed on /gfs/brick-a/a. Since both the copies have data, metadata changes that are not on the other file, it is in both data and metadata split-brain.","title":"According to Changelog extended attributes on file /gfs/brick-b/a:"},{"location":"Troubleshooting/resolving-splitbrain/#deciding-on-the-correct-copy","text":"The user may have to inspect stat,getfattr output of the files to decide which metadata to retain and contents of the file to decide which data to retain. Continuing with the example above, lets say we want to retain the data of /gfs/brick-a/a and metadata of /gfs/brick-b/a.","title":"Deciding on the correct copy:"},{"location":"Troubleshooting/resolving-splitbrain/#resetting-the-relevant-changelogs-to-resolve-the-split-brain","text":"For resolving data-split-brain: We need to change the changelog extended attributes on the files as if some data operations succeeded on /gfs/brick-a/a but failed on /gfs/brick-b/a. But /gfs/brick-b/a should NOT have any changelog which says some data operations succeeded on /gfs/brick-b/a but failed on /gfs/brick-a/a. We need to reset the data part of the changelog on trusted.afr.vol-client-0 of /gfs/brick-b/a. For resolving metadata-split-brain: We need to change the changelog extended attributes on the files as if some metadata operations succeeded on /gfs/brick-b/a but failed on /gfs/brick-a/a. But /gfs/brick-a/a should NOT have any changelog which says some metadata operations succeeded on /gfs/brick-a/a but failed on /gfs/brick-b/a. We need to reset metadata part of the changelog on trusted.afr.vol-client-1 of /gfs/brick-a/a So, the intended changes are: On /gfs/brick-b/a: For trusted.afr.vol-client-0 0x000003b00000000100000000 to 0x000000000000000100000000 (Note that the metadata part is still not all zeros) Hence execute setfattr -n trusted.afr.vol-client-0 -v 0x000000000000000100000000 /gfs/brick-b/a On /gfs/brick-a/a: For trusted.afr.vol-client-1 0x000003d70000000100000000 to 0x000003d70000000000000000 (Note that the data part is still not all zeros) Hence execute setfattr -n trusted.afr.vol-client-1 -v 0x000003d70000000000000000 /gfs/brick-a/a Thus after the above operations are done, the changelogs look like this: [root@pranithk-laptop vol]# getfattr -d -m . -e hex /gfs/brick-?/a getfattr: Removing leading '/' from absolute path names #file: gfs/brick-a/a trusted.afr.vol-client-0=0x000000000000000000000000 trusted.afr.vol-client-1=0x000003d70000000000000000 trusted.gfid=0x80acdbd886524f6fbefa21fc356fed57 #file: gfs/brick-b/a trusted.afr.vol-client-0=0x000000000000000100000000 trusted.afr.vol-client-1=0x000000000000000000000000 trusted.gfid=0x80acdbd886524f6fbefa21fc356fed57","title":"Resetting the relevant changelogs to resolve the split-brain:"},{"location":"Troubleshooting/resolving-splitbrain/#triggering-self-heal","text":"Perform ls -l <file-path-on-gluster-mount> to trigger healing. Fixing Directory entry split-brain: Afr has the ability to conservatively merge different entries in the directories when there is a split-brain on directory. If on one brick directory 'd' has entries '1', '2' and has entries '3', '4' on the other brick then afr will merge all of the entries in the directory to have '1', '2', '3', '4' entries in the same directory. (Note: this may result in deleted files to re-appear in case the split-brain happens because of deletion of files on the directory) Split-brain resolution needs human intervention when there is at least one entry which has same file name but different gfid in that directory. Example: On brick-a the directory has entries '1' (with gfid g1), '2' and on brick-b directory has entries '1' (with gfid g2) and '3'. These kinds of directory split-brains need human intervention to resolve. The user needs to remove either file '1' on brick-a or the file '1' on brick-b to resolve the split-brain. In addition, the corresponding gfid-link file also needs to be removed.The gfid-link files are present in the .glusterfs folder in the top-level directory of the brick. If the gfid of the file is 0x307a5c9efddd4e7c96e94fd4bcdcbd1b (the trusted.gfid extended attribute got from the getfattr command earlier),the gfid-link file can be found at /gfs/brick-a/.glusterfs/30/7a/307a5c9efddd4e7c96e94fd4bcdcbd1b","title":"Triggering Self-heal:"},{"location":"Troubleshooting/resolving-splitbrain/#word-of-caution","text":"Before deleting the gfid-link, we have to ensure that there are no hard links to the file present on that brick. If hard-links exist,they must be deleted as well.","title":"Word of caution:"},{"location":"Troubleshooting/statedump/","text":"Statedump A statedump is, as the name suggests, a dump of the internal state of a glusterfs process. It captures information about in-memory structures such as frames, call stacks, active inodes, fds, mempools, iobufs, and locks as well as xlator specific data structures. This can be an invaluable tool for debugging memory leaks and hung processes. Generate a Statedump Read a Statedump Debug with a Statedump Generate a Statedump Run the command # gluster --print-statedumpdir on a gluster server node to find out which directory the statedumps will be created in. This directory may need to be created if not already present. For the rest of this document, we will refer to this directory as statedump-directory . To generate a statedump for a process, run kill -USR1 <pid-of-gluster-process> For client mounts: Run the following command on the client system kill -USR1 <pid-of-gluster-mount-process> There are specific commands to generate statedumps for all brick processes/nfs server/quotad which can be used instead of the above. Run the following commands on one of the server nodes: For bricks: gluster volume statedump <volname> For the NFS server: gluster volume statedump <volname> nfs For quotad: gluster volume statedump <volname> quotad The statedumps will be created in statedump-directory on each node. The statedumps for brick processes will be created with the filename hyphenated-brick-path.<pid>.dump.timestamp while for all other processes it will be glusterdump.<pid>.dump.timestamp . Read a Statedump Statedumps are text files and can be opened in any text editor. The first and last lines of the file contain the start and end time (in UTC)respectively of when the statedump file was written. Mallinfo The mallinfo return status is printed in the following format. Please read man mallinfo for more information about what each field means. [mallinfo] mallinfo_arena=100020224 /* Non-mmapped space allocated (bytes) */ mallinfo_ordblks=69467 /* Number of free chunks */ mallinfo_smblks=449 /* Number of free fastbin blocks */ mallinfo_hblks=13 /* Number of mmapped regions */ mallinfo_hblkhd=20144128 /* Space allocated in mmapped regions (bytes) */ mallinfo_usmblks=0 /* Maximum total allocated space (bytes) */ mallinfo_fsmblks=39264 /* Space in freed fastbin blocks (bytes) */ mallinfo_uordblks=96710112 /* Total allocated space (bytes) */ mallinfo_fordblks=3310112 /* Total free space (bytes) */ mallinfo_keepcost=133712 /* Top-most, releasable space (bytes) */ Memory accounting stats Each xlator defines data structures specific to its requirements. The statedump captures information about the memory usage and allocations of these structures for each xlator in the call-stack and prints them in the following format: For the xlator with the name glusterfs [global.glusterfs - Memory usage] #[global.<xlator-name> - Memory usage] num_types=119 #The number of data types it is using followed by the memory usage for each data-type for that translator. The following example displays a sample for the gf_common_mt_gf_timer_t type [global.glusterfs - usage-type gf_common_mt_gf_timer_t memusage] #[global.<xlator-name> - usage-type <tag associated with the data-type> memusage] size=112 #Total size allocated for data-type when the statedump was taken i.e. num_allocs * sizeof (data-type) num_allocs=2 #Number of allocations of the data-type which are active at the time of taking the statedump. max_size=168 #max_num_allocs times the sizeof(data-type) i.e. max_num_allocs * sizeof (data-type) max_num_allocs=3 #Maximum number of active allocations at any point in the life of the process. total_allocs=7 #Number of times this data-type was allocated in the life of the process. This information is useful while debugging high memory usage issues as steadily increasing values for num_allocs may indicate a memory leak for that data-type. Mempools Mempools are an optimization intended to reduce the number of allocations of a data type. By creating a mempool of 1024 elements for a data-type, new elements of that type will be allocated from the heap using syscalls like calloc only if all the 1024 elements in the pool are in active use. Memory pool allocations by each xlator are displayed in the following format: [mempool] #Section name -----=----- pool-name=fuse:fd_t #pool-name=<xlator-name>:<data-type> hot-count=1 #number of mempool elements in active use. i.e. for this pool it is the number of 'fd_t' elements in active use. cold-count=1023 #number of mempool elements that are not in use. New allocation requests will be served from here until all the elements in the pool are in use i.e. cold-count becomes 0. padded_sizeof=108 #Element size including padding. Each mempool element is padded with a doubly-linked-list + ptr of mempool + is-in-use info to operate the pool of elements pool-misses=0 #Number of times the element was allocated from heap because all elements from the pool were in active use. alloc-count=314 #Number of times this type of data was allocated through out the life of this process. This may include pool-misses as well. max-alloc=3 #Maximum number of elements from the pool in active use at any point in the life of the process. This does *not* include pool-misses. cur-stdalloc=0 #Number of allocations made from heap that are yet to be released via mem_put(). max-stdalloc=0 #Maximum number of allocations from heap that were in active use at any point in the life of the process. This information is also useful while debugging high memory usage issues as large hot_count and cur-stdalloc values may point to an element not being freed after it has been used. Iobufs [iobuf.global] iobuf_pool=0x1f0d970 #The memory pool for iobufs iobuf_pool.default_page_size=131072 #The default size of iobuf (if no iobuf size is specified the default size is allocated) #iobuf_arena: One arena represents a group of iobufs of a particular size iobuf_pool.arena_size=12976128 # The initial size of the iobuf pool (doesn't include the stdalloc'd memory or newly added arenas) iobuf_pool.arena_cnt=8 #Total number of arenas in the pool iobuf_pool.request_misses=0 #The number of iobufs that were stdalloc'd (as they exceeded the default max page size provided by iobuf_pool). There are 3 lists of arenas Arena list: arenas allocated during iobuf pool creation and the arenas that are in use(active_cnt != 0) will be part of this list. Purge list: arenas that can be purged(no active iobufs, active_cnt == 0). Filled list: arenas without free iobufs. [purge.1] #purge.<S.No.> purge.1.mem_base=0x7fc47b35f000 #The address of the arena structure purge.1.active_cnt=0 #The number of iobufs active in that arena purge.1.passive_cnt=1024 #The number of unused iobufs in the arena purge.1.alloc_cnt=22853 #Total allocs in this pool(number of times the iobuf was allocated from this arena) purge.1.max_active=7 #Max active iobufs from this arena, at any point in the life of this process. purge.1.page_size=128 #Size of all the iobufs in this arena. [arena.5] #arena.<S.No.> arena.5.mem_base=0x7fc47af1f000 arena.5.active_cnt=0 arena.5.passive_cnt=64 arena.5.alloc_cnt=0 arena.5.max_active=0 arena.5.page_size=32768 If the active_cnt of any arena is non zero, then the statedump will also have the iobuf list. [arena.6.active_iobuf.1] #arena.<S.No>.active_iobuf.<iobuf.S.No.> arena.6.active_iobuf.1.ref=1 #refcount of the iobuf arena.6.active_iobuf.1.ptr=0x7fdb921a9000 #address of the iobuf [arena.6.active_iobuf.2] arena.6.active_iobuf.2.ref=1 arena.6.active_iobuf.2.ptr=0x7fdb92189000 A lot of filled arenas at any given point in time could be a sign of iobuf leaks. Call stack The fops received by gluster are handled using call stacks. A call stack contains information about the uid/gid/pid etc of the process that is executing the fop. Each call stack contains different call-frames for each xlator which handles that fop. [global.callpool.stack.3] #global.callpool.stack.<Serial-Number> stack=0x7fc47a44bbe0 #Stack address uid=0 #Uid of the process executing the fop gid=0 #Gid of the process executing the fop pid=6223 #Pid of the process executing the fop unique=2778 #Some Xlators like afr do copy_frame and perform the operation in a different stack. This id is used to determine the stacks that are inter-related because of copy-frame lk-owner=0000000000000000 #Some of the fuse fops have lk-owner. op=LOOKUP #Fop type=1 #Type of the op i.e. FOP/MGMT-OP cnt=9 #Number of frames in this stack. Call-frame Each frame will have information about which xlator the frame belongs to, which function it wound to/from and which it will be unwound to, and whether it has unwound. [global.callpool.stack.3.frame.2] #global.callpool.stack.<stack-serial-number>.frame.<frame-serial-number> frame=0x7fc47a611dbc #Frame address ref_count=0 #Incremented at the time of wind and decremented at the time of unwind. translator=r2-client-1 #Xlator this frame belongs to complete=0 #1 if this frame is already unwound. 0 if it is yet to unwind. parent=r2-replicate-0 #Parent xlator of this frame wind_from=afr_lookup #Parent xlator function from which it was wound wind_to=priv->children[i]->fops->lookup unwind_to=afr_lookup_cbk #Parent xlator function to unwind to To debug hangs in the system, see which xlator has not yet unwound its fop by checking the value of the complete tag in the statedump. ( complete=0 indicates the xlator has not yet unwound). FUSE Operation History Gluster Fuse maintains a history of the operations that it has performed. [xlator.mount.fuse.history] TIME=2014-07-09 16:44:57.523364 message=[0] fuse_release: RELEASE(): 4590:, fd: 0x1fef0d8, gfid: 3afb4968-5100-478d-91e9-76264e634c9f TIME=2014-07-09 16:44:57.523373 message=[0] send_fuse_err: Sending Success for operation 18 on inode 3afb4968-5100-478d-91e9-76264e634c9f TIME=2014-07-09 16:44:57.523394 message=[0] fuse_getattr_resume: 4591, STAT, path: (/iozone.tmp), gfid: (3afb4968-5100-478d-91e9-76264e634c9f) Xlator configuration [cluster/replicate.r2-replicate-0] #Xlator type, name information child_count=2 #Number of children for the xlator #Xlator specific configuration below child_up[0]=1 pending_key[0]=trusted.afr.r2-client-0 child_up[1]=1 pending_key[1]=trusted.afr.r2-client-1 data_self_heal=on metadata_self_heal=1 entry_self_heal=1 data_change_log=1 metadata_change_log=1 entry-change_log=1 read_child=1 favorite_child=-1 wait_count=1 Graph/inode table [active graph - 1] conn.1.bound_xl./data/brick01a/homegfs.hashsize=14057 conn.1.bound_xl./data/brick01a/homegfs.name=/data/brick01a/homegfs/inode conn.1.bound_xl./data/brick01a/homegfs.lru_limit=16384 #Least recently used size limit conn.1.bound_xl./data/brick01a/homegfs.active_size=690 #Number of inodes undergoing some kind of fop ie., on which there is at least one ref. conn.1.bound_xl./data/brick01a/homegfs.lru_size=183 #Number of inodes present in lru list conn.1.bound_xl./data/brick01a/homegfs.purge_size=0 #Number of inodes present in purge list Inode [conn.1.bound_xl./data/brick01a/homegfs.active.324] #324th inode in active inode list gfid=e6d337cf-97eb-44b3-9492-379ba3f6ad42 #Gfid of the inode nlookup=13 #Number of times lookups happened from the client or from fuse kernel fd-count=4 #Number of fds opened on the inode ref=11 #Number of refs taken on the inode ia_type=1 #Type of the inode. This should be changed to some string :-( [conn.1.bound_xl./data/brick01a/homegfs.lru.1] #1st inode in lru list. Note that ref count is zero for these inodes. gfid=5114574e-69bc-412b-9e52-f13ff087c6fc nlookup=5 fd-count=0 ref=0 ia_type=2 Inode context Each xlator can store information specific to it in the inode context. This context can also be printed in the statedump. Here is the inode context of the locks xlator [xlator.features.locks.homegfs-locks.inode] path=/homegfs/users/dfrobins/gfstest/r4/SCRATCH/fort.5102 - path of the file mandatory=0 inodelk-count=5 #Number of inode locks lock-dump.domain.domain=homegfs-replicate-0:self-heal #Domain on which the lock was taken. In this case, this domain is used by the selfheal to prevent more than one heal on the same file inodelk.inodelk[0](ACTIVE)=type=WRITE, whence=0, start=0, len=0, pid = 18446744073709551615, owner=080b1ada117f0000, client=0xb7fc30, connection-id=compute-30-029.com-3505-2014/06/29-14:46:12:477358-homegfs-client-0-0-1, granted at Sun Jun 29 11:01:00 2014 #Active lock information inodelk.inodelk[1](BLOCKED)=type=WRITE, whence=0, start=0, len=0, pid = 18446744073709551615, owner=c0cb091a277f0000, client=0xad4f10, connection-id=gfs01a.com-4080-2014/06/29-14:41:36:917768-homegfs-client-0-0-0, blocked at Sun Jun 29 11:04:44 2014 #Blocked lock information lock-dump.domain.domain=homegfs-replicate-0:metadata #Domain name where metadata operations take locks to maintain replication consistency lock-dump.domain.domain=homegfs-replicate-0 #Domain name where entry/data operations take locks to maintain replication consistency inodelk.inodelk[0](ACTIVE)=type=WRITE, whence=0, start=11141120, len=131072, pid = 18446744073709551615, owner=080b1ada117f0000, client=0xb7fc30, connection-id=compute-30-029.com-3505-2014/06/29-14:46:12:477358-homegfs-client-0-0-1, granted at Sun Jun 29 11:10:36 2014 #Active lock information Debug With Statedumps Memory leaks Statedumps can be used to determine whether the high memory usage of a process is caused by a leak. To debug the issue, generate statedumps for that process at regular intervals, or before and after running the steps that cause the memory used to increase. Once you have multiple statedumps, compare the memory allocation stats to see if any of them are increasing steadily as those could indicate a potential memory leak. The following examples walk through using statedumps to debug two different memory leaks. With the memory accounting feature: BZ 1120151 reported high memory usage by the self heal daemon whenever one of the bricks was wiped in a replicate volume and a full self-heal was invoked to heal the contents. This issue was debugged using statedumps to determine which data-structure was leaking memory. A statedump of the self heal daemon process was taken using kill -USR1 `<pid-of-gluster-self-heal-daemon>` On examining the statedump: grep -w num_allocs glusterdump.5225.dump.1405493251 num_allocs=77078 num_allocs=87070 num_allocs=117376 .... grep hot-count glusterdump.5225.dump.1405493251 hot-count=16384 hot-count=16384 hot-count=4095 .... On searching for num_allocs with high values in the statedump, a grep of the statedump revealed a large number of allocations for the following data-types under the replicate xlator: 1. gf_common_mt_asprintf 2. gf_common_mt_char 3. gf_common_mt_mem_pool. On checking the afr-code for allocations with tag gf_common_mt_char , it was found that the data-self-heal code path does not free one such allocated data structure. gf_common_mt_mem_pool suggests that there is a leak in pool memory. The replicate-0:dict_t , glusterfs:data_t and glusterfs:data_pair_t pools are using a lot of memory, i.e. cold_count is 0 and there are too many allocations. Checking the source code of dict.c shows that key in dict is allocated with gf_common_mt_char i.e. 2. tag and value is created using gf_asprintf which in-turn uses gf_common_mt_asprintf i.e. 1. . Checking the code for leaks in self-heal code paths led to a line which over-writes a variable with new dictionary even when it was already holding a reference to another dictionary. After fixing these leaks, we ran the same test to verify that none of the num_allocs values increased in the statedump of the self-daemon after healing 10,000 files. Please check http://review.gluster.org/8316 for more info about the patch/code. Leaks in mempools: The statedump output of mempools was used to test and verify the fixes for BZ 1134221 . On code analysis, dict_t objects were found to be leaking (due to missing unref's) during name self-heal. Glusterfs was compiled with the -DDEBUG flags to have cold count set to 0 by default. The test involved creating 100 files on plain replicate volume, removing them from one of the backend bricks, and then triggering lookups on them from the mount point. A statedump of the mount process was taken before executing the test case and after it was completed. Statedump output of the fuse mount process before the test case was executed: pool-name=glusterfs:dict_t hot-count=0 cold-count=0 padded_sizeof=140 alloc-count=33 max-alloc=0 pool-misses=33 cur-stdalloc=14 max-stdalloc=18 Statedump output of the fuse mount process after the test case was executed: pool-name=glusterfs:dict_t hot-count=0 cold-count=0 padded_sizeof=140 alloc-count=2841 max-alloc=0 pool-misses=2841 cur-stdalloc=214 max-stdalloc=220 Here, as cold count was 0 by default, cur-stdalloc indicates the number of dict_t objects that were allocated from the heap using mem_get(), and are yet to be freed using mem_put(). After running the test case (named selfheal of 100 files), there was a rise in the cur-stdalloc value (from 14 to 214) for dict_t. After the leaks were fixed, glusterfs was again compiled with -DDEBUG flags and the steps were repeated. Statedumps of the FUSE mount were taken before and after executing the test case to ascertain the validity of the fix. And the results were as follows: Statedump output of the fuse mount process before executing the test case: pool-name=glusterfs:dict_t hot-count=0 cold-count=0 padded_sizeof=140 alloc-count=33 max-alloc=0 pool-misses=33 cur-stdalloc=14 max-stdalloc=18 Statedump output of the fuse mount process after executing the test case: pool-name=glusterfs:dict_t hot-count=0 cold-count=0 padded_sizeof=140 alloc-count=2837 max-alloc=0 pool-misses=2837 cur-stdalloc=14 max-stdalloc=119 The value of cur-stdalloc remained 14 after the test, indicating that the fix indeed does what it's supposed to do. Hangs caused by frame loss BZ 994959 reported that the Fuse mount hangs on a readdirp operation. Here are the steps used to locate the cause of the hang using statedump. Statedumps were taken for all gluster processes after reproducing the issue. The following stack was seen in the FUSE mount's statedump: [global.callpool.stack.1.frame.1] ref_count=1 translator=fuse complete=0 [global.callpool.stack.1.frame.2] ref_count=0 translator=r2-client-1 complete=1 <<----- Client xlator has completed the readdirp call and unwound to afr parent=r2-replicate-0 wind_from=afr_do_readdir wind_to=children[call_child]->fops->readdirp unwind_from=client3_3_readdirp_cbk unwind_to=afr_readdirp_cbk [global.callpool.stack.1.frame.3] ref_count=0 translator=r2-replicate-0 complete=0 <<---- But the Afr xlator is not unwinding for some reason. parent=r2-dht wind_from=dht_do_readdir wind_to=xvol->fops->readdirp unwind_to=dht_readdirp_cbk [global.callpool.stack.1.frame.4] ref_count=1 translator=r2-dht complete=0 parent=r2-io-cache wind_from=ioc_readdirp wind_to=FIRST_CHILD(this)->fops->readdirp unwind_to=ioc_readdirp_cbk [global.callpool.stack.1.frame.5] ref_count=1 translator=r2-io-cache complete=0 parent=r2-quick-read wind_from=qr_readdirp wind_to=FIRST_CHILD (this)->fops->readdirp unwind_to=qr_readdirp_cbk unwind_to shows that call was unwound to afr_readdirp_cbk from the r2-client-1 xlator. Inspecting that function revealed that afr is not unwinding the stack when fop failed. Check http://review.gluster.org/5531 for more info about patch/code changes.","title":"Statedump"},{"location":"Troubleshooting/statedump/#statedump","text":"A statedump is, as the name suggests, a dump of the internal state of a glusterfs process. It captures information about in-memory structures such as frames, call stacks, active inodes, fds, mempools, iobufs, and locks as well as xlator specific data structures. This can be an invaluable tool for debugging memory leaks and hung processes. Generate a Statedump Read a Statedump Debug with a Statedump","title":"Statedump"},{"location":"Troubleshooting/statedump/#generate-a-statedump","text":"Run the command # gluster --print-statedumpdir on a gluster server node to find out which directory the statedumps will be created in. This directory may need to be created if not already present. For the rest of this document, we will refer to this directory as statedump-directory . To generate a statedump for a process, run kill -USR1 <pid-of-gluster-process> For client mounts: Run the following command on the client system kill -USR1 <pid-of-gluster-mount-process> There are specific commands to generate statedumps for all brick processes/nfs server/quotad which can be used instead of the above. Run the following commands on one of the server nodes: For bricks: gluster volume statedump <volname> For the NFS server: gluster volume statedump <volname> nfs For quotad: gluster volume statedump <volname> quotad The statedumps will be created in statedump-directory on each node. The statedumps for brick processes will be created with the filename hyphenated-brick-path.<pid>.dump.timestamp while for all other processes it will be glusterdump.<pid>.dump.timestamp .","title":"Generate a Statedump"},{"location":"Troubleshooting/statedump/#read-a-statedump","text":"Statedumps are text files and can be opened in any text editor. The first and last lines of the file contain the start and end time (in UTC)respectively of when the statedump file was written.","title":"Read a Statedump"},{"location":"Troubleshooting/statedump/#mallinfo","text":"The mallinfo return status is printed in the following format. Please read man mallinfo for more information about what each field means. [mallinfo] mallinfo_arena=100020224 /* Non-mmapped space allocated (bytes) */ mallinfo_ordblks=69467 /* Number of free chunks */ mallinfo_smblks=449 /* Number of free fastbin blocks */ mallinfo_hblks=13 /* Number of mmapped regions */ mallinfo_hblkhd=20144128 /* Space allocated in mmapped regions (bytes) */ mallinfo_usmblks=0 /* Maximum total allocated space (bytes) */ mallinfo_fsmblks=39264 /* Space in freed fastbin blocks (bytes) */ mallinfo_uordblks=96710112 /* Total allocated space (bytes) */ mallinfo_fordblks=3310112 /* Total free space (bytes) */ mallinfo_keepcost=133712 /* Top-most, releasable space (bytes) */","title":"Mallinfo"},{"location":"Troubleshooting/statedump/#memory-accounting-stats","text":"Each xlator defines data structures specific to its requirements. The statedump captures information about the memory usage and allocations of these structures for each xlator in the call-stack and prints them in the following format: For the xlator with the name glusterfs [global.glusterfs - Memory usage] #[global.<xlator-name> - Memory usage] num_types=119 #The number of data types it is using followed by the memory usage for each data-type for that translator. The following example displays a sample for the gf_common_mt_gf_timer_t type [global.glusterfs - usage-type gf_common_mt_gf_timer_t memusage] #[global.<xlator-name> - usage-type <tag associated with the data-type> memusage] size=112 #Total size allocated for data-type when the statedump was taken i.e. num_allocs * sizeof (data-type) num_allocs=2 #Number of allocations of the data-type which are active at the time of taking the statedump. max_size=168 #max_num_allocs times the sizeof(data-type) i.e. max_num_allocs * sizeof (data-type) max_num_allocs=3 #Maximum number of active allocations at any point in the life of the process. total_allocs=7 #Number of times this data-type was allocated in the life of the process. This information is useful while debugging high memory usage issues as steadily increasing values for num_allocs may indicate a memory leak for that data-type.","title":"Memory accounting stats"},{"location":"Troubleshooting/statedump/#mempools","text":"Mempools are an optimization intended to reduce the number of allocations of a data type. By creating a mempool of 1024 elements for a data-type, new elements of that type will be allocated from the heap using syscalls like calloc only if all the 1024 elements in the pool are in active use. Memory pool allocations by each xlator are displayed in the following format: [mempool] #Section name -----=----- pool-name=fuse:fd_t #pool-name=<xlator-name>:<data-type> hot-count=1 #number of mempool elements in active use. i.e. for this pool it is the number of 'fd_t' elements in active use. cold-count=1023 #number of mempool elements that are not in use. New allocation requests will be served from here until all the elements in the pool are in use i.e. cold-count becomes 0. padded_sizeof=108 #Element size including padding. Each mempool element is padded with a doubly-linked-list + ptr of mempool + is-in-use info to operate the pool of elements pool-misses=0 #Number of times the element was allocated from heap because all elements from the pool were in active use. alloc-count=314 #Number of times this type of data was allocated through out the life of this process. This may include pool-misses as well. max-alloc=3 #Maximum number of elements from the pool in active use at any point in the life of the process. This does *not* include pool-misses. cur-stdalloc=0 #Number of allocations made from heap that are yet to be released via mem_put(). max-stdalloc=0 #Maximum number of allocations from heap that were in active use at any point in the life of the process. This information is also useful while debugging high memory usage issues as large hot_count and cur-stdalloc values may point to an element not being freed after it has been used.","title":"Mempools"},{"location":"Troubleshooting/statedump/#iobufs","text":"[iobuf.global] iobuf_pool=0x1f0d970 #The memory pool for iobufs iobuf_pool.default_page_size=131072 #The default size of iobuf (if no iobuf size is specified the default size is allocated) #iobuf_arena: One arena represents a group of iobufs of a particular size iobuf_pool.arena_size=12976128 # The initial size of the iobuf pool (doesn't include the stdalloc'd memory or newly added arenas) iobuf_pool.arena_cnt=8 #Total number of arenas in the pool iobuf_pool.request_misses=0 #The number of iobufs that were stdalloc'd (as they exceeded the default max page size provided by iobuf_pool). There are 3 lists of arenas Arena list: arenas allocated during iobuf pool creation and the arenas that are in use(active_cnt != 0) will be part of this list. Purge list: arenas that can be purged(no active iobufs, active_cnt == 0). Filled list: arenas without free iobufs. [purge.1] #purge.<S.No.> purge.1.mem_base=0x7fc47b35f000 #The address of the arena structure purge.1.active_cnt=0 #The number of iobufs active in that arena purge.1.passive_cnt=1024 #The number of unused iobufs in the arena purge.1.alloc_cnt=22853 #Total allocs in this pool(number of times the iobuf was allocated from this arena) purge.1.max_active=7 #Max active iobufs from this arena, at any point in the life of this process. purge.1.page_size=128 #Size of all the iobufs in this arena. [arena.5] #arena.<S.No.> arena.5.mem_base=0x7fc47af1f000 arena.5.active_cnt=0 arena.5.passive_cnt=64 arena.5.alloc_cnt=0 arena.5.max_active=0 arena.5.page_size=32768 If the active_cnt of any arena is non zero, then the statedump will also have the iobuf list. [arena.6.active_iobuf.1] #arena.<S.No>.active_iobuf.<iobuf.S.No.> arena.6.active_iobuf.1.ref=1 #refcount of the iobuf arena.6.active_iobuf.1.ptr=0x7fdb921a9000 #address of the iobuf [arena.6.active_iobuf.2] arena.6.active_iobuf.2.ref=1 arena.6.active_iobuf.2.ptr=0x7fdb92189000 A lot of filled arenas at any given point in time could be a sign of iobuf leaks.","title":"Iobufs"},{"location":"Troubleshooting/statedump/#call-stack","text":"The fops received by gluster are handled using call stacks. A call stack contains information about the uid/gid/pid etc of the process that is executing the fop. Each call stack contains different call-frames for each xlator which handles that fop. [global.callpool.stack.3] #global.callpool.stack.<Serial-Number> stack=0x7fc47a44bbe0 #Stack address uid=0 #Uid of the process executing the fop gid=0 #Gid of the process executing the fop pid=6223 #Pid of the process executing the fop unique=2778 #Some Xlators like afr do copy_frame and perform the operation in a different stack. This id is used to determine the stacks that are inter-related because of copy-frame lk-owner=0000000000000000 #Some of the fuse fops have lk-owner. op=LOOKUP #Fop type=1 #Type of the op i.e. FOP/MGMT-OP cnt=9 #Number of frames in this stack.","title":"Call stack"},{"location":"Troubleshooting/statedump/#call-frame","text":"Each frame will have information about which xlator the frame belongs to, which function it wound to/from and which it will be unwound to, and whether it has unwound. [global.callpool.stack.3.frame.2] #global.callpool.stack.<stack-serial-number>.frame.<frame-serial-number> frame=0x7fc47a611dbc #Frame address ref_count=0 #Incremented at the time of wind and decremented at the time of unwind. translator=r2-client-1 #Xlator this frame belongs to complete=0 #1 if this frame is already unwound. 0 if it is yet to unwind. parent=r2-replicate-0 #Parent xlator of this frame wind_from=afr_lookup #Parent xlator function from which it was wound wind_to=priv->children[i]->fops->lookup unwind_to=afr_lookup_cbk #Parent xlator function to unwind to To debug hangs in the system, see which xlator has not yet unwound its fop by checking the value of the complete tag in the statedump. ( complete=0 indicates the xlator has not yet unwound).","title":"Call-frame"},{"location":"Troubleshooting/statedump/#fuse-operation-history","text":"Gluster Fuse maintains a history of the operations that it has performed. [xlator.mount.fuse.history] TIME=2014-07-09 16:44:57.523364 message=[0] fuse_release: RELEASE(): 4590:, fd: 0x1fef0d8, gfid: 3afb4968-5100-478d-91e9-76264e634c9f TIME=2014-07-09 16:44:57.523373 message=[0] send_fuse_err: Sending Success for operation 18 on inode 3afb4968-5100-478d-91e9-76264e634c9f TIME=2014-07-09 16:44:57.523394 message=[0] fuse_getattr_resume: 4591, STAT, path: (/iozone.tmp), gfid: (3afb4968-5100-478d-91e9-76264e634c9f)","title":"FUSE Operation History"},{"location":"Troubleshooting/statedump/#xlator-configuration","text":"[cluster/replicate.r2-replicate-0] #Xlator type, name information child_count=2 #Number of children for the xlator #Xlator specific configuration below child_up[0]=1 pending_key[0]=trusted.afr.r2-client-0 child_up[1]=1 pending_key[1]=trusted.afr.r2-client-1 data_self_heal=on metadata_self_heal=1 entry_self_heal=1 data_change_log=1 metadata_change_log=1 entry-change_log=1 read_child=1 favorite_child=-1 wait_count=1","title":"Xlator configuration"},{"location":"Troubleshooting/statedump/#graphinode-table","text":"[active graph - 1] conn.1.bound_xl./data/brick01a/homegfs.hashsize=14057 conn.1.bound_xl./data/brick01a/homegfs.name=/data/brick01a/homegfs/inode conn.1.bound_xl./data/brick01a/homegfs.lru_limit=16384 #Least recently used size limit conn.1.bound_xl./data/brick01a/homegfs.active_size=690 #Number of inodes undergoing some kind of fop ie., on which there is at least one ref. conn.1.bound_xl./data/brick01a/homegfs.lru_size=183 #Number of inodes present in lru list conn.1.bound_xl./data/brick01a/homegfs.purge_size=0 #Number of inodes present in purge list","title":"Graph/inode table"},{"location":"Troubleshooting/statedump/#inode","text":"[conn.1.bound_xl./data/brick01a/homegfs.active.324] #324th inode in active inode list gfid=e6d337cf-97eb-44b3-9492-379ba3f6ad42 #Gfid of the inode nlookup=13 #Number of times lookups happened from the client or from fuse kernel fd-count=4 #Number of fds opened on the inode ref=11 #Number of refs taken on the inode ia_type=1 #Type of the inode. This should be changed to some string :-( [conn.1.bound_xl./data/brick01a/homegfs.lru.1] #1st inode in lru list. Note that ref count is zero for these inodes. gfid=5114574e-69bc-412b-9e52-f13ff087c6fc nlookup=5 fd-count=0 ref=0 ia_type=2","title":"Inode"},{"location":"Troubleshooting/statedump/#inode-context","text":"Each xlator can store information specific to it in the inode context. This context can also be printed in the statedump. Here is the inode context of the locks xlator [xlator.features.locks.homegfs-locks.inode] path=/homegfs/users/dfrobins/gfstest/r4/SCRATCH/fort.5102 - path of the file mandatory=0 inodelk-count=5 #Number of inode locks lock-dump.domain.domain=homegfs-replicate-0:self-heal #Domain on which the lock was taken. In this case, this domain is used by the selfheal to prevent more than one heal on the same file inodelk.inodelk[0](ACTIVE)=type=WRITE, whence=0, start=0, len=0, pid = 18446744073709551615, owner=080b1ada117f0000, client=0xb7fc30, connection-id=compute-30-029.com-3505-2014/06/29-14:46:12:477358-homegfs-client-0-0-1, granted at Sun Jun 29 11:01:00 2014 #Active lock information inodelk.inodelk[1](BLOCKED)=type=WRITE, whence=0, start=0, len=0, pid = 18446744073709551615, owner=c0cb091a277f0000, client=0xad4f10, connection-id=gfs01a.com-4080-2014/06/29-14:41:36:917768-homegfs-client-0-0-0, blocked at Sun Jun 29 11:04:44 2014 #Blocked lock information lock-dump.domain.domain=homegfs-replicate-0:metadata #Domain name where metadata operations take locks to maintain replication consistency lock-dump.domain.domain=homegfs-replicate-0 #Domain name where entry/data operations take locks to maintain replication consistency inodelk.inodelk[0](ACTIVE)=type=WRITE, whence=0, start=11141120, len=131072, pid = 18446744073709551615, owner=080b1ada117f0000, client=0xb7fc30, connection-id=compute-30-029.com-3505-2014/06/29-14:46:12:477358-homegfs-client-0-0-1, granted at Sun Jun 29 11:10:36 2014 #Active lock information","title":"Inode context"},{"location":"Troubleshooting/statedump/#debug-with-statedumps","text":"","title":"Debug With Statedumps"},{"location":"Troubleshooting/statedump/#memory-leaks","text":"Statedumps can be used to determine whether the high memory usage of a process is caused by a leak. To debug the issue, generate statedumps for that process at regular intervals, or before and after running the steps that cause the memory used to increase. Once you have multiple statedumps, compare the memory allocation stats to see if any of them are increasing steadily as those could indicate a potential memory leak. The following examples walk through using statedumps to debug two different memory leaks.","title":"Memory leaks"},{"location":"Troubleshooting/statedump/#with-the-memory-accounting-feature","text":"BZ 1120151 reported high memory usage by the self heal daemon whenever one of the bricks was wiped in a replicate volume and a full self-heal was invoked to heal the contents. This issue was debugged using statedumps to determine which data-structure was leaking memory. A statedump of the self heal daemon process was taken using kill -USR1 `<pid-of-gluster-self-heal-daemon>` On examining the statedump: grep -w num_allocs glusterdump.5225.dump.1405493251 num_allocs=77078 num_allocs=87070 num_allocs=117376 .... grep hot-count glusterdump.5225.dump.1405493251 hot-count=16384 hot-count=16384 hot-count=4095 .... On searching for num_allocs with high values in the statedump, a grep of the statedump revealed a large number of allocations for the following data-types under the replicate xlator: 1. gf_common_mt_asprintf 2. gf_common_mt_char 3. gf_common_mt_mem_pool. On checking the afr-code for allocations with tag gf_common_mt_char , it was found that the data-self-heal code path does not free one such allocated data structure. gf_common_mt_mem_pool suggests that there is a leak in pool memory. The replicate-0:dict_t , glusterfs:data_t and glusterfs:data_pair_t pools are using a lot of memory, i.e. cold_count is 0 and there are too many allocations. Checking the source code of dict.c shows that key in dict is allocated with gf_common_mt_char i.e. 2. tag and value is created using gf_asprintf which in-turn uses gf_common_mt_asprintf i.e. 1. . Checking the code for leaks in self-heal code paths led to a line which over-writes a variable with new dictionary even when it was already holding a reference to another dictionary. After fixing these leaks, we ran the same test to verify that none of the num_allocs values increased in the statedump of the self-daemon after healing 10,000 files. Please check http://review.gluster.org/8316 for more info about the patch/code.","title":"With the memory accounting feature:"},{"location":"Troubleshooting/statedump/#leaks-in-mempools","text":"The statedump output of mempools was used to test and verify the fixes for BZ 1134221 . On code analysis, dict_t objects were found to be leaking (due to missing unref's) during name self-heal. Glusterfs was compiled with the -DDEBUG flags to have cold count set to 0 by default. The test involved creating 100 files on plain replicate volume, removing them from one of the backend bricks, and then triggering lookups on them from the mount point. A statedump of the mount process was taken before executing the test case and after it was completed. Statedump output of the fuse mount process before the test case was executed: pool-name=glusterfs:dict_t hot-count=0 cold-count=0 padded_sizeof=140 alloc-count=33 max-alloc=0 pool-misses=33 cur-stdalloc=14 max-stdalloc=18 Statedump output of the fuse mount process after the test case was executed: pool-name=glusterfs:dict_t hot-count=0 cold-count=0 padded_sizeof=140 alloc-count=2841 max-alloc=0 pool-misses=2841 cur-stdalloc=214 max-stdalloc=220 Here, as cold count was 0 by default, cur-stdalloc indicates the number of dict_t objects that were allocated from the heap using mem_get(), and are yet to be freed using mem_put(). After running the test case (named selfheal of 100 files), there was a rise in the cur-stdalloc value (from 14 to 214) for dict_t. After the leaks were fixed, glusterfs was again compiled with -DDEBUG flags and the steps were repeated. Statedumps of the FUSE mount were taken before and after executing the test case to ascertain the validity of the fix. And the results were as follows: Statedump output of the fuse mount process before executing the test case: pool-name=glusterfs:dict_t hot-count=0 cold-count=0 padded_sizeof=140 alloc-count=33 max-alloc=0 pool-misses=33 cur-stdalloc=14 max-stdalloc=18 Statedump output of the fuse mount process after executing the test case: pool-name=glusterfs:dict_t hot-count=0 cold-count=0 padded_sizeof=140 alloc-count=2837 max-alloc=0 pool-misses=2837 cur-stdalloc=14 max-stdalloc=119 The value of cur-stdalloc remained 14 after the test, indicating that the fix indeed does what it's supposed to do.","title":"Leaks in mempools:"},{"location":"Troubleshooting/statedump/#hangs-caused-by-frame-loss","text":"BZ 994959 reported that the Fuse mount hangs on a readdirp operation. Here are the steps used to locate the cause of the hang using statedump. Statedumps were taken for all gluster processes after reproducing the issue. The following stack was seen in the FUSE mount's statedump: [global.callpool.stack.1.frame.1] ref_count=1 translator=fuse complete=0 [global.callpool.stack.1.frame.2] ref_count=0 translator=r2-client-1 complete=1 <<----- Client xlator has completed the readdirp call and unwound to afr parent=r2-replicate-0 wind_from=afr_do_readdir wind_to=children[call_child]->fops->readdirp unwind_from=client3_3_readdirp_cbk unwind_to=afr_readdirp_cbk [global.callpool.stack.1.frame.3] ref_count=0 translator=r2-replicate-0 complete=0 <<---- But the Afr xlator is not unwinding for some reason. parent=r2-dht wind_from=dht_do_readdir wind_to=xvol->fops->readdirp unwind_to=dht_readdirp_cbk [global.callpool.stack.1.frame.4] ref_count=1 translator=r2-dht complete=0 parent=r2-io-cache wind_from=ioc_readdirp wind_to=FIRST_CHILD(this)->fops->readdirp unwind_to=ioc_readdirp_cbk [global.callpool.stack.1.frame.5] ref_count=1 translator=r2-io-cache complete=0 parent=r2-quick-read wind_from=qr_readdirp wind_to=FIRST_CHILD (this)->fops->readdirp unwind_to=qr_readdirp_cbk unwind_to shows that call was unwound to afr_readdirp_cbk from the r2-client-1 xlator. Inspecting that function revealed that afr is not unwinding the stack when fop failed. Check http://review.gluster.org/5531 for more info about patch/code changes.","title":"Hangs caused by frame loss"},{"location":"Troubleshooting/troubleshooting-afr/","text":"The first level of analysis always starts with looking at the log files. Which ones, you ask? /var/log/glusterfs/$fuse-mount-point.log \u2013> Fuse client log /var/log/glusterfs/glfsheal-$volname.log \u2013> This is the log file to look at when you run the heal info/split-brain resolution commands. /var/log/glusterfs/glustershd.log \u2013> This is the self-heal daemon log that prints the names of files undergoing heal, the sources and sinks for each file etc. It is common for all volumes. /var/log/glusterfs/bricks/$brick.log\u2013>Some errors in clients are simply propagated from the bricks themselves, so correlating client log errors with the logs from the brick is necessary. Sometimes, you might need more verbose logging to figure out what\u2019s going on: gluster volume set $volname client-log-level $LEVEL where LEVEL can be any one of DEBUG, WARNING, ERROR, INFO, CRITICAL, NONE, TRACE . This should ideally make all the log files mentioned above to start logging at $LEVEL . The default is INFO but you can temporarily toggle it to DEBUG or TRACE if you want to see under-the-hood messages. Useful when the normal logs don\u2019t give a clue as to what is happening. Heal related issues: Most issues I\u2019ve seen on the mailing list and with customers can broadly fit into the following buckets: ( Note: Not discussing split-brains here. If they occur, you need to use split-brain resolution CLI or cluster.favorite-child-policy options to fix them. They usually occur in replica 2 volumes and can be prevented by using replica 3 or arbiter volumes.) i) Heal info appears to hang/takes a long time to complete If the number of entries are large, then heal info will take longer than usual. While there are performance improvements to heal info being planned, a faster way to get an approx. count of the pending entries is to use the gluster volume heal $VOLNAME statistics heal-count command. Knowledge Hack: Since we know that during the write transaction. the xattrop folder will capture the gfid-string of the file if it needs heal, we can also do an ls /brick/.glusterfs/indices/xattrop|wc -l on each brick to get the approx. no of entries that need heal. If this number reduces over time, it is a sign that the heal backlog is reducing. You will also see messages whenever a particular type of heal starts/ends for a given gfid, like so: [2019-05-07 12:05:14.460442] I [MSGID: 108026] [afr-self-heal-entry.c:883:afr_selfheal_entry_do] 0-testvol-replicate-0: performing entry selfheal on d120c0cf-6e87-454b-965b-0d83a4c752bb [2019-05-07 12:05:14.474710] I [MSGID: 108026] [afr-self-heal-common.c:1741:afr_log_selfheal] 0-testvol-replicate-0: Completed entry selfheal on d120c0cf-6e87-454b-965b-0d83a4c752bb. sources=[0] 2 sinks=1 [2019-05-07 12:05:14.493506] I [MSGID: 108026] [afr-self-heal-common.c:1741:afr_log_selfheal] 0-testvol-replicate-0: Completed data selfheal on a9b5f183-21eb-4fb3-a342-287d3a7dddc5. sources=[0] 2 sinks=1 [2019-05-07 12:05:14.494577] I [MSGID: 108026] [afr-self-heal-metadata.c:52:__afr_selfheal_metadata_do] 0-testvol-replicate-0: performing metadata selfheal on a9b5f183-21eb-4fb3-a342-287d3a7dddc5 [2019-05-07 12:05:14.498398] I [MSGID: 108026] [afr-self-heal-common.c:1741:afr_log_selfheal] 0-testvol-replicate-0: Completed metadata selfheal on a9b5f183-21eb-4fb3-a342-287d3a7dddc5. sources=[0] 2 sinks=1 ii) Self-heal is stuck/ not getting completed. If a file seems to be forever appearing in heal info and not healing, check the following: Examine the afr xattrs- Do they clearly indicate the good and bad copies? If there isn\u2019t at least one good copy, then the file is in split-brain and you would need to use the split-brain resolution CLI. Identify which node\u2019s shds would be picking up the file for heal. If a file is listed in the heal info output under brick1 and brick2, then the shds on the nodes which host those bricks would attempt (and one of them would succeed) in doing the heal. Once the shd is identified, look at the shd logs to see if it is indeed connected to the bricks. This is good: [2019-05-07 09:53:02.912923] I [MSGID: 114046] [client-handshake.c:1106:client_setvolume_cbk] 0-testvol-client-2: Connected to testvol-client-2, attached to remote volume '/bricks/brick3' This indicates a disconnect: [2019-05-07 11:44:47.602862] I [MSGID: 114018] [client.c:2334:client_rpc_notify] 0-testvol-client-2: disconnected from testvol-client-2. Client process will keep trying to connect to glusterd until brick's port is available [2019-05-07 11:44:50.953516] E [MSGID: 114058] [client-handshake.c:1456:client_query_portmap_cbk] 0-testvol-client-2: failed to get the port number for remote subvolume. Please run 'gluster volume status' on server to see if brick process is running. Alternatively, take a statedump of the self-heal daemon (shd) and check if all client xlators are connected to the respective bricks. The shd must have connected=1 for all the client xlators, meaning it can talk to all the bricks. Shd\u2019s statedump entry of a client xlator that is connected to the 3rd brick Shd\u2019s statedump entry of the same client xlator if it is diconnected from the 3rd brick [xlator.protocol.client.testvol-client-2.priv] connected=1 total_bytes_read=75004 ping_timeout=42 total_bytes_written=50608 ping_msgs_sent=0 msgs_sent=0 [xlator.protocol.client.testvol-client-2.priv] connected=0 total_bytes_read=75004 ping_timeout=42 total_bytes_written=50608 ping_msgs_sent=0 msgs_sent=0 If there are connection issues (i.e. connected=0 ), you would need to investigate and fix them. Check if the pid and the TCP/RDMA Port of the brick proceess from gluster volume status $VOLNAME matches that of ps aux|grep glusterfsd|grep $brick-path [root@tuxpad glusterfs]# gluster volume status Status of volume: testvol Gluster process TCP Port RDMA Port Online Pid Brick 127.0.0.2:/bricks/brick1 49152 0 Y 12527 [root@tuxpad glusterfs]# ps aux|grep brick1 root 12527 0.0 0.1 1459208 20104 ? Ssl 11:20 0:01 /usr/local/sbin/glusterfsd -s 127.0.0.2 --volfile-id testvol.127.0.0.2.bricks-brick1 -p /var/run/gluster/vols/testvol/127.0.0.2-bricks-brick1.pid -S /var/run/gluster/70529980362a17d6.socket --brick-name /bricks/brick1 -l /var/log/glusterfs/bricks/bricks-brick1.log --xlator-option *-posix.glusterd-uuid=d90b1532-30e5-4f9d-a75b-3ebb1c3682d4 --process-name brick --brick-port 49152 --xlator-option testvol-server.listen-port=49152 Though this will likely match, sometimes there could be a bug leading to stale port usage. A quick workaround would be to restart glusterd on that node and check if things match. Report the issue to the devs if you see this problem. I have seen some cases where a file is listed in heal info, and the afr xattrs indicate pending metadata or data heal but the file itself is not present on all bricks. Ideally, the parent directory of the file must have pending entry heal xattrs so that the file either gets created on the missing bricks or gets deleted from the ones where it is present. But if the parent dir doesn\u2019t have xattrs, the entry heal can\u2019t proceed. In such cases, you can -- Either do a lookup directly on the file from the mount so that name heal is triggered and then shd can pickup the data/metadata heal. -- Or manually set entry xattrs on the parent dir to emulate an entry heal so that the file gets created as a part of it. -- If a brick\u2019s underlying filesystem/lvm was damaged and fsck\u2019d to recovery, some files/dirs might be missing on it. If there is a lot of missing info on the recovered bricks, it might be better to just to a replace-brick or reset-brick and let the heal fully sync everything rather than fiddling with afr xattrs of individual entries. Hack: How to trigger heal on any file/directory Knowing about self-heal logic and index heal from the previous post, we can sort of emulate a heal with the following steps. This is not something that you should be doing on your cluster but it pays to at least know that it is possible when push comes to shove. Picking one brick as good and setting the afr pending xattr on it blaming the bad bricks. Capture the gfid inside .glusterfs/indices/xattrop so that the shd can pick it up during index heal. Finally, trigger index heal: gluster volume heal $VOLNAME . Example: Let us say a FILE-1 exists with trusted.gfid=0x1ad2144928124da9b7117d27393fea5c on all bricks of a replica 3 volume called testvol. It has no afr xattrs. But you still need to emulate a heal. Let us say you choose brick-2 as the source. Let us do the steps listed above: Make brick-2 blame the other 2 bricks: [root@tuxpad fuse_mnt]# setfattr -n trusted.afr.testvol-client-2 -v 0x000000010000000000000000 /bricks/brick2/FILE-1 [root@tuxpad fuse_mnt]# setfattr -n trusted.afr.testvol-client-1 -v 0x000000010000000000000000 /bricks/brick2/FILE-1 Store the gfid string inside xattrop folder as a hardlink to the base entry: root@tuxpad ~]# cd /bricks/brick2/.glusterfs/indices/xattrop/ [root@tuxpad xattrop]# ls -li total 0 17829255 ----------. 1 root root 0 May 10 11:20 xattrop-a400ca91-cec9-4463-a183-aca9eaff9fa7` [root@tuxpad xattrop]# ln xattrop-a400ca91-cec9-4463-a183-aca9eaff9fa7 1ad21449-2812-4da9-b711-7d27393fea5c [root@tuxpad xattrop]# ll total 0 ----------. 2 root root 0 May 10 11:20 1ad21449-2812-4da9-b711-7d27393fea5c ----------. 2 root root 0 May 10 11:20 xattrop-a400ca91-cec9-4463-a183-aca9eaff9fa7 Trigger heal: gluster volume heal testvol The glustershd.log of node-2 should log about the heal. [2019-05-10 06:10:46.027238] I [MSGID: 108026] [afr-self-heal-common.c:1741:afr_log_selfheal] 0-testvol-replicate-0: Completed data selfheal on 1ad21449-2812-4da9-b711-7d27393fea5c. sources=[1] sinks=0 2 So the data was healed from the second brick to the first and third brick. iii) Self-heal is too slow If the heal backlog is decreasing and you see glustershd logging heals but you\u2019re not happy with the rate of healing, then you can play around with shd-max-threads and shd-wait-qlength volume options. Option: cluster.shd-max-threads Default Value: 1 Description: Maximum number of parallel heals SHD can do per local brick. This can substantially lower heal times, but can also crush your bricks if you don\u2019t have the storage hardware to support this. Option: cluster.shd-wait-qlength Default Value: 1024 Description: This option can be used to control number of heals that can wait in SHD per subvolume I\u2019m not covering it here but it is possible to launch multiple shd instances (and kill them later on) on your node for increasing heal throughput. It is documented at https://access.redhat.com/solutions/3794011. iv) Self-heal is too aggressive and slows down the system. If shd-max-threads are at the lowest value (i.e. 1) and you see if CPU usage of the bricks is too high, you can check if the volume\u2019s profile info shows a lot of RCHECKSUM fops. Data self-heal does checksum calculation (i.e the posix_rchecksum() FOP) which can be CPU intensive. You can the cluster.data-self-heal-algorithm option to full. This does a full file copy instead of computing rolling checksums and syncing only the mismatching blocks. The tradeoff is that the network consumption will be increased. You can also disable all client-side heals if they are turned on so that the client bandwidth is consumed entirely by the application FOPs and not the ones by client side background heals. i.e. turn off cluster.metadata-self-heal, cluster.data-self-heal and cluster.entry-self-heal . Note: In recent versions of gluster, client-side heals are disabled by default. Mount related issues: ### i) All fops are failing with ENOTCONN Check mount log/ statedump for loss of quorum, just like for glustershd. If this is a fuse client (as opposed to an nfs/ gfapi client), you can also check the .meta folder to check the connection status to the bricks. [root@tuxpad ~]# cat /mnt/fuse_mnt/.meta/graphs/active/testvol-client-*/private |grep connected connected = 0 connected = 1 connected = 1 If connected=0 , the connection to that brick is lost. Find out why. If the client is not connected to quorum number of bricks, then AFR fails lookups (and therefore any subsequent FOP) with Transport endpoint is not connected ii) FOPs on some files are failing with ENOTCONN Check mount log for the file being unreadable: [2019-05-10 11:04:01.607046] W [MSGID: 108027] [afr-common.c:2268:afr_attempt_readsubvol_set] 13-testvol-replicate-0: no read subvols for /FILE.txt [2019-05-10 11:04:01.607775] W [fuse-bridge.c:939:fuse_entry_cbk] 0-glusterfs-fuse: 234: LOOKUP() /FILE.txt => -1 (Transport endpoint is not connected) This means there was only 1 good copy and the client has lost connection to that brick. You need to ensure that the client is connected to all bricks. iii) Mount is hung It can be difficult to pin-point the issue immediately and might require assistance from the developers but the first steps to debugging could be to strace the fuse mount; see where it is hung. Take a statedump of the mount to see which xlator has frames that are not wound (i.e. complete=0) and for which FOP. Then check the source code to see if there are any unhanded cases where the xlator doesn\u2019t wind the FOP to its child. Take statedump of bricks to see if there are any stale locks. An indication of stale locks is the same lock being present in multiple statedumps or the \u2018granted\u2019 date being very old. Excerpt from a brick statedump: [xlator.features.locks.testvol-locks.inode] path=/FILE mandatory=0 inodelk-count=1 lock-dump.domain.domain=testvol-replicate-0:self-heal lock-dump.domain.domain=testvol-replicate-0 inodelk.inodelk[0](ACTIVE)=type=WRITE, whence=0, start=0, len=0, pid = 18446744073709551610, owner=700a0060037f0000, client=0x7fc57c09c1c0, connection-id=vm1-17902-2018/10/14-07:18:17:132969-testvol-client-0-0-0, granted at 2018-10-14 07:18:40 While stale lock issues are candidates for bug reports, the locks xlator on the brick releases locks from a particular client upon a network disconnect. That can be used as a workaround to release the stale locks- i.e. restart the brick or restart the client or induce a network disconnect between them.","title":"Troubleshooting Self-heal"},{"location":"Troubleshooting/troubleshooting-afr/#heal-related-issues","text":"Most issues I\u2019ve seen on the mailing list and with customers can broadly fit into the following buckets: ( Note: Not discussing split-brains here. If they occur, you need to use split-brain resolution CLI or cluster.favorite-child-policy options to fix them. They usually occur in replica 2 volumes and can be prevented by using replica 3 or arbiter volumes.)","title":"Heal related issues:"},{"location":"Troubleshooting/troubleshooting-afr/#i-heal-info-appears-to-hangtakes-a-long-time-to-complete","text":"If the number of entries are large, then heal info will take longer than usual. While there are performance improvements to heal info being planned, a faster way to get an approx. count of the pending entries is to use the gluster volume heal $VOLNAME statistics heal-count command. Knowledge Hack: Since we know that during the write transaction. the xattrop folder will capture the gfid-string of the file if it needs heal, we can also do an ls /brick/.glusterfs/indices/xattrop|wc -l on each brick to get the approx. no of entries that need heal. If this number reduces over time, it is a sign that the heal backlog is reducing. You will also see messages whenever a particular type of heal starts/ends for a given gfid, like so: [2019-05-07 12:05:14.460442] I [MSGID: 108026] [afr-self-heal-entry.c:883:afr_selfheal_entry_do] 0-testvol-replicate-0: performing entry selfheal on d120c0cf-6e87-454b-965b-0d83a4c752bb [2019-05-07 12:05:14.474710] I [MSGID: 108026] [afr-self-heal-common.c:1741:afr_log_selfheal] 0-testvol-replicate-0: Completed entry selfheal on d120c0cf-6e87-454b-965b-0d83a4c752bb. sources=[0] 2 sinks=1 [2019-05-07 12:05:14.493506] I [MSGID: 108026] [afr-self-heal-common.c:1741:afr_log_selfheal] 0-testvol-replicate-0: Completed data selfheal on a9b5f183-21eb-4fb3-a342-287d3a7dddc5. sources=[0] 2 sinks=1 [2019-05-07 12:05:14.494577] I [MSGID: 108026] [afr-self-heal-metadata.c:52:__afr_selfheal_metadata_do] 0-testvol-replicate-0: performing metadata selfheal on a9b5f183-21eb-4fb3-a342-287d3a7dddc5 [2019-05-07 12:05:14.498398] I [MSGID: 108026] [afr-self-heal-common.c:1741:afr_log_selfheal] 0-testvol-replicate-0: Completed metadata selfheal on a9b5f183-21eb-4fb3-a342-287d3a7dddc5. sources=[0] 2 sinks=1","title":"i) Heal info appears to hang/takes a long time to complete"},{"location":"Troubleshooting/troubleshooting-afr/#ii-self-heal-is-stuck-not-getting-completed","text":"If a file seems to be forever appearing in heal info and not healing, check the following: Examine the afr xattrs- Do they clearly indicate the good and bad copies? If there isn\u2019t at least one good copy, then the file is in split-brain and you would need to use the split-brain resolution CLI. Identify which node\u2019s shds would be picking up the file for heal. If a file is listed in the heal info output under brick1 and brick2, then the shds on the nodes which host those bricks would attempt (and one of them would succeed) in doing the heal. Once the shd is identified, look at the shd logs to see if it is indeed connected to the bricks. This is good: [2019-05-07 09:53:02.912923] I [MSGID: 114046] [client-handshake.c:1106:client_setvolume_cbk] 0-testvol-client-2: Connected to testvol-client-2, attached to remote volume '/bricks/brick3' This indicates a disconnect: [2019-05-07 11:44:47.602862] I [MSGID: 114018] [client.c:2334:client_rpc_notify] 0-testvol-client-2: disconnected from testvol-client-2. Client process will keep trying to connect to glusterd until brick's port is available [2019-05-07 11:44:50.953516] E [MSGID: 114058] [client-handshake.c:1456:client_query_portmap_cbk] 0-testvol-client-2: failed to get the port number for remote subvolume. Please run 'gluster volume status' on server to see if brick process is running. Alternatively, take a statedump of the self-heal daemon (shd) and check if all client xlators are connected to the respective bricks. The shd must have connected=1 for all the client xlators, meaning it can talk to all the bricks. Shd\u2019s statedump entry of a client xlator that is connected to the 3rd brick Shd\u2019s statedump entry of the same client xlator if it is diconnected from the 3rd brick [xlator.protocol.client.testvol-client-2.priv] connected=1 total_bytes_read=75004 ping_timeout=42 total_bytes_written=50608 ping_msgs_sent=0 msgs_sent=0 [xlator.protocol.client.testvol-client-2.priv] connected=0 total_bytes_read=75004 ping_timeout=42 total_bytes_written=50608 ping_msgs_sent=0 msgs_sent=0 If there are connection issues (i.e. connected=0 ), you would need to investigate and fix them. Check if the pid and the TCP/RDMA Port of the brick proceess from gluster volume status $VOLNAME matches that of ps aux|grep glusterfsd|grep $brick-path [root@tuxpad glusterfs]# gluster volume status Status of volume: testvol Gluster process TCP Port RDMA Port Online Pid Brick 127.0.0.2:/bricks/brick1 49152 0 Y 12527 [root@tuxpad glusterfs]# ps aux|grep brick1 root 12527 0.0 0.1 1459208 20104 ? Ssl 11:20 0:01 /usr/local/sbin/glusterfsd -s 127.0.0.2 --volfile-id testvol.127.0.0.2.bricks-brick1 -p /var/run/gluster/vols/testvol/127.0.0.2-bricks-brick1.pid -S /var/run/gluster/70529980362a17d6.socket --brick-name /bricks/brick1 -l /var/log/glusterfs/bricks/bricks-brick1.log --xlator-option *-posix.glusterd-uuid=d90b1532-30e5-4f9d-a75b-3ebb1c3682d4 --process-name brick --brick-port 49152 --xlator-option testvol-server.listen-port=49152 Though this will likely match, sometimes there could be a bug leading to stale port usage. A quick workaround would be to restart glusterd on that node and check if things match. Report the issue to the devs if you see this problem. I have seen some cases where a file is listed in heal info, and the afr xattrs indicate pending metadata or data heal but the file itself is not present on all bricks. Ideally, the parent directory of the file must have pending entry heal xattrs so that the file either gets created on the missing bricks or gets deleted from the ones where it is present. But if the parent dir doesn\u2019t have xattrs, the entry heal can\u2019t proceed. In such cases, you can -- Either do a lookup directly on the file from the mount so that name heal is triggered and then shd can pickup the data/metadata heal. -- Or manually set entry xattrs on the parent dir to emulate an entry heal so that the file gets created as a part of it. -- If a brick\u2019s underlying filesystem/lvm was damaged and fsck\u2019d to recovery, some files/dirs might be missing on it. If there is a lot of missing info on the recovered bricks, it might be better to just to a replace-brick or reset-brick and let the heal fully sync everything rather than fiddling with afr xattrs of individual entries. Hack: How to trigger heal on any file/directory Knowing about self-heal logic and index heal from the previous post, we can sort of emulate a heal with the following steps. This is not something that you should be doing on your cluster but it pays to at least know that it is possible when push comes to shove. Picking one brick as good and setting the afr pending xattr on it blaming the bad bricks. Capture the gfid inside .glusterfs/indices/xattrop so that the shd can pick it up during index heal. Finally, trigger index heal: gluster volume heal $VOLNAME . Example: Let us say a FILE-1 exists with trusted.gfid=0x1ad2144928124da9b7117d27393fea5c on all bricks of a replica 3 volume called testvol. It has no afr xattrs. But you still need to emulate a heal. Let us say you choose brick-2 as the source. Let us do the steps listed above: Make brick-2 blame the other 2 bricks: [root@tuxpad fuse_mnt]# setfattr -n trusted.afr.testvol-client-2 -v 0x000000010000000000000000 /bricks/brick2/FILE-1 [root@tuxpad fuse_mnt]# setfattr -n trusted.afr.testvol-client-1 -v 0x000000010000000000000000 /bricks/brick2/FILE-1 Store the gfid string inside xattrop folder as a hardlink to the base entry: root@tuxpad ~]# cd /bricks/brick2/.glusterfs/indices/xattrop/ [root@tuxpad xattrop]# ls -li total 0 17829255 ----------. 1 root root 0 May 10 11:20 xattrop-a400ca91-cec9-4463-a183-aca9eaff9fa7` [root@tuxpad xattrop]# ln xattrop-a400ca91-cec9-4463-a183-aca9eaff9fa7 1ad21449-2812-4da9-b711-7d27393fea5c [root@tuxpad xattrop]# ll total 0 ----------. 2 root root 0 May 10 11:20 1ad21449-2812-4da9-b711-7d27393fea5c ----------. 2 root root 0 May 10 11:20 xattrop-a400ca91-cec9-4463-a183-aca9eaff9fa7 Trigger heal: gluster volume heal testvol The glustershd.log of node-2 should log about the heal. [2019-05-10 06:10:46.027238] I [MSGID: 108026] [afr-self-heal-common.c:1741:afr_log_selfheal] 0-testvol-replicate-0: Completed data selfheal on 1ad21449-2812-4da9-b711-7d27393fea5c. sources=[1] sinks=0 2 So the data was healed from the second brick to the first and third brick.","title":"ii) Self-heal is stuck/ not getting completed."},{"location":"Troubleshooting/troubleshooting-afr/#iii-self-heal-is-too-slow","text":"If the heal backlog is decreasing and you see glustershd logging heals but you\u2019re not happy with the rate of healing, then you can play around with shd-max-threads and shd-wait-qlength volume options. Option: cluster.shd-max-threads Default Value: 1 Description: Maximum number of parallel heals SHD can do per local brick. This can substantially lower heal times, but can also crush your bricks if you don\u2019t have the storage hardware to support this. Option: cluster.shd-wait-qlength Default Value: 1024 Description: This option can be used to control number of heals that can wait in SHD per subvolume I\u2019m not covering it here but it is possible to launch multiple shd instances (and kill them later on) on your node for increasing heal throughput. It is documented at https://access.redhat.com/solutions/3794011.","title":"iii) Self-heal is too slow"},{"location":"Troubleshooting/troubleshooting-afr/#iv-self-heal-is-too-aggressive-and-slows-down-the-system","text":"If shd-max-threads are at the lowest value (i.e. 1) and you see if CPU usage of the bricks is too high, you can check if the volume\u2019s profile info shows a lot of RCHECKSUM fops. Data self-heal does checksum calculation (i.e the posix_rchecksum() FOP) which can be CPU intensive. You can the cluster.data-self-heal-algorithm option to full. This does a full file copy instead of computing rolling checksums and syncing only the mismatching blocks. The tradeoff is that the network consumption will be increased. You can also disable all client-side heals if they are turned on so that the client bandwidth is consumed entirely by the application FOPs and not the ones by client side background heals. i.e. turn off cluster.metadata-self-heal, cluster.data-self-heal and cluster.entry-self-heal . Note: In recent versions of gluster, client-side heals are disabled by default.","title":"iv) Self-heal is too aggressive and slows down the system."},{"location":"Troubleshooting/troubleshooting-afr/#mount-related-issues","text":"### i) All fops are failing with ENOTCONN Check mount log/ statedump for loss of quorum, just like for glustershd. If this is a fuse client (as opposed to an nfs/ gfapi client), you can also check the .meta folder to check the connection status to the bricks. [root@tuxpad ~]# cat /mnt/fuse_mnt/.meta/graphs/active/testvol-client-*/private |grep connected connected = 0 connected = 1 connected = 1 If connected=0 , the connection to that brick is lost. Find out why. If the client is not connected to quorum number of bricks, then AFR fails lookups (and therefore any subsequent FOP) with Transport endpoint is not connected","title":"Mount related issues:"},{"location":"Troubleshooting/troubleshooting-afr/#ii-fops-on-some-files-are-failing-with-enotconn","text":"Check mount log for the file being unreadable: [2019-05-10 11:04:01.607046] W [MSGID: 108027] [afr-common.c:2268:afr_attempt_readsubvol_set] 13-testvol-replicate-0: no read subvols for /FILE.txt [2019-05-10 11:04:01.607775] W [fuse-bridge.c:939:fuse_entry_cbk] 0-glusterfs-fuse: 234: LOOKUP() /FILE.txt => -1 (Transport endpoint is not connected) This means there was only 1 good copy and the client has lost connection to that brick. You need to ensure that the client is connected to all bricks.","title":"ii) FOPs on some files are failing with ENOTCONN"},{"location":"Troubleshooting/troubleshooting-afr/#iii-mount-is-hung","text":"It can be difficult to pin-point the issue immediately and might require assistance from the developers but the first steps to debugging could be to strace the fuse mount; see where it is hung. Take a statedump of the mount to see which xlator has frames that are not wound (i.e. complete=0) and for which FOP. Then check the source code to see if there are any unhanded cases where the xlator doesn\u2019t wind the FOP to its child. Take statedump of bricks to see if there are any stale locks. An indication of stale locks is the same lock being present in multiple statedumps or the \u2018granted\u2019 date being very old. Excerpt from a brick statedump: [xlator.features.locks.testvol-locks.inode] path=/FILE mandatory=0 inodelk-count=1 lock-dump.domain.domain=testvol-replicate-0:self-heal lock-dump.domain.domain=testvol-replicate-0 inodelk.inodelk[0](ACTIVE)=type=WRITE, whence=0, start=0, len=0, pid = 18446744073709551610, owner=700a0060037f0000, client=0x7fc57c09c1c0, connection-id=vm1-17902-2018/10/14-07:18:17:132969-testvol-client-0-0-0, granted at 2018-10-14 07:18:40 While stale lock issues are candidates for bug reports, the locks xlator on the brick releases locks from a particular client upon a network disconnect. That can be used as a workaround to release the stale locks- i.e. restart the brick or restart the client or induce a network disconnect between them.","title":"iii) Mount is hung"},{"location":"Troubleshooting/troubleshooting-filelocks/","text":"Troubleshooting File Locks Use statedumps to find and list the locks held on files. The statedump output also provides information on each lock with its range, basename, PID of the application holding the lock, and so on. You can analyze the output to know about the locks whose owner/application is no longer running or interested in that lock. After ensuring that the no application is using the file, you can clear the lock using the following clear lock commands. Perform statedump on the volume to view the files that are locked using the following command: # gluster volume statedump inode For example, to display statedump of test-volume: # gluster volume statedump test-volume Volume statedump successful The statedump files are created on the brick servers in the /tmp directory or in the directory set using server.statedump-path volume option. The naming convention of the dump file is <brick-path>.<brick-pid>.dump . The following are the sample contents of the statedump file. It indicates that GlusterFS has entered into a state where there is an entry lock (entrylk) and an inode lock (inodelk). Ensure that those are stale locks and no resources own them. [xlator.features.locks.vol-locks.inode] path=/ mandatory=0 entrylk-count=1 lock-dump.domain.domain=vol-replicate-0 xlator.feature.locks.lock-dump.domain.entrylk.entrylk[0](ACTIVE)=type=ENTRYLK_WRLCK on basename=file1, pid = 714782904, owner=ffffff2a3c7f0000, transport=0x20e0670, , granted at Mon Feb 27 16:01:01 2012 conn.2.bound_xl./gfs/brick1.hashsize=14057 conn.2.bound_xl./gfs/brick1.name=/gfs/brick1/inode conn.2.bound_xl./gfs/brick1.lru_limit=16384 conn.2.bound_xl./gfs/brick1.active_size=2 conn.2.bound_xl./gfs/brick1.lru_size=0 conn.2.bound_xl./gfs/brick1.purge_size=0 [conn.2.bound_xl./gfs/brick1.active.1] gfid=538a3d4a-01b0-4d03-9dc9-843cd8704d07 nlookup=1 ref=2 ia_type=1 [xlator.features.locks.vol-locks.inode] path=/file1 mandatory=0 inodelk-count=1 lock-dump.domain.domain=vol-replicate-0 inodelk.inodelk[0](ACTIVE)=type=WRITE, whence=0, start=0, len=0, pid = 714787072, owner=00ffff2a3c7f0000, transport=0x20e0670, , granted at Mon Feb 27 16:01:01 2012 Clear the lock using the following command: # gluster volume clear-locks For example, to clear the entry lock on file1 of test-volume: # gluster volume clear-locks test-volume / kind granted entry file1 Volume clear-locks successful vol-locks: entry blocked locks=0 granted locks=1 Clear the inode lock using the following command: # gluster volume clear-locks For example, to clear the inode lock on file1 of test-volume: # gluster volume clear-locks test-volume /file1 kind granted inode 0,0-0 Volume clear-locks successful vol-locks: inode blocked locks=0 granted locks=1 Perform statedump on test-volume again to verify that the above inode and entry locks are cleared.","title":"Troubleshooting File Locks"},{"location":"Troubleshooting/troubleshooting-filelocks/#troubleshooting-file-locks","text":"Use statedumps to find and list the locks held on files. The statedump output also provides information on each lock with its range, basename, PID of the application holding the lock, and so on. You can analyze the output to know about the locks whose owner/application is no longer running or interested in that lock. After ensuring that the no application is using the file, you can clear the lock using the following clear lock commands. Perform statedump on the volume to view the files that are locked using the following command: # gluster volume statedump inode For example, to display statedump of test-volume: # gluster volume statedump test-volume Volume statedump successful The statedump files are created on the brick servers in the /tmp directory or in the directory set using server.statedump-path volume option. The naming convention of the dump file is <brick-path>.<brick-pid>.dump . The following are the sample contents of the statedump file. It indicates that GlusterFS has entered into a state where there is an entry lock (entrylk) and an inode lock (inodelk). Ensure that those are stale locks and no resources own them. [xlator.features.locks.vol-locks.inode] path=/ mandatory=0 entrylk-count=1 lock-dump.domain.domain=vol-replicate-0 xlator.feature.locks.lock-dump.domain.entrylk.entrylk[0](ACTIVE)=type=ENTRYLK_WRLCK on basename=file1, pid = 714782904, owner=ffffff2a3c7f0000, transport=0x20e0670, , granted at Mon Feb 27 16:01:01 2012 conn.2.bound_xl./gfs/brick1.hashsize=14057 conn.2.bound_xl./gfs/brick1.name=/gfs/brick1/inode conn.2.bound_xl./gfs/brick1.lru_limit=16384 conn.2.bound_xl./gfs/brick1.active_size=2 conn.2.bound_xl./gfs/brick1.lru_size=0 conn.2.bound_xl./gfs/brick1.purge_size=0 [conn.2.bound_xl./gfs/brick1.active.1] gfid=538a3d4a-01b0-4d03-9dc9-843cd8704d07 nlookup=1 ref=2 ia_type=1 [xlator.features.locks.vol-locks.inode] path=/file1 mandatory=0 inodelk-count=1 lock-dump.domain.domain=vol-replicate-0 inodelk.inodelk[0](ACTIVE)=type=WRITE, whence=0, start=0, len=0, pid = 714787072, owner=00ffff2a3c7f0000, transport=0x20e0670, , granted at Mon Feb 27 16:01:01 2012 Clear the lock using the following command: # gluster volume clear-locks For example, to clear the entry lock on file1 of test-volume: # gluster volume clear-locks test-volume / kind granted entry file1 Volume clear-locks successful vol-locks: entry blocked locks=0 granted locks=1 Clear the inode lock using the following command: # gluster volume clear-locks For example, to clear the inode lock on file1 of test-volume: # gluster volume clear-locks test-volume /file1 kind granted inode 0,0-0 Volume clear-locks successful vol-locks: inode blocked locks=0 granted locks=1 Perform statedump on test-volume again to verify that the above inode and entry locks are cleared.","title":"Troubleshooting File Locks"},{"location":"Troubleshooting/troubleshooting-georep/","text":"Troubleshooting Geo-replication This section describes the most common troubleshooting scenarios related to GlusterFS Geo-replication. Locating Log Files For every Geo-replication session, the following three log files are associated to it (four, if the secondary is a gluster volume): Primary-log-file - log file for the process which monitors the Primary volume Secondary-log-file - log file for process which initiates the changes in secondary Primary-gluster-log-file - log file for the maintenance mount point that Geo-replication module uses to monitor the Primary volume Secondary-gluster-log-file - is the secondary's counterpart of it Primary Log File To get the Primary-log-file for geo-replication, use the following command: gluster volume geo-replication <session> config log-file For example: # gluster volume geo-replication Volume1 example.com:/data/remote_dir config log-file Secondary Log File To get the log file for geo-replication on secondary (glusterd must be running on secondary machine), use the following commands: On primary, run the following command: # gluster volume geo-replication Volume1 example.com:/data/remote_dir config session-owner 5f6e5200-756f-11e0-a1f0-0800200c9a66 Displays the session owner details. On secondary, run the following command: # gluster volume geo-replication /data/remote_dir config log-file /var/log/gluster/${session-owner}:remote-mirror.log Replace the session owner details (output of Step 1) to the output of Step 2 to get the location of the log file. /var/log/gluster/5f6e5200-756f-11e0-a1f0-0800200c9a66:remote-mirror.log Rotating Geo-replication Logs Administrators can rotate the log file of a particular primary-secondary session, as needed. When you run geo-replication's log-rotate command, the log file is backed up with the current timestamp suffixed to the file name and signal is sent to gsyncd to start logging to a new log file. To rotate a geo-replication log file Rotate log file for a particular primary-secondary session using the following command: # gluster volume geo-replication log-rotate For example, to rotate the log file of primary Volume1 and secondary example.com:/data/remote_dir : # gluster volume geo-replication Volume1 example.com:/data/remote_dir log rotate log rotate successful Rotate log file for all sessions for a primary volume using the following command: # gluster volume geo-replication log-rotate For example, to rotate the log file of primary Volume1 : # gluster volume geo-replication Volume1 log rotate log rotate successful Rotate log file for all sessions using the following command: # gluster volume geo-replication log-rotate For example, to rotate the log file for all sessions: # gluster volume geo-replication log rotate log rotate successful Synchronization is not complete Description : GlusterFS geo-replication did not synchronize the data completely but the geo-replication status displayed is OK. Solution : You can enforce a full sync of the data by erasing the index and restarting GlusterFS geo-replication. After restarting, GlusterFS geo-replication begins synchronizing all the data. All files are compared using checksum, which can be a lengthy and high resource utilization operation on large data sets. Issues in Data Synchronization Description : Geo-replication display status as OK, but the files do not get synced, only directories and symlink gets synced with the following error message in the log: [2011-05-02 13:42:13.467644] E [primary:288:regjob] GMaster: failed to sync ./some\\_file\\` Solution : Geo-replication invokes rsync v3.0.0 or higher on the host and the remote machine. You must verify if you have installed the required version. Geo-replication status displays Faulty very often Description : Geo-replication displays status as faulty very often with a backtrace similar to the following: 2011-04-28 14:06:18.378859] E [syncdutils:131:log\\_raise\\_exception] \\<top\\>: FAIL: Traceback (most recent call last): File \"/usr/local/libexec/glusterfs/python/syncdaemon/syncdutils.py\", line 152, in twraptf(\\*aa) File \"/usr/local/libexec/glusterfs/python/syncdaemon/repce.py\", line 118, in listen rid, exc, res = recv(self.inf) File \"/usr/local/libexec/glusterfs/python/syncdaemon/repce.py\", line 42, in recv return pickle.load(inf) EOFError Solution : This error indicates that the RPC communication between the primary gsyncd module and secondary gsyncd module is broken and this can happen for various reasons. Check if it satisfies all the following pre-requisites: Password-less SSH is set up properly between the host and the remote machine. If FUSE is installed in the machine, because geo-replication module mounts the GlusterFS volume using FUSE to sync data. If the Secondary is a volume, check if that volume is started. If the Secondary is a plain directory, verify if the directory has been created already with the required permissions. If GlusterFS 3.2 or higher is not installed in the default location (in Primary) and has been prefixed to be installed in a custom location, configure the gluster-command for it to point to the exact location. If GlusterFS 3.2 or higher is not installed in the default location (in secondary) and has been prefixed to be installed in a custom location, configure the remote-gsyncd-command for it to point to the exact place where gsyncd is located. Intermediate Primary goes to Faulty State Description : In a cascading set-up, the intermediate primary goes to faulty state with the following log: raise RuntimeError (\"aborting on uuid change from %s to %s\" % \\\\ RuntimeError: aborting on uuid change from af07e07c-427f-4586-ab9f- 4bf7d299be81 to de6b5040-8f4e-4575-8831-c4f55bd41154 Solution : In a cascading set-up the Intermediate primary is loyal to the original primary. The above log means that the geo-replication module has detected change in original primary. If this is the desired behavior, delete the config option volume-id in the session initiated from the intermediate primary.","title":"Troubleshooting Geo-replication"},{"location":"Troubleshooting/troubleshooting-georep/#troubleshooting-geo-replication","text":"This section describes the most common troubleshooting scenarios related to GlusterFS Geo-replication.","title":"Troubleshooting Geo-replication"},{"location":"Troubleshooting/troubleshooting-georep/#locating-log-files","text":"For every Geo-replication session, the following three log files are associated to it (four, if the secondary is a gluster volume): Primary-log-file - log file for the process which monitors the Primary volume Secondary-log-file - log file for process which initiates the changes in secondary Primary-gluster-log-file - log file for the maintenance mount point that Geo-replication module uses to monitor the Primary volume Secondary-gluster-log-file - is the secondary's counterpart of it Primary Log File To get the Primary-log-file for geo-replication, use the following command: gluster volume geo-replication <session> config log-file For example: # gluster volume geo-replication Volume1 example.com:/data/remote_dir config log-file Secondary Log File To get the log file for geo-replication on secondary (glusterd must be running on secondary machine), use the following commands: On primary, run the following command: # gluster volume geo-replication Volume1 example.com:/data/remote_dir config session-owner 5f6e5200-756f-11e0-a1f0-0800200c9a66 Displays the session owner details. On secondary, run the following command: # gluster volume geo-replication /data/remote_dir config log-file /var/log/gluster/${session-owner}:remote-mirror.log Replace the session owner details (output of Step 1) to the output of Step 2 to get the location of the log file. /var/log/gluster/5f6e5200-756f-11e0-a1f0-0800200c9a66:remote-mirror.log","title":"Locating Log Files"},{"location":"Troubleshooting/troubleshooting-georep/#rotating-geo-replication-logs","text":"Administrators can rotate the log file of a particular primary-secondary session, as needed. When you run geo-replication's log-rotate command, the log file is backed up with the current timestamp suffixed to the file name and signal is sent to gsyncd to start logging to a new log file. To rotate a geo-replication log file Rotate log file for a particular primary-secondary session using the following command: # gluster volume geo-replication log-rotate For example, to rotate the log file of primary Volume1 and secondary example.com:/data/remote_dir : # gluster volume geo-replication Volume1 example.com:/data/remote_dir log rotate log rotate successful Rotate log file for all sessions for a primary volume using the following command: # gluster volume geo-replication log-rotate For example, to rotate the log file of primary Volume1 : # gluster volume geo-replication Volume1 log rotate log rotate successful Rotate log file for all sessions using the following command: # gluster volume geo-replication log-rotate For example, to rotate the log file for all sessions: # gluster volume geo-replication log rotate log rotate successful","title":"Rotating Geo-replication Logs"},{"location":"Troubleshooting/troubleshooting-georep/#synchronization-is-not-complete","text":"Description : GlusterFS geo-replication did not synchronize the data completely but the geo-replication status displayed is OK. Solution : You can enforce a full sync of the data by erasing the index and restarting GlusterFS geo-replication. After restarting, GlusterFS geo-replication begins synchronizing all the data. All files are compared using checksum, which can be a lengthy and high resource utilization operation on large data sets.","title":"Synchronization is not complete"},{"location":"Troubleshooting/troubleshooting-georep/#issues-in-data-synchronization","text":"Description : Geo-replication display status as OK, but the files do not get synced, only directories and symlink gets synced with the following error message in the log: [2011-05-02 13:42:13.467644] E [primary:288:regjob] GMaster: failed to sync ./some\\_file\\` Solution : Geo-replication invokes rsync v3.0.0 or higher on the host and the remote machine. You must verify if you have installed the required version.","title":"Issues in Data Synchronization"},{"location":"Troubleshooting/troubleshooting-georep/#geo-replication-status-displays-faulty-very-often","text":"Description : Geo-replication displays status as faulty very often with a backtrace similar to the following: 2011-04-28 14:06:18.378859] E [syncdutils:131:log\\_raise\\_exception] \\<top\\>: FAIL: Traceback (most recent call last): File \"/usr/local/libexec/glusterfs/python/syncdaemon/syncdutils.py\", line 152, in twraptf(\\*aa) File \"/usr/local/libexec/glusterfs/python/syncdaemon/repce.py\", line 118, in listen rid, exc, res = recv(self.inf) File \"/usr/local/libexec/glusterfs/python/syncdaemon/repce.py\", line 42, in recv return pickle.load(inf) EOFError Solution : This error indicates that the RPC communication between the primary gsyncd module and secondary gsyncd module is broken and this can happen for various reasons. Check if it satisfies all the following pre-requisites: Password-less SSH is set up properly between the host and the remote machine. If FUSE is installed in the machine, because geo-replication module mounts the GlusterFS volume using FUSE to sync data. If the Secondary is a volume, check if that volume is started. If the Secondary is a plain directory, verify if the directory has been created already with the required permissions. If GlusterFS 3.2 or higher is not installed in the default location (in Primary) and has been prefixed to be installed in a custom location, configure the gluster-command for it to point to the exact location. If GlusterFS 3.2 or higher is not installed in the default location (in secondary) and has been prefixed to be installed in a custom location, configure the remote-gsyncd-command for it to point to the exact place where gsyncd is located.","title":"Geo-replication status displays Faulty very often"},{"location":"Troubleshooting/troubleshooting-georep/#intermediate-primary-goes-to-faulty-state","text":"Description : In a cascading set-up, the intermediate primary goes to faulty state with the following log: raise RuntimeError (\"aborting on uuid change from %s to %s\" % \\\\ RuntimeError: aborting on uuid change from af07e07c-427f-4586-ab9f- 4bf7d299be81 to de6b5040-8f4e-4575-8831-c4f55bd41154 Solution : In a cascading set-up the Intermediate primary is loyal to the original primary. The above log means that the geo-replication module has detected change in original primary. If this is the desired behavior, delete the config option volume-id in the session initiated from the intermediate primary.","title":"Intermediate Primary goes to Faulty State"},{"location":"Troubleshooting/troubleshooting-glusterd/","text":"Troubleshooting the gluster CLI and glusterd The glusterd daemon runs on every trusted server node and is responsible for the management of the trusted pool and volumes. The gluster CLI sends commands to the glusterd daemon on the local node, which executes the operation and returns the result to the user. Debugging glusterd Logs Start by looking at the log files for clues as to what went wrong when you hit a problem. The default directory for Gluster logs is /var/log/glusterfs. The logs for the CLI and glusterd are: glusterd : /var/log/glusterfs/glusterd.log gluster CLI : /var/log/glusterfs/cli.log Statedumps Statedumps are useful in debugging memory leaks and hangs. See Statedump for more details. Common Issues and How to Resolve Them \" Another transaction is in progress for volname \" or \" Locking failed on xxx.xxx.xxx.xxx\" As Gluster is distributed by nature, glusterd takes locks when performing operations to ensure that configuration changes made to a volume are atomic across the cluster. These errors are returned when: More than one transaction contends on the same lock. Solution : These are likely to be transient errors and the operation will succeed if retried once the other transaction is complete. A stale lock exists on one of the nodes. Solution : Repeating the operation will not help until the stale lock is cleaned up. Restart the glusterd process holding the lock Check the glusterd.log file to find out which node holds the stale lock. Look for the message: lock being held by <uuid> Run gluster peer status to identify the node with the uuid in the log message. Restart glusterd on that node. \" Transport endpoint is not connected \" errors but all bricks are up This is usually seen when a brick process does not shut down cleanly, leaving stale data behind in the glusterd process. Gluster client processes query glusterd for the ports the bricks processes are listening on and attempt to connect to that port. If the port information in glusterd is incorrect, the client will fail to connect to the brick even though it is up. Operations which would need to access that brick may fail with \"Transport endpoint is not connected\". Solution : Restart the glusterd service. \"Peer Rejected\" gluster peer status returns \"Peer Rejected\" for a node. Hostname: <hostname> Uuid: <xxxx-xxx-xxxx> State: Peer Rejected (Connected) This indicates that the volume configuration on the node is not in sync with the rest of the trusted storage pool. You should see the following message in the glusterd log for the node on which the peer status command was run: Version of Cksums <vol-name> differ. local cksum = xxxxxx, remote cksum = xxxxyx on peer <hostname> Solution : Update the cluster.op-version Run gluster volume get all cluster.max-op-version to get the latest supported op-version. Update the cluster.op-version to the latest supported op-version by executing gluster volume set all cluster.op-version <op-version> . \"Accepted Peer Request\" If the glusterd handshake fails while expanding a cluster, the view of the cluster will be inconsistent. The state of the peer in gluster peer status will be \u201caccepted peer request\u201d and subsequent CLI commands will fail with an error. Eg. Volume create command will fail with \"volume create: testvol: failed: Host <hostname> is not in 'Peer in Cluster' state In this case the value of the state field in /var/lib/glusterd/peers/<UUID> will be other than 3. Solution : Stop glusterd Open /var/lib/glusterd/peers/<UUID> Change state to 3 Start glusterd","title":"Troubleshooting CLI and glusterd"},{"location":"Troubleshooting/troubleshooting-glusterd/#troubleshooting-the-gluster-cli-and-glusterd","text":"The glusterd daemon runs on every trusted server node and is responsible for the management of the trusted pool and volumes. The gluster CLI sends commands to the glusterd daemon on the local node, which executes the operation and returns the result to the user.","title":"Troubleshooting the gluster CLI and glusterd"},{"location":"Troubleshooting/troubleshooting-glusterd/#debugging-glusterd","text":"","title":"Debugging glusterd"},{"location":"Troubleshooting/troubleshooting-glusterd/#logs","text":"Start by looking at the log files for clues as to what went wrong when you hit a problem. The default directory for Gluster logs is /var/log/glusterfs. The logs for the CLI and glusterd are: glusterd : /var/log/glusterfs/glusterd.log gluster CLI : /var/log/glusterfs/cli.log","title":"Logs"},{"location":"Troubleshooting/troubleshooting-glusterd/#statedumps","text":"Statedumps are useful in debugging memory leaks and hangs. See Statedump for more details.","title":"Statedumps"},{"location":"Troubleshooting/troubleshooting-glusterd/#common-issues-and-how-to-resolve-them","text":"\" Another transaction is in progress for volname \" or \" Locking failed on xxx.xxx.xxx.xxx\" As Gluster is distributed by nature, glusterd takes locks when performing operations to ensure that configuration changes made to a volume are atomic across the cluster. These errors are returned when: More than one transaction contends on the same lock. Solution : These are likely to be transient errors and the operation will succeed if retried once the other transaction is complete. A stale lock exists on one of the nodes. Solution : Repeating the operation will not help until the stale lock is cleaned up. Restart the glusterd process holding the lock Check the glusterd.log file to find out which node holds the stale lock. Look for the message: lock being held by <uuid> Run gluster peer status to identify the node with the uuid in the log message. Restart glusterd on that node. \" Transport endpoint is not connected \" errors but all bricks are up This is usually seen when a brick process does not shut down cleanly, leaving stale data behind in the glusterd process. Gluster client processes query glusterd for the ports the bricks processes are listening on and attempt to connect to that port. If the port information in glusterd is incorrect, the client will fail to connect to the brick even though it is up. Operations which would need to access that brick may fail with \"Transport endpoint is not connected\". Solution : Restart the glusterd service. \"Peer Rejected\" gluster peer status returns \"Peer Rejected\" for a node. Hostname: <hostname> Uuid: <xxxx-xxx-xxxx> State: Peer Rejected (Connected) This indicates that the volume configuration on the node is not in sync with the rest of the trusted storage pool. You should see the following message in the glusterd log for the node on which the peer status command was run: Version of Cksums <vol-name> differ. local cksum = xxxxxx, remote cksum = xxxxyx on peer <hostname> Solution : Update the cluster.op-version Run gluster volume get all cluster.max-op-version to get the latest supported op-version. Update the cluster.op-version to the latest supported op-version by executing gluster volume set all cluster.op-version <op-version> . \"Accepted Peer Request\" If the glusterd handshake fails while expanding a cluster, the view of the cluster will be inconsistent. The state of the peer in gluster peer status will be \u201caccepted peer request\u201d and subsequent CLI commands will fail with an error. Eg. Volume create command will fail with \"volume create: testvol: failed: Host <hostname> is not in 'Peer in Cluster' state In this case the value of the state field in /var/lib/glusterd/peers/<UUID> will be other than 3. Solution : Stop glusterd Open /var/lib/glusterd/peers/<UUID> Change state to 3 Start glusterd","title":"Common Issues and How to Resolve Them"},{"location":"Troubleshooting/troubleshooting-gnfs/","text":"Troubleshooting Gluster NFS This section describes the most common troubleshooting issues related to NFS . mount command on NFS client fails with \u201cRPC Error: Program not registered\u201d Start portmap or rpcbind service on the NFS server. This error is encountered when the server has not started correctly. On most Linux distributions this is fixed by starting portmap: # /etc/init.d/portmap start On some distributions where portmap has been replaced by rpcbind, the following command is required: # /etc/init.d/rpcbind start After starting portmap or rpcbind, gluster NFS server needs to be restarted. NFS server start-up fails with \u201cPort is already in use\u201d error in the log file. Another Gluster NFS server is running on the same machine. This error can arise in case there is already a Gluster NFS server running on the same machine. This situation can be confirmed from the log file, if the following error lines exist: [2010-05-26 23:40:49] E [rpc-socket.c:126:rpcsvc_socket_listen] rpc-socket: binding socket failed:Address already in use [2010-05-26 23:40:49] E [rpc-socket.c:129:rpcsvc_socket_listen] rpc-socket: Port is already in use [2010-05-26 23:40:49] E [rpcsvc.c:2636:rpcsvc_stage_program_register] rpc-service: could not create listening connection [2010-05-26 23:40:49] E [rpcsvc.c:2675:rpcsvc_program_register] rpc-service: stage registration of program failed [2010-05-26 23:40:49] E [rpcsvc.c:2695:rpcsvc_program_register] rpc-service: Program registration failed: MOUNT3, Num: 100005, Ver: 3, Port: 38465 [2010-05-26 23:40:49] E [nfs.c:125:nfs_init_versions] nfs: Program init failed [2010-05-26 23:40:49] C [nfs.c:531:notify] nfs: Failed to initialize protocols To resolve this error one of the Gluster NFS servers will have to be shutdown. At this time, Gluster NFS server does not support running multiple NFS servers on the same machine. mount command fails with \u201crpc.statd\u201d related error message If the mount command fails with the following error message: mount.nfs: rpc.statd is not running but is required for remote locking. mount.nfs: Either use '-o nolock' to keep locks local, or start statd. For NFS clients to mount the NFS server, rpc.statd service must be running on the clients. Start rpc.statd service by running the following command: # rpc.statd mount command takes too long to finish. Start rpcbind service on the NFS client The problem is that the rpcbind or portmap service is not running on the NFS client. The resolution for this is to start either of these services by running the following command: # /etc/init.d/portmap start On some distributions where portmap has been replaced by rpcbind, the following command is required: # /etc/init.d/rpcbind start NFS server glusterfsd starts but initialization fails with \u201cnfsrpc- service: portmap registration of program failed\u201d error message in the log. NFS start-up can succeed but the initialization of the NFS service can still fail preventing clients from accessing the mount points. Such a situation can be confirmed from the following error messages in the log file: [2010-05-26 23:33:47] E [rpcsvc.c:2598:rpcsvc_program_register_portmap] rpc-service: Could notregister with portmap [2010-05-26 23:33:47] E [rpcsvc.c:2682:rpcsvc_program_register] rpc-service: portmap registration of program failed [2010-05-26 23:33:47] E [rpcsvc.c:2695:rpcsvc_program_register] rpc-service: Program registration failed: MOUNT3, Num: 100005, Ver: 3, Port: 38465 [2010-05-26 23:33:47] E [nfs.c:125:nfs_init_versions] nfs: Program init failed [2010-05-26 23:33:47] C [nfs.c:531:notify] nfs: Failed to initialize protocols [2010-05-26 23:33:49] E [rpcsvc.c:2614:rpcsvc_program_unregister_portmap] rpc-service: Could not unregister with portmap [2010-05-26 23:33:49] E [rpcsvc.c:2731:rpcsvc_program_unregister] rpc-service: portmap unregistration of program failed [2010-05-26 23:33:49] E [rpcsvc.c:2744:rpcsvc_program_unregister] rpc-service: Program unregistration failed: MOUNT3, Num: 100005, Ver: 3, Port: 38465 Start portmap or rpcbind service on the NFS server On most Linux distributions, portmap can be started using the following command: # /etc/init.d/portmap start On some distributions where portmap has been replaced by rpcbind, run the following command: # /etc/init.d/rpcbind start After starting portmap or rpcbind, gluster NFS server needs to be restarted. Stop another NFS server running on the same machine Such an error is also seen when there is another NFS server running on the same machine but it is not the Gluster NFS server. On Linux systems, this could be the kernel NFS server. Resolution involves stopping the other NFS server or not running the Gluster NFS server on the machine. Before stopping the kernel NFS server, ensure that no critical service depends on access to that NFS server's exports. On Linux, kernel NFS servers can be stopped by using either of the following commands depending on the distribution in use: # /etc/init.d/nfs-kernel-server stop # /etc/init.d/nfs stop Restart Gluster NFS server mount command fails with NFS server failed error. mount command fails with following error mount: mount to NFS server '10.1.10.11' failed: timed out (retrying). Perform one of the following to resolve this issue: Disable name lookup requests from NFS server to a DNS server The NFS server attempts to authenticate NFS clients by performing a reverse DNS lookup to match hostnames in the volume file with the client IP addresses. There can be a situation where the NFS server either is not able to connect to the DNS server or the DNS server is taking too long to responsd to DNS request. These delays can result in delayed replies from the NFS server to the NFS client resulting in the timeout error seen above. NFS server provides a work-around that disables DNS requests, instead relying only on the client IP addresses for authentication. The following option can be added for successful mounting in such situations: option rpc-auth.addr.namelookup off Note : Remember that disabling the NFS server forces authentication of clients to use only IP addresses and if the authentication rules in the volume file use hostnames, those authentication rules will fail and disallow mounting for those clients. OR NFS version used by the NFS client is other than version 3 Gluster NFS server supports version 3 of NFS protocol. In recent Linux kernels, the default NFS version has been changed from 3 to 4. It is possible that the client machine is unable to connect to the Gluster NFS server because it is using version 4 messages which are not understood by Gluster NFS server. The timeout can be resolved by forcing the NFS client to use version 3. The vers option to mount command is used for this purpose: # mount -o vers=3 showmount fails with clnt_create: RPC: Unable to receive Check your firewall setting to open ports 111 for portmap requests/replies and Gluster NFS server requests/replies. Gluster NFS server operates over the following port numbers: 38465, 38466, and 38467. Application fails with \"Invalid argument\" or \"Value too large for defined data type\" error. These two errors generally happen for 32-bit nfs clients or applications that do not support 64-bit inode numbers or large files. Use the following option from the CLI to make Gluster NFS return 32-bit inode numbers instead: nfs.enable-ino32 \\<on|off> Applications that will benefit are those that were either: built 32-bit and run on 32-bit machines such that they do not support large files by default built 32-bit on 64-bit systems This option is disabled by default so NFS returns 64-bit inode numbers by default. Applications which can be rebuilt from source are recommended to rebuild using the following flag with gcc: -D_FILE_OFFSET_BITS=64","title":"Troubleshooting gNFS"},{"location":"Troubleshooting/troubleshooting-gnfs/#troubleshooting-gluster-nfs","text":"This section describes the most common troubleshooting issues related to NFS .","title":"Troubleshooting Gluster NFS"},{"location":"Troubleshooting/troubleshooting-gnfs/#mount-command-on-nfs-client-fails-with-rpc-error-program-not-registered","text":"Start portmap or rpcbind service on the NFS server. This error is encountered when the server has not started correctly. On most Linux distributions this is fixed by starting portmap: # /etc/init.d/portmap start On some distributions where portmap has been replaced by rpcbind, the following command is required: # /etc/init.d/rpcbind start After starting portmap or rpcbind, gluster NFS server needs to be restarted.","title":"mount command on NFS client fails with \u201cRPC Error: Program not registered\u201d"},{"location":"Troubleshooting/troubleshooting-gnfs/#nfs-server-start-up-fails-with-port-is-already-in-use-error-in-the-log-file","text":"Another Gluster NFS server is running on the same machine. This error can arise in case there is already a Gluster NFS server running on the same machine. This situation can be confirmed from the log file, if the following error lines exist: [2010-05-26 23:40:49] E [rpc-socket.c:126:rpcsvc_socket_listen] rpc-socket: binding socket failed:Address already in use [2010-05-26 23:40:49] E [rpc-socket.c:129:rpcsvc_socket_listen] rpc-socket: Port is already in use [2010-05-26 23:40:49] E [rpcsvc.c:2636:rpcsvc_stage_program_register] rpc-service: could not create listening connection [2010-05-26 23:40:49] E [rpcsvc.c:2675:rpcsvc_program_register] rpc-service: stage registration of program failed [2010-05-26 23:40:49] E [rpcsvc.c:2695:rpcsvc_program_register] rpc-service: Program registration failed: MOUNT3, Num: 100005, Ver: 3, Port: 38465 [2010-05-26 23:40:49] E [nfs.c:125:nfs_init_versions] nfs: Program init failed [2010-05-26 23:40:49] C [nfs.c:531:notify] nfs: Failed to initialize protocols To resolve this error one of the Gluster NFS servers will have to be shutdown. At this time, Gluster NFS server does not support running multiple NFS servers on the same machine.","title":"NFS server start-up fails with \u201cPort is already in use\u201d error in the log file."},{"location":"Troubleshooting/troubleshooting-gnfs/#mount-command-fails-with-rpcstatd-related-error-message","text":"If the mount command fails with the following error message: mount.nfs: rpc.statd is not running but is required for remote locking. mount.nfs: Either use '-o nolock' to keep locks local, or start statd. For NFS clients to mount the NFS server, rpc.statd service must be running on the clients. Start rpc.statd service by running the following command: # rpc.statd","title":"mount command fails with \u201crpc.statd\u201d related error message"},{"location":"Troubleshooting/troubleshooting-gnfs/#mount-command-takes-too-long-to-finish","text":"Start rpcbind service on the NFS client The problem is that the rpcbind or portmap service is not running on the NFS client. The resolution for this is to start either of these services by running the following command: # /etc/init.d/portmap start On some distributions where portmap has been replaced by rpcbind, the following command is required: # /etc/init.d/rpcbind start","title":"mount command takes too long to finish."},{"location":"Troubleshooting/troubleshooting-gnfs/#nfs-server-glusterfsd-starts-but-initialization-fails-with-nfsrpc-service-portmap-registration-of-program-failed-error-message-in-the-log","text":"NFS start-up can succeed but the initialization of the NFS service can still fail preventing clients from accessing the mount points. Such a situation can be confirmed from the following error messages in the log file: [2010-05-26 23:33:47] E [rpcsvc.c:2598:rpcsvc_program_register_portmap] rpc-service: Could notregister with portmap [2010-05-26 23:33:47] E [rpcsvc.c:2682:rpcsvc_program_register] rpc-service: portmap registration of program failed [2010-05-26 23:33:47] E [rpcsvc.c:2695:rpcsvc_program_register] rpc-service: Program registration failed: MOUNT3, Num: 100005, Ver: 3, Port: 38465 [2010-05-26 23:33:47] E [nfs.c:125:nfs_init_versions] nfs: Program init failed [2010-05-26 23:33:47] C [nfs.c:531:notify] nfs: Failed to initialize protocols [2010-05-26 23:33:49] E [rpcsvc.c:2614:rpcsvc_program_unregister_portmap] rpc-service: Could not unregister with portmap [2010-05-26 23:33:49] E [rpcsvc.c:2731:rpcsvc_program_unregister] rpc-service: portmap unregistration of program failed [2010-05-26 23:33:49] E [rpcsvc.c:2744:rpcsvc_program_unregister] rpc-service: Program unregistration failed: MOUNT3, Num: 100005, Ver: 3, Port: 38465 Start portmap or rpcbind service on the NFS server On most Linux distributions, portmap can be started using the following command: # /etc/init.d/portmap start On some distributions where portmap has been replaced by rpcbind, run the following command: # /etc/init.d/rpcbind start After starting portmap or rpcbind, gluster NFS server needs to be restarted. Stop another NFS server running on the same machine Such an error is also seen when there is another NFS server running on the same machine but it is not the Gluster NFS server. On Linux systems, this could be the kernel NFS server. Resolution involves stopping the other NFS server or not running the Gluster NFS server on the machine. Before stopping the kernel NFS server, ensure that no critical service depends on access to that NFS server's exports. On Linux, kernel NFS servers can be stopped by using either of the following commands depending on the distribution in use: # /etc/init.d/nfs-kernel-server stop # /etc/init.d/nfs stop Restart Gluster NFS server","title":"NFS server glusterfsd starts but initialization fails with \u201cnfsrpc- service: portmap registration of program failed\u201d error message in the log."},{"location":"Troubleshooting/troubleshooting-gnfs/#mount-command-fails-with-nfs-server-failed-error","text":"mount command fails with following error mount: mount to NFS server '10.1.10.11' failed: timed out (retrying). Perform one of the following to resolve this issue: Disable name lookup requests from NFS server to a DNS server The NFS server attempts to authenticate NFS clients by performing a reverse DNS lookup to match hostnames in the volume file with the client IP addresses. There can be a situation where the NFS server either is not able to connect to the DNS server or the DNS server is taking too long to responsd to DNS request. These delays can result in delayed replies from the NFS server to the NFS client resulting in the timeout error seen above. NFS server provides a work-around that disables DNS requests, instead relying only on the client IP addresses for authentication. The following option can be added for successful mounting in such situations: option rpc-auth.addr.namelookup off Note : Remember that disabling the NFS server forces authentication of clients to use only IP addresses and if the authentication rules in the volume file use hostnames, those authentication rules will fail and disallow mounting for those clients. OR NFS version used by the NFS client is other than version 3 Gluster NFS server supports version 3 of NFS protocol. In recent Linux kernels, the default NFS version has been changed from 3 to 4. It is possible that the client machine is unable to connect to the Gluster NFS server because it is using version 4 messages which are not understood by Gluster NFS server. The timeout can be resolved by forcing the NFS client to use version 3. The vers option to mount command is used for this purpose: # mount -o vers=3","title":"mount command fails with NFS server failed error."},{"location":"Troubleshooting/troubleshooting-gnfs/#showmount-fails-with-clnt_create-rpc-unable-to-receive","text":"Check your firewall setting to open ports 111 for portmap requests/replies and Gluster NFS server requests/replies. Gluster NFS server operates over the following port numbers: 38465, 38466, and 38467.","title":"showmount fails with clnt_create: RPC: Unable to receive"},{"location":"Troubleshooting/troubleshooting-gnfs/#application-fails-with-invalid-argument-or-value-too-large-for-defined-data-type-error","text":"These two errors generally happen for 32-bit nfs clients or applications that do not support 64-bit inode numbers or large files. Use the following option from the CLI to make Gluster NFS return 32-bit inode numbers instead: nfs.enable-ino32 \\<on|off> Applications that will benefit are those that were either: built 32-bit and run on 32-bit machines such that they do not support large files by default built 32-bit on 64-bit systems This option is disabled by default so NFS returns 64-bit inode numbers by default. Applications which can be rebuilt from source are recommended to rebuild using the following flag with gcc: -D_FILE_OFFSET_BITS=64","title":"Application fails with \"Invalid argument\" or \"Value too large for defined data type\" error."},{"location":"Troubleshooting/troubleshooting-memory/","text":"Troubleshooting High Memory Utilization If the memory utilization of a Gluster process increases significantly with time, it could be a leak caused by resources not being freed. If you suspect that you may have hit such an issue, try using statedumps to debug the issue. If you are unable to figure out where the leak is, please file an issue and provide the following details: Gluster version The affected process The output of gluster volume info Steps to reproduce the issue if available Statedumps for the process collected at intervals as the memory utilization increases The Gluster log files for the process (if possible)","title":"Debugging Memory Leaks"},{"location":"Troubleshooting/troubleshooting-memory/#troubleshooting-high-memory-utilization","text":"If the memory utilization of a Gluster process increases significantly with time, it could be a leak caused by resources not being freed. If you suspect that you may have hit such an issue, try using statedumps to debug the issue. If you are unable to figure out where the leak is, please file an issue and provide the following details: Gluster version The affected process The output of gluster volume info Steps to reproduce the issue if available Statedumps for the process collected at intervals as the memory utilization increases The Gluster log files for the process (if possible)","title":"Troubleshooting High Memory Utilization"},{"location":"Upgrade-Guide/","text":"Upgrading GlusterFS About op-version If you are using GlusterFS version 6.x or above, you can upgrade it to the following: Upgrading to 10 Upgrading to 9 Upgrading to 8 Upgrading to 7 If you are using GlusterFS version 5.x or above, you can upgrade it to the following: Upgrading to 8 Upgrading to 7 Upgrading to 6 If you are using GlusterFS version 4.x or above, you can upgrade it to the following: Upgrading to 6 Upgrading to 5 If you are using GlusterFS version 3.4.x or above, you can upgrade it to following: Upgrading to 3.5 Upgrading to 3.6 Upgrading to 3.7 Upgrading to 3.9 Upgrading to 3.10 Upgrading to 3.11 Upgrading to 3.12 Upgrading to 3.13","title":"Upgrade-Guide Index"},{"location":"Upgrade-Guide/#upgrading-glusterfs","text":"About op-version If you are using GlusterFS version 6.x or above, you can upgrade it to the following: Upgrading to 10 Upgrading to 9 Upgrading to 8 Upgrading to 7 If you are using GlusterFS version 5.x or above, you can upgrade it to the following: Upgrading to 8 Upgrading to 7 Upgrading to 6 If you are using GlusterFS version 4.x or above, you can upgrade it to the following: Upgrading to 6 Upgrading to 5 If you are using GlusterFS version 3.4.x or above, you can upgrade it to following: Upgrading to 3.5 Upgrading to 3.6 Upgrading to 3.7 Upgrading to 3.9 Upgrading to 3.10 Upgrading to 3.11 Upgrading to 3.12 Upgrading to 3.13","title":"Upgrading GlusterFS"},{"location":"Upgrade-Guide/generic-upgrade-procedure/","text":"Generic Upgrade procedure Pre-upgrade notes Online upgrade is only possible with replicated and distributed replicate volumes Online upgrade is not supported for dispersed or distributed dispersed volumes Ensure no configuration changes are done during the upgrade If you are using geo-replication, please upgrade the slave cluster(s) before upgrading the master Upgrading the servers ahead of the clients is recommended It is recommended to have the same client and server, major versions running eventually Online upgrade procedure for servers This procedure involves upgrading one server at a time , while keeping the volume(s) online and client IO ongoing. This procedure assumes that multiple replicas of a replica set, are not part of the same server in the trusted storage pool. ALERT: If there are disperse or, pure distributed volumes in the storage pool being upgraded, this procedure is NOT recommended, use the Offline upgrade procedure instead. Repeat the following steps, on each server in the trusted storage pool, to upgrade the entire pool to new-version : Stop all gluster services, either using the command below, or through other means. # systemctl stop glusterd # systemctl stop glustereventsd # killall glusterfs glusterfsd glusterd Stop all applications that run on this server and access the volumes via gfapi (qemu, NFS-Ganesha, Samba, etc.) Install Gluster new-version, below example shows how to create a repository on fedora and use it to upgrade : 3.1 Create a private repository (assuming /new-gluster-rpms/ folder has the new rpms ): # createrepo /new-gluster-rpms/ 3.2 Create the .repo file in /etc/yum.d/ : # cat /etc/yum.d/newglusterrepo.repo [newglusterrepo] name=NewGlusterRepo baseurl=\"file:///new-gluster-rpms/\" gpgcheck=0 enabled=1 3.3 Upgrade glusterfs, for example to upgrade glusterfs-server to x.y version : # yum update glusterfs-server-x.y.fc30.x86_64.rpm Ensure that version reflects new-version in the output of, # gluster --version Start glusterd on the upgraded server # systemctl start glusterd Ensure that all gluster processes are online by checking the output of, # gluster volume status If the glustereventsd service was previously enabled, it is required to start it using the commands below, or, through other means, # systemctl start glustereventsd Invoke self-heal on all the gluster volumes by running, # for i in `gluster volume list`; do gluster volume heal $i; done Verify that there are no heal backlog by running the command for all the volumes, # gluster volume heal <volname> info NOTE: Before proceeding to upgrade the next server in the pool it is recommended to check the heal backlog. If there is a heal backlog, it is recommended to wait until the backlog is empty, or, the backlog does not contain any entries requiring a sync to the just upgraded server. Restart any gfapi based application stopped previously in step (2) Offline upgrade procedure This procedure involves cluster downtime and during the upgrade window, clients are not allowed access to the volumes. Steps to perform an offline upgrade: On every server in the trusted storage pool, stop all gluster services, either using the command below, or through other means, # systemctl stop glusterd # systemctl stop glustereventsd # killall glusterfs glusterfsd glusterd Stop all applications that access the volumes via gfapi (qemu, NFS-Ganesha, Samba, etc.), across all servers Install Gluster new-version, on all servers Ensure that version reflects new-version in the output of the following command on all servers, # gluster --version Start glusterd on all the upgraded servers # systemctl start glusterd Ensure that all gluster processes are online by checking the output of, # gluster volume status If the glustereventsd service was previously enabled, it is required to start it using the commands below, or, through other means, # systemctl start glustereventsd Restart any gfapi based application stopped previously in step (2) Post upgrade steps Perform the following steps post upgrading the entire trusted storage pool, It is recommended to update the op-version of the cluster. Refer, to the op-version section for further details Proceed to upgrade the clients to new-version version as well Post upgrading the clients, for replicate volumes, it is recommended to enable the option gluster volume set <volname> fips-mode-rchecksum on to turn off usage of MD5 checksums during healing. This enables running Gluster on FIPS compliant systems. If upgrading from a version lesser than Gluster 7.0 NOTE: If you have ever enabled quota on your volumes then after the upgrade is done, you will have to restart all the nodes in the cluster one by one so as to fix the checksum values in the quota.cksum file under the /var/lib/glusterd/vols/<volname>/ directory. The peers may go into Peer rejected state while doing so but once all the nodes are rebooted everything will be back to normal. Upgrade procedure for clients Following are the steps to upgrade clients to the new-version version, Unmount all glusterfs mount points on the client Stop all applications that access the volumes via gfapi (qemu, etc.) Install Gluster new-version Mount all gluster shares Start any applications that were stopped previously in step (2)","title":"Generic upgrade procedure"},{"location":"Upgrade-Guide/generic-upgrade-procedure/#generic-upgrade-procedure","text":"","title":"Generic Upgrade procedure"},{"location":"Upgrade-Guide/generic-upgrade-procedure/#pre-upgrade-notes","text":"Online upgrade is only possible with replicated and distributed replicate volumes Online upgrade is not supported for dispersed or distributed dispersed volumes Ensure no configuration changes are done during the upgrade If you are using geo-replication, please upgrade the slave cluster(s) before upgrading the master Upgrading the servers ahead of the clients is recommended It is recommended to have the same client and server, major versions running eventually","title":"Pre-upgrade notes"},{"location":"Upgrade-Guide/generic-upgrade-procedure/#online-upgrade-procedure-for-servers","text":"This procedure involves upgrading one server at a time , while keeping the volume(s) online and client IO ongoing. This procedure assumes that multiple replicas of a replica set, are not part of the same server in the trusted storage pool. ALERT: If there are disperse or, pure distributed volumes in the storage pool being upgraded, this procedure is NOT recommended, use the Offline upgrade procedure instead.","title":"Online upgrade procedure for servers"},{"location":"Upgrade-Guide/generic-upgrade-procedure/#repeat-the-following-steps-on-each-server-in-the-trusted-storage-pool-to-upgrade-the-entire-pool-to-new-version","text":"Stop all gluster services, either using the command below, or through other means. # systemctl stop glusterd # systemctl stop glustereventsd # killall glusterfs glusterfsd glusterd Stop all applications that run on this server and access the volumes via gfapi (qemu, NFS-Ganesha, Samba, etc.) Install Gluster new-version, below example shows how to create a repository on fedora and use it to upgrade : 3.1 Create a private repository (assuming /new-gluster-rpms/ folder has the new rpms ): # createrepo /new-gluster-rpms/ 3.2 Create the .repo file in /etc/yum.d/ : # cat /etc/yum.d/newglusterrepo.repo [newglusterrepo] name=NewGlusterRepo baseurl=\"file:///new-gluster-rpms/\" gpgcheck=0 enabled=1 3.3 Upgrade glusterfs, for example to upgrade glusterfs-server to x.y version : # yum update glusterfs-server-x.y.fc30.x86_64.rpm Ensure that version reflects new-version in the output of, # gluster --version Start glusterd on the upgraded server # systemctl start glusterd Ensure that all gluster processes are online by checking the output of, # gluster volume status If the glustereventsd service was previously enabled, it is required to start it using the commands below, or, through other means, # systemctl start glustereventsd Invoke self-heal on all the gluster volumes by running, # for i in `gluster volume list`; do gluster volume heal $i; done Verify that there are no heal backlog by running the command for all the volumes, # gluster volume heal <volname> info NOTE: Before proceeding to upgrade the next server in the pool it is recommended to check the heal backlog. If there is a heal backlog, it is recommended to wait until the backlog is empty, or, the backlog does not contain any entries requiring a sync to the just upgraded server. Restart any gfapi based application stopped previously in step (2)","title":"Repeat the following steps, on each server in the trusted storage pool, to upgrade the entire pool to new-version :"},{"location":"Upgrade-Guide/generic-upgrade-procedure/#offline-upgrade-procedure","text":"This procedure involves cluster downtime and during the upgrade window, clients are not allowed access to the volumes.","title":"Offline upgrade procedure"},{"location":"Upgrade-Guide/generic-upgrade-procedure/#steps-to-perform-an-offline-upgrade","text":"On every server in the trusted storage pool, stop all gluster services, either using the command below, or through other means, # systemctl stop glusterd # systemctl stop glustereventsd # killall glusterfs glusterfsd glusterd Stop all applications that access the volumes via gfapi (qemu, NFS-Ganesha, Samba, etc.), across all servers Install Gluster new-version, on all servers Ensure that version reflects new-version in the output of the following command on all servers, # gluster --version Start glusterd on all the upgraded servers # systemctl start glusterd Ensure that all gluster processes are online by checking the output of, # gluster volume status If the glustereventsd service was previously enabled, it is required to start it using the commands below, or, through other means, # systemctl start glustereventsd Restart any gfapi based application stopped previously in step (2)","title":"Steps to perform an offline upgrade:"},{"location":"Upgrade-Guide/generic-upgrade-procedure/#post-upgrade-steps","text":"Perform the following steps post upgrading the entire trusted storage pool, It is recommended to update the op-version of the cluster. Refer, to the op-version section for further details Proceed to upgrade the clients to new-version version as well Post upgrading the clients, for replicate volumes, it is recommended to enable the option gluster volume set <volname> fips-mode-rchecksum on to turn off usage of MD5 checksums during healing. This enables running Gluster on FIPS compliant systems.","title":"Post upgrade steps"},{"location":"Upgrade-Guide/generic-upgrade-procedure/#if-upgrading-from-a-version-lesser-than-gluster-70","text":"NOTE: If you have ever enabled quota on your volumes then after the upgrade is done, you will have to restart all the nodes in the cluster one by one so as to fix the checksum values in the quota.cksum file under the /var/lib/glusterd/vols/<volname>/ directory. The peers may go into Peer rejected state while doing so but once all the nodes are rebooted everything will be back to normal.","title":"If upgrading from a version lesser than Gluster 7.0"},{"location":"Upgrade-Guide/generic-upgrade-procedure/#upgrade-procedure-for-clients","text":"Following are the steps to upgrade clients to the new-version version, Unmount all glusterfs mount points on the client Stop all applications that access the volumes via gfapi (qemu, etc.) Install Gluster new-version Mount all gluster shares Start any applications that were stopped previously in step (2)","title":"Upgrade procedure for clients"},{"location":"Upgrade-Guide/op-version/","text":"op-version op-version is the operating version of the Gluster which is running. op-version was introduced to ensure gluster running with different versions do not end up in a problem and backward compatibility issues can be tackled. After Gluster upgrade, it is advisable to have op-version updated. Updating op-version Current op-version can be queried as below: For 3.10 onwards: # gluster volume get all cluster.op-version For release < 3.10: # gluster volume get <VOLNAME> cluster.op-version To get the maximum possible op-version a cluster can support, the following query can be used (this is available 3.10 release onwards): # gluster volume get all cluster.max-op-version For example, if some nodes in a cluster have been upgraded to X and some to X+, then the maximum op-version supported by the cluster is X, and the cluster.op-version can be bumped up to X to support new features. op-version can be updated as below. For example, after upgrading to glusterfs-4.0.0, set op-version as: # gluster volume set all cluster.op-version 40000 Note: This is not mandatory, but advisable to have updated op-version if you want to make use of latest features in the updated gluster. Client op-version When trying to set a volume option, it might happen that one or more of the connected clients cannot support the feature being set and might need to be upgraded to the op-version the cluster is currently running on. To check op-version information for the connected clients and find the offending client, the following query can be used for 3.10 release onwards: # gluster volume status <all|VOLNAME> clients The respective clients can then be upgraded to the required version. This information could also be used to make an informed decision while bumping up the op-version of a cluster, so that connected clients can support all the new features provided by the upgraded cluster as well.","title":"Op-version"},{"location":"Upgrade-Guide/op-version/#op-version","text":"op-version is the operating version of the Gluster which is running. op-version was introduced to ensure gluster running with different versions do not end up in a problem and backward compatibility issues can be tackled. After Gluster upgrade, it is advisable to have op-version updated.","title":"op-version"},{"location":"Upgrade-Guide/op-version/#updating-op-version","text":"Current op-version can be queried as below: For 3.10 onwards: # gluster volume get all cluster.op-version For release < 3.10: # gluster volume get <VOLNAME> cluster.op-version To get the maximum possible op-version a cluster can support, the following query can be used (this is available 3.10 release onwards): # gluster volume get all cluster.max-op-version For example, if some nodes in a cluster have been upgraded to X and some to X+, then the maximum op-version supported by the cluster is X, and the cluster.op-version can be bumped up to X to support new features. op-version can be updated as below. For example, after upgrading to glusterfs-4.0.0, set op-version as: # gluster volume set all cluster.op-version 40000 Note: This is not mandatory, but advisable to have updated op-version if you want to make use of latest features in the updated gluster.","title":"Updating op-version"},{"location":"Upgrade-Guide/op-version/#client-op-version","text":"When trying to set a volume option, it might happen that one or more of the connected clients cannot support the feature being set and might need to be upgraded to the op-version the cluster is currently running on. To check op-version information for the connected clients and find the offending client, the following query can be used for 3.10 release onwards: # gluster volume status <all|VOLNAME> clients The respective clients can then be upgraded to the required version. This information could also be used to make an informed decision while bumping up the op-version of a cluster, so that connected clients can support all the new features provided by the upgraded cluster as well.","title":"Client op-version"},{"location":"Upgrade-Guide/upgrade-to-10/","text":"Upgrade procedure to Gluster 10, from Gluster 9.x, 8.x and 7.x We recommend reading the release notes for 10.0 to be aware of the features and fixes provided with the release. NOTE: Before following the generic upgrade procedure checkout the \" Major Issues \" section given below. Refer, to the generic upgrade procedure guide and follow documented instructions. Major issues The following options are removed from the code base and require to be unset before an upgrade from releases older than release 4.1.0, - features.lock-heal - features.grace-timeout To check if these options are set use, # gluster volume info and ensure that the above options are not part of the Options Reconfigured: section in the output of all volumes in the cluster. If these are set, then unset them using the following commands, # gluster volume reset <volname> <option> Make sure you are not using any of the following depricated features : - Block device (bd) xlator - Decompounder feature - Crypt xlator - Symlink-cache xlator - Stripe feature - Tiering support (tier xlator and changetimerecorder) - Glupy NOTE: Failure to do the above may result in failure during online upgrades, and the reset of these options to their defaults needs to be done prior to upgrading the cluster. Deprecated translators and upgrade procedure for volumes using these features If you are upgrading from a release prior to release-6 be aware of deprecated xlators and functionality .","title":"Upgrade to 10"},{"location":"Upgrade-Guide/upgrade-to-10/#upgrade-procedure-to-gluster-10-from-gluster-9x-8x-and-7x","text":"We recommend reading the release notes for 10.0 to be aware of the features and fixes provided with the release. NOTE: Before following the generic upgrade procedure checkout the \" Major Issues \" section given below. Refer, to the generic upgrade procedure guide and follow documented instructions.","title":"Upgrade procedure to Gluster 10, from Gluster 9.x, 8.x and 7.x"},{"location":"Upgrade-Guide/upgrade-to-10/#major-issues","text":"","title":"Major issues"},{"location":"Upgrade-Guide/upgrade-to-10/#the-following-options-are-removed-from-the-code-base-and-require-to-be-unset","text":"before an upgrade from releases older than release 4.1.0, - features.lock-heal - features.grace-timeout To check if these options are set use, # gluster volume info and ensure that the above options are not part of the Options Reconfigured: section in the output of all volumes in the cluster. If these are set, then unset them using the following commands, # gluster volume reset <volname> <option>","title":"The following options are removed from the code base and require to be unset"},{"location":"Upgrade-Guide/upgrade-to-10/#make-sure-you-are-not-using-any-of-the-following-depricated-features","text":"- Block device (bd) xlator - Decompounder feature - Crypt xlator - Symlink-cache xlator - Stripe feature - Tiering support (tier xlator and changetimerecorder) - Glupy NOTE: Failure to do the above may result in failure during online upgrades, and the reset of these options to their defaults needs to be done prior to upgrading the cluster.","title":"Make sure you are not using any of the following depricated features :"},{"location":"Upgrade-Guide/upgrade-to-10/#deprecated-translators-and-upgrade-procedure-for-volumes-using-these-features","text":"If you are upgrading from a release prior to release-6 be aware of deprecated xlators and functionality .","title":"Deprecated translators and upgrade procedure for volumes using these features"},{"location":"Upgrade-Guide/upgrade-to-3.10/","text":"Upgrade procedure to Gluster 3.10.0, from Gluster 3.9.x, 3.8.x and 3.7.x Pre-upgrade notes Online upgrade is only possible with replicated and distributed replicate volumes Online upgrade is not supported for dispersed or distributed dispersed volumes Ensure no configuration changes are done during the upgrade If you are using geo-replication, please upgrade the slave cluster(s) before upgrading the master Upgrading the servers ahead of the clients is recommended It is recommended to have the same client and server, major versions running eventually Online upgrade procedure for servers This procedure involves upgrading one server at a time , while keeping the volume(s) online and client IO ongoing. This procedure assumes that multiple replicas of a replica set, are not part of the same server in the trusted storage pool. ALERT : If any of your volumes, in the trusted storage pool that is being upgraded, uses disperse or is a pure distributed volume, this procedure is NOT recommended, use the Offline upgrade procedure instead. Repeat the following steps, on each server in the trusted storage pool, to upgrade the entire pool to 3.10 version: Stop all gluster services, either using the command below, or through other means, #killall glusterfs glusterfsd glusterd Stop all applications that run on this server and access the volumes via gfapi (qemu, NFS-Ganesha, Samba, etc.) Install Gluster 3.10 Ensure that version reflects 3.10.0 in the output of, #gluster --version Start glusterd on the upgraded server #glusterd Ensure that all gluster processes are online by checking the output of, #gluster volume status Self-heal all gluster volumes by running #for i in `gluster volume list`; do gluster volume heal $i; done Ensure that there is no heal backlog by running the below command for all volumes #gluster volume heal <volname> info NOTE: If there is a heal backlog, wait till the backlog is empty, or the backlog does not have any entries needing a sync to the just upgraded server, before proceeding to upgrade the next server in the pool Restart any gfapi based application stopped previously in step (2) Offline upgrade procedure This procedure involves cluster downtime and during the upgrade window, clients are not allowed access to the volumes. Steps to perform an offline upgrade: On every server in the trusted storage pool, stop all gluster services, either using the command below, or through other means, #killall glusterfs glusterfsd glusterd Stop all applications that access the volumes via gfapi (qemu, NFS-Ganesha, Samba, etc.), across all servers Install Gluster 3.10, on all servers Ensure that version reflects 3.10.0 in the output of the following command on all servers, #gluster --version Start glusterd on all the upgraded servers #glusterd Ensure that all gluster processes are online by checking the output of, #gluster volume status Restart any gfapi based application stopped previously in step (2) Post upgrade steps Perform the following steps post upgrading the entire trusted storage pool, - It is recommended to update the op-version of the cluster. Refer, to the op-version section for further details - Proceed to upgrade the clients to 3.10 version as well Upgrade procedure for clients Following are the steps to upgrade clients to the 3.10.0 version, Unmount all glusterfs mount points on the client Stop all applications that access the volumes via gfapi (qemu, etc.) Install Gluster 3.10 Mount all gluster shares Start any applications that were stopped previously in step (2)","title":"Upgrade to 3.10"},{"location":"Upgrade-Guide/upgrade-to-3.10/#upgrade-procedure-to-gluster-3100-from-gluster-39x-38x-and-37x","text":"","title":"Upgrade procedure to Gluster 3.10.0, from Gluster 3.9.x, 3.8.x and 3.7.x"},{"location":"Upgrade-Guide/upgrade-to-3.10/#pre-upgrade-notes","text":"Online upgrade is only possible with replicated and distributed replicate volumes Online upgrade is not supported for dispersed or distributed dispersed volumes Ensure no configuration changes are done during the upgrade If you are using geo-replication, please upgrade the slave cluster(s) before upgrading the master Upgrading the servers ahead of the clients is recommended It is recommended to have the same client and server, major versions running eventually","title":"Pre-upgrade notes"},{"location":"Upgrade-Guide/upgrade-to-3.10/#online-upgrade-procedure-for-servers","text":"This procedure involves upgrading one server at a time , while keeping the volume(s) online and client IO ongoing. This procedure assumes that multiple replicas of a replica set, are not part of the same server in the trusted storage pool. ALERT : If any of your volumes, in the trusted storage pool that is being upgraded, uses disperse or is a pure distributed volume, this procedure is NOT recommended, use the Offline upgrade procedure instead.","title":"Online upgrade procedure for servers"},{"location":"Upgrade-Guide/upgrade-to-3.10/#repeat-the-following-steps-on-each-server-in-the-trusted-storage-pool-to-upgrade-the-entire-pool-to-310-version","text":"Stop all gluster services, either using the command below, or through other means, #killall glusterfs glusterfsd glusterd Stop all applications that run on this server and access the volumes via gfapi (qemu, NFS-Ganesha, Samba, etc.) Install Gluster 3.10 Ensure that version reflects 3.10.0 in the output of, #gluster --version Start glusterd on the upgraded server #glusterd Ensure that all gluster processes are online by checking the output of, #gluster volume status Self-heal all gluster volumes by running #for i in `gluster volume list`; do gluster volume heal $i; done Ensure that there is no heal backlog by running the below command for all volumes #gluster volume heal <volname> info NOTE: If there is a heal backlog, wait till the backlog is empty, or the backlog does not have any entries needing a sync to the just upgraded server, before proceeding to upgrade the next server in the pool Restart any gfapi based application stopped previously in step (2)","title":"Repeat the following steps, on each server in the trusted storage pool, to upgrade the entire pool to 3.10 version:"},{"location":"Upgrade-Guide/upgrade-to-3.10/#offline-upgrade-procedure","text":"This procedure involves cluster downtime and during the upgrade window, clients are not allowed access to the volumes.","title":"Offline upgrade procedure"},{"location":"Upgrade-Guide/upgrade-to-3.10/#steps-to-perform-an-offline-upgrade","text":"On every server in the trusted storage pool, stop all gluster services, either using the command below, or through other means, #killall glusterfs glusterfsd glusterd Stop all applications that access the volumes via gfapi (qemu, NFS-Ganesha, Samba, etc.), across all servers Install Gluster 3.10, on all servers Ensure that version reflects 3.10.0 in the output of the following command on all servers, #gluster --version Start glusterd on all the upgraded servers #glusterd Ensure that all gluster processes are online by checking the output of, #gluster volume status Restart any gfapi based application stopped previously in step (2)","title":"Steps to perform an offline upgrade:"},{"location":"Upgrade-Guide/upgrade-to-3.10/#post-upgrade-steps","text":"Perform the following steps post upgrading the entire trusted storage pool, - It is recommended to update the op-version of the cluster. Refer, to the op-version section for further details - Proceed to upgrade the clients to 3.10 version as well","title":"Post upgrade steps"},{"location":"Upgrade-Guide/upgrade-to-3.10/#upgrade-procedure-for-clients","text":"Following are the steps to upgrade clients to the 3.10.0 version, Unmount all glusterfs mount points on the client Stop all applications that access the volumes via gfapi (qemu, etc.) Install Gluster 3.10 Mount all gluster shares Start any applications that were stopped previously in step (2)","title":"Upgrade procedure for clients"},{"location":"Upgrade-Guide/upgrade-to-3.11/","text":"Upgrade procedure to Gluster 3.11, from Gluster 3.10.x, and 3.8.x NOTE: Upgrade procedure remains the same as with the 3.10 release Pre-upgrade notes Online upgrade is only possible with replicated and distributed replicate volumes Online upgrade is not supported for dispersed or distributed dispersed volumes Ensure no configuration changes are done during the upgrade If you are using geo-replication, please upgrade the slave cluster(s) before upgrading the master Upgrading the servers ahead of the clients is recommended It is recommended to have the same client and server, major versions running eventually Online upgrade procedure for servers This procedure involves upgrading one server at a time , while keeping the volume(s) online and client IO ongoing. This procedure assumes that multiple replicas of a replica set, are not part of the same server in the trusted storage pool. ALERT : If any of your volumes, in the trusted storage pool that is being upgraded, uses disperse or is a pure distributed volume, this procedure is NOT recommended, use the Offline upgrade procedure instead. Repeat the following steps, on each server in the trusted storage pool, to upgrade the entire pool to 3.11 version: Stop all gluster services, either using the command below, or through other means, #killall glusterfs glusterfsd glusterd Stop all applications that run on this server and access the volumes via gfapi (qemu, NFS-Ganesha, Samba, etc.) Install Gluster 3.11 Ensure that version reflects 3.11.x in the output of, #gluster --version NOTE: x is the minor release number for the release Start glusterd on the upgraded server #glusterd Ensure that all gluster processes are online by checking the output of, #gluster volume status Self-heal all gluster volumes by running #for i in `gluster volume list`; do gluster volume heal $i; done Ensure that there is no heal backlog by running the below command for all volumes #gluster volume heal <volname> info NOTE: If there is a heal backlog, wait till the backlog is empty, or the backlog does not have any entries needing a sync to the just upgraded server, before proceeding to upgrade the next server in the pool Restart any gfapi based application stopped previously in step (2) Offline upgrade procedure This procedure involves cluster downtime and during the upgrade window, clients are not allowed access to the volumes. Steps to perform an offline upgrade: On every server in the trusted storage pool, stop all gluster services, either using the command below, or through other means, #killall glusterfs glusterfsd glusterd Stop all applications that access the volumes via gfapi (qemu, NFS-Ganesha, Samba, etc.), across all servers Install Gluster 3.11, on all servers Ensure that version reflects 3.11.x in the output of the following command on all servers, #gluster --version NOTE: x is the minor release number for the release Start glusterd on all the upgraded servers #glusterd Ensure that all gluster processes are online by checking the output of, #gluster volume status Restart any gfapi based application stopped previously in step (2) Post upgrade steps Perform the following steps post upgrading the entire trusted storage pool, - It is recommended to update the op-version of the cluster. Refer, to the op-version section for further details - Proceed to upgrade the clients to 3.11 version as well Upgrade procedure for clients Following are the steps to upgrade clients to the 3.11.x version, NOTE: x is the minor release number for the release Unmount all glusterfs mount points on the client Stop all applications that access the volumes via gfapi (qemu, etc.) Install Gluster 3.11 Mount all gluster shares Start any applications that were stopped previously in step (2)","title":"Upgrade to 3.11"},{"location":"Upgrade-Guide/upgrade-to-3.11/#upgrade-procedure-to-gluster-311-from-gluster-310x-and-38x","text":"NOTE: Upgrade procedure remains the same as with the 3.10 release","title":"Upgrade procedure to Gluster 3.11, from Gluster 3.10.x, and 3.8.x"},{"location":"Upgrade-Guide/upgrade-to-3.11/#pre-upgrade-notes","text":"Online upgrade is only possible with replicated and distributed replicate volumes Online upgrade is not supported for dispersed or distributed dispersed volumes Ensure no configuration changes are done during the upgrade If you are using geo-replication, please upgrade the slave cluster(s) before upgrading the master Upgrading the servers ahead of the clients is recommended It is recommended to have the same client and server, major versions running eventually","title":"Pre-upgrade notes"},{"location":"Upgrade-Guide/upgrade-to-3.11/#online-upgrade-procedure-for-servers","text":"This procedure involves upgrading one server at a time , while keeping the volume(s) online and client IO ongoing. This procedure assumes that multiple replicas of a replica set, are not part of the same server in the trusted storage pool. ALERT : If any of your volumes, in the trusted storage pool that is being upgraded, uses disperse or is a pure distributed volume, this procedure is NOT recommended, use the Offline upgrade procedure instead.","title":"Online upgrade procedure for servers"},{"location":"Upgrade-Guide/upgrade-to-3.11/#repeat-the-following-steps-on-each-server-in-the-trusted-storage-pool-to-upgrade-the-entire-pool-to-311-version","text":"Stop all gluster services, either using the command below, or through other means, #killall glusterfs glusterfsd glusterd Stop all applications that run on this server and access the volumes via gfapi (qemu, NFS-Ganesha, Samba, etc.) Install Gluster 3.11 Ensure that version reflects 3.11.x in the output of, #gluster --version NOTE: x is the minor release number for the release Start glusterd on the upgraded server #glusterd Ensure that all gluster processes are online by checking the output of, #gluster volume status Self-heal all gluster volumes by running #for i in `gluster volume list`; do gluster volume heal $i; done Ensure that there is no heal backlog by running the below command for all volumes #gluster volume heal <volname> info NOTE: If there is a heal backlog, wait till the backlog is empty, or the backlog does not have any entries needing a sync to the just upgraded server, before proceeding to upgrade the next server in the pool Restart any gfapi based application stopped previously in step (2)","title":"Repeat the following steps, on each server in the trusted storage pool, to upgrade the entire pool to 3.11 version:"},{"location":"Upgrade-Guide/upgrade-to-3.11/#offline-upgrade-procedure","text":"This procedure involves cluster downtime and during the upgrade window, clients are not allowed access to the volumes.","title":"Offline upgrade procedure"},{"location":"Upgrade-Guide/upgrade-to-3.11/#steps-to-perform-an-offline-upgrade","text":"On every server in the trusted storage pool, stop all gluster services, either using the command below, or through other means, #killall glusterfs glusterfsd glusterd Stop all applications that access the volumes via gfapi (qemu, NFS-Ganesha, Samba, etc.), across all servers Install Gluster 3.11, on all servers Ensure that version reflects 3.11.x in the output of the following command on all servers, #gluster --version NOTE: x is the minor release number for the release Start glusterd on all the upgraded servers #glusterd Ensure that all gluster processes are online by checking the output of, #gluster volume status Restart any gfapi based application stopped previously in step (2)","title":"Steps to perform an offline upgrade:"},{"location":"Upgrade-Guide/upgrade-to-3.11/#post-upgrade-steps","text":"Perform the following steps post upgrading the entire trusted storage pool, - It is recommended to update the op-version of the cluster. Refer, to the op-version section for further details - Proceed to upgrade the clients to 3.11 version as well","title":"Post upgrade steps"},{"location":"Upgrade-Guide/upgrade-to-3.11/#upgrade-procedure-for-clients","text":"Following are the steps to upgrade clients to the 3.11.x version, NOTE: x is the minor release number for the release Unmount all glusterfs mount points on the client Stop all applications that access the volumes via gfapi (qemu, etc.) Install Gluster 3.11 Mount all gluster shares Start any applications that were stopped previously in step (2)","title":"Upgrade procedure for clients"},{"location":"Upgrade-Guide/upgrade-to-3.12/","text":"Upgrade procedure to Gluster 3.12, from Gluster 3.11.x, 3.10.x, and 3.8.x NOTE: Upgrade procedure remains the same as with 3.11 and 3.10 releases Pre-upgrade notes Online upgrade is only possible with replicated and distributed replicate volumes Online upgrade is not supported for dispersed or distributed dispersed volumes Ensure no configuration changes are done during the upgrade If you are using geo-replication, please upgrade the slave cluster(s) before upgrading the master Upgrading the servers ahead of the clients is recommended It is recommended to have the same client and server, major versions running eventually Online upgrade procedure for servers This procedure involves upgrading one server at a time , while keeping the volume(s) online and client IO ongoing. This procedure assumes that multiple replicas of a replica set, are not part of the same server in the trusted storage pool. ALERT: If there are disperse or, pure distributed volumes in the storage pool being upgraded, this procedure is NOT recommended, use the Offline upgrade procedure instead. Repeat the following steps, on each server in the trusted storage pool, to upgrade the entire pool to 3.12 version: Stop all gluster services, either using the command below, or through other means, # killall glusterfs glusterfsd glusterd # systemctl stop glustereventsd Stop all applications that run on this server and access the volumes via gfapi (qemu, NFS-Ganesha, Samba, etc.) Install Gluster 3.12 Ensure that version reflects 3.12.x in the output of, # gluster --version NOTE: x is the minor release number for the release Start glusterd on the upgraded server # glusterd Ensure that all gluster processes are online by checking the output of, # gluster volume status If the glustereventsd service was previously enabled, it is required to start it using the commands below, or, through other means, # systemctl start glustereventsd Invoke self-heal on all the gluster volumes by running, # for i in `gluster volume list`; do gluster volume heal $i; done Verify that there are no heal backlog by running the command for all the volumes, # gluster volume heal <volname> info NOTE: Before proceeding to upgrade the next server in the pool it is recommended to check the heal backlog. If there is a heal backlog, it is recommended to wait until the backlog is empty, or, the backlog does not contain any entries requiring a sync to the just upgraded server. Restart any gfapi based application stopped previously in step (2) Offline upgrade procedure This procedure involves cluster downtime and during the upgrade window, clients are not allowed access to the volumes. Steps to perform an offline upgrade: On every server in the trusted storage pool, stop all gluster services, either using the command below, or through other means, # killall glusterfs glusterfsd glusterd glustereventsd # systemctl stop glustereventsd Stop all applications that access the volumes via gfapi (qemu, NFS-Ganesha, Samba, etc.), across all servers Install Gluster 3.12, on all servers Ensure that version reflects 3.12.x in the output of the following command on all servers, # gluster --version NOTE: x is the minor release number for the release Start glusterd on all the upgraded servers # glusterd Ensure that all gluster processes are online by checking the output of, # gluster volume status If the glustereventsd service was previously enabled, it is required to start it using the commands below, or, through other means, # systemctl start glustereventsd Restart any gfapi based application stopped previously in step (2) Post upgrade steps Perform the following steps post upgrading the entire trusted storage pool, It is recommended to update the op-version of the cluster. Refer, to the op-version section for further details Proceed to upgrade the clients to 3.12 version as well Upgrade procedure for clients Following are the steps to upgrade clients to the 3.12.x version, NOTE: x is the minor release number for the release Unmount all glusterfs mount points on the client Stop all applications that access the volumes via gfapi (qemu, etc.) Install Gluster 3.12 Mount all gluster shares Start any applications that were stopped previously in step (2)","title":"Upgrade to 3.12"},{"location":"Upgrade-Guide/upgrade-to-3.12/#upgrade-procedure-to-gluster-312-from-gluster-311x-310x-and-38x","text":"NOTE: Upgrade procedure remains the same as with 3.11 and 3.10 releases","title":"Upgrade procedure to Gluster 3.12, from Gluster 3.11.x, 3.10.x, and 3.8.x"},{"location":"Upgrade-Guide/upgrade-to-3.12/#pre-upgrade-notes","text":"Online upgrade is only possible with replicated and distributed replicate volumes Online upgrade is not supported for dispersed or distributed dispersed volumes Ensure no configuration changes are done during the upgrade If you are using geo-replication, please upgrade the slave cluster(s) before upgrading the master Upgrading the servers ahead of the clients is recommended It is recommended to have the same client and server, major versions running eventually","title":"Pre-upgrade notes"},{"location":"Upgrade-Guide/upgrade-to-3.12/#online-upgrade-procedure-for-servers","text":"This procedure involves upgrading one server at a time , while keeping the volume(s) online and client IO ongoing. This procedure assumes that multiple replicas of a replica set, are not part of the same server in the trusted storage pool. ALERT: If there are disperse or, pure distributed volumes in the storage pool being upgraded, this procedure is NOT recommended, use the Offline upgrade procedure instead.","title":"Online upgrade procedure for servers"},{"location":"Upgrade-Guide/upgrade-to-3.12/#repeat-the-following-steps-on-each-server-in-the-trusted-storage-pool-to-upgrade-the-entire-pool-to-312-version","text":"Stop all gluster services, either using the command below, or through other means, # killall glusterfs glusterfsd glusterd # systemctl stop glustereventsd Stop all applications that run on this server and access the volumes via gfapi (qemu, NFS-Ganesha, Samba, etc.) Install Gluster 3.12 Ensure that version reflects 3.12.x in the output of, # gluster --version NOTE: x is the minor release number for the release Start glusterd on the upgraded server # glusterd Ensure that all gluster processes are online by checking the output of, # gluster volume status If the glustereventsd service was previously enabled, it is required to start it using the commands below, or, through other means, # systemctl start glustereventsd Invoke self-heal on all the gluster volumes by running, # for i in `gluster volume list`; do gluster volume heal $i; done Verify that there are no heal backlog by running the command for all the volumes, # gluster volume heal <volname> info NOTE: Before proceeding to upgrade the next server in the pool it is recommended to check the heal backlog. If there is a heal backlog, it is recommended to wait until the backlog is empty, or, the backlog does not contain any entries requiring a sync to the just upgraded server. Restart any gfapi based application stopped previously in step (2)","title":"Repeat the following steps, on each server in the trusted storage pool, to upgrade the entire pool to 3.12 version:"},{"location":"Upgrade-Guide/upgrade-to-3.12/#offline-upgrade-procedure","text":"This procedure involves cluster downtime and during the upgrade window, clients are not allowed access to the volumes.","title":"Offline upgrade procedure"},{"location":"Upgrade-Guide/upgrade-to-3.12/#steps-to-perform-an-offline-upgrade","text":"On every server in the trusted storage pool, stop all gluster services, either using the command below, or through other means, # killall glusterfs glusterfsd glusterd glustereventsd # systemctl stop glustereventsd Stop all applications that access the volumes via gfapi (qemu, NFS-Ganesha, Samba, etc.), across all servers Install Gluster 3.12, on all servers Ensure that version reflects 3.12.x in the output of the following command on all servers, # gluster --version NOTE: x is the minor release number for the release Start glusterd on all the upgraded servers # glusterd Ensure that all gluster processes are online by checking the output of, # gluster volume status If the glustereventsd service was previously enabled, it is required to start it using the commands below, or, through other means, # systemctl start glustereventsd Restart any gfapi based application stopped previously in step (2)","title":"Steps to perform an offline upgrade:"},{"location":"Upgrade-Guide/upgrade-to-3.12/#post-upgrade-steps","text":"Perform the following steps post upgrading the entire trusted storage pool, It is recommended to update the op-version of the cluster. Refer, to the op-version section for further details Proceed to upgrade the clients to 3.12 version as well","title":"Post upgrade steps"},{"location":"Upgrade-Guide/upgrade-to-3.12/#upgrade-procedure-for-clients","text":"Following are the steps to upgrade clients to the 3.12.x version, NOTE: x is the minor release number for the release Unmount all glusterfs mount points on the client Stop all applications that access the volumes via gfapi (qemu, etc.) Install Gluster 3.12 Mount all gluster shares Start any applications that were stopped previously in step (2)","title":"Upgrade procedure for clients"},{"location":"Upgrade-Guide/upgrade-to-3.13/","text":"Upgrade procedure to Gluster 3.13, from Gluster 3.12.x, and 3.10.x NOTE: Upgrade procedure remains the same as with 3.12 and 3.10 releases Pre-upgrade notes Online upgrade is only possible with replicated and distributed replicate volumes Online upgrade is not supported for dispersed or distributed dispersed volumes Ensure no configuration changes are done during the upgrade If you are using geo-replication, please upgrade the slave cluster(s) before upgrading the master Upgrading the servers ahead of the clients is recommended It is recommended to have the same client and server, major versions running eventually Online upgrade procedure for servers This procedure involves upgrading one server at a time , while keeping the volume(s) online and client IO ongoing. This procedure assumes that multiple replicas of a replica set, are not part of the same server in the trusted storage pool. ALERT : If any of your volumes, in the trusted storage pool that is being upgraded, uses disperse or is a pure distributed volume, this procedure is NOT recommended, use the Offline upgrade procedure instead. Repeat the following steps, on each server in the trusted storage pool, to upgrade the entire pool to 3.13 version: Stop all gluster services, either using the command below, or through other means, # killall glusterfs glusterfsd glusterd Stop all applications that run on this server and access the volumes via gfapi (qemu, NFS-Ganesha, Samba, etc.) Install Gluster 3.13 Ensure that version reflects 3.13.x in the output of, # gluster --version NOTE: x is the minor release number for the release Start glusterd on the upgraded server # glusterd Ensure that all gluster processes are online by checking the output of, # gluster volume status Self-heal all gluster volumes by running # for i in `gluster volume list`; do gluster volume heal $i; done Ensure that there is no heal backlog by running the below command for all volumes # gluster volume heal <volname> info NOTE: If there is a heal backlog, wait till the backlog is empty, or the backlog does not have any entries needing a sync to the just upgraded server, before proceeding to upgrade the next server in the pool Restart any gfapi based application stopped previously in step (2) Offline upgrade procedure This procedure involves cluster downtime and during the upgrade window, clients are not allowed access to the volumes. Steps to perform an offline upgrade: On every server in the trusted storage pool, stop all gluster services, either using the command below, or through other means, # killall glusterfs glusterfsd glusterd Stop all applications that access the volumes via gfapi (qemu, NFS-Ganesha, Samba, etc.), across all servers Install Gluster 3.13, on all servers Ensure that version reflects 3.13.x in the output of the following command on all servers, # gluster --version NOTE: x is the minor release number for the release Start glusterd on all the upgraded servers # glusterd Ensure that all gluster processes are online by checking the output of, # gluster volume status Restart any gfapi based application stopped previously in step (2) Post upgrade steps Perform the following steps post upgrading the entire trusted storage pool, It is recommended to update the op-version of the cluster. Refer, to the op-version section for further details Proceed to upgrade the clients to 3.13 version as well Upgrade procedure for clients Following are the steps to upgrade clients to the 3.13.x version, NOTE: x is the minor release number for the release Unmount all glusterfs mount points on the client Stop all applications that access the volumes via gfapi (qemu, etc.) Install Gluster 3.13 Mount all gluster shares Start any applications that were stopped previously in step (2)","title":"Upgrade to 3.13"},{"location":"Upgrade-Guide/upgrade-to-3.13/#upgrade-procedure-to-gluster-313-from-gluster-312x-and-310x","text":"NOTE: Upgrade procedure remains the same as with 3.12 and 3.10 releases","title":"Upgrade procedure to Gluster 3.13, from Gluster 3.12.x, and 3.10.x"},{"location":"Upgrade-Guide/upgrade-to-3.13/#pre-upgrade-notes","text":"Online upgrade is only possible with replicated and distributed replicate volumes Online upgrade is not supported for dispersed or distributed dispersed volumes Ensure no configuration changes are done during the upgrade If you are using geo-replication, please upgrade the slave cluster(s) before upgrading the master Upgrading the servers ahead of the clients is recommended It is recommended to have the same client and server, major versions running eventually","title":"Pre-upgrade notes"},{"location":"Upgrade-Guide/upgrade-to-3.13/#online-upgrade-procedure-for-servers","text":"This procedure involves upgrading one server at a time , while keeping the volume(s) online and client IO ongoing. This procedure assumes that multiple replicas of a replica set, are not part of the same server in the trusted storage pool. ALERT : If any of your volumes, in the trusted storage pool that is being upgraded, uses disperse or is a pure distributed volume, this procedure is NOT recommended, use the Offline upgrade procedure instead.","title":"Online upgrade procedure for servers"},{"location":"Upgrade-Guide/upgrade-to-3.13/#repeat-the-following-steps-on-each-server-in-the-trusted-storage-pool-to-upgrade-the-entire-pool-to-313-version","text":"Stop all gluster services, either using the command below, or through other means, # killall glusterfs glusterfsd glusterd Stop all applications that run on this server and access the volumes via gfapi (qemu, NFS-Ganesha, Samba, etc.) Install Gluster 3.13 Ensure that version reflects 3.13.x in the output of, # gluster --version NOTE: x is the minor release number for the release Start glusterd on the upgraded server # glusterd Ensure that all gluster processes are online by checking the output of, # gluster volume status Self-heal all gluster volumes by running # for i in `gluster volume list`; do gluster volume heal $i; done Ensure that there is no heal backlog by running the below command for all volumes # gluster volume heal <volname> info NOTE: If there is a heal backlog, wait till the backlog is empty, or the backlog does not have any entries needing a sync to the just upgraded server, before proceeding to upgrade the next server in the pool Restart any gfapi based application stopped previously in step (2)","title":"Repeat the following steps, on each server in the trusted storage pool, to upgrade the entire pool to 3.13 version:"},{"location":"Upgrade-Guide/upgrade-to-3.13/#offline-upgrade-procedure","text":"This procedure involves cluster downtime and during the upgrade window, clients are not allowed access to the volumes.","title":"Offline upgrade procedure"},{"location":"Upgrade-Guide/upgrade-to-3.13/#steps-to-perform-an-offline-upgrade","text":"On every server in the trusted storage pool, stop all gluster services, either using the command below, or through other means, # killall glusterfs glusterfsd glusterd Stop all applications that access the volumes via gfapi (qemu, NFS-Ganesha, Samba, etc.), across all servers Install Gluster 3.13, on all servers Ensure that version reflects 3.13.x in the output of the following command on all servers, # gluster --version NOTE: x is the minor release number for the release Start glusterd on all the upgraded servers # glusterd Ensure that all gluster processes are online by checking the output of, # gluster volume status Restart any gfapi based application stopped previously in step (2)","title":"Steps to perform an offline upgrade:"},{"location":"Upgrade-Guide/upgrade-to-3.13/#post-upgrade-steps","text":"Perform the following steps post upgrading the entire trusted storage pool, It is recommended to update the op-version of the cluster. Refer, to the op-version section for further details Proceed to upgrade the clients to 3.13 version as well","title":"Post upgrade steps"},{"location":"Upgrade-Guide/upgrade-to-3.13/#upgrade-procedure-for-clients","text":"Following are the steps to upgrade clients to the 3.13.x version, NOTE: x is the minor release number for the release Unmount all glusterfs mount points on the client Stop all applications that access the volumes via gfapi (qemu, etc.) Install Gluster 3.13 Mount all gluster shares Start any applications that were stopped previously in step (2)","title":"Upgrade procedure for clients"},{"location":"Upgrade-Guide/upgrade-to-3.5/","text":"Glusterfs upgrade from 3.4.x to 3.5 Now that GlusterFS 3.5.0 is out, here are some mechanisms to upgrade from earlier installed versions of GlusterFS. Upgrade from GlusterFS 3.4.x: GlusterFS 3.5.0 is compatible with 3.4.x (yes, you read it right!). You can upgrade your deployment by following one of the two procedures below. a) Scheduling a downtime (Recommended) For this approach, schedule a downtime and prevent all your clients from accessing the servers. If you have quota configured, you need to perform step 1 and 6, otherwise you can skip it. If you have geo-replication session running, stop the session using the geo-rep stop command (please refer to step 1 of geo-rep upgrade steps provided below) Execute \"pre-upgrade-script-for-quota.sh\" mentioned under \"Upgrade Steps For Quota\" section. Stop all glusterd, glusterfsd and glusterfs processes on your server. Install GlusterFS 3.5.0 Start glusterd. Ensure that all started volumes have processes online in \u201cgluster volume status\u201d. Execute \"Post-Upgrade Script\" mentioned under \"Upgrade Steps For Quota\" section. You would need to repeat these steps on all servers that form your trusted storage pool. To upgrade geo-replication session, please refer to geo-rep upgrade steps provided below (from step 2) After upgrading the servers, it is recommended to upgrade all client installations to 3.5.0. b) Rolling upgrades with no downtime If you have replicated or distributed replicated volumes with bricks placed in the right fashion for redundancy, have no data to be self-healed and feel adventurous, you can perform a rolling upgrade through the following procedure: NOTE: Rolling upgrade of geo-replication session from glusterfs version \\< 3.5 to 3.5.x is not supported. If you have quota configured, you need to perform step 1 and 7, otherwise you can skip it. Execute \"pre-upgrade-script-for-quota.sh\" mentioned under \"Upgrade Steps For Quota\" section. Stop all glusterd, glusterfs and glusterfsd processes on your server. Install GlusterFS 3.5.0. Start glusterd. Run \u201cgluster volume heal <volname> info\u201d on all volumes and ensure that there is nothing left to be self-healed on every volume. If you have pending data for self-heal, run \u201cgluster volume heal <volname> \u201d and wait for self-heal to complete. Ensure that all started volumes have processes online in \u201cgluster volume status\u201d. Execute \"Post-Upgrade Script\" mentioned under \"Upgrade Steps For Quota\" section. Repeat the above steps on all servers that are part of your trusted storage pool. Again after upgrading the servers, it is recommended to upgrade all client installations to 3.5.0. Do report your findings on 3.5.0 in gluster-users, #gluster on Freenode and bugzilla. Please note that this may not work for all installations & upgrades. If you notice anything amiss and would like to see it covered here, please point the same. Upgrade Steps For Quota The upgrade process for quota involves executing two upgrade scripts: pre-upgrade-script-for-quota.sh, and\\ post-upgrade-script-for-quota.sh Pre-Upgrade Script: What it does: The pre-upgrade script (pre-upgrade-script-for-quota.sh) iterates over the list of volumes that have quota enabled and captures the configured quota limits for each such volume in a file under /var/tmp/glusterfs/quota-config-backup/vol_\\<VOLNAME> by executing 'quota list' command on each one of them. Pre-requisites for running Pre-Upgrade Script: Make sure glusterd and the brick processes are running on all nodes in the cluster. The pre-upgrade script must be run prior to upgradation. The pre-upgrade script must be run on only one of the nodes in the cluster. Location: pre-upgrade-script-for-quota.sh must be retrieved from the source tree under the 'extras' directory. Invocation: Invoke the script by executing `./pre-upgrade-script-for-quota.sh` from the shell on any one of the nodes in the cluster. Example: [root@server1 extras]#./pre-upgrade-script-for-quota.sh Post-Upgrade Script: What it does: The post-upgrade script (post-upgrade-script-for-quota.sh) picks the volumes that have quota enabled. Because the cluster must be operating at op-version 3 for quota to work, the 'default-soft-limit' for each of these volumes is set to 80% (which is its default value) via `volume set` operation as an explicit trigger to bump up the op-version of the cluster and also to trigger a re-write of volfiles which knocks quota off client volume file. Once this is done, these volumes are started forcefully using `volume start force` to launch the Quota Daemon on all the nodes. Thereafter, for each of these volumes, the paths and the limits configured on them are retrieved from the backed up file /var/tmp/glusterfs/quota-config-backup/vol_\\<VOLNAME> and limits are set on them via the `quota limit-usage` interface. Note: In the new version of quota, the command `quota limit-usage` will fail if the directory on which quota limit is to be set for a given volume does not exist. Therefore, it is advised that you create these directories first before running post-upgrade-script-for-quota.sh if you want limits to be set on these directories. Pre-requisites for running Post-Upgrade Script: The post-upgrade script must be executed after all the nodes in the cluster have upgraded. Also, all the clients accessing the given volume must also be upgraded before the script is run. Make sure glusterd and the brick processes are running on all nodes in the cluster post upgrade. The script must be run from the same node where the pre-upgrade script was run. Location: post-upgrade-script-for-quota.sh can be found under the 'extras' directory of the source tree for glusterfs. Invocation: post-upgrade-script-for-quota.sh takes one command line argument. This argument could be one of the following: ''the name of the volume which has quota enabled; or' '' 'all'.'' In the first case, invoke post-upgrade-script-for-quota.sh from the shell for each volume with quota enabled, with the name of the volume passed as an argument in the command-line: Example: For a volume \"vol1\" on which quota is enabled, invoke the script in the following way: [root@server1 extras]#./post-upgrade-script-for-quota.sh vol1 In the second case, the post-upgrade script picks on its own, the volumes on which quota is enabled, and executes the post-upgrade procedure on each one of them. In this case, invoke post-upgrade-script-for-quota.sh from the shell with 'all' passed as an argument in the command-line: Example: [root@server1 extras]#./post-upgrade-script-for-quota.sh all Note: In the second case, post-upgrade-script-for-quota.sh exits prematurely upon failure to ugprade any given volume. In that case, you may run post-upgrade-script-for-quota.sh individually (using the volume name as command line argument) on this volume and also on all volumes appearing after this volume in the output of `gluster volume list`, that have quota enabled. The backed up files under /var/tmp/glusterfs/quota-config-backup/ are retained after the post-upgrade procedure for reference. Upgrade steps for geo replication: Here are the steps to upgrade your existing geo-replication setup to new distributed geo-replication in glusterfs-3.5. The new version leverges all the nodes in your master volume and provides better performace. Note: Since new version of geo-rep very much different from the older one, this has to be done offline. New version supports only syncing between two gluster volumes via ssh+gluster. This doc deals with upgrading geo-rep. So upgrading the volumes are not covered in detail here. Below are the steps to upgrade: \u200b1. Stop the geo-replication session in older version ( \\< 3.5) using the below command #gluster volume geo-replication `<master_vol>` `<slave_host>`::`<slave_vol>` stop \u200b2. Now since the new geo-replication requires gfids of master and slave volume to be same, generate a file containing the gfids of all the files in master cd /usr/share/glusterfs/scripts/ ; bash generate-gfid-file.sh localhost:`<master_vol>` $PWD/get-gfid.sh /tmp/master_gfid_file.txt ; scp /tmp/master_gfid_file.txt root@`<slave_host>`:/tmp \u200b3. Now go to the slave host and aplly the gfid to the slave volume. cd /usr/share/glusterfs/scripts/ bash slave-upgrade.sh localhost:`<slave_vol>` /tmp/master_gfid_file.txt $PWD/gsync-sync-gfid This will ask you for password of all the nodes in slave cluster. Please provide them, if asked. \u200b4. Also note that this will restart your slave gluster volume (stop and start) \u200b5. Now create and start the geo-rep session between master and slave. For instruction on creating new geo-rep seesion please refer distributed-geo-rep admin guide. gluster volume geo-replication `<master_volume>` `<slave_host>`::`<slave_volume>` create push-pem force gluster volume geo-replication `<master_volume>` `<slave_host>`::`<slave_volume>` start \u200b6. Now your session is upgraded to use distributed-geo-rep","title":"Upgrade to 3.5"},{"location":"Upgrade-Guide/upgrade-to-3.5/#glusterfs-upgrade-from-34x-to-35","text":"Now that GlusterFS 3.5.0 is out, here are some mechanisms to upgrade from earlier installed versions of GlusterFS. Upgrade from GlusterFS 3.4.x: GlusterFS 3.5.0 is compatible with 3.4.x (yes, you read it right!). You can upgrade your deployment by following one of the two procedures below. a) Scheduling a downtime (Recommended) For this approach, schedule a downtime and prevent all your clients from accessing the servers. If you have quota configured, you need to perform step 1 and 6, otherwise you can skip it. If you have geo-replication session running, stop the session using the geo-rep stop command (please refer to step 1 of geo-rep upgrade steps provided below) Execute \"pre-upgrade-script-for-quota.sh\" mentioned under \"Upgrade Steps For Quota\" section. Stop all glusterd, glusterfsd and glusterfs processes on your server. Install GlusterFS 3.5.0 Start glusterd. Ensure that all started volumes have processes online in \u201cgluster volume status\u201d. Execute \"Post-Upgrade Script\" mentioned under \"Upgrade Steps For Quota\" section. You would need to repeat these steps on all servers that form your trusted storage pool. To upgrade geo-replication session, please refer to geo-rep upgrade steps provided below (from step 2) After upgrading the servers, it is recommended to upgrade all client installations to 3.5.0. b) Rolling upgrades with no downtime If you have replicated or distributed replicated volumes with bricks placed in the right fashion for redundancy, have no data to be self-healed and feel adventurous, you can perform a rolling upgrade through the following procedure: NOTE: Rolling upgrade of geo-replication session from glusterfs version \\< 3.5 to 3.5.x is not supported. If you have quota configured, you need to perform step 1 and 7, otherwise you can skip it. Execute \"pre-upgrade-script-for-quota.sh\" mentioned under \"Upgrade Steps For Quota\" section. Stop all glusterd, glusterfs and glusterfsd processes on your server. Install GlusterFS 3.5.0. Start glusterd. Run \u201cgluster volume heal <volname> info\u201d on all volumes and ensure that there is nothing left to be self-healed on every volume. If you have pending data for self-heal, run \u201cgluster volume heal <volname> \u201d and wait for self-heal to complete. Ensure that all started volumes have processes online in \u201cgluster volume status\u201d. Execute \"Post-Upgrade Script\" mentioned under \"Upgrade Steps For Quota\" section. Repeat the above steps on all servers that are part of your trusted storage pool. Again after upgrading the servers, it is recommended to upgrade all client installations to 3.5.0. Do report your findings on 3.5.0 in gluster-users, #gluster on Freenode and bugzilla. Please note that this may not work for all installations & upgrades. If you notice anything amiss and would like to see it covered here, please point the same.","title":"Glusterfs upgrade from 3.4.x to 3.5"},{"location":"Upgrade-Guide/upgrade-to-3.5/#upgrade-steps-for-quota","text":"The upgrade process for quota involves executing two upgrade scripts: pre-upgrade-script-for-quota.sh, and\\ post-upgrade-script-for-quota.sh Pre-Upgrade Script: What it does: The pre-upgrade script (pre-upgrade-script-for-quota.sh) iterates over the list of volumes that have quota enabled and captures the configured quota limits for each such volume in a file under /var/tmp/glusterfs/quota-config-backup/vol_\\<VOLNAME> by executing 'quota list' command on each one of them. Pre-requisites for running Pre-Upgrade Script: Make sure glusterd and the brick processes are running on all nodes in the cluster. The pre-upgrade script must be run prior to upgradation. The pre-upgrade script must be run on only one of the nodes in the cluster. Location: pre-upgrade-script-for-quota.sh must be retrieved from the source tree under the 'extras' directory. Invocation: Invoke the script by executing `./pre-upgrade-script-for-quota.sh` from the shell on any one of the nodes in the cluster. Example: [root@server1 extras]#./pre-upgrade-script-for-quota.sh Post-Upgrade Script: What it does: The post-upgrade script (post-upgrade-script-for-quota.sh) picks the volumes that have quota enabled. Because the cluster must be operating at op-version 3 for quota to work, the 'default-soft-limit' for each of these volumes is set to 80% (which is its default value) via `volume set` operation as an explicit trigger to bump up the op-version of the cluster and also to trigger a re-write of volfiles which knocks quota off client volume file. Once this is done, these volumes are started forcefully using `volume start force` to launch the Quota Daemon on all the nodes. Thereafter, for each of these volumes, the paths and the limits configured on them are retrieved from the backed up file /var/tmp/glusterfs/quota-config-backup/vol_\\<VOLNAME> and limits are set on them via the `quota limit-usage` interface. Note: In the new version of quota, the command `quota limit-usage` will fail if the directory on which quota limit is to be set for a given volume does not exist. Therefore, it is advised that you create these directories first before running post-upgrade-script-for-quota.sh if you want limits to be set on these directories. Pre-requisites for running Post-Upgrade Script: The post-upgrade script must be executed after all the nodes in the cluster have upgraded. Also, all the clients accessing the given volume must also be upgraded before the script is run. Make sure glusterd and the brick processes are running on all nodes in the cluster post upgrade. The script must be run from the same node where the pre-upgrade script was run. Location: post-upgrade-script-for-quota.sh can be found under the 'extras' directory of the source tree for glusterfs. Invocation: post-upgrade-script-for-quota.sh takes one command line argument. This argument could be one of the following: ''the name of the volume which has quota enabled; or' '' 'all'.'' In the first case, invoke post-upgrade-script-for-quota.sh from the shell for each volume with quota enabled, with the name of the volume passed as an argument in the command-line: Example: For a volume \"vol1\" on which quota is enabled, invoke the script in the following way: [root@server1 extras]#./post-upgrade-script-for-quota.sh vol1 In the second case, the post-upgrade script picks on its own, the volumes on which quota is enabled, and executes the post-upgrade procedure on each one of them. In this case, invoke post-upgrade-script-for-quota.sh from the shell with 'all' passed as an argument in the command-line: Example: [root@server1 extras]#./post-upgrade-script-for-quota.sh all Note: In the second case, post-upgrade-script-for-quota.sh exits prematurely upon failure to ugprade any given volume. In that case, you may run post-upgrade-script-for-quota.sh individually (using the volume name as command line argument) on this volume and also on all volumes appearing after this volume in the output of `gluster volume list`, that have quota enabled. The backed up files under /var/tmp/glusterfs/quota-config-backup/ are retained after the post-upgrade procedure for reference.","title":"Upgrade Steps For Quota"},{"location":"Upgrade-Guide/upgrade-to-3.5/#upgrade-steps-for-geo-replication","text":"Here are the steps to upgrade your existing geo-replication setup to new distributed geo-replication in glusterfs-3.5. The new version leverges all the nodes in your master volume and provides better performace. Note: Since new version of geo-rep very much different from the older one, this has to be done offline. New version supports only syncing between two gluster volumes via ssh+gluster. This doc deals with upgrading geo-rep. So upgrading the volumes are not covered in detail here. Below are the steps to upgrade: \u200b1. Stop the geo-replication session in older version ( \\< 3.5) using the below command #gluster volume geo-replication `<master_vol>` `<slave_host>`::`<slave_vol>` stop \u200b2. Now since the new geo-replication requires gfids of master and slave volume to be same, generate a file containing the gfids of all the files in master cd /usr/share/glusterfs/scripts/ ; bash generate-gfid-file.sh localhost:`<master_vol>` $PWD/get-gfid.sh /tmp/master_gfid_file.txt ; scp /tmp/master_gfid_file.txt root@`<slave_host>`:/tmp \u200b3. Now go to the slave host and aplly the gfid to the slave volume. cd /usr/share/glusterfs/scripts/ bash slave-upgrade.sh localhost:`<slave_vol>` /tmp/master_gfid_file.txt $PWD/gsync-sync-gfid This will ask you for password of all the nodes in slave cluster. Please provide them, if asked. \u200b4. Also note that this will restart your slave gluster volume (stop and start) \u200b5. Now create and start the geo-rep session between master and slave. For instruction on creating new geo-rep seesion please refer distributed-geo-rep admin guide. gluster volume geo-replication `<master_volume>` `<slave_host>`::`<slave_volume>` create push-pem force gluster volume geo-replication `<master_volume>` `<slave_host>`::`<slave_volume>` start \u200b6. Now your session is upgraded to use distributed-geo-rep","title":"Upgrade steps for geo replication:"},{"location":"Upgrade-Guide/upgrade-to-3.6/","text":"GlusterFS upgrade from 3.5.x to 3.6.x Now that GlusterFS 3.6.0 is out, here is the process to upgrade from earlier installed versions of GlusterFS. If you are using GlusterFS replication ( \\< 3.6) in your setup , please note that the new afrv2 implementation is only compatible with 3.6 GlusterFS clients. If you are not updating your clients to GlusterFS version 3.6 you need to disable client self healing process. You can perform this by below steps. # gluster v set testvol cluster.entry-self-heal off volume set: success # # gluster v set testvol cluster.data-self-heal off volume set: success # gluster v set testvol cluster.metadata-self-heal off volume set: success # GlusterFS upgrade from 3.5.x to 3.6.x a) Scheduling a downtime (Recommended) For this approach, schedule a downtime and prevent all your clients from accessing ( umount your volumes, stop gluster Volumes..etc)the servers. Stop all glusterd, glusterfsd and glusterfs processes on your server. Install GlusterFS 3.6.0 Start glusterd. Ensure that all started volumes have processes online in \u201cgluster volume status\u201d. You would need to repeat these steps on all servers that form your trusted storage pool. After upgrading the servers, it is recommended to upgrade all client installations to 3.6.0 GlusterFS upgrade from 3.4.x to 3.6.X Upgrade from GlusterFS 3.4.x: GlusterFS 3.6.0 is compatible with 3.4.x (yes, you read it right!). You can upgrade your deployment by following one of the two procedures below. a) Scheduling a downtime (Recommended) For this approach, schedule a downtime and prevent all your clients from accessing ( umount your volumes, stop gluster Volumes..etc)the servers. If you have quota configured, you need to perform step 1 and 6, otherwise you can skip it. If you have geo-replication session running, stop the session using the geo-rep stop command (please refer to step 1 of geo-rep upgrade steps provided below) Execute \"pre-upgrade-script-for-quota.sh\" mentioned under \"Upgrade Steps For Quota\" section. Stop all glusterd, glusterfsd and glusterfs processes on your server. Install GlusterFS 3.6.0 Start glusterd. Ensure that all started volumes have processes online in \u201cgluster volume status\u201d. Execute \"Post-Upgrade Script\" mentioned under \"Upgrade Steps For Quota\" section. You would need to repeat these steps on all servers that form your trusted storage pool. To upgrade geo-replication session, please refer to geo-rep upgrade steps provided below (from step 2) After upgrading the servers, it is recommended to upgrade all client installations to 3.6.0. Do report your findings on 3.6.0 in gluster-users, #gluster on Freenode and bugzilla. Please note that this may not work for all installations & upgrades. If you notice anything amiss and would like to see it covered here, please point the same. Upgrade Steps For Quota The upgrade process for quota involves executing two upgrade scripts: pre-upgrade-script-for-quota.sh, and\\ post-upgrade-script-for-quota.sh Pre-Upgrade Script: What it does: The pre-upgrade script (pre-upgrade-script-for-quota.sh) iterates over the list of volumes that have quota enabled and captures the configured quota limits for each such volume in a file under /var/tmp/glusterfs/quota-config-backup/vol_\\<VOLNAME> by executing 'quota list' command on each one of them. Pre-requisites for running Pre-Upgrade Script: Make sure glusterd and the brick processes are running on all nodes in the cluster. The pre-upgrade script must be run prior to upgradation. The pre-upgrade script must be run on only one of the nodes in the cluster. Location: pre-upgrade-script-for-quota.sh must be retrieved from the source tree under the 'extras' directory. Invocation: Invoke the script by executing `./pre-upgrade-script-for-quota.sh` from the shell on any one of the nodes in the cluster. Example: [root@server1 extras]#./pre-upgrade-script-for-quota.sh Post-Upgrade Script: What it does: The post-upgrade script (post-upgrade-script-for-quota.sh) picks the volumes that have quota enabled. Because the cluster must be operating at op-version 3 for quota to work, the 'default-soft-limit' for each of these volumes is set to 80% (which is its default value) via `volume set` operation as an explicit trigger to bump up the op-version of the cluster and also to trigger a re-write of volfiles which knocks quota off client volume file. Once this is done, these volumes are started forcefully using `volume start force` to launch the Quota Daemon on all the nodes. Thereafter, for each of these volumes, the paths and the limits configured on them are retrieved from the backed up file /var/tmp/glusterfs/quota-config-backup/vol_\\<VOLNAME> and limits are set on them via the `quota limit-usage` interface. Note: In the new version of quota, the command `quota limit-usage` will fail if the directory on which quota limit is to be set for a given volume does not exist. Therefore, it is advised that you create these directories first before running post-upgrade-script-for-quota.sh if you want limits to be set on these directories. Pre-requisites for running Post-Upgrade Script: The post-upgrade script must be executed after all the nodes in the cluster have upgraded. Also, all the clients accessing the given volume must also be upgraded before the script is run. Make sure glusterd and the brick processes are running on all nodes in the cluster post upgrade. The script must be run from the same node where the pre-upgrade script was run. Location: post-upgrade-script-for-quota.sh can be found under the 'extras' directory of the source tree for glusterfs. Invocation: post-upgrade-script-for-quota.sh takes one command line argument. This argument could be one of the following: ''the name of the volume which has quota enabled; or' '' 'all'.'' In the first case, invoke post-upgrade-script-for-quota.sh from the shell for each volume with quota enabled, with the name of the volume passed as an argument in the command-line: Example: For a volume \"vol1\" on which quota is enabled, invoke the script in the following way: [root@server1 extras]#./post-upgrade-script-for-quota.sh vol1 In the second case, the post-upgrade script picks on its own, the volumes on which quota is enabled, and executes the post-upgrade procedure on each one of them. In this case, invoke post-upgrade-script-for-quota.sh from the shell with 'all' passed as an argument in the command-line: Example: [root@server1 extras]#./post-upgrade-script-for-quota.sh all Note: In the second case, post-upgrade-script-for-quota.sh exits prematurely upon failure to ugprade any given volume. In that case, you may run post-upgrade-script-for-quota.sh individually (using the volume name as command line argument) on this volume and also on all volumes appearing after this volume in the output of `gluster volume list`, that have quota enabled. The backed up files under /var/tmp/glusterfs/quota-config-backup/ are retained after the post-upgrade procedure for reference. Upgrade steps for geo replication: Here are the steps to upgrade your existing geo-replication setup to new distributed geo-replication in glusterfs-3.5. The new version leverges all the nodes in your master volume and provides better performace. Note: Since new version of geo-rep very much different from the older one, this has to be done offline. New version supports only syncing between two gluster volumes via ssh+gluster. This doc deals with upgrading geo-rep. So upgrading the volumes are not covered in detail here. Below are the steps to upgrade: \u200b1. Stop the geo-replication session in older version ( \\< 3.5) using the below command # gluster volume geo-replication `<master_vol>` `<slave_host>`::`<slave_vol>` stop \u200b2. Now since the new geo-replication requires gfids of master and slave volume to be same, generate a file containing the gfids of all the files in master # cd /usr/share/glusterfs/scripts/ ; # bash generate-gfid-file.sh localhost:`<master_vol>` $PWD/get-gfid.sh /tmp/master_gfid_file.txt ; # scp /tmp/master_gfid_file.txt root@`<slave_host>`:/tmp \u200b3. Now go to the slave host and aplly the gfid to the slave volume. # cd /usr/share/glusterfs/scripts/ # bash slave-upgrade.sh localhost:`<slave_vol>` /tmp/master_gfid_file.txt $PWD/gsync-sync-gfid This will ask you for password of all the nodes in slave cluster. Please provide them, if asked. \u200b4. Also note that this will restart your slave gluster volume (stop and start) \u200b5. Now create and start the geo-rep session between master and slave. For instruction on creating new geo-rep seesion please refer distributed-geo-rep admin guide. # gluster volume geo-replication `<master_volume>` `<slave_host>`::`<slave_volume>` create push-pem force # gluster volume geo-replication `<master_volume>` `<slave_host>`::`<slave_volume>` start \u200b6. Now your session is upgraded to use distributed-geo-rep","title":"Upgrade to 3.6"},{"location":"Upgrade-Guide/upgrade-to-3.6/#glusterfs-upgrade-from-35x-to-36x","text":"Now that GlusterFS 3.6.0 is out, here is the process to upgrade from earlier installed versions of GlusterFS. If you are using GlusterFS replication ( \\< 3.6) in your setup , please note that the new afrv2 implementation is only compatible with 3.6 GlusterFS clients. If you are not updating your clients to GlusterFS version 3.6 you need to disable client self healing process. You can perform this by below steps. # gluster v set testvol cluster.entry-self-heal off volume set: success # # gluster v set testvol cluster.data-self-heal off volume set: success # gluster v set testvol cluster.metadata-self-heal off volume set: success #","title":"GlusterFS upgrade from 3.5.x to 3.6.x"},{"location":"Upgrade-Guide/upgrade-to-3.6/#glusterfs-upgrade-from-35x-to-36x_1","text":"a) Scheduling a downtime (Recommended) For this approach, schedule a downtime and prevent all your clients from accessing ( umount your volumes, stop gluster Volumes..etc)the servers. Stop all glusterd, glusterfsd and glusterfs processes on your server. Install GlusterFS 3.6.0 Start glusterd. Ensure that all started volumes have processes online in \u201cgluster volume status\u201d. You would need to repeat these steps on all servers that form your trusted storage pool. After upgrading the servers, it is recommended to upgrade all client installations to 3.6.0","title":"GlusterFS upgrade from 3.5.x to 3.6.x"},{"location":"Upgrade-Guide/upgrade-to-3.6/#glusterfs-upgrade-from-34x-to-36x","text":"Upgrade from GlusterFS 3.4.x: GlusterFS 3.6.0 is compatible with 3.4.x (yes, you read it right!). You can upgrade your deployment by following one of the two procedures below. a) Scheduling a downtime (Recommended) For this approach, schedule a downtime and prevent all your clients from accessing ( umount your volumes, stop gluster Volumes..etc)the servers. If you have quota configured, you need to perform step 1 and 6, otherwise you can skip it. If you have geo-replication session running, stop the session using the geo-rep stop command (please refer to step 1 of geo-rep upgrade steps provided below) Execute \"pre-upgrade-script-for-quota.sh\" mentioned under \"Upgrade Steps For Quota\" section. Stop all glusterd, glusterfsd and glusterfs processes on your server. Install GlusterFS 3.6.0 Start glusterd. Ensure that all started volumes have processes online in \u201cgluster volume status\u201d. Execute \"Post-Upgrade Script\" mentioned under \"Upgrade Steps For Quota\" section. You would need to repeat these steps on all servers that form your trusted storage pool. To upgrade geo-replication session, please refer to geo-rep upgrade steps provided below (from step 2) After upgrading the servers, it is recommended to upgrade all client installations to 3.6.0. Do report your findings on 3.6.0 in gluster-users, #gluster on Freenode and bugzilla. Please note that this may not work for all installations & upgrades. If you notice anything amiss and would like to see it covered here, please point the same.","title":"GlusterFS upgrade from 3.4.x to 3.6.X"},{"location":"Upgrade-Guide/upgrade-to-3.6/#upgrade-steps-for-quota","text":"The upgrade process for quota involves executing two upgrade scripts: pre-upgrade-script-for-quota.sh, and\\ post-upgrade-script-for-quota.sh Pre-Upgrade Script: What it does: The pre-upgrade script (pre-upgrade-script-for-quota.sh) iterates over the list of volumes that have quota enabled and captures the configured quota limits for each such volume in a file under /var/tmp/glusterfs/quota-config-backup/vol_\\<VOLNAME> by executing 'quota list' command on each one of them. Pre-requisites for running Pre-Upgrade Script: Make sure glusterd and the brick processes are running on all nodes in the cluster. The pre-upgrade script must be run prior to upgradation. The pre-upgrade script must be run on only one of the nodes in the cluster. Location: pre-upgrade-script-for-quota.sh must be retrieved from the source tree under the 'extras' directory. Invocation: Invoke the script by executing `./pre-upgrade-script-for-quota.sh` from the shell on any one of the nodes in the cluster. Example: [root@server1 extras]#./pre-upgrade-script-for-quota.sh Post-Upgrade Script: What it does: The post-upgrade script (post-upgrade-script-for-quota.sh) picks the volumes that have quota enabled. Because the cluster must be operating at op-version 3 for quota to work, the 'default-soft-limit' for each of these volumes is set to 80% (which is its default value) via `volume set` operation as an explicit trigger to bump up the op-version of the cluster and also to trigger a re-write of volfiles which knocks quota off client volume file. Once this is done, these volumes are started forcefully using `volume start force` to launch the Quota Daemon on all the nodes. Thereafter, for each of these volumes, the paths and the limits configured on them are retrieved from the backed up file /var/tmp/glusterfs/quota-config-backup/vol_\\<VOLNAME> and limits are set on them via the `quota limit-usage` interface. Note: In the new version of quota, the command `quota limit-usage` will fail if the directory on which quota limit is to be set for a given volume does not exist. Therefore, it is advised that you create these directories first before running post-upgrade-script-for-quota.sh if you want limits to be set on these directories. Pre-requisites for running Post-Upgrade Script: The post-upgrade script must be executed after all the nodes in the cluster have upgraded. Also, all the clients accessing the given volume must also be upgraded before the script is run. Make sure glusterd and the brick processes are running on all nodes in the cluster post upgrade. The script must be run from the same node where the pre-upgrade script was run. Location: post-upgrade-script-for-quota.sh can be found under the 'extras' directory of the source tree for glusterfs. Invocation: post-upgrade-script-for-quota.sh takes one command line argument. This argument could be one of the following: ''the name of the volume which has quota enabled; or' '' 'all'.'' In the first case, invoke post-upgrade-script-for-quota.sh from the shell for each volume with quota enabled, with the name of the volume passed as an argument in the command-line: Example: For a volume \"vol1\" on which quota is enabled, invoke the script in the following way: [root@server1 extras]#./post-upgrade-script-for-quota.sh vol1 In the second case, the post-upgrade script picks on its own, the volumes on which quota is enabled, and executes the post-upgrade procedure on each one of them. In this case, invoke post-upgrade-script-for-quota.sh from the shell with 'all' passed as an argument in the command-line: Example: [root@server1 extras]#./post-upgrade-script-for-quota.sh all Note: In the second case, post-upgrade-script-for-quota.sh exits prematurely upon failure to ugprade any given volume. In that case, you may run post-upgrade-script-for-quota.sh individually (using the volume name as command line argument) on this volume and also on all volumes appearing after this volume in the output of `gluster volume list`, that have quota enabled. The backed up files under /var/tmp/glusterfs/quota-config-backup/ are retained after the post-upgrade procedure for reference.","title":"Upgrade Steps For Quota"},{"location":"Upgrade-Guide/upgrade-to-3.6/#upgrade-steps-for-geo-replication","text":"Here are the steps to upgrade your existing geo-replication setup to new distributed geo-replication in glusterfs-3.5. The new version leverges all the nodes in your master volume and provides better performace. Note: Since new version of geo-rep very much different from the older one, this has to be done offline. New version supports only syncing between two gluster volumes via ssh+gluster. This doc deals with upgrading geo-rep. So upgrading the volumes are not covered in detail here. Below are the steps to upgrade: \u200b1. Stop the geo-replication session in older version ( \\< 3.5) using the below command # gluster volume geo-replication `<master_vol>` `<slave_host>`::`<slave_vol>` stop \u200b2. Now since the new geo-replication requires gfids of master and slave volume to be same, generate a file containing the gfids of all the files in master # cd /usr/share/glusterfs/scripts/ ; # bash generate-gfid-file.sh localhost:`<master_vol>` $PWD/get-gfid.sh /tmp/master_gfid_file.txt ; # scp /tmp/master_gfid_file.txt root@`<slave_host>`:/tmp \u200b3. Now go to the slave host and aplly the gfid to the slave volume. # cd /usr/share/glusterfs/scripts/ # bash slave-upgrade.sh localhost:`<slave_vol>` /tmp/master_gfid_file.txt $PWD/gsync-sync-gfid This will ask you for password of all the nodes in slave cluster. Please provide them, if asked. \u200b4. Also note that this will restart your slave gluster volume (stop and start) \u200b5. Now create and start the geo-rep session between master and slave. For instruction on creating new geo-rep seesion please refer distributed-geo-rep admin guide. # gluster volume geo-replication `<master_volume>` `<slave_host>`::`<slave_volume>` create push-pem force # gluster volume geo-replication `<master_volume>` `<slave_host>`::`<slave_volume>` start \u200b6. Now your session is upgraded to use distributed-geo-rep","title":"Upgrade steps for geo replication:"},{"location":"Upgrade-Guide/upgrade-to-3.7/","text":"GlusterFS upgrade to 3.7.x Now that GlusterFS 3.7.0 is out, here is the process to upgrade from earlier installed versions of GlusterFS. Please read the entire howto before proceeding with an upgrade of your deployment Pre-upgrade GlusterFS contains afrv2 implementation from 3.6.0 by default. If you are using GlusterFS replication ( \\< 3.6) in your setup, please note that the new afrv2 implementation is only compatible with 3.6 or greater GlusterFS clients. If you are not updating your clients to GlusterFS version 3.6 along with your servers you would need to disable client self healing process before the upgrade. You can perform this by below steps. # gluster v set testvol cluster.entry-self-heal off volume set: success # # gluster v set testvol cluster.data-self-heal off volume set: success # gluster v set testvol cluster.metadata-self-heal off volume set: success # GlusterFS upgrade to 3.7.x a) Scheduling a downtime For this approach, schedule a downtime and prevent all your clients from accessing (umount your volumes, stop gluster Volumes..etc) the servers. 1. Stop all glusterd, glusterfsd and glusterfs processes on your server. 2. Install GlusterFS 3.7.0 3. Start glusterd. 4. Ensure that all started volumes have processes online in \u201cgluster volume status\u201d. You would need to repeat these steps on all servers that form your trusted storage pool. After upgrading the servers, it is recommended to upgrade all client installations to 3.7.0. b) Rolling Upgrade If you have replicated or distributed replicated volumes with bricks placed in the right fashion for redundancy, have no data to be self-healed and feel adventurous, you can perform a rolling upgrade through the following procedure: 1.Stop all glusterd, glusterfs and glusterfsd processes on your server. 2.Install GlusterFS 3.7.0. 3.Start glusterd. 4.Run \u201cgluster volume heal <volname> info\u201d on all volumes and ensure that there is nothing left to be 5.self-healed on every volume. If you have pending data for self-heal, run \u201cgluster volume heal <volname>\u201d and wait for self-heal to complete. 6.Ensure that all started volumes have processes online in \u201cgluster volume status\u201d. Repeat the above steps on all servers that are part of your trusted storage pool. Again after upgrading the servers, it is recommended to upgrade all client installations to 3.7.0. Special notes for upgrading from 3.4.x to 3.7.X If you have quota or geo-replication configured in 3.4.x, please read below. Else you can skip this section. Architectural changes in Quota & geo-replication were introduced in Gluster 3.5.0. Hence scheduling a downtime is recommended for upgrading from 3.4.x to 3.7.x if you have these features enabled. Upgrade Steps For Quota The upgrade process for quota involves the following: Run pre-upgrade-script-for-quota.sh Upgrade to 3.7.0 Run post-upgrade-script-for-quota.sh More details on the scripts are as under. Pre-Upgrade Script: What it does: The pre-upgrade script (pre-upgrade-script-for-quota.sh) iterates over the list of volumes that have quota enabled and captures the configured quota limits for each such volume in a file under /var/tmp/glusterfs/quota-config-backup/vol_\\<VOLNAME> by executing 'quota list' command on each one of them. Pre-requisites for running Pre-Upgrade Script: Make sure glusterd and the brick processes are running on all nodes in the cluster. The pre-upgrade script must be run prior to upgradation. The pre-upgrade script must be run on only one of the nodes in the cluster. Location: pre-upgrade-script-for-quota.sh must be retrieved from the source tree under the 'extras' directory. Invocation: Invoke the script by executing `./pre-upgrade-script-for-quota.sh` from the shell on any one of the nodes in the cluster. Example: [root@server1 extras]#./pre-upgrade-script-for-quota.sh Post-Upgrade Script: What it does: The post-upgrade script (post-upgrade-script-for-quota.sh) picks the volumes that have quota enabled. Because the cluster must be operating at op-version 3 for quota to work, the 'default-soft-limit' for each of these volumes is set to 80% (which is its default value) via `volume set` operation as an explicit trigger to bump up the op-version of the cluster and also to trigger a re-write of volfiles which knocks quota off client volume file. Once this is done, these volumes are started forcefully using `volume start force` to launch the Quota Daemon on all the nodes. Thereafter, for each of these volumes, the paths and the limits configured on them are retrieved from the backed up file /var/tmp/glusterfs/quota-config-backup/vol_\\<VOLNAME> and limits are set on them via the `quota limit-usage` interface. Note: In the new version of quota, the command `quota limit-usage` will fail if the directory on which quota limit is to be set for a given volume does not exist. Therefore, it is advised that you create these directories first before running post-upgrade-script-for-quota.sh if you want limits to be set on these directories. Pre-requisites for running Post-Upgrade Script: The post-upgrade script must be executed after all the nodes in the cluster have upgraded. Also, all the clients accessing the given volume must also be upgraded before the script is run. Make sure glusterd and the brick processes are running on all nodes in the cluster post upgrade. The script must be run from the same node where the pre-upgrade script was run. Location: post-upgrade-script-for-quota.sh can be found under the 'extras' directory of the source tree for glusterfs. Invocation: post-upgrade-script-for-quota.sh takes one command line argument. This argument could be one of the following: ''the name of the volume which has quota enabled; or' '' 'all'.'' In the first case, invoke post-upgrade-script-for-quota.sh from the shell for each volume with quota enabled, with the name of the volume passed as an argument in the command-line: Example: For a volume \"vol1\" on which quota is enabled, invoke the script in the following way: [root@server1 extras]#./post-upgrade-script-for-quota.sh vol1 In the second case, the post-upgrade script picks on its own, the volumes on which quota is enabled, and executes the post-upgrade procedure on each one of them. In this case, invoke post-upgrade-script-for-quota.sh from the shell with 'all' passed as an argument in the command-line: Example: [root@server1 extras]#./post-upgrade-script-for-quota.sh all Note: In the second case, post-upgrade-script-for-quota.sh exits prematurely upon failure to ugprade any given volume. In that case, you may run post-upgrade-script-for-quota.sh individually (using the volume name as command line argument) on this volume and also on all volumes appearing after this volume in the output of `gluster volume list`, that have quota enabled. The backed up files under /var/tmp/glusterfs/quota-config-backup/ are retained after the post-upgrade procedure for reference. Upgrade steps for geo replication: New version supports only syncing between two gluster volumes via ssh+gluster. ''Below are the steps to upgrade. '' \u200b1. Stop the geo-replication session in older version ( \\< 3.5) using the below command # gluster volume geo-replication <master_vol> <slave_host>::<slave_vol> stop \u200b2. Now since the new geo-replication requires gfids of master and slave volume to be same, generate a file containing the gfids of all the files in master # cd /usr/share/glusterfs/scripts/ ; # bash generate-gfid-file.sh localhost:<master_vol> $PWD/get-gfid.sh /tmp/master_gfid_file.txt ; # scp /tmp/master_gfid_file.txt root@<slave_host>:/tmp \u200b3. Upgrade the slave cluster installation to 3.7.0 \u200b4. Now go to the slave host and apply the gfid to the slave volume. # cd /usr/share/glusterfs/scripts/ # bash slave-upgrade.sh localhost:<slave_vol> /tmp/master_gfid_file.txt $PWD/gsync-sync-gfid This will ask you for password of all the nodes in slave cluster. Please provide them, if asked. Also note that this will restart your slave gluster volume (stop and start) \u200b5. Upgrade the master cluster to 3.7.0 \u200b6. Now create and start the geo-rep session between master and slave. For instruction on creating new geo-rep session please refer distributed-geo-rep chapter in admin guide. # gluster volume geo-replication <master_volume> <slave_host>::<slave_volume> create push-pem force # gluster volume geo-replication <master_volume> <slave_host>::<slave_volume> start At this point, your distributed geo-replication should be configured appropriately.","title":"Upgrade to 3.7"},{"location":"Upgrade-Guide/upgrade-to-3.7/#glusterfs-upgrade-to-37x","text":"Now that GlusterFS 3.7.0 is out, here is the process to upgrade from earlier installed versions of GlusterFS. Please read the entire howto before proceeding with an upgrade of your deployment","title":"GlusterFS upgrade to 3.7.x"},{"location":"Upgrade-Guide/upgrade-to-3.7/#pre-upgrade","text":"GlusterFS contains afrv2 implementation from 3.6.0 by default. If you are using GlusterFS replication ( \\< 3.6) in your setup, please note that the new afrv2 implementation is only compatible with 3.6 or greater GlusterFS clients. If you are not updating your clients to GlusterFS version 3.6 along with your servers you would need to disable client self healing process before the upgrade. You can perform this by below steps. # gluster v set testvol cluster.entry-self-heal off volume set: success # # gluster v set testvol cluster.data-self-heal off volume set: success # gluster v set testvol cluster.metadata-self-heal off volume set: success #","title":"Pre-upgrade"},{"location":"Upgrade-Guide/upgrade-to-3.7/#glusterfs-upgrade-to-37x_1","text":"a) Scheduling a downtime For this approach, schedule a downtime and prevent all your clients from accessing (umount your volumes, stop gluster Volumes..etc) the servers. 1. Stop all glusterd, glusterfsd and glusterfs processes on your server. 2. Install GlusterFS 3.7.0 3. Start glusterd. 4. Ensure that all started volumes have processes online in \u201cgluster volume status\u201d. You would need to repeat these steps on all servers that form your trusted storage pool. After upgrading the servers, it is recommended to upgrade all client installations to 3.7.0. b) Rolling Upgrade If you have replicated or distributed replicated volumes with bricks placed in the right fashion for redundancy, have no data to be self-healed and feel adventurous, you can perform a rolling upgrade through the following procedure: 1.Stop all glusterd, glusterfs and glusterfsd processes on your server. 2.Install GlusterFS 3.7.0. 3.Start glusterd. 4.Run \u201cgluster volume heal <volname> info\u201d on all volumes and ensure that there is nothing left to be 5.self-healed on every volume. If you have pending data for self-heal, run \u201cgluster volume heal <volname>\u201d and wait for self-heal to complete. 6.Ensure that all started volumes have processes online in \u201cgluster volume status\u201d. Repeat the above steps on all servers that are part of your trusted storage pool. Again after upgrading the servers, it is recommended to upgrade all client installations to 3.7.0.","title":"GlusterFS upgrade to 3.7.x"},{"location":"Upgrade-Guide/upgrade-to-3.7/#special-notes-for-upgrading-from-34x-to-37x","text":"If you have quota or geo-replication configured in 3.4.x, please read below. Else you can skip this section. Architectural changes in Quota & geo-replication were introduced in Gluster 3.5.0. Hence scheduling a downtime is recommended for upgrading from 3.4.x to 3.7.x if you have these features enabled.","title":"Special notes for upgrading from 3.4.x to 3.7.X"},{"location":"Upgrade-Guide/upgrade-to-3.7/#upgrade-steps-for-quota","text":"The upgrade process for quota involves the following: Run pre-upgrade-script-for-quota.sh Upgrade to 3.7.0 Run post-upgrade-script-for-quota.sh More details on the scripts are as under. Pre-Upgrade Script: What it does: The pre-upgrade script (pre-upgrade-script-for-quota.sh) iterates over the list of volumes that have quota enabled and captures the configured quota limits for each such volume in a file under /var/tmp/glusterfs/quota-config-backup/vol_\\<VOLNAME> by executing 'quota list' command on each one of them. Pre-requisites for running Pre-Upgrade Script: Make sure glusterd and the brick processes are running on all nodes in the cluster. The pre-upgrade script must be run prior to upgradation. The pre-upgrade script must be run on only one of the nodes in the cluster. Location: pre-upgrade-script-for-quota.sh must be retrieved from the source tree under the 'extras' directory. Invocation: Invoke the script by executing `./pre-upgrade-script-for-quota.sh` from the shell on any one of the nodes in the cluster. Example: [root@server1 extras]#./pre-upgrade-script-for-quota.sh Post-Upgrade Script: What it does: The post-upgrade script (post-upgrade-script-for-quota.sh) picks the volumes that have quota enabled. Because the cluster must be operating at op-version 3 for quota to work, the 'default-soft-limit' for each of these volumes is set to 80% (which is its default value) via `volume set` operation as an explicit trigger to bump up the op-version of the cluster and also to trigger a re-write of volfiles which knocks quota off client volume file. Once this is done, these volumes are started forcefully using `volume start force` to launch the Quota Daemon on all the nodes. Thereafter, for each of these volumes, the paths and the limits configured on them are retrieved from the backed up file /var/tmp/glusterfs/quota-config-backup/vol_\\<VOLNAME> and limits are set on them via the `quota limit-usage` interface. Note: In the new version of quota, the command `quota limit-usage` will fail if the directory on which quota limit is to be set for a given volume does not exist. Therefore, it is advised that you create these directories first before running post-upgrade-script-for-quota.sh if you want limits to be set on these directories. Pre-requisites for running Post-Upgrade Script: The post-upgrade script must be executed after all the nodes in the cluster have upgraded. Also, all the clients accessing the given volume must also be upgraded before the script is run. Make sure glusterd and the brick processes are running on all nodes in the cluster post upgrade. The script must be run from the same node where the pre-upgrade script was run. Location: post-upgrade-script-for-quota.sh can be found under the 'extras' directory of the source tree for glusterfs. Invocation: post-upgrade-script-for-quota.sh takes one command line argument. This argument could be one of the following: ''the name of the volume which has quota enabled; or' '' 'all'.'' In the first case, invoke post-upgrade-script-for-quota.sh from the shell for each volume with quota enabled, with the name of the volume passed as an argument in the command-line: Example: For a volume \"vol1\" on which quota is enabled, invoke the script in the following way: [root@server1 extras]#./post-upgrade-script-for-quota.sh vol1 In the second case, the post-upgrade script picks on its own, the volumes on which quota is enabled, and executes the post-upgrade procedure on each one of them. In this case, invoke post-upgrade-script-for-quota.sh from the shell with 'all' passed as an argument in the command-line: Example: [root@server1 extras]#./post-upgrade-script-for-quota.sh all Note: In the second case, post-upgrade-script-for-quota.sh exits prematurely upon failure to ugprade any given volume. In that case, you may run post-upgrade-script-for-quota.sh individually (using the volume name as command line argument) on this volume and also on all volumes appearing after this volume in the output of `gluster volume list`, that have quota enabled. The backed up files under /var/tmp/glusterfs/quota-config-backup/ are retained after the post-upgrade procedure for reference.","title":"Upgrade Steps For Quota"},{"location":"Upgrade-Guide/upgrade-to-3.7/#upgrade-steps-for-geo-replication","text":"New version supports only syncing between two gluster volumes via ssh+gluster. ''Below are the steps to upgrade. '' \u200b1. Stop the geo-replication session in older version ( \\< 3.5) using the below command # gluster volume geo-replication <master_vol> <slave_host>::<slave_vol> stop \u200b2. Now since the new geo-replication requires gfids of master and slave volume to be same, generate a file containing the gfids of all the files in master # cd /usr/share/glusterfs/scripts/ ; # bash generate-gfid-file.sh localhost:<master_vol> $PWD/get-gfid.sh /tmp/master_gfid_file.txt ; # scp /tmp/master_gfid_file.txt root@<slave_host>:/tmp \u200b3. Upgrade the slave cluster installation to 3.7.0 \u200b4. Now go to the slave host and apply the gfid to the slave volume. # cd /usr/share/glusterfs/scripts/ # bash slave-upgrade.sh localhost:<slave_vol> /tmp/master_gfid_file.txt $PWD/gsync-sync-gfid This will ask you for password of all the nodes in slave cluster. Please provide them, if asked. Also note that this will restart your slave gluster volume (stop and start) \u200b5. Upgrade the master cluster to 3.7.0 \u200b6. Now create and start the geo-rep session between master and slave. For instruction on creating new geo-rep session please refer distributed-geo-rep chapter in admin guide. # gluster volume geo-replication <master_volume> <slave_host>::<slave_volume> create push-pem force # gluster volume geo-replication <master_volume> <slave_host>::<slave_volume> start At this point, your distributed geo-replication should be configured appropriately.","title":"Upgrade steps for geo replication:"},{"location":"Upgrade-Guide/upgrade-to-3.8/","text":"Upgrade procedure from Gluster 3.7.x Pre-upgrade Notes Online upgrade is only possible with replicated and distributed replicate volumes. Online upgrade is not yet supported for dispersed or distributed dispersed volumes. Ensure no configuration changes are done during the upgrade. If you are using geo-replication, please upgrade the slave cluster(s) before upgrading the master. Upgrading the servers ahead of the clients is recommended. Upgrade the clients after the servers are upgraded. It is recommended to have the same client and server major versions. Online Upgrade Procedure for Servers The procedure involves upgrading one server at a time . On every storage server in your trusted storage pool: Stop all gluster services using the below command or through your favorite way to stop them. # killall glusterfs glusterfsd glusterd If you are using gfapi based applications (qemu, NFS-Ganesha, Samba etc.) on the servers, please stop those applications too. Install Gluster 3.8 Ensure that version reflects 3.8.x in the output of # gluster --version Start glusterd on the upgraded server # glusterd Ensure that all gluster processes are online by executing # gluster volume status Self-heal all gluster volumes by running # for i in `gluster volume list`; do gluster volume heal $i; done Ensure that there is no heal backlog by running the below command for all volumes # gluster volume heal <volname> info Restart any gfapi based application stopped previously. After the upgrade is complete on all servers, run the following command: # gluster volume set all cluster.op-version 30800 Offline Upgrade Procedure For this procedure, schedule a downtime and prevent all your clients from accessing the servers. On every storage server in your trusted storage pool: - Stop all gluster services using the below command or through your favorite way to stop them. # killall glusterfs glusterfsd glusterd If you are using gfapi based applications (qemu, NFS-Ganesha, Samba etc.) on the servers, please stop those applications too. Install Gluster 3.8 Ensure that version reflects 3.8.x in the output of # gluster --version Start glusterd on the upgraded server # glusterd Ensure that all gluster processes are online by executing # gluster volume status Restart any gfapi based application stopped previously. After the upgrade is complete on all servers, run the following command: # gluster volume set all cluster.op-version 30800 Upgrade Procedure for Clients Unmount all glusterfs mount points on the client Stop applications using gfapi (qemu etc.) Install Gluster 3.8 Mount all gluster shares Start applications using libgfapi that were stopped previously","title":"Upgrade to 3.8"},{"location":"Upgrade-Guide/upgrade-to-3.8/#upgrade-procedure-from-gluster-37x","text":"","title":"Upgrade procedure from Gluster 3.7.x"},{"location":"Upgrade-Guide/upgrade-to-3.8/#pre-upgrade-notes","text":"Online upgrade is only possible with replicated and distributed replicate volumes. Online upgrade is not yet supported for dispersed or distributed dispersed volumes. Ensure no configuration changes are done during the upgrade. If you are using geo-replication, please upgrade the slave cluster(s) before upgrading the master. Upgrading the servers ahead of the clients is recommended. Upgrade the clients after the servers are upgraded. It is recommended to have the same client and server major versions.","title":"Pre-upgrade Notes"},{"location":"Upgrade-Guide/upgrade-to-3.8/#online-upgrade-procedure-for-servers","text":"The procedure involves upgrading one server at a time . On every storage server in your trusted storage pool: Stop all gluster services using the below command or through your favorite way to stop them. # killall glusterfs glusterfsd glusterd If you are using gfapi based applications (qemu, NFS-Ganesha, Samba etc.) on the servers, please stop those applications too. Install Gluster 3.8 Ensure that version reflects 3.8.x in the output of # gluster --version Start glusterd on the upgraded server # glusterd Ensure that all gluster processes are online by executing # gluster volume status Self-heal all gluster volumes by running # for i in `gluster volume list`; do gluster volume heal $i; done Ensure that there is no heal backlog by running the below command for all volumes # gluster volume heal <volname> info Restart any gfapi based application stopped previously. After the upgrade is complete on all servers, run the following command: # gluster volume set all cluster.op-version 30800","title":"Online Upgrade Procedure for Servers"},{"location":"Upgrade-Guide/upgrade-to-3.8/#offline-upgrade-procedure","text":"For this procedure, schedule a downtime and prevent all your clients from accessing the servers. On every storage server in your trusted storage pool: - Stop all gluster services using the below command or through your favorite way to stop them. # killall glusterfs glusterfsd glusterd If you are using gfapi based applications (qemu, NFS-Ganesha, Samba etc.) on the servers, please stop those applications too. Install Gluster 3.8 Ensure that version reflects 3.8.x in the output of # gluster --version Start glusterd on the upgraded server # glusterd Ensure that all gluster processes are online by executing # gluster volume status Restart any gfapi based application stopped previously. After the upgrade is complete on all servers, run the following command: # gluster volume set all cluster.op-version 30800","title":"Offline Upgrade Procedure"},{"location":"Upgrade-Guide/upgrade-to-3.8/#upgrade-procedure-for-clients","text":"Unmount all glusterfs mount points on the client Stop applications using gfapi (qemu etc.) Install Gluster 3.8 Mount all gluster shares Start applications using libgfapi that were stopped previously","title":"Upgrade Procedure for Clients"},{"location":"Upgrade-Guide/upgrade-to-3.9/","text":"Upgrade procedure from Gluster 3.8.x and 3.7.x The steps to uprade to Gluster 3.9 are the same as for upgrading to Gluster 3.8. Please follow the detailed instructions from the 3.8 upgrade guide . Note that there is only a single difference, related to the op-version : After the upgrade is complete on all servers, run the following command: # gluster volume set all cluster.op-version 30900","title":"Upgrade to 3.9"},{"location":"Upgrade-Guide/upgrade-to-3.9/#upgrade-procedure-from-gluster-38x-and-37x","text":"The steps to uprade to Gluster 3.9 are the same as for upgrading to Gluster 3.8. Please follow the detailed instructions from the 3.8 upgrade guide . Note that there is only a single difference, related to the op-version : After the upgrade is complete on all servers, run the following command: # gluster volume set all cluster.op-version 30900","title":"Upgrade procedure from Gluster 3.8.x and 3.7.x"},{"location":"Upgrade-Guide/upgrade-to-4.0/","text":"Upgrade procedure to Gluster 4.0, from Gluster 3.13.x, 3.12.x, and 3.10.x NOTE: Upgrade procedure remains the same as with 3.12 and 3.10 releases Pre-upgrade notes Online upgrade is only possible with replicated and distributed replicate volumes Online upgrade is not supported for dispersed or distributed dispersed volumes Ensure no configuration changes are done during the upgrade If you are using geo-replication, please upgrade the slave cluster(s) before upgrading the master Upgrading the servers ahead of the clients is recommended It is recommended to have the same client and server, major versions running eventually Online upgrade procedure for servers This procedure involves upgrading one server at a time , while keeping the volume(s) online and client IO ongoing. This procedure assumes that multiple replicas of a replica set, are not part of the same server in the trusted storage pool. ALERT : If any of your volumes, in the trusted storage pool that is being upgraded, uses disperse or is a pure distributed volume, this procedure is NOT recommended, use the Offline upgrade procedure instead. Repeat the following steps, on each server in the trusted storage pool, to upgrade the entire pool to 4.0 version: Stop all gluster services, either using the command below, or through other means, # killall glusterfs glusterfsd glusterd Stop all applications that run on this server and access the volumes via gfapi (qemu, NFS-Ganesha, Samba, etc.) Install Gluster 4.0 Ensure that version reflects 4.0.x in the output of, # gluster --version NOTE: x is the minor release number for the release Start glusterd on the upgraded server # glusterd Ensure that all gluster processes are online by checking the output of, # gluster volume status Self-heal all gluster volumes by running # for i in `gluster volume list`; do gluster volume heal $i; done Ensure that there is no heal backlog by running the below command for all volumes # gluster volume heal <volname> info NOTE: If there is a heal backlog, wait till the backlog is empty, or the backlog does not have any entries needing a sync to the just upgraded server, before proceeding to upgrade the next server in the pool Restart any gfapi based application stopped previously in step (2) Offline upgrade procedure This procedure involves cluster downtime and during the upgrade window, clients are not allowed access to the volumes. Steps to perform an offline upgrade: On every server in the trusted storage pool, stop all gluster services, either using the command below, or through other means, # killall glusterfs glusterfsd glusterd Stop all applications that access the volumes via gfapi (qemu, NFS-Ganesha, Samba, etc.), across all servers Install Gluster 4.0, on all servers Ensure that version reflects 4.0.x in the output of the following command on all servers, # gluster --version NOTE: x is the minor release number for the release Start glusterd on all the upgraded servers # glusterd Ensure that all gluster processes are online by checking the output of, # gluster volume status Restart any gfapi based application stopped previously in step (2) Post upgrade steps Perform the following steps post upgrading the entire trusted storage pool, It is recommended to update the op-version of the cluster. Refer, to the op-version section for further details Proceed to upgrade the clients to 4.0 version as well Post upgrading the clients, for replicate volumes, it is recommended to enable the option gluster volume set <volname> fips-mode-rchecksum on to turn off usage of MD5 checksums during healing. This enables running Gluster on FIPS compliant systems. Upgrade procedure for clients Following are the steps to upgrade clients to the 4.0.x version, NOTE: x is the minor release number for the release Unmount all glusterfs mount points on the client Stop all applications that access the volumes via gfapi (qemu, etc.) Install Gluster 4.0 Mount all gluster shares Start any applications that were stopped previously in step (2)","title":"Upgrade to 4.0"},{"location":"Upgrade-Guide/upgrade-to-4.0/#upgrade-procedure-to-gluster-40-from-gluster-313x-312x-and-310x","text":"NOTE: Upgrade procedure remains the same as with 3.12 and 3.10 releases","title":"Upgrade procedure to Gluster 4.0, from Gluster 3.13.x, 3.12.x, and 3.10.x"},{"location":"Upgrade-Guide/upgrade-to-4.0/#pre-upgrade-notes","text":"Online upgrade is only possible with replicated and distributed replicate volumes Online upgrade is not supported for dispersed or distributed dispersed volumes Ensure no configuration changes are done during the upgrade If you are using geo-replication, please upgrade the slave cluster(s) before upgrading the master Upgrading the servers ahead of the clients is recommended It is recommended to have the same client and server, major versions running eventually","title":"Pre-upgrade notes"},{"location":"Upgrade-Guide/upgrade-to-4.0/#online-upgrade-procedure-for-servers","text":"This procedure involves upgrading one server at a time , while keeping the volume(s) online and client IO ongoing. This procedure assumes that multiple replicas of a replica set, are not part of the same server in the trusted storage pool. ALERT : If any of your volumes, in the trusted storage pool that is being upgraded, uses disperse or is a pure distributed volume, this procedure is NOT recommended, use the Offline upgrade procedure instead.","title":"Online upgrade procedure for servers"},{"location":"Upgrade-Guide/upgrade-to-4.0/#repeat-the-following-steps-on-each-server-in-the-trusted-storage-pool-to-upgrade-the-entire-pool-to-40-version","text":"Stop all gluster services, either using the command below, or through other means, # killall glusterfs glusterfsd glusterd Stop all applications that run on this server and access the volumes via gfapi (qemu, NFS-Ganesha, Samba, etc.) Install Gluster 4.0 Ensure that version reflects 4.0.x in the output of, # gluster --version NOTE: x is the minor release number for the release Start glusterd on the upgraded server # glusterd Ensure that all gluster processes are online by checking the output of, # gluster volume status Self-heal all gluster volumes by running # for i in `gluster volume list`; do gluster volume heal $i; done Ensure that there is no heal backlog by running the below command for all volumes # gluster volume heal <volname> info NOTE: If there is a heal backlog, wait till the backlog is empty, or the backlog does not have any entries needing a sync to the just upgraded server, before proceeding to upgrade the next server in the pool Restart any gfapi based application stopped previously in step (2)","title":"Repeat the following steps, on each server in the trusted storage pool, to upgrade the entire pool to 4.0 version:"},{"location":"Upgrade-Guide/upgrade-to-4.0/#offline-upgrade-procedure","text":"This procedure involves cluster downtime and during the upgrade window, clients are not allowed access to the volumes.","title":"Offline upgrade procedure"},{"location":"Upgrade-Guide/upgrade-to-4.0/#steps-to-perform-an-offline-upgrade","text":"On every server in the trusted storage pool, stop all gluster services, either using the command below, or through other means, # killall glusterfs glusterfsd glusterd Stop all applications that access the volumes via gfapi (qemu, NFS-Ganesha, Samba, etc.), across all servers Install Gluster 4.0, on all servers Ensure that version reflects 4.0.x in the output of the following command on all servers, # gluster --version NOTE: x is the minor release number for the release Start glusterd on all the upgraded servers # glusterd Ensure that all gluster processes are online by checking the output of, # gluster volume status Restart any gfapi based application stopped previously in step (2)","title":"Steps to perform an offline upgrade:"},{"location":"Upgrade-Guide/upgrade-to-4.0/#post-upgrade-steps","text":"Perform the following steps post upgrading the entire trusted storage pool, It is recommended to update the op-version of the cluster. Refer, to the op-version section for further details Proceed to upgrade the clients to 4.0 version as well Post upgrading the clients, for replicate volumes, it is recommended to enable the option gluster volume set <volname> fips-mode-rchecksum on to turn off usage of MD5 checksums during healing. This enables running Gluster on FIPS compliant systems.","title":"Post upgrade steps"},{"location":"Upgrade-Guide/upgrade-to-4.0/#upgrade-procedure-for-clients","text":"Following are the steps to upgrade clients to the 4.0.x version, NOTE: x is the minor release number for the release Unmount all glusterfs mount points on the client Stop all applications that access the volumes via gfapi (qemu, etc.) Install Gluster 4.0 Mount all gluster shares Start any applications that were stopped previously in step (2)","title":"Upgrade procedure for clients"},{"location":"Upgrade-Guide/upgrade-to-4.1/","text":"Upgrade procedure to Gluster 4.1, from Gluster 4.0.x, 3.12.x, and 3.10.x NOTE: Upgrade procedure remains the same as with 3.12 and 3.10 releases Pre-upgrade notes Online upgrade is only possible with replicated and distributed replicate volumes Online upgrade is not supported for dispersed or distributed dispersed volumes Ensure no configuration changes are done during the upgrade If you are using geo-replication, please upgrade the slave cluster(s) before upgrading the master Upgrading the servers ahead of the clients is recommended It is recommended to have the same client and server, major versions running eventually Online upgrade procedure for servers This procedure involves upgrading one server at a time , while keeping the volume(s) online and client IO ongoing. This procedure assumes that multiple replicas of a replica set, are not part of the same server in the trusted storage pool. ALERT: If there are disperse or, pure distributed volumes in the storage pool being upgraded, this procedure is NOT recommended, use the Offline upgrade procedure instead. Repeat the following steps, on each server in the trusted storage pool, to upgrade the entire pool to 4.1 version: Stop all gluster services, either using the command below, or through other means, # killall glusterfs glusterfsd glusterd # systemctl stop glustereventsd Stop all applications that run on this server and access the volumes via gfapi (qemu, NFS-Ganesha, Samba, etc.) Install Gluster 4.1 Ensure that version reflects 4.1.x in the output of, # gluster --version NOTE: x is the minor release number for the release Start glusterd on the upgraded server # glusterd Ensure that all gluster processes are online by checking the output of, # gluster volume status If the glustereventsd service was previously enabled, it is required to start it using the commands below, or, through other means, # systemctl start glustereventsd Invoke self-heal on all the gluster volumes by running, # for i in `gluster volume list`; do gluster volume heal $i; done Verify that there are no heal backlog by running the command for all the volumes, # gluster volume heal <volname> info NOTE: Before proceeding to upgrade the next server in the pool it is recommended to check the heal backlog. If there is a heal backlog, it is recommended to wait until the backlog is empty, or, the backlog does not contain any entries requiring a sync to the just upgraded server. Restart any gfapi based application stopped previously in step (2) Offline upgrade procedure This procedure involves cluster downtime and during the upgrade window, clients are not allowed access to the volumes. Steps to perform an offline upgrade: On every server in the trusted storage pool, stop all gluster services, either using the command below, or through other means, #killall glusterfs glusterfsd glusterd glustereventsd #systemctl stop glustereventsd Stop all applications that access the volumes via gfapi (qemu, NFS-Ganesha, Samba, etc.), across all servers Install Gluster 4.1, on all servers Ensure that version reflects 4.1.x in the output of the following command on all servers, #gluster --version NOTE: x is the minor release number for the release Start glusterd on all the upgraded servers #glusterd Ensure that all gluster processes are online by checking the output of, #gluster volume status If the glustereventsd service was previously enabled, it is required to start it using the commands below, or, through other means, #systemctl start glustereventsd Restart any gfapi based application stopped previously in step (2) Post upgrade steps Perform the following steps post upgrading the entire trusted storage pool, It is recommended to update the op-version of the cluster. Refer, to the op-version section for further details Proceed to upgrade the clients to 4.1 version as well Post upgrading the clients, for replicate volumes, it is recommended to enable the option gluster volume set <volname> fips-mode-rchecksum on to turn off usage of MD5 checksums during healing. This enables running Gluster on FIPS compliant systems. Upgrade procedure for clients Following are the steps to upgrade clients to the 4.1.x version, NOTE: x is the minor release number for the release Unmount all glusterfs mount points on the client Stop all applications that access the volumes via gfapi (qemu, etc.) Install Gluster 4.1 Mount all gluster shares Start any applications that were stopped previously in step (2)","title":"Upgrade to 4.1"},{"location":"Upgrade-Guide/upgrade-to-4.1/#upgrade-procedure-to-gluster-41-from-gluster-40x-312x-and-310x","text":"NOTE: Upgrade procedure remains the same as with 3.12 and 3.10 releases","title":"Upgrade procedure to Gluster 4.1, from Gluster 4.0.x, 3.12.x, and 3.10.x"},{"location":"Upgrade-Guide/upgrade-to-4.1/#pre-upgrade-notes","text":"Online upgrade is only possible with replicated and distributed replicate volumes Online upgrade is not supported for dispersed or distributed dispersed volumes Ensure no configuration changes are done during the upgrade If you are using geo-replication, please upgrade the slave cluster(s) before upgrading the master Upgrading the servers ahead of the clients is recommended It is recommended to have the same client and server, major versions running eventually","title":"Pre-upgrade notes"},{"location":"Upgrade-Guide/upgrade-to-4.1/#online-upgrade-procedure-for-servers","text":"This procedure involves upgrading one server at a time , while keeping the volume(s) online and client IO ongoing. This procedure assumes that multiple replicas of a replica set, are not part of the same server in the trusted storage pool. ALERT: If there are disperse or, pure distributed volumes in the storage pool being upgraded, this procedure is NOT recommended, use the Offline upgrade procedure instead.","title":"Online upgrade procedure for servers"},{"location":"Upgrade-Guide/upgrade-to-4.1/#repeat-the-following-steps-on-each-server-in-the-trusted-storage-pool-to-upgrade-the-entire-pool-to-41-version","text":"Stop all gluster services, either using the command below, or through other means, # killall glusterfs glusterfsd glusterd # systemctl stop glustereventsd Stop all applications that run on this server and access the volumes via gfapi (qemu, NFS-Ganesha, Samba, etc.) Install Gluster 4.1 Ensure that version reflects 4.1.x in the output of, # gluster --version NOTE: x is the minor release number for the release Start glusterd on the upgraded server # glusterd Ensure that all gluster processes are online by checking the output of, # gluster volume status If the glustereventsd service was previously enabled, it is required to start it using the commands below, or, through other means, # systemctl start glustereventsd Invoke self-heal on all the gluster volumes by running, # for i in `gluster volume list`; do gluster volume heal $i; done Verify that there are no heal backlog by running the command for all the volumes, # gluster volume heal <volname> info NOTE: Before proceeding to upgrade the next server in the pool it is recommended to check the heal backlog. If there is a heal backlog, it is recommended to wait until the backlog is empty, or, the backlog does not contain any entries requiring a sync to the just upgraded server. Restart any gfapi based application stopped previously in step (2)","title":"Repeat the following steps, on each server in the trusted storage pool, to upgrade the entire pool to 4.1 version:"},{"location":"Upgrade-Guide/upgrade-to-4.1/#offline-upgrade-procedure","text":"This procedure involves cluster downtime and during the upgrade window, clients are not allowed access to the volumes.","title":"Offline upgrade procedure"},{"location":"Upgrade-Guide/upgrade-to-4.1/#steps-to-perform-an-offline-upgrade","text":"On every server in the trusted storage pool, stop all gluster services, either using the command below, or through other means, #killall glusterfs glusterfsd glusterd glustereventsd #systemctl stop glustereventsd Stop all applications that access the volumes via gfapi (qemu, NFS-Ganesha, Samba, etc.), across all servers Install Gluster 4.1, on all servers Ensure that version reflects 4.1.x in the output of the following command on all servers, #gluster --version NOTE: x is the minor release number for the release Start glusterd on all the upgraded servers #glusterd Ensure that all gluster processes are online by checking the output of, #gluster volume status If the glustereventsd service was previously enabled, it is required to start it using the commands below, or, through other means, #systemctl start glustereventsd Restart any gfapi based application stopped previously in step (2)","title":"Steps to perform an offline upgrade:"},{"location":"Upgrade-Guide/upgrade-to-4.1/#post-upgrade-steps","text":"Perform the following steps post upgrading the entire trusted storage pool, It is recommended to update the op-version of the cluster. Refer, to the op-version section for further details Proceed to upgrade the clients to 4.1 version as well Post upgrading the clients, for replicate volumes, it is recommended to enable the option gluster volume set <volname> fips-mode-rchecksum on to turn off usage of MD5 checksums during healing. This enables running Gluster on FIPS compliant systems.","title":"Post upgrade steps"},{"location":"Upgrade-Guide/upgrade-to-4.1/#upgrade-procedure-for-clients","text":"Following are the steps to upgrade clients to the 4.1.x version, NOTE: x is the minor release number for the release Unmount all glusterfs mount points on the client Stop all applications that access the volumes via gfapi (qemu, etc.) Install Gluster 4.1 Mount all gluster shares Start any applications that were stopped previously in step (2)","title":"Upgrade procedure for clients"},{"location":"Upgrade-Guide/upgrade-to-5/","text":"Upgrade procedure to Gluster 5, from Gluster 4.1.x, 4.0.x, 3.12.x and 3.10.x NOTE: Upgrade procedure remains the same as with 4.1 release Refer, to the Upgrading to 4.1 guide and follow documented instructions, replacing 5 when you encounter 4.1 in the guide as the version reference. Major issues The following options are removed from the code base and require to be unset before an upgrade from releases older than release 4.1.0, features.lock-heal features.grace-timeout To check if these options are set use, # gluster volume info and ensure that the above options are not part of the Options Reconfigured: section in the output of all volumes in the cluster. If these are set, then unset them using the following commands, # gluster volume reset <volname> <option> NOTE: Failure to do the above may result in failure during online upgrades, and the reset of these options to their defaults needs to be done prior to upgrading the cluster.","title":"Upgrade to 5"},{"location":"Upgrade-Guide/upgrade-to-5/#upgrade-procedure-to-gluster-5-from-gluster-41x-40x-312x-and-310x","text":"NOTE: Upgrade procedure remains the same as with 4.1 release Refer, to the Upgrading to 4.1 guide and follow documented instructions, replacing 5 when you encounter 4.1 in the guide as the version reference.","title":"Upgrade procedure to Gluster 5, from Gluster 4.1.x, 4.0.x, 3.12.x and 3.10.x"},{"location":"Upgrade-Guide/upgrade-to-5/#major-issues","text":"The following options are removed from the code base and require to be unset before an upgrade from releases older than release 4.1.0, features.lock-heal features.grace-timeout To check if these options are set use, # gluster volume info and ensure that the above options are not part of the Options Reconfigured: section in the output of all volumes in the cluster. If these are set, then unset them using the following commands, # gluster volume reset <volname> <option> NOTE: Failure to do the above may result in failure during online upgrades, and the reset of these options to their defaults needs to be done prior to upgrading the cluster.","title":"Major issues"},{"location":"Upgrade-Guide/upgrade-to-6/","text":"Upgrade procedure to Gluster 6, from Gluster 5.x, 4.1.x, and 3.12.x We recommend reading the release notes for 6.0 to be aware of the features and fixes provided with the release. NOTE: Upgrade procedure remains the same as with 4.1.x release Refer, to the Upgrading to 4.1 guide and follow documented instructions, replacing 6 when you encounter 4.1 in the guide as the version reference. Major issues The following options are removed from the code base and require to be unset before an upgrade from releases older than release 4.1.0, features.lock-heal features.grace-timeout To check if these options are set use, # gluster volume info and ensure that the above options are not part of the Options Reconfigured: section in the output of all volumes in the cluster. If these are set, then unset them using the following commands, # gluster volume reset <volname> <option> NOTE: Failure to do the above may result in failure during online upgrades, and the reset of these options to their defaults needs to be done prior to upgrading the cluster. Deprecated translators and upgrade procedure for volumes using these features With this release of Gluster, the following xlator/features are deprecated and are not available in the distribution specific packages. If any of these xlators or features are in use, refer to instructions on steps needed pre-upgrade to plan for an upgrade to this release. Stripe volume Stripe xlator, provided the ability to stripe data across bricks. This functionality was used to create and support files larger than a single brick and also to provide better disk utilization across large file IO, by spreading the IO blocks across bricks and hence physical disks. This functionality is now provided by the shard xlator . There is no in place upgrade feasible for volumes using the stripe feature, and users are encouraged to migrate their data from existing stripe based volumes to sharded volumes. Tier volume Tier feature is no longer supported with this release. There is no replacement for the tiering feature as well. Volumes using the existing Tier feature need to be converted to regular volumes before upgrading to this release. Command reference: volume tier <VOLNAME> detach <start|stop|status|commit|[force]> Other miscellaneous features BD xlator glupy The above translators were not supported in previous versions as well, but users had an option to create volumes using these features. If such volumes were in use, data from the same need to me migrated into a new volume without the feature, before upgrading the clusters.","title":"Upgrade to 6"},{"location":"Upgrade-Guide/upgrade-to-6/#upgrade-procedure-to-gluster-6-from-gluster-5x-41x-and-312x","text":"We recommend reading the release notes for 6.0 to be aware of the features and fixes provided with the release. NOTE: Upgrade procedure remains the same as with 4.1.x release Refer, to the Upgrading to 4.1 guide and follow documented instructions, replacing 6 when you encounter 4.1 in the guide as the version reference.","title":"Upgrade procedure to Gluster 6, from Gluster 5.x, 4.1.x, and 3.12.x"},{"location":"Upgrade-Guide/upgrade-to-6/#major-issues","text":"The following options are removed from the code base and require to be unset before an upgrade from releases older than release 4.1.0, features.lock-heal features.grace-timeout To check if these options are set use, # gluster volume info and ensure that the above options are not part of the Options Reconfigured: section in the output of all volumes in the cluster. If these are set, then unset them using the following commands, # gluster volume reset <volname> <option> NOTE: Failure to do the above may result in failure during online upgrades, and the reset of these options to their defaults needs to be done prior to upgrading the cluster.","title":"Major issues"},{"location":"Upgrade-Guide/upgrade-to-6/#deprecated-translators-and-upgrade-procedure-for-volumes-using-these-features","text":"With this release of Gluster, the following xlator/features are deprecated and are not available in the distribution specific packages. If any of these xlators or features are in use, refer to instructions on steps needed pre-upgrade to plan for an upgrade to this release.","title":"Deprecated translators and upgrade procedure for volumes using these features"},{"location":"Upgrade-Guide/upgrade-to-6/#stripe-volume","text":"Stripe xlator, provided the ability to stripe data across bricks. This functionality was used to create and support files larger than a single brick and also to provide better disk utilization across large file IO, by spreading the IO blocks across bricks and hence physical disks. This functionality is now provided by the shard xlator . There is no in place upgrade feasible for volumes using the stripe feature, and users are encouraged to migrate their data from existing stripe based volumes to sharded volumes.","title":"Stripe volume"},{"location":"Upgrade-Guide/upgrade-to-6/#tier-volume","text":"Tier feature is no longer supported with this release. There is no replacement for the tiering feature as well. Volumes using the existing Tier feature need to be converted to regular volumes before upgrading to this release. Command reference: volume tier <VOLNAME> detach <start|stop|status|commit|[force]>","title":"Tier volume"},{"location":"Upgrade-Guide/upgrade-to-6/#other-miscellaneous-features","text":"BD xlator glupy The above translators were not supported in previous versions as well, but users had an option to create volumes using these features. If such volumes were in use, data from the same need to me migrated into a new volume without the feature, before upgrading the clusters.","title":"Other miscellaneous features"},{"location":"Upgrade-Guide/upgrade-to-7/","text":"Upgrade procedure to Gluster 7, from Gluster 6.x, 5.x, 4.1.x, and 3.12.x We recommend reading the release notes for 7.0 to be aware of the features and fixes provided with the release. NOTE: Upgrade procedure remains the same as with 4.1.x release Refer, to the Upgrading to 4.1 guide and follow documented instructions, replacing 7 when you encounter 4.1 in the guide as the version reference. NOTE: If you have ever enabled quota on your volumes then after the upgrade is done, you will have to restart all the nodes in the cluster one by one so as to fix the checksum values in the quota.cksum file under the /var/lib/glusterd/vols/<volname>/ directory. The peers may go into Peer rejected state while doing so but once all the nodes are rebooted everything will be back to normal. Major issues The following options are removed from the code base and require to be unset before an upgrade from releases older than release 4.1.0, features.lock-heal features.grace-timeout To check if these options are set use, # gluster volume info and ensure that the above options are not part of the Options Reconfigured: section in the output of all volumes in the cluster. If these are set, then unset them using the following commands, # gluster volume reset <volname> <option> NOTE: Failure to do the above may result in failure during online upgrades, and the reset of these options to their defaults needs to be done prior to upgrading the cluster. Deprecated translators and upgrade procedure for volumes using these features If you are upgrading from a release prior to release-6 be aware of deprecated xlators and functionality .","title":"Upgrade to 7"},{"location":"Upgrade-Guide/upgrade-to-7/#upgrade-procedure-to-gluster-7-from-gluster-6x-5x-41x-and-312x","text":"We recommend reading the release notes for 7.0 to be aware of the features and fixes provided with the release. NOTE: Upgrade procedure remains the same as with 4.1.x release Refer, to the Upgrading to 4.1 guide and follow documented instructions, replacing 7 when you encounter 4.1 in the guide as the version reference. NOTE: If you have ever enabled quota on your volumes then after the upgrade is done, you will have to restart all the nodes in the cluster one by one so as to fix the checksum values in the quota.cksum file under the /var/lib/glusterd/vols/<volname>/ directory. The peers may go into Peer rejected state while doing so but once all the nodes are rebooted everything will be back to normal.","title":"Upgrade procedure to Gluster 7, from Gluster 6.x, 5.x, 4.1.x, and 3.12.x"},{"location":"Upgrade-Guide/upgrade-to-7/#major-issues","text":"The following options are removed from the code base and require to be unset before an upgrade from releases older than release 4.1.0, features.lock-heal features.grace-timeout To check if these options are set use, # gluster volume info and ensure that the above options are not part of the Options Reconfigured: section in the output of all volumes in the cluster. If these are set, then unset them using the following commands, # gluster volume reset <volname> <option> NOTE: Failure to do the above may result in failure during online upgrades, and the reset of these options to their defaults needs to be done prior to upgrading the cluster.","title":"Major issues"},{"location":"Upgrade-Guide/upgrade-to-7/#deprecated-translators-and-upgrade-procedure-for-volumes-using-these-features","text":"If you are upgrading from a release prior to release-6 be aware of deprecated xlators and functionality .","title":"Deprecated translators and upgrade procedure for volumes using these features"},{"location":"Upgrade-Guide/upgrade-to-8/","text":"Upgrade procedure to Gluster 8, from Gluster 7.x, 6.x and 5.x We recommend reading the release notes for 8.0 to be aware of the features and fixes provided with the release. NOTE: Before following the generic upgrade procedure checkout the \" Major Issues \" section given below. With version 8, there are certain changes introduced to the directory structure of changelog files in gluster geo-replication. Thus, before the upgrade of geo-rep packages, we need to execute the upgrade script with the brick path as argument, as described below: 1. Stop the geo-rep session 2. Run the upgrade script with the brick path as the argument. Script can be used in loop for multiple bricks. 3. Start the upgradation process. This script will update the existing changelog directory structure and the paths inside the htime files to a new format introduced in version 8. If the above mentioned script is not executed, the search algorithm, used during the history crawl will fail with the wrong result for upgradation from version 7 and below to version 8 and above. Refer, to the generic upgrade procedure guide and follow documented instructions. Major issues The following options are removed from the code base and require to be unset before an upgrade from releases older than release 4.1.0, - features.lock-heal - features.grace-timeout To check if these options are set use, # gluster volume info and ensure that the above options are not part of the Options Reconfigured: section in the output of all volumes in the cluster. If these are set, then unset them using the following commands, # gluster volume reset <volname> <option> Make sure you are not using any of the following depricated features : - Block device (bd) xlator - Decompounder feature - Crypt xlator - Symlink-cache xlator - Stripe feature - Tiering support (tier xlator and changetimerecorder) - Glupy NOTE: Failure to do the above may result in failure during online upgrades, and the reset of these options to their defaults needs to be done prior to upgrading the cluster. Deprecated translators and upgrade procedure for volumes using these features If you are upgrading from a release prior to release-6 be aware of deprecated xlators and functionality .","title":"Upgrade to 8"},{"location":"Upgrade-Guide/upgrade-to-8/#upgrade-procedure-to-gluster-8-from-gluster-7x-6x-and-5x","text":"We recommend reading the release notes for 8.0 to be aware of the features and fixes provided with the release. NOTE: Before following the generic upgrade procedure checkout the \" Major Issues \" section given below. With version 8, there are certain changes introduced to the directory structure of changelog files in gluster geo-replication. Thus, before the upgrade of geo-rep packages, we need to execute the upgrade script with the brick path as argument, as described below: 1. Stop the geo-rep session 2. Run the upgrade script with the brick path as the argument. Script can be used in loop for multiple bricks. 3. Start the upgradation process. This script will update the existing changelog directory structure and the paths inside the htime files to a new format introduced in version 8. If the above mentioned script is not executed, the search algorithm, used during the history crawl will fail with the wrong result for upgradation from version 7 and below to version 8 and above. Refer, to the generic upgrade procedure guide and follow documented instructions.","title":"Upgrade procedure to Gluster 8, from Gluster 7.x, 6.x and 5.x"},{"location":"Upgrade-Guide/upgrade-to-8/#major-issues","text":"","title":"Major issues"},{"location":"Upgrade-Guide/upgrade-to-8/#the-following-options-are-removed-from-the-code-base-and-require-to-be-unset","text":"before an upgrade from releases older than release 4.1.0, - features.lock-heal - features.grace-timeout To check if these options are set use, # gluster volume info and ensure that the above options are not part of the Options Reconfigured: section in the output of all volumes in the cluster. If these are set, then unset them using the following commands, # gluster volume reset <volname> <option>","title":"The following options are removed from the code base and require to be unset"},{"location":"Upgrade-Guide/upgrade-to-8/#make-sure-you-are-not-using-any-of-the-following-depricated-features","text":"- Block device (bd) xlator - Decompounder feature - Crypt xlator - Symlink-cache xlator - Stripe feature - Tiering support (tier xlator and changetimerecorder) - Glupy NOTE: Failure to do the above may result in failure during online upgrades, and the reset of these options to their defaults needs to be done prior to upgrading the cluster.","title":"Make sure you are not using any of the following depricated features :"},{"location":"Upgrade-Guide/upgrade-to-8/#deprecated-translators-and-upgrade-procedure-for-volumes-using-these-features","text":"If you are upgrading from a release prior to release-6 be aware of deprecated xlators and functionality .","title":"Deprecated translators and upgrade procedure for volumes using these features"},{"location":"Upgrade-Guide/upgrade-to-9/","text":"Upgrade procedure to Gluster 9, from Gluster 8.x, 7.x and 6.x We recommend reading the release notes for 9.0 to be aware of the features and fixes provided with the release. NOTE: Before following the generic upgrade procedure checkout the \" Major Issues \" section given below. Refer, to the generic upgrade procedure guide and follow documented instructions. Major issues The following options are removed from the code base and require to be unset before an upgrade from releases older than release 4.1.0, - features.lock-heal - features.grace-timeout To check if these options are set use, # gluster volume info and ensure that the above options are not part of the Options Reconfigured: section in the output of all volumes in the cluster. If these are set, then unset them using the following commands, # gluster volume reset <volname> <option> Make sure you are not using any of the following depricated features : - Block device (bd) xlator - Decompounder feature - Crypt xlator - Symlink-cache xlator - Stripe feature - Tiering support (tier xlator and changetimerecorder) - Glupy NOTE: Failure to do the above may result in failure during online upgrades, and the reset of these options to their defaults needs to be done prior to upgrading the cluster. Deprecated translators and upgrade procedure for volumes using these features If you are upgrading from a release prior to release-6 be aware of deprecated xlators and functionality .","title":"Upgrade to 9"},{"location":"Upgrade-Guide/upgrade-to-9/#upgrade-procedure-to-gluster-9-from-gluster-8x-7x-and-6x","text":"We recommend reading the release notes for 9.0 to be aware of the features and fixes provided with the release. NOTE: Before following the generic upgrade procedure checkout the \" Major Issues \" section given below. Refer, to the generic upgrade procedure guide and follow documented instructions.","title":"Upgrade procedure to Gluster 9, from Gluster 8.x, 7.x and 6.x"},{"location":"Upgrade-Guide/upgrade-to-9/#major-issues","text":"","title":"Major issues"},{"location":"Upgrade-Guide/upgrade-to-9/#the-following-options-are-removed-from-the-code-base-and-require-to-be-unset","text":"before an upgrade from releases older than release 4.1.0, - features.lock-heal - features.grace-timeout To check if these options are set use, # gluster volume info and ensure that the above options are not part of the Options Reconfigured: section in the output of all volumes in the cluster. If these are set, then unset them using the following commands, # gluster volume reset <volname> <option>","title":"The following options are removed from the code base and require to be unset"},{"location":"Upgrade-Guide/upgrade-to-9/#make-sure-you-are-not-using-any-of-the-following-depricated-features","text":"- Block device (bd) xlator - Decompounder feature - Crypt xlator - Symlink-cache xlator - Stripe feature - Tiering support (tier xlator and changetimerecorder) - Glupy NOTE: Failure to do the above may result in failure during online upgrades, and the reset of these options to their defaults needs to be done prior to upgrading the cluster.","title":"Make sure you are not using any of the following depricated features :"},{"location":"Upgrade-Guide/upgrade-to-9/#deprecated-translators-and-upgrade-procedure-for-volumes-using-these-features","text":"If you are upgrading from a release prior to release-6 be aware of deprecated xlators and functionality .","title":"Deprecated translators and upgrade procedure for volumes using these features"},{"location":"presentations/","text":"Presentations This is a collection of Gluster presentations from all over the world. We have a slideshare account where most of these presentations are stored. FOSDEM 2020 @ Brussels, Belgium - 1st & 2nd February 2020 Evolution of path based Geo-replication in Gluster - Hari Gowtham A 'Thin Arbiter' for glusterfs replication - Ravishankar N FOSDEM 2017 @ Brussels, Belgium - February 5, 2017 GlusterD-2.0: The next generation of GlusterFS management - Kaushal M Gluster Roadmap and Features - Niels de Vos SELinux Support over GlusterFS - Jiffin Tony Thottan Hyper-converged, persistent storage for containers with GlusterFS - Jose Rivera, Mohamed Ashiq Liyazudeen Kubernetes+GlusterFS Lightning Ver. - Jose Rivera, Mohamed Ashiq Liyazudeen PyCon India 2016 @ New Delhi, India - September 25, 2016 Python bindings to GlusterFS - a distributed filesystem - Prashanth Pai Openshift Meetup India 2016 @Bangalore, India - June 11, 2016 \u201cGlusterFS and Openshift (slideshare)\u201d - Humble Devassy Chirammal, Mohamed Ashiq Liyazudeen GlusterFS Meetup Bangalore 2016 @ Bangalore, India - June 4, 2016 \u201cGlusterFS Containers (slideshare)\u201d - Humble Devassy Chirammal, Mohamed Ashiq Liyazudeen \"gdeploy 2.0 (slideshare)\" - Sachidananda Urs, Nandaja Varma OpenStack Days Istanbul 2016 @ Istanbul, Turkey - May 31, 2016 \u201cBuilding Clouds That Scale-Out with GlusterFS\u201d - Mustafa Resul CETINEL NLUUG Voorjaarsconferentie 2016 @ Bunnik, The Netherlands - May 26, 2016 Replication Techniques in Gluster ( .odp ) ( .pdf ) - Niels de Vos Vault 2016 @ Raleigh, NC, US - Apr 20-21, 2016 GlusterD 2.0 ( slideshare ) - Atin Mukherjee Incontro DevOps Italia 2016 @ Bologna, Italy - Apr 1, 2016 Gluster roadmap, recent improvements and upcoming features slideshare Vimeo recording - Niels de Vos LinuxConfAU 2016 @ Geelong, Australia - Feb 03, 2016 GlusterD thread synchronization using Userspace Read Copy Update (URCU) slideshare - Atin Mukherjee DevConf.CZ 2016 @ Brno, Czech Republic - February 5, 2016 [Ceph, Gluster, Swift : Similarities and differences] (https://speakerdeck.com/prashanthpai/ceph-gluster-swift-similarities-and-differences) - Prashanth Pai, Thiago da Silva FOSDEM 2016 @ Brussels, Belgium - January 30, 2016 Gluster roadmap: Recent improvements and upcoming features slideshare - Niels de Vos T-DOSE 2015 @ Eindhoven, The Netherlands - Nov 28, 2015 Introduction into Scale-out Storage with Gluster slideshare - Niels de Vos Usenix LISA 2015 @ Washington DC, USA - Nov 8, 2015 GlusterFS Tutorial - Architecture - Rajesh Joseph & Poornima Gurusiddaiah GlusterFS Tutorial - Hands-on - Rajesh Joseph & Poornima Gurusiddaiah Open Source Backup Conference @ Cologne, Germany - September 30, 2015 Scale-Out backups with Bareos and Gluster ( slideshare ) - Niels de Vos 2015 Storage Developer Conference Achieving Coherent and Aggressive Client Caching in Gluster, a Distributed System pdf - Poornima Gurusiddaiah, Soumya Koduri Introduction to Highly Available NFS Server on Scale-Out Storage Systems Based on GlusterFS slideshare - Soumya Koduri, Meghana Madhusudhan Gluster Summit 2015 @ Barcelona, Spain Bug Triage in Gluster - Niels de Vos Responsibilities of Gluster Maintainers - Niels de Vos Leases and caching - Poornima Gurusiddaiah & Soumya Koduri Cache tiering in GlusterFS and future directions - Dan Lambright Yet Another Deduplication Library (YADL) - Dan Lambright Gluster Conference @ NMAMIT, Nitte - Apr 11, 2015 Introduction to Open Source - Niels de Vos, Red Hat Software Defined Storage - Dan Lambright, Red Hat Introduction to GlusterFS - Kaleb S. KEITHLEY, Red Hat Data Deduplication - Joseph Fernandes, Red Hat Quality of Service - Karthik US & Sukumar Poojary, 4th SEM, MCA, NMAMIT, Nitte Ceph & Gluster FS - Software Defined Storage Meetup - Jan 22, 2015 GlusterFS - Current Features & Roadmap - Niels de Vos, Red Hat Open source storage for bigdata :Fifth Elephant event - Jun 21, 2014 GlusterFS_Hadoop_fifth-elephant.odp - Lalatendu Mohanty, Red Hat Red Hat Summit 2014, San Francisco, California, USA - Apr 14-17, 2014 Red Hat Storage Server Administration Deep Dive - slideshare - Dustin Black, Red Hat GlusterFS Stack Diagram Gluster Community Night, Amsterdam, The Netherlands - Mar 4th, 2014 GlusterFS for System Administrators - Niels de Vos, Red Hat Gluster Community Day, London, United Kingdom - Oct 29th, 2013 Developing Apps and Integrating with GlusterFS - Libgfapi.odp - Justin Clift, Red Hat Gluster Community Day / LinuxCon Europ 2013, Edinburgh, United Kingdom - Oct 22-24, 2013 GlusterFS Architecture & Roadmap - Vijay Bellur Integrating GlusterFS, qemu and oVirt - Vijay Bellur Gluster Community Day, Stockholm, Sweden - Sep 4th, 2013 Gluster related development - Niels de Vos, Red Hat LOADays, Belgium - April 6th, 2013 Glusterfs_for_sysadmins-justin_clift - GlusterFS for SysAdmins, Justin Clift. For LOADays 2013 conference. CIALUG Des Moines, IA - March 21st, 2013 Converged infrastruture with oVirt, KVM, and Gluster - Theron Conrey, Red Hat Gluster Community Summit, Bangalore - March 7 & 8, 2013 SMB-GlusterDevMar2013 - Chris Hertel, Red Hat Video recording kkeithley-UFONFS-GlusterSummit - Kaleb Keithley, Red Hat HDFS + GlusterFS integration - Jay Vyas Video Join_the_SuperColony_-_Feb2013.odp - JMW, Red Hat GlusterFS API Introduction slideshare (Jeff Darcy, Red Hat) Gluster Community Workshop at CERN in Geneva - February 26, 2013 Debugging GlusterFS with Wireshark ( additional files ), Niels de Vos Gluster Community Workshop at LinuxCon Europe - November 8, 2012 Gluster for Sysadmins - Dustin Black, Red Hat On-demand_File_Caching_-_Gustavo_Brand - On-demand File Caching, Gustavo Brand, Scalus Project Gluster_Wireshark_Niels_de_Vos - Gluster and Wireshark Integration, Niels de Vos, Red Hat Accessing_Gluster_UFO_-_Eco_Willson Unified File and Object with GlusterFS, Eco Willson, Red Hat Disperse_Xlator_Ramon_Datalab.pdf - Disperse Translator, Ramon , Datalab State_of_the_Gluster_-_LCEU.pdf State of the Gluster Community, John Mark Walker, Red Hat QEMU_GlusterFS - QEMU integration with GlusterFS, Bharata Rao, IBM Linux Technology Center Software Developers' Conference (SNIA) - Sep 17, 2012 Challenges and Futures slideshare (Jeff Darcy, Red Hat) Gluster Workshop at LinuxCon North America - Aug 28, 2012 Translator tutorial slideshare (Jeff Darcy, Red Hat) Translator example slideshare (Jeff Darcy, Red Hat)","title":"Presentations"},{"location":"presentations/#presentations","text":"This is a collection of Gluster presentations from all over the world. We have a slideshare account where most of these presentations are stored.","title":"Presentations"},{"location":"presentations/#fosdem-2020-brussels-belgium-1st-2nd-february-2020","text":"Evolution of path based Geo-replication in Gluster - Hari Gowtham A 'Thin Arbiter' for glusterfs replication - Ravishankar N","title":"FOSDEM 2020 @ Brussels, Belgium -  1st &amp; 2nd February 2020"},{"location":"presentations/#fosdem-2017-brussels-belgium-february-5-2017","text":"GlusterD-2.0: The next generation of GlusterFS management - Kaushal M Gluster Roadmap and Features - Niels de Vos SELinux Support over GlusterFS - Jiffin Tony Thottan Hyper-converged, persistent storage for containers with GlusterFS - Jose Rivera, Mohamed Ashiq Liyazudeen Kubernetes+GlusterFS Lightning Ver. - Jose Rivera, Mohamed Ashiq Liyazudeen","title":"FOSDEM 2017 @ Brussels, Belgium - February 5, 2017"},{"location":"presentations/#pycon-india-2016-new-delhi-india-september-25-2016","text":"Python bindings to GlusterFS - a distributed filesystem - Prashanth Pai","title":"PyCon India 2016 @ New Delhi, India - September 25, 2016"},{"location":"presentations/#openshift-meetup-india-2016-bangalore-india-june-11-2016","text":"\u201cGlusterFS and Openshift (slideshare)\u201d - Humble Devassy Chirammal, Mohamed Ashiq Liyazudeen","title":"Openshift Meetup  India 2016 @Bangalore, India - June 11, 2016"},{"location":"presentations/#glusterfs-meetup-bangalore-2016-bangalore-india-june-4-2016","text":"\u201cGlusterFS Containers (slideshare)\u201d - Humble Devassy Chirammal, Mohamed Ashiq Liyazudeen \"gdeploy 2.0 (slideshare)\" - Sachidananda Urs, Nandaja Varma","title":"GlusterFS Meetup Bangalore 2016 @ Bangalore, India - June 4, 2016"},{"location":"presentations/#openstack-days-istanbul-2016-istanbul-turkey-may-31-2016","text":"\u201cBuilding Clouds That Scale-Out with GlusterFS\u201d - Mustafa Resul CETINEL","title":"OpenStack Days Istanbul  2016 @ Istanbul, Turkey - May 31, 2016"},{"location":"presentations/#nluug-voorjaarsconferentie-2016-bunnik-the-netherlands-may-26-2016","text":"Replication Techniques in Gluster ( .odp ) ( .pdf ) - Niels de Vos","title":"NLUUG Voorjaarsconferentie 2016 @ Bunnik, The Netherlands - May 26, 2016"},{"location":"presentations/#vault-2016-raleigh-nc-us-apr-20-21-2016","text":"GlusterD 2.0 ( slideshare ) - Atin Mukherjee","title":"Vault 2016 @ Raleigh, NC, US - Apr 20-21, 2016"},{"location":"presentations/#incontro-devops-italia-2016-bologna-italy-apr-1-2016","text":"Gluster roadmap, recent improvements and upcoming features slideshare Vimeo recording - Niels de Vos","title":"Incontro DevOps Italia 2016 @ Bologna, Italy - Apr 1, 2016"},{"location":"presentations/#linuxconfau-2016-geelong-australia-feb-03-2016","text":"GlusterD thread synchronization using Userspace Read Copy Update (URCU) slideshare - Atin Mukherjee","title":"LinuxConfAU 2016 @ Geelong, Australia - Feb 03, 2016"},{"location":"presentations/#devconfcz-2016-brno-czech-republic-february-5-2016","text":"[Ceph, Gluster, Swift : Similarities and differences] (https://speakerdeck.com/prashanthpai/ceph-gluster-swift-similarities-and-differences) - Prashanth Pai, Thiago da Silva","title":"DevConf.CZ 2016 @ Brno, Czech Republic - February 5, 2016"},{"location":"presentations/#fosdem-2016-brussels-belgium-january-30-2016","text":"Gluster roadmap: Recent improvements and upcoming features slideshare - Niels de Vos","title":"FOSDEM 2016 @ Brussels, Belgium - January 30, 2016"},{"location":"presentations/#t-dose-2015-eindhoven-the-netherlands-nov-28-2015","text":"Introduction into Scale-out Storage with Gluster slideshare - Niels de Vos","title":"T-DOSE 2015 @ Eindhoven, The Netherlands - Nov 28, 2015"},{"location":"presentations/#usenix-lisa-2015-washington-dc-usa-nov-8-2015","text":"GlusterFS Tutorial - Architecture - Rajesh Joseph & Poornima Gurusiddaiah GlusterFS Tutorial - Hands-on - Rajesh Joseph & Poornima Gurusiddaiah","title":"Usenix LISA 2015 @ Washington DC, USA - Nov 8, 2015"},{"location":"presentations/#open-source-backup-conference-cologne-germany-september-30-2015","text":"Scale-Out backups with Bareos and Gluster ( slideshare ) - Niels de Vos","title":"Open Source Backup Conference @ Cologne, Germany - September 30, 2015"},{"location":"presentations/#2015-storage-developer-conference","text":"Achieving Coherent and Aggressive Client Caching in Gluster, a Distributed System pdf - Poornima Gurusiddaiah, Soumya Koduri Introduction to Highly Available NFS Server on Scale-Out Storage Systems Based on GlusterFS slideshare - Soumya Koduri, Meghana Madhusudhan","title":"2015 Storage Developer Conference"},{"location":"presentations/#gluster-summit-2015-barcelona-spain","text":"Bug Triage in Gluster - Niels de Vos Responsibilities of Gluster Maintainers - Niels de Vos Leases and caching - Poornima Gurusiddaiah & Soumya Koduri Cache tiering in GlusterFS and future directions - Dan Lambright Yet Another Deduplication Library (YADL) - Dan Lambright","title":"Gluster Summit 2015 @ Barcelona, Spain"},{"location":"presentations/#gluster-conference-nmamit-nitte-apr-11-2015","text":"Introduction to Open Source - Niels de Vos, Red Hat Software Defined Storage - Dan Lambright, Red Hat Introduction to GlusterFS - Kaleb S. KEITHLEY, Red Hat Data Deduplication - Joseph Fernandes, Red Hat Quality of Service - Karthik US & Sukumar Poojary, 4th SEM, MCA, NMAMIT, Nitte","title":"Gluster Conference @ NMAMIT, Nitte - Apr 11, 2015"},{"location":"presentations/#ceph-gluster-fs-software-defined-storage-meetup-jan-22-2015","text":"GlusterFS - Current Features & Roadmap - Niels de Vos, Red Hat","title":"Ceph &amp; Gluster FS - Software Defined Storage Meetup - Jan 22, 2015"},{"location":"presentations/#open-source-storage-for-bigdata-fifth-elephant-event-jun-21-2014","text":"GlusterFS_Hadoop_fifth-elephant.odp - Lalatendu Mohanty, Red Hat","title":"Open source storage for bigdata :Fifth Elephant event - Jun 21, 2014"},{"location":"presentations/#red-hat-summit-2014-san-francisco-california-usa-apr-14-17-2014","text":"Red Hat Storage Server Administration Deep Dive - slideshare - Dustin Black, Red Hat GlusterFS Stack Diagram","title":"Red Hat Summit 2014, San Francisco, California, USA - Apr 14-17, 2014"},{"location":"presentations/#gluster-community-night-amsterdam-the-netherlands-mar-4th-2014","text":"GlusterFS for System Administrators - Niels de Vos, Red Hat","title":"Gluster Community Night, Amsterdam, The Netherlands - Mar 4th, 2014"},{"location":"presentations/#gluster-community-day-london-united-kingdom-oct-29th-2013","text":"Developing Apps and Integrating with GlusterFS - Libgfapi.odp - Justin Clift, Red Hat","title":"Gluster Community Day, London, United Kingdom - Oct 29th, 2013"},{"location":"presentations/#gluster-community-day-linuxcon-europ-2013-edinburgh-united-kingdom-oct-22-24-2013","text":"GlusterFS Architecture & Roadmap - Vijay Bellur Integrating GlusterFS, qemu and oVirt - Vijay Bellur","title":"Gluster Community Day / LinuxCon Europ 2013, Edinburgh, United Kingdom - Oct 22-24, 2013"},{"location":"presentations/#gluster-community-day-stockholm-sweden-sep-4th-2013","text":"Gluster related development - Niels de Vos, Red Hat","title":"Gluster Community Day, Stockholm, Sweden - Sep 4th, 2013"},{"location":"presentations/#loadays-belgium-april-6th-2013","text":"Glusterfs_for_sysadmins-justin_clift - GlusterFS for SysAdmins, Justin Clift. For LOADays 2013 conference.","title":"LOADays, Belgium - April 6th, 2013"},{"location":"presentations/#cialug-des-moines-ia-march-21st-2013","text":"Converged infrastruture with oVirt, KVM, and Gluster - Theron Conrey, Red Hat","title":"CIALUG Des Moines, IA - March 21st, 2013"},{"location":"presentations/#gluster-community-summit-bangalore-march-7-8-2013","text":"SMB-GlusterDevMar2013 - Chris Hertel, Red Hat Video recording kkeithley-UFONFS-GlusterSummit - Kaleb Keithley, Red Hat HDFS + GlusterFS integration - Jay Vyas Video Join_the_SuperColony_-_Feb2013.odp - JMW, Red Hat GlusterFS API Introduction slideshare (Jeff Darcy, Red Hat)","title":"Gluster Community Summit, Bangalore - March 7 &amp; 8, 2013"},{"location":"presentations/#gluster-community-workshop-at-cern-in-geneva-february-26-2013","text":"Debugging GlusterFS with Wireshark ( additional files ), Niels de Vos","title":"Gluster Community Workshop at CERN in Geneva - February 26, 2013"},{"location":"presentations/#gluster-community-workshop-at-linuxcon-europe-november-8-2012","text":"Gluster for Sysadmins - Dustin Black, Red Hat On-demand_File_Caching_-_Gustavo_Brand - On-demand File Caching, Gustavo Brand, Scalus Project Gluster_Wireshark_Niels_de_Vos - Gluster and Wireshark Integration, Niels de Vos, Red Hat Accessing_Gluster_UFO_-_Eco_Willson Unified File and Object with GlusterFS, Eco Willson, Red Hat Disperse_Xlator_Ramon_Datalab.pdf - Disperse Translator, Ramon , Datalab State_of_the_Gluster_-_LCEU.pdf State of the Gluster Community, John Mark Walker, Red Hat QEMU_GlusterFS - QEMU integration with GlusterFS, Bharata Rao, IBM Linux Technology Center","title":"Gluster Community Workshop at LinuxCon Europe - November 8, 2012"},{"location":"presentations/#software-developers-conference-snia-sep-17-2012","text":"Challenges and Futures slideshare (Jeff Darcy, Red Hat)","title":"Software Developers' Conference (SNIA) - Sep 17, 2012"},{"location":"presentations/#gluster-workshop-at-linuxcon-north-america-aug-28-2012","text":"Translator tutorial slideshare (Jeff Darcy, Red Hat) Translator example slideshare (Jeff Darcy, Red Hat)","title":"Gluster Workshop at LinuxCon North America - Aug 28, 2012"},{"location":"release-notes/","text":"Gluster releases are separated into major and minor releases. Major releases typically contain newer functionality (in addition to bug fixes) and minor releases improve the stability of a major releases by providing bug fixes that are found or reported against them. Major releases are made once every 1 year and receive minor updates for the next 12 months, after which they are no longer maintained (or termed EOL (End-Of-Life)). NOTE: From Gluster 10 major release, the release cycle for major releases is changed from 6 months to 1 year. Minor releases will follow every alternate month for a period of 12 months. Like wise minor releases of the previous major version will happen every three months. Detailed release schedule here Release Notes GlusterFS seLinux release notes 2.0.1 GlusterFS 10 release notes 10.1 10.0 GlusterFS 9 release notes 9.5 9.4 9.3 9.2 9.1 9.0 GlusterFS 8 release notes 8.6 8.5 8.4 8.3 8.2 8.1 8.0 GlusterFS 7 release notes 7.9 7.8 7.7 7.6 7.5 7.4 7.3 7.2 7.1 7.0 GlusterFS 6 release notes 6.10 6.9 6.8 6.7 6.6 6.5 6.4 6.3 6.2 6.1 6.0 GlusterFS 5 release notes 5.13 5.12 5.11 5.10 5.9 5.8 5.6 5.5 5.3 5.2 5.1 5.0 GlusterFS 4.1 release notes 4.1.10 4.1.9 4.1.8 4.1.7 4.1.6 4.1.5 4.1.4 4.1.3 4.1.2 4.1.1 4.1.0 GlusterFS 4.0 release notes 4.0.2 4.0.1 4.0.0 GlusterFS 3.13 release notes 3.13.2 3.13.1 3.13.0 GlusterFS 3.12 release notes 3.12.15 3.12.14 3.12.13 3.12.12 3.12.11 3.12.10 3.12.9 3.12.8 3.12.7 3.12.6 3.12.5 3.12.4 3.12.3 3.12.2 3.12.1 3.12.0 GlusterFS 3.11 release notes 3.11.3 3.11.2 3.11.1 3.11.0 GlusterFS 3.10 release notes 3.10.12 3.10.11 3.10.10 3.10.9 3.10.8 3.10.7 3.10.6 3.10.5 3.10.4 3.10.3 3.10.2 3.10.1 3.10.0 GlusterFS 3.9 release notes 3.9.0 GlusterFS 3.7 release notes 3.7.1 3.7.0 GlusterFS 3.6 release notes 3.6.3 3.6.0 GlusterFS 3.5 release notes 3.5.4 3.5.3 3.5.2 3.5.1 3.5.0","title":"index"},{"location":"release-notes/#release-notes","text":"","title":"Release Notes"},{"location":"release-notes/#glusterfs-selinux-release-notes","text":"2.0.1","title":"GlusterFS seLinux release notes"},{"location":"release-notes/#glusterfs-10-release-notes","text":"10.1 10.0","title":"GlusterFS 10 release notes"},{"location":"release-notes/#glusterfs-9-release-notes","text":"9.5 9.4 9.3 9.2 9.1 9.0","title":"GlusterFS 9 release notes"},{"location":"release-notes/#glusterfs-8-release-notes","text":"8.6 8.5 8.4 8.3 8.2 8.1 8.0","title":"GlusterFS 8 release notes"},{"location":"release-notes/#glusterfs-7-release-notes","text":"7.9 7.8 7.7 7.6 7.5 7.4 7.3 7.2 7.1 7.0","title":"GlusterFS 7 release notes"},{"location":"release-notes/#glusterfs-6-release-notes","text":"6.10 6.9 6.8 6.7 6.6 6.5 6.4 6.3 6.2 6.1 6.0","title":"GlusterFS 6 release notes"},{"location":"release-notes/#glusterfs-5-release-notes","text":"5.13 5.12 5.11 5.10 5.9 5.8 5.6 5.5 5.3 5.2 5.1 5.0","title":"GlusterFS 5 release notes"},{"location":"release-notes/#glusterfs-41-release-notes","text":"4.1.10 4.1.9 4.1.8 4.1.7 4.1.6 4.1.5 4.1.4 4.1.3 4.1.2 4.1.1 4.1.0","title":"GlusterFS 4.1 release notes"},{"location":"release-notes/#glusterfs-40-release-notes","text":"4.0.2 4.0.1 4.0.0","title":"GlusterFS 4.0 release notes"},{"location":"release-notes/#glusterfs-313-release-notes","text":"3.13.2 3.13.1 3.13.0","title":"GlusterFS 3.13 release notes"},{"location":"release-notes/#glusterfs-312-release-notes","text":"3.12.15 3.12.14 3.12.13 3.12.12 3.12.11 3.12.10 3.12.9 3.12.8 3.12.7 3.12.6 3.12.5 3.12.4 3.12.3 3.12.2 3.12.1 3.12.0","title":"GlusterFS 3.12 release notes"},{"location":"release-notes/#glusterfs-311-release-notes","text":"3.11.3 3.11.2 3.11.1 3.11.0","title":"GlusterFS 3.11 release notes"},{"location":"release-notes/#glusterfs-310-release-notes","text":"3.10.12 3.10.11 3.10.10 3.10.9 3.10.8 3.10.7 3.10.6 3.10.5 3.10.4 3.10.3 3.10.2 3.10.1 3.10.0","title":"GlusterFS 3.10 release notes"},{"location":"release-notes/#glusterfs-39-release-notes","text":"3.9.0","title":"GlusterFS 3.9 release notes"},{"location":"release-notes/#glusterfs-37-release-notes","text":"3.7.1 3.7.0","title":"GlusterFS 3.7 release notes"},{"location":"release-notes/#glusterfs-36-release-notes","text":"3.6.3 3.6.0","title":"GlusterFS 3.6 release notes"},{"location":"release-notes/#glusterfs-35-release-notes","text":"3.5.4 3.5.3 3.5.2 3.5.1 3.5.0","title":"GlusterFS 3.5 release notes"},{"location":"release-notes/10.0/","text":"Release notes for Gluster 10.0 Release date: 16-Nov-2021 This is a major release that includes a range of features, code improvements and stability fixes as noted below. A selection of the key features and changes are documented in this page. A full list of bugs that have been addressed is included further below. Announcements Highlights Bugs addressed in the release Announcements Releases that receive maintenance updates post release 10 is 9 ( reference ) Release 10 will receive maintenance updates around the 15th of every alternative month, and the release 9 will recieve maintainance updates around 15th every three months. Builds are available at - https://download.gluster.org/pub/gluster/glusterfs/10/10.0/ Highlights Major performance improvement of ~20% w.r.t small files as well as large files testing in controlled lab environments #2771 NOTE : The above improvement requires tcmalloc library to be enabled for building. We have tested and verified tcmalloc in X86_64 platforms and is enabled only for x86_64 builds in current release. Randomized port selection for bricks, improves startup time #786 Performance improvement with use of readdir instead of readdirp in fix-layout #2241 Heal time improvement with bigger window size #2067 Bugs addressed Bugs addressed since release-10 are listed below. #504 AFR: remove memcpy() + ntoh32() pattern #705 gf_backtrace_save inefficiencies #782 Do not explicitly call strerror(errnum) when logging #786 glusterd-pmap binds to 10K ports on startup (using IPv4) #904 [bug:1649037] Translators allocate too much memory in their xlator_ #1000 [bug:1193929] GlusterFS can be improved #1002 [bug:1679998] GlusterFS can be improved #1052 [bug:1693692] Increase code coverage from regression tests #1060 [bug:789278] Issues reported by Coverity static analysis tool #1096 [bug:1622665] clang-scan report: glusterfs issues #1101 [bug:1813029] volume brick fails to come online because other proce #1251 performance: improve __afr_fd_ctx_get() function #1339 Rebalance status is not shown correctly after node reboot #1358 features/shard: wrong \"inode->ref\" leading to ASSERT in inode_unref #1359 Cleanup --disable-mempool #1380 fd_unref() optimization - do an atomic decrement outside the lock a #1384 mount glusterfs volume, files larger than 64Mb only show 64Mb #1406 shared storage volume fails to mount in ipv6 environment #1415 Removing problematic language in geo-replication #1423 shard_make_block_abspath() should be called with a string of of the #1536 Improve dict_reset() efficiency #1545 fuse_invalidate_entry() - too many repetitive calls to uuid_utoa() #1583 Rework stats structure (xl->stats.total.metrics[fop_idx] and friend #1584 MAINTAINERS file needs to be revisited and updated #1596 'this' NULL check relies on 'THIS' not being NULL #1600 Save and re-use MYUUID #1678 Improve gf_error_to_errno() and gf_errno_to_error() positive flow #1695 Rebalance has a redundant lookup operation #1702 Move GF_CLIENT_PID_GSYNCD check to start of the function. #1703 Remove trivial check for GF_XATTR_SHARD_FILE_SIZE before calling sh #1707 PL_LOCAL_GET_REQUESTS access the dictionary twice for the same info #1717 glusterd: sequence of rebalance and replace/reset-brick presents re #1723 DHT: further investigation for treating an ongoing mknod's linkto file #1749 brick-process: call 'notify()' and 'fini()' of brick xlators in a p #1755 Reduce calls to 'THIS' in fd_destroy() and others, where 'THIS' is #1761 CONTRIBUTING.md regression can only be run by maintainers #1764 Slow write on ZFS bricks after healing millions of files due to add #1772 build: add LTO as a configure option #1773 DHT/Rebalance - Remove unused variable dht_migrate_file #1779 Add-brick command should check hostnames with bricks present in vol #1825 Latency in io-stats should be in nanoseconds resolution, not micros #1872 Question: How to check heal info without glusterd management layer #1885 __posix_writev() - reduce memory copies and unneeded zeroing #1888 GD_OP_VERSION needs to be updated for release-10 #1898 schedule_georep.py resulting in failure when used with python3 #1909 core: Avoid several dict OR key is NULL message in brick logs #1925 dht_pt_getxattr does not seem to handle virtual xattrs. #1935 logging to syslog instead of any glusterfs logs #1943 glusterd-volgen: Add functionality to accept any custom xlator #1952 posix-aio: implement GF_FOP_FSYNC #1959 Broken links in the 2 replicas split-brain-issue - [Bug][Enhancemen #1960 Add missing LOCK_DESTROY() calls #1966 Can't print trace details due to memory allocation issues #1977 Inconsistent locking in presence of disconnects #1978 test case ./tests/bugs/core/bug-1432542-mpx-restart-crash.t is gett #1981 Reduce posix_fdstat() calls in IO paths #1991 mdcache: bug causes getxattr() to report ENODATA when fetching samb #1992 dht: var decommission_subvols_cnt becomes invalid when config is up #1996 Analyze if spinlocks have any benefit and remove them if not #2001 Error handling in /usr/sbin/gluster-eventsapi produces AttributeErr #2005 ./tests/bugs/replicate/bug-921231.t is continuously failing #2013 dict_t hash-calculation can be removed when hash_size=1 #2024 Remove gfs_id variable or at least set to appropriate value #2025 list_del() should not set prev and next #2033 tests/bugs/nfs/bug-1053579.t fails on CentOS 8 #2038 shard_unlink() fails due to no space to create marker file #2039 Do not allow POSIX IO backend switch when the volume is running #2042 mount ipv6 gluster volume with serveral backup-volfile-servers,use #2052 Revert the commit 50e953e2450b5183988c12e87bdfbc997e0ad8a8 #2054 cleanup call_stub_t from unused variables #2063 Provide autoconf option to enable/disable storage.linux-io_uring du #2067 Change self-heal-window-size to 1MB by default #2075 Annotate synctasks with valgrind API if --enable-valgrind[=memcheck #2080 Glustereventsd default port #2083 GD_MSG_DICT_GET_FAILED should not include 'errno' but 'ret' #2086 Move tests/00-geo-rep/00-georep-verify-non-root-setup.t to tests/00 #2096 iobuf_arena structure doesn't need passive and active iobufs, but l #2099 'force' option does not work in the replicated volume snapshot crea #2101 Move 00-georep-verify-non-root-setup.t back to tests/00-geo-rep/ #2107 mount crashes when setfattr -n distribute.fix.layout -v \"yes\" is ex #2116 enable quota for multiple volumes take more time #2117 Concurrent quota enable causes glusterd deadlock #2123 Implement an I/O framework #2129 CID 1445996 Null pointer dereferences (FORWARD_NULL) /xlators/mgmt/ #2130 stack.h/c: remove unused variable and reorder struct #2133 Changelog History Crawl failed after resuming stopped geo-replicati #2134 Fix spurious failures caused by change in profile info duration to #2138 glfs_write() dumps a core file file when buffer size is 1GB #2154 \"Operation not supported\" doing a chmod on a symlink #2159 Remove unused component tests #2161 Crash caused by memory corruption #2169 Stack overflow when parallel-readdir is enabled #2180 CID 1446716: Memory - illegal accesses (USE_AFTER_FREE) /xlators/mg #2187 [Input/output error] IO failure while performing shrink operation w #2190 Move a test case tests/basic/glusterd-restart-shd-mux.t to flaky #2192 4+1 arbiter setup is broken #2198 There are blocked inodelks for a long time #2216 Fix coverity issues #2232 \"Invalid argument\" when reading a directory with gfapi #2234 Segmentation fault in directory quota daemon for replicated volume #2239 rebalance crashes in dht on master #2241 Using readdir instead of readdirp for fix-layout increases performa #2253 Disable lookup-optimize by default in the virt group #2258 Provide option to disable fsync in data migration #2260 failed to list quota info after setting limit-usage #2268 dht_layout_unref() only uses 'this' to check that 'this->private' i #2278 nfs-ganesha does not start due to shared storage not ready, but ret #2287 runner infrastructure fails to provide platfrom independent error c #2294 dict.c: remove some strlen() calls if using DICT_LIST_IMP #2308 Developer sessions for glusterfs #2313 Long setting names mess up the columns and break parsing #2317 Rebalance doesn't migrate some sparse files #2328 \"gluster volume set group samba\" needs to include write-b #2330 gf_msg can cause relock deadlock #2334 posix_handle_soft() is doing an unnecessary stat #2337 memory leak observed in lock fop #2348 Gluster's test suite on RHEL 8 runs slower than on RHEL 7 #2351 glusterd: After upgrade on release 9.1 glusterd protocol is broken #2353 Permission issue after upgrading to Gluster v9.1 #2360 extras: postscript fails on logrotation of snapd logs #2364 After the service is restarted, a large number of handles are not r #2370 glusterd: Issues with custom xlator changes #2378 Remove sys_fstatat() from posix_handle_unset_gfid() function - not #2380 Remove sys_lstat() from posix_acl_xattr_set() - not needed #2388 Geo-replication gets delayed when there are many renames on primary #2394 Spurious failure in tests/basic/fencing/afr-lock-heal-basic.t #2398 Bitrot and scrub process showed like unknown in the gluster volume #2404 Spurious failure of tests/bugs/ec/bug-1236065.t #2407 configure glitch with CC=clang #2410 dict_xxx_sizen variant compilation should fail on passing a variabl #2414 Prefer mallinfo2() to mallinfo() if available #2421 rsync should not try to sync internal xattrs. #2429 Use file timestamps with nanosecond precision #2431 Drop --disable-syslog configuration option #2440 Geo-replication not working on Ubuntu 21.04 #2443 Core dumps on Gluster 9 - 3 replicas #2446 client_add_lock_for_recovery() - new_client_lock() should be called #2467 failed to open /proc/0/status: No such file or directory #2470 sharding: [inode.c:1255:__inode_unlink] 0-inode: dentry not found #2480 Brick going offline on another host as well as the host which reboo #2502 xlator/features/locks/src/common.c has code duplication #2507 Use appropriate msgid in gf_msg() #2515 Unable to mount the gluster volume using fuse unless iptables is fl #2522 ganesha_ha (extras/ganesha/ocf): ganesha_grace RA fails in start() #2540 delay-gen doesn't work correctly for delays longer than 2 seconds #2551 Sometimes the lock notification feature doesn't work #2581 With strict-locks enabled clients which are holding posix locks sti #2590 trusted.io-stats-dump extended attribute usage description error #2611 Granular entry self-heal is taking more time than full entry self h #2617 High CPU utilization of thread glfs_fusenoti and huge delays in som #2620 Granular entry heal purging of index name trigger two lookups in th #2625 auth.allow value is corrupted after add-brick operation #2626 entry self-heal does xattrops unnecessarily in many cases #2649 glustershd failed in bind with error \"Address already in use\" #2652 Removal of deadcode: Pump #2659 tests/basic/afr/afr-anon-inode.t crashed #2664 Test suite produce uncompressed logs #2693 dht: dht_local_wipe is crashed while running rename operation #2771 Smallfile improvement in glusterfs #2782 Glustereventsd does not listen on IPv4 when IPv6 is not available #2789 An improper locking bug(e.g., deadlock) on the lock up_inode_ctx->c #2798 FUSE mount option for localtime-logging is not exposed #2816 Glusterfsd memory leak when subdir_mounting a volume #2835 dht: found anomalies in dht_layout after commit c4cbdbcb3d02fb56a62 #2857 variable twice initialization.","title":10.0},{"location":"release-notes/10.0/#release-notes-for-gluster-100","text":"Release date: 16-Nov-2021 This is a major release that includes a range of features, code improvements and stability fixes as noted below. A selection of the key features and changes are documented in this page. A full list of bugs that have been addressed is included further below. Announcements Highlights Bugs addressed in the release","title":"Release notes for Gluster 10.0"},{"location":"release-notes/10.0/#announcements","text":"Releases that receive maintenance updates post release 10 is 9 ( reference ) Release 10 will receive maintenance updates around the 15th of every alternative month, and the release 9 will recieve maintainance updates around 15th every three months.","title":"Announcements"},{"location":"release-notes/10.0/#builds-are-available-at-","text":"https://download.gluster.org/pub/gluster/glusterfs/10/10.0/","title":"Builds are available at -"},{"location":"release-notes/10.0/#highlights","text":"Major performance improvement of ~20% w.r.t small files as well as large files testing in controlled lab environments #2771 NOTE : The above improvement requires tcmalloc library to be enabled for building. We have tested and verified tcmalloc in X86_64 platforms and is enabled only for x86_64 builds in current release. Randomized port selection for bricks, improves startup time #786 Performance improvement with use of readdir instead of readdirp in fix-layout #2241 Heal time improvement with bigger window size #2067","title":"Highlights"},{"location":"release-notes/10.0/#bugs-addressed","text":"Bugs addressed since release-10 are listed below. #504 AFR: remove memcpy() + ntoh32() pattern #705 gf_backtrace_save inefficiencies #782 Do not explicitly call strerror(errnum) when logging #786 glusterd-pmap binds to 10K ports on startup (using IPv4) #904 [bug:1649037] Translators allocate too much memory in their xlator_ #1000 [bug:1193929] GlusterFS can be improved #1002 [bug:1679998] GlusterFS can be improved #1052 [bug:1693692] Increase code coverage from regression tests #1060 [bug:789278] Issues reported by Coverity static analysis tool #1096 [bug:1622665] clang-scan report: glusterfs issues #1101 [bug:1813029] volume brick fails to come online because other proce #1251 performance: improve __afr_fd_ctx_get() function #1339 Rebalance status is not shown correctly after node reboot #1358 features/shard: wrong \"inode->ref\" leading to ASSERT in inode_unref #1359 Cleanup --disable-mempool #1380 fd_unref() optimization - do an atomic decrement outside the lock a #1384 mount glusterfs volume, files larger than 64Mb only show 64Mb #1406 shared storage volume fails to mount in ipv6 environment #1415 Removing problematic language in geo-replication #1423 shard_make_block_abspath() should be called with a string of of the #1536 Improve dict_reset() efficiency #1545 fuse_invalidate_entry() - too many repetitive calls to uuid_utoa() #1583 Rework stats structure (xl->stats.total.metrics[fop_idx] and friend #1584 MAINTAINERS file needs to be revisited and updated #1596 'this' NULL check relies on 'THIS' not being NULL #1600 Save and re-use MYUUID #1678 Improve gf_error_to_errno() and gf_errno_to_error() positive flow #1695 Rebalance has a redundant lookup operation #1702 Move GF_CLIENT_PID_GSYNCD check to start of the function. #1703 Remove trivial check for GF_XATTR_SHARD_FILE_SIZE before calling sh #1707 PL_LOCAL_GET_REQUESTS access the dictionary twice for the same info #1717 glusterd: sequence of rebalance and replace/reset-brick presents re #1723 DHT: further investigation for treating an ongoing mknod's linkto file #1749 brick-process: call 'notify()' and 'fini()' of brick xlators in a p #1755 Reduce calls to 'THIS' in fd_destroy() and others, where 'THIS' is #1761 CONTRIBUTING.md regression can only be run by maintainers #1764 Slow write on ZFS bricks after healing millions of files due to add #1772 build: add LTO as a configure option #1773 DHT/Rebalance - Remove unused variable dht_migrate_file #1779 Add-brick command should check hostnames with bricks present in vol #1825 Latency in io-stats should be in nanoseconds resolution, not micros #1872 Question: How to check heal info without glusterd management layer #1885 __posix_writev() - reduce memory copies and unneeded zeroing #1888 GD_OP_VERSION needs to be updated for release-10 #1898 schedule_georep.py resulting in failure when used with python3 #1909 core: Avoid several dict OR key is NULL message in brick logs #1925 dht_pt_getxattr does not seem to handle virtual xattrs. #1935 logging to syslog instead of any glusterfs logs #1943 glusterd-volgen: Add functionality to accept any custom xlator #1952 posix-aio: implement GF_FOP_FSYNC #1959 Broken links in the 2 replicas split-brain-issue - [Bug][Enhancemen #1960 Add missing LOCK_DESTROY() calls #1966 Can't print trace details due to memory allocation issues #1977 Inconsistent locking in presence of disconnects #1978 test case ./tests/bugs/core/bug-1432542-mpx-restart-crash.t is gett #1981 Reduce posix_fdstat() calls in IO paths #1991 mdcache: bug causes getxattr() to report ENODATA when fetching samb #1992 dht: var decommission_subvols_cnt becomes invalid when config is up #1996 Analyze if spinlocks have any benefit and remove them if not #2001 Error handling in /usr/sbin/gluster-eventsapi produces AttributeErr #2005 ./tests/bugs/replicate/bug-921231.t is continuously failing #2013 dict_t hash-calculation can be removed when hash_size=1 #2024 Remove gfs_id variable or at least set to appropriate value #2025 list_del() should not set prev and next #2033 tests/bugs/nfs/bug-1053579.t fails on CentOS 8 #2038 shard_unlink() fails due to no space to create marker file #2039 Do not allow POSIX IO backend switch when the volume is running #2042 mount ipv6 gluster volume with serveral backup-volfile-servers,use #2052 Revert the commit 50e953e2450b5183988c12e87bdfbc997e0ad8a8 #2054 cleanup call_stub_t from unused variables #2063 Provide autoconf option to enable/disable storage.linux-io_uring du #2067 Change self-heal-window-size to 1MB by default #2075 Annotate synctasks with valgrind API if --enable-valgrind[=memcheck #2080 Glustereventsd default port #2083 GD_MSG_DICT_GET_FAILED should not include 'errno' but 'ret' #2086 Move tests/00-geo-rep/00-georep-verify-non-root-setup.t to tests/00 #2096 iobuf_arena structure doesn't need passive and active iobufs, but l #2099 'force' option does not work in the replicated volume snapshot crea #2101 Move 00-georep-verify-non-root-setup.t back to tests/00-geo-rep/ #2107 mount crashes when setfattr -n distribute.fix.layout -v \"yes\" is ex #2116 enable quota for multiple volumes take more time #2117 Concurrent quota enable causes glusterd deadlock #2123 Implement an I/O framework #2129 CID 1445996 Null pointer dereferences (FORWARD_NULL) /xlators/mgmt/ #2130 stack.h/c: remove unused variable and reorder struct #2133 Changelog History Crawl failed after resuming stopped geo-replicati #2134 Fix spurious failures caused by change in profile info duration to #2138 glfs_write() dumps a core file file when buffer size is 1GB #2154 \"Operation not supported\" doing a chmod on a symlink #2159 Remove unused component tests #2161 Crash caused by memory corruption #2169 Stack overflow when parallel-readdir is enabled #2180 CID 1446716: Memory - illegal accesses (USE_AFTER_FREE) /xlators/mg #2187 [Input/output error] IO failure while performing shrink operation w #2190 Move a test case tests/basic/glusterd-restart-shd-mux.t to flaky #2192 4+1 arbiter setup is broken #2198 There are blocked inodelks for a long time #2216 Fix coverity issues #2232 \"Invalid argument\" when reading a directory with gfapi #2234 Segmentation fault in directory quota daemon for replicated volume #2239 rebalance crashes in dht on master #2241 Using readdir instead of readdirp for fix-layout increases performa #2253 Disable lookup-optimize by default in the virt group #2258 Provide option to disable fsync in data migration #2260 failed to list quota info after setting limit-usage #2268 dht_layout_unref() only uses 'this' to check that 'this->private' i #2278 nfs-ganesha does not start due to shared storage not ready, but ret #2287 runner infrastructure fails to provide platfrom independent error c #2294 dict.c: remove some strlen() calls if using DICT_LIST_IMP #2308 Developer sessions for glusterfs #2313 Long setting names mess up the columns and break parsing #2317 Rebalance doesn't migrate some sparse files #2328 \"gluster volume set group samba\" needs to include write-b #2330 gf_msg can cause relock deadlock #2334 posix_handle_soft() is doing an unnecessary stat #2337 memory leak observed in lock fop #2348 Gluster's test suite on RHEL 8 runs slower than on RHEL 7 #2351 glusterd: After upgrade on release 9.1 glusterd protocol is broken #2353 Permission issue after upgrading to Gluster v9.1 #2360 extras: postscript fails on logrotation of snapd logs #2364 After the service is restarted, a large number of handles are not r #2370 glusterd: Issues with custom xlator changes #2378 Remove sys_fstatat() from posix_handle_unset_gfid() function - not #2380 Remove sys_lstat() from posix_acl_xattr_set() - not needed #2388 Geo-replication gets delayed when there are many renames on primary #2394 Spurious failure in tests/basic/fencing/afr-lock-heal-basic.t #2398 Bitrot and scrub process showed like unknown in the gluster volume #2404 Spurious failure of tests/bugs/ec/bug-1236065.t #2407 configure glitch with CC=clang #2410 dict_xxx_sizen variant compilation should fail on passing a variabl #2414 Prefer mallinfo2() to mallinfo() if available #2421 rsync should not try to sync internal xattrs. #2429 Use file timestamps with nanosecond precision #2431 Drop --disable-syslog configuration option #2440 Geo-replication not working on Ubuntu 21.04 #2443 Core dumps on Gluster 9 - 3 replicas #2446 client_add_lock_for_recovery() - new_client_lock() should be called #2467 failed to open /proc/0/status: No such file or directory #2470 sharding: [inode.c:1255:__inode_unlink] 0-inode: dentry not found #2480 Brick going offline on another host as well as the host which reboo #2502 xlator/features/locks/src/common.c has code duplication #2507 Use appropriate msgid in gf_msg() #2515 Unable to mount the gluster volume using fuse unless iptables is fl #2522 ganesha_ha (extras/ganesha/ocf): ganesha_grace RA fails in start() #2540 delay-gen doesn't work correctly for delays longer than 2 seconds #2551 Sometimes the lock notification feature doesn't work #2581 With strict-locks enabled clients which are holding posix locks sti #2590 trusted.io-stats-dump extended attribute usage description error #2611 Granular entry self-heal is taking more time than full entry self h #2617 High CPU utilization of thread glfs_fusenoti and huge delays in som #2620 Granular entry heal purging of index name trigger two lookups in th #2625 auth.allow value is corrupted after add-brick operation #2626 entry self-heal does xattrops unnecessarily in many cases #2649 glustershd failed in bind with error \"Address already in use\" #2652 Removal of deadcode: Pump #2659 tests/basic/afr/afr-anon-inode.t crashed #2664 Test suite produce uncompressed logs #2693 dht: dht_local_wipe is crashed while running rename operation #2771 Smallfile improvement in glusterfs #2782 Glustereventsd does not listen on IPv4 when IPv6 is not available #2789 An improper locking bug(e.g., deadlock) on the lock up_inode_ctx->c #2798 FUSE mount option for localtime-logging is not exposed #2816 Glusterfsd memory leak when subdir_mounting a volume #2835 dht: found anomalies in dht_layout after commit c4cbdbcb3d02fb56a62 #2857 variable twice initialization.","title":"Bugs addressed"},{"location":"release-notes/10.1/","text":"Release notes for Gluster 10.1 Release date: 1st-Feb-2021 This is a bugfix and improvement release. The release notes for 10.0 contain a listing of all the new features that were added and bugs fixed in the GlusterFS 10 stable release. NOTE: - Next minor release tentative date: Week of 15th Mar, 2022 - Users are highly encouraged to upgrade to newer releases of GlusterFS. Important fixes in this release Fix missing stripe count issue with upgrade from 9.x to 10.x Fix IO failure when shrinking distributed dispersed volume with ongoing IO Fix log spam introduced with glusterfs 10.0 Enable ltcmalloc_minimal instead of ltcmalloc Builds are available at - https://download.gluster.org/pub/gluster/glusterfs/10/10.1/ Bugs addressed #2846 Avoid redundant logs in gluster #2903 Fix worker disconnect due to AttributeError in geo-replication #2910 Check for available ports in port_range in glusterd #2939 Remove the deprecated commands from gluster man page #2947 Fix IO failure when shrinking distributed dispersed volume with ongoing IO #3071 Fix log spam introduced with glusterfs 10.0 #3000 Enable ltcmalloc_minimal instead of ltcmalloc #3086 Handle excessive log in case dict is NUL #3133 Fix missing stripe count issue with upgrade from 9.x to 10.x #2962 Fix volume create failures without disperse count and ip addresses","title":10.1},{"location":"release-notes/10.1/#release-notes-for-gluster-101","text":"Release date: 1st-Feb-2021 This is a bugfix and improvement release. The release notes for 10.0 contain a listing of all the new features that were added and bugs fixed in the GlusterFS 10 stable release. NOTE: - Next minor release tentative date: Week of 15th Mar, 2022 - Users are highly encouraged to upgrade to newer releases of GlusterFS.","title":"Release notes for Gluster 10.1"},{"location":"release-notes/10.1/#important-fixes-in-this-release","text":"Fix missing stripe count issue with upgrade from 9.x to 10.x Fix IO failure when shrinking distributed dispersed volume with ongoing IO Fix log spam introduced with glusterfs 10.0 Enable ltcmalloc_minimal instead of ltcmalloc","title":"Important fixes in this release"},{"location":"release-notes/10.1/#builds-are-available-at-","text":"https://download.gluster.org/pub/gluster/glusterfs/10/10.1/","title":"Builds are available at -"},{"location":"release-notes/10.1/#bugs-addressed","text":"#2846 Avoid redundant logs in gluster #2903 Fix worker disconnect due to AttributeError in geo-replication #2910 Check for available ports in port_range in glusterd #2939 Remove the deprecated commands from gluster man page #2947 Fix IO failure when shrinking distributed dispersed volume with ongoing IO #3071 Fix log spam introduced with glusterfs 10.0 #3000 Enable ltcmalloc_minimal instead of ltcmalloc #3086 Handle excessive log in case dict is NUL #3133 Fix missing stripe count issue with upgrade from 9.x to 10.x #2962 Fix volume create failures without disperse count and ip addresses","title":"Bugs addressed"},{"location":"release-notes/3.10.0/","text":"Release notes for Gluster 3.10.0 This is a major Gluster release that includes some substantial changes. The features revolve around, better support in container environments, scaling to larger number of bricks per node, and a few usability and performance improvements, among other bug fixes. The most notable features and changes are documented on this page. A full list of bugs that has been addressed is included further below. Major changes and features Brick multiplexing Notes for users: Multiplexing reduces both port and memory usage. It does not improve performance vs. non-multiplexing except when memory is the limiting factor, though there are other related changes that improve performance overall (e.g. compared to 3.9). Multiplexing is off by default. It can be enabled with # gluster volume set all cluster.brick-multiplex on Limitations: There are currently no tuning options for multiplexing - it's all or nothing. This will change in the near future. Known Issues: The only feature or combination of features known not to work with multiplexing is USS and SSL. Anyone using that combination should leave multiplexing off. Support to display op-version information from clients Notes for users: To get information on what op-version are supported by the clients, users can invoke the gluster volume status command for clients. Along with information on hostname, port, bytes read, bytes written and number of clients connected per brick, we now also get the op-version on which the respective clients operate. Following is the example usage: # gluster volume status <VOLNAME|all> clients Limitations: Known Issues: Support to get maximum op-version in a heterogeneous cluster Notes for users: A heterogeneous cluster operates on a common op-version that can be supported across all the nodes in the trusted storage pool. Upon upgrade of the nodes in the cluster, the cluster might support a higher op-version. Users can retrieve the maximum op-version to which the cluster could be bumped up to by invoking the gluster volume get command on the newly introduced global option, cluster.max-op-version . The usage is as follows: # gluster volume get all cluster.max-op-version Limitations: Known Issues: Support for rebalance time to completion estimation Notes for users: Users can now see approximately how much time the rebalance operation will take to complete across all nodes. The estimated time left for rebalance to complete is displayed as part of the rebalance status. Use the command: # gluster volume rebalance <VOLNAME> status Limitations: The rebalance process calculates the time left based on the rate at while files are processed on the node and the total number of files on the brick which is determined using statfs. The limitations of this are: A single fs partition must host only one brick. Multiple bricks on the same fs partition will cause the statfs results to be invalid. The estimates are dynamic and are recalculated every time the rebalance status command is invoked.The estimates become more accurate over time so short running rebalance operations may not benefit. Known Issues: As glusterfs does not stored the number of files on the brick, we use statfs to guess the number. The .glusterfs directory contents can significantly skew this number and affect the calculated estimates. Separation of tier as its own service Notes for users: This change is to move the management of the tier daemon into the gluster service framework, thereby improving it stability and manageability by the service framework. This has no change to any of the tier commands or user facing interfaces and operations. Limitations: Known Issues: Statedump support for gfapi based applications Notes for users: gfapi based applications now can dump state information for better trouble shooting of issues. A statedump can be triggered in two ways: by executing the following on one of the Gluster servers, # gluster volume statedump <VOLNAME> client <HOST>:<PID> <VOLNAME> should be replaced by the name of the volume <HOST> should be replaced by the hostname of the system running the gfapi application <PID> should be replaced by the PID of the gfapi application through calling glfs_sysrq(<FS>, GLFS_SYSRQ_STATEDUMP) within the application <FS> should be replaced by a pointer to a glfs_t structure All statedumps ( *.dump.* files) will be located at the usual location, on most distributions this would be /var/run/gluster/ . Limitations: It is not possible to trigger statedumps from the Gluster CLI when the gfapi application has lost its management connection to the GlusterD servers. GlusterFS 3.10 is the first release that contains support for the new glfs_sysrq() function. Applications that include features for debugging will need to be adapted to call this function. At the time of the release of 3.10, no applications are known to call glfs_sysrq() . Known Issues: Disabled creation of trash directory by default Notes for users: From now onwards trash directory, namely .trashcan, will not be be created by default upon creation of new volumes unless and until the feature is turned ON and the restrictions on the same will be applicable as long as features.trash is set for a particular volume. Limitations: After upgrade for pre-existing volumes, trash directory will be still present at root of the volume. Those who are not interested in this feature may have to manually delete the directory from the mount point. Known Issues: Implemented parallel readdirp with distribute xlator Notes for users: Currently the directory listing gets slower as the number of bricks/nodes increases in a volume, though the file/directory numbers remain unchanged. With this feature, the performance of directory listing is made mostly independent of the number of nodes/bricks in the volume. Thus scale doesn't exponentially reduce the directory listing performance. (On a 2, 5, 10, 25 brick setup we saw ~5, 100, 400, 450% improvement consecutively) To enable this feature: # gluster volume set <VOLNAME> performance.readdir-ahead on # gluster volume set <VOLNAME> performance.parallel-readdir on To disable this feature: # gluster volume set <VOLNAME> performance.parallel-readdir off If there are more than 50 bricks in the volume it is good to increase the cache size to be more than 10Mb (default value): # gluster volume set <VOLNAME> performance.rda-cache-limit <CACHE SIZE> Limitations: Known Issues: md-cache can optionally -ve cache security.ima xattr Notes for users: From kernel version 3.X or greater, creating of a file results in removexattr call on security.ima xattr. This xattr is not set on the file unless IMA feature is active. With this patch, removxattr call returns ENODATA if it is not found in the cache. The end benefit is faster create operations where IMA is not enabled. To cache this xattr use, # gluster volume set <VOLNAME> performance.cache-ima-xattrs on The above option is on by default. Limitations: Known Issues: Added support for CPU extensions in disperse computations Notes for users: To improve disperse computations, a new way of generating dynamic code targeting specific CPU extensions like SSE and AVX on Intel processors is implemented. The available extensions are detected on run time. This can roughly double encoding and decoding speeds (or halve CPU usage). This change is 100% compatible with the old method. No change is needed if an existing volume is upgraded. You can control which extensions to use or disable them with the following command: # gluster volume set <VOLNAME> disperse.cpu-extensions <type> Valid values are: none: Completely disable dynamic code generation auto: Automatically detect available extensions and use the best one x64: Use dynamic code generation using standard 64 bits instructions sse: Use dynamic code generation using SSE extensions (128 bits) avx: Use dynamic code generation using AVX extensions (256 bits) The default value is 'auto'. If a value is specified that is not detected on run-time, it will automatically fall back to the next available option. Limitations: Known Issues: To solve a conflict between the dynamic code generator and SELinux, it has been necessary to create a dynamic file on runtime in the directory /usr/libexec/glusterfs. This directory only exists if the server package is installed. On nodes with only the client package installed, this directory won't exist and the dynamic code won't be used. It also needs root privileges to create the file there, so any gfapi application not running as root won't be able to use dynamic code generation. In these cases, disperse volumes will continue working normally but using the old implementation (equivalent to setting disperse.cpu-extensions to none). More information and a discussion on how to solve this can be found here: https://bugzilla.redhat.com/1421649 Bugs addressed Bugs addressed since release-3.9 are listed below. #789278 : Issues reported by Coverity static analysis tool #1198849 : Minor improvements and cleanup for the build system #1211863 : RFE: Support in md-cache to use upcall notifications to invalidate its cache #1231224 : Misleading error messages on brick logs while creating directory (mkdir) on fuse mount #1234054 : `gluster volume heal split-brain' does not heal if data/metadata/entry self-heal options are turned off #1289922 : Implement SIMD support on EC #1290304 : [RFE]Reducing number of network round trips #1297182 : Mounting with \"-o noatime\" or \"-o noexec\" causes \"nosuid,nodev\" to be set as well #1313838 : Tiering as separate process and in v status moving tier task to tier process #1316873 : EC: Set/unset dirty flag for all the update operations #1325531 : Statedump: Add per xlator ref counting for inode #1325792 : \"gluster vol heal test statistics heal-count replica\" seems doesn't work #1330604 : out-of-tree builds generate XDR headers and source files in the original directory #1336371 : Sequential volume start&stop is failing with SSL enabled setup. #1341948 : DHT: Rebalance- Misleading log messages from __dht_check_free_space function #1344714 : removal of file from nfs mount crashs ganesha server #1349385 : [FEAT]jbr: Add rollbacking of failed fops #1355956 : RFE : move ganesha related configuration into shared storage #1356076 : DHT doesn't evenly balance files on FreeBSD with ZFS #1356960 : OOM Kill on client when heal is in progress on 1*(2+1) arbiter volume #1357753 : JSON output for all Events CLI commands #1357754 : Delayed Events if any one Webhook is slow #1358296 : tier: breaking down the monolith processing function tier_migrate_using_query_file() #1359612 : [RFE] Geo-replication Logging Improvements #1360670 : Add output option --xml to man page of gluster #1363595 : Node remains in stopped state in pcs status with \"/usr/lib/ocf/resource.d/heartbeat/ganesha_mon: line 137: [: too many arguments ]\" messages in logs. #1363965 : geo-replication *changes.log does not respect the log-level configured #1364420 : [RFE] History Crawl performance improvement #1365395 : Support for rc.d and init for Service management #1365740 : dht: Update stbuf from servers having layout #1365791 : Geo-rep worker Faulty with OSError: [Errno 21] Is a directory #1365822 : [RFE] cli command to get max supported cluster.op-version #1366494 : Rebalance is not considering the brick sizes while fixing the layout #1366495 : 1 mkdir generates tons of log messages from dht xlator #1366648 : [GSS] A hot tier brick becomes full, causing the entire volume to have issues and returns stale file handle and input/output error. #1366815 : spurious heal info as pending heal entries never end on an EC volume while IOs are going on #1368012 : gluster fails to propagate permissions on the root of a gluster export when adding bricks #1368138 : Crash of glusterd when using long username with geo-replication #1368312 : Value of `replica.split-brain-status' attribute of a directory in metadata split-brain in a dist-rep volume reads that it is not in split-brain #1368336 : [RFE] Tier Events #1369077 : The directories get renamed when data bricks are offline in 4*(2+1) volume #1369124 : fix unused variable warnings from out-of-tree builds generate XDR headers and source files i... #1369397 : segment fault in changelog_cleanup_dispatchers #1369403 : [RFE]: events from protocol server #1369523 : worm: variable reten_mode is invalid to be free by mem_put in fini() #1370410 : [granular entry sh] - Provide a CLI to enable/disable the feature that checks that there are no heals pending before allowing the operation #1370567 : [RFE] Provide snapshot events for the new eventing framework #1370931 : glfs_realpath() should not return malloc()'d allocated memory #1371353 : posix: Integrate important events with events framework #1371470 : disperse: Integrate important events with events framework #1371485 : [RFE]: AFR events #1371539 : Quota version not changing in the quota.conf after upgrading to 3.7.1 from 3.6.1 #1371540 : Spurious regression in tests/basic/gfapi/bug1291259.t #1371874 : [RFE] DHT Events #1372193 : [geo-rep]: AttributeError: 'Popen' object has no attribute 'elines' #1372211 : write-behind: flush stuck by former failed write #1372356 : glusterd experiencing repeated connect/disconnect messages when shd is down #1372553 : \"gluster vol status all clients --xml\" doesn't generate xml if there is a failure in between #1372584 : Fix the test case http://review.gluster.org/#/c/15385/ #1373072 : Event pushed even if Answer is No in the Volume Stop and Delete prompt #1373373 : Worker crashes with EINVAL errors #1373520 : [Bitrot]: Recovery fails of a corrupted hardlink (and the corresponding parent file) in a disperse volume #1373741 : [geo-replication]: geo-rep Status is not showing bricks from one of the nodes #1374093 : glusterfs: create a directory with 0464 mode return EIO error #1374286 : [geo-rep]: defunct tar process while using tar+ssh sync #1374584 : Detach tier commit is allowed when detach tier start goes into failed state #1374587 : gf_event python fails with ImportError #1374993 : bug-963541.t spurious failure #1375181 : /var/tmp/rpm-tmp.KPCugR: line 2: /bin/systemctl: No such file or directory #1375431 : [RFE] enable sharding and strict-o-direct with virt profile - /var/lib/glusterd/groups/virt #1375526 : Kill rpc.statd on Linux machines #1375532 : Rpm installation fails with conflicts error for eventsconfig.json file #1376671 : Rebalance fails to start if a brick is down #1376693 : RFE: Provide a prompt when enabling gluster-NFS #1377097 : The GlusterFS Callback RPC-calls always use RPC/XID 42 #1377341 : out-of-tree builds generate XDR headers and source files in the original directory #1377427 : incorrect fuse dumping for WRITE #1377556 : Files not being opened with o_direct flag during random read operation (Glusterfs 3.8.2) #1377584 : memory leak problems are found in daemon:glusterd, server:glusterfsd and client:glusterfs #1377607 : Volume restart couldn't re-export the volume exported via ganesha. #1377864 : Creation of files on hot tier volume taking very long time #1378057 : glusterd fails to start without installing glusterfs-events package #1378072 : Modifications to AFR Events #1378305 : DHT: remove unused structure members #1378436 : build: python-ctypes no longer exists in Fedora Rawhide #1378492 : warning messages seen in glusterd logs for each 'gluster volume status' command #1378684 : Poor smallfile read performance on Arbiter volume compared to Replica 3 volume #1378778 : Add a test script for compound fops changes in AFR #1378842 : [RFE] 'gluster volume get' should implement the way to retrieve volume options using the volume name 'all' #1379223 : \"nfs.disable: on\" is not showing in Vol info by default for the 3.7.x volumes after updating to 3.9.0 #1379285 : gfapi: Fix fd ref leaks #1379328 : Boolean attributes are published as string #1379330 : eventsapi/georep: Events are not available for Checkpoint and Status Change #1379511 : Fix spurious failures in open-behind.t #1379655 : Recording (ffmpeg) processes on FUSE get hung #1379720 : errors appear in brick and nfs logs and getting stale files on NFS clients #1379769 : GlusterFS fails to build on old Linux distros with linux/oom.h missing #1380249 : Huge memory usage of FUSE client #1380275 : client ID should logged when SSL connection fails #1381115 : Polling failure errors getting when volume is started&stopped with SSL enabled setup. #1381421 : afr fix shd log message error #1381830 : Regression caused by enabling client-io-threads by default #1382236 : glusterfind pre session hangs indefinitely #1382258 : RFE: Support to update NFS-Ganesha export options dynamically #1382266 : md-cache: Invalidate cache entry in case of OPEN with O_TRUNC #1384142 : crypt: changes needed for openssl-1.1 (coming in Fedora 26) #1384297 : glusterfs can't self heal character dev file for invalid dev_t parameters #1384906 : arbiter volume write performance is bad with sharding #1385104 : invalid argument warning messages seen in fuse client logs 2016-09-30 06:34:58.938667] W [dict.c:418ict_set] (-->/usr/lib64/glusterfs/3.8.4/xlator/cluster/replicate.so(+0x58722) 0-dict: !this || !value for key=link-count [Invalid argument] #1385575 : pmap_signin event fails to update brickinfo->signed_in flag #1385593 : Fix some spelling mistakes in comments and log messages #1385839 : Incorrect volume type in the \"glusterd_state\" file generated using CLI \"gluster get-state\" #1386088 : Memory Leaks in snapshot code path #1386097 : 4 of 8 bricks (2 dht subvols) crashed on systemic setup #1386123 : geo-replica slave node goes faulty for non-root user session due to fail to locate gluster binary #1386141 : Error and warning message getting while removing glusterfs-events package #1386188 : Asynchronous Unsplit-brain still causes Input/Output Error on system calls #1386200 : Log all published events #1386247 : [Eventing]: 'gluster volume tier start force' does not generate a TIER_START event #1386450 : Continuous warning messages getting when one of the cluster node is down on SSL setup. #1386516 : [Eventing]: UUID is showing zeros in the event message for the peer probe operation. #1386626 : fuse mount point not accessible #1386766 : trashcan max file limit cannot go beyond 1GB #1387160 : clone creation with older names in a system fails #1387207 : [Eventing]: Random VOLUME_SET events seen when no operation is done on the gluster cluster #1387241 : Pass proper permission to acl_permit() in posix_acl_open() #1387652 : [Eventing]: BRICK_DISCONNECTED events seen when a tier volume is stopped #1387864 : [Eventing]: 'gluster vol bitrot scrub ondemand' does not produce an event #1388010 : [Eventing]: 'VOLUME_REBALANCE' event messages have an incorrect volume name #1388062 : throw warning to show that older tier commands are depricated and will be removed. #1388292 : performance.read-ahead on results in processes on client stuck in IO wait #1388348 : glusterd: Display proper error message and fail the command if S32gluster_enable_shared_storage.sh hook script is not present during gluster volume set all cluster.enable-shared-storage command #1388401 : Labelled geo-rep checkpoints hide geo-replication status #1388861 : build: python on Debian-based dists use .../lib/python2.7/dist-packages instead of .../site-packages #1388862 : [Eventing]: Events not seen when command is triggered from one of the peer nodes #1388877 : Continuous errors getting in the mount log when the volume mount server glusterd is down. #1389293 : build: incorrect Requires: for portblock resource agent #1389481 : glusterfind fails to list files from tiered volume #1389697 : Remove-brick status output is showing status of fix-layout instead of original remove-brick status output #1389746 : Refresh config fails while exporting subdirectories within a volume #1390050 : Elasticsearch get CorruptIndexException errors when running with GlusterFS persistent storage #1391086 : gfapi clients crash while using async calls due to double fd_unref #1391387 : The FUSE client log is filling up with posix_acl_default and posix_acl_access messages #1392167 : SMB[md-cache Private Build]:Error messages in brick logs related to upcall_cache_invalidate gf_uuid_is_null #1392445 : Hosted Engine VM paused post replace-brick operation #1392713 : inconsistent file permissions b/w write permission and sticky bits(---------T ) displayed when IOs are going on with md-cache enabled (and within the invalidation cycle) #1392772 : [setxattr_cbk] \"Permission denied\" warning messages are seen in logs while running pjd-fstest suite #1392865 : Better logging when reporting failures of the kind \" Failing MKNOD as quorum is not met\" #1393259 : stat of file is hung with possible deadlock #1393678 : Worker restarts on log-rsync-performance config update #1394131 : [md-cache]: All bricks crashed while performing symlink and rename from client at the same time #1394224 : \"nfs-grace-monitor\" timed out messages observed #1394548 : Make debugging EACCES errors easier to debug #1394719 : libgfapi core dumps #1394881 : Failed to enable nfs-ganesha after disabling nfs-ganesha cluster #1395261 : Seeing error messages [snapview-client.c:283:gf_svc_lookup_cbk] and [dht-helper.c:1666ht_inode_ctx_time_update] (-->/usr/lib64/glusterfs/3.8.4/xlator/cluster/replicate.so(+0x5d75c) #1395648 : ganesha-ha.conf --status should validate if the VIPs are assigned to right nodes #1395660 : Checkpoint completed event missing master node detail #1395687 : Client side IObuff leaks at a high pace consumes complete client memory and hence making gluster volume inaccessible #1395993 : heal info --xml when bricks are down in a systemic environment is not displaying anything even after more than 30minutes #1396038 : refresh-config fails and crashes ganesha when mdcache is enabled on the volume. #1396048 : A hard link is lost during rebalance+lookup #1396062 : [geo-rep]: Worker crashes seen while renaming directories in loop #1396081 : Wrong value in Last Synced column during Hybrid Crawl #1396364 : Scheduler : Scheduler should not depend on glusterfs-events package #1396793 : [Ganesha] : Ganesha crashes intermittently during nfs-ganesha restarts. #1396807 : capture volume tunables in get-state dump #1396952 : I/O errors on FUSE mount point when reading and writing from 2 clients #1397052 : OOM kill of nfs-ganesha on one node while fs-sanity test suite is executed. #1397177 : memory leak when using libgfapi #1397419 : glusterfs_ctx_defaults_init is re-initializing ctx->locks #1397424 : PEER_REJECT, EVENT_BRICKPATH_RESOLVE_FAILED, EVENT_COMPARE_FRIEND_VOLUME_FAILED are not seen #1397754 : [SAMBA-CIFS] : IO hungs in cifs mount while graph switch on & off #1397795 : NFS-Ganesha:Volume reset for any option causes reset of ganesha enable option and bring down the ganesha services #1398076 : SEEK_HOLE/ SEEK_DATA doesn't return the correct offset #1398226 : With compound fops on, client process crashes when a replica is brought down while IO is in progress #1398566 : self-heal info command hangs after triggering self-heal #1399031 : build: add systemd dependency to glusterfs sub-package #1399072 : [Disperse] healing should not start if only data bricks are UP #1399134 : GlusterFS client crashes during remove-brick operation #1399154 : After ganesha node reboot/shutdown, portblock process goes to FAILED state #1399186 : [GANESHA] Export ID changed during volume start and stop with message \"lookup_export failed with Export id not found\" in ganesha.log #1399578 : [compound FOPs]: Memory leak while doing FOPs with brick down #1399592 : Memory leak when self healing daemon queue is full #1399780 : Use standard refcounting for structures where possible #1399995 : Dump volume specific options in get-state output in a more parseable manner #1400013 : [USS,SSL] .snaps directory is not reachable when I/O encryption (SSL) is enabled #1400026 : Duplicate value assigned to GD_MSG_DAEMON_STATE_REQ_RCVD and GD_MSG_BRICK_CLEANUP_SUCCESS messages #1400237 : Ganesha services are not stopped when pacemaker quorum is lost #1400613 : [GANESHA] failed to create directory of hostname of new node in var/lib/nfs/ganesha/ in already existing cluster nodes #1400818 : possible memory leak on client when writing to a file while another client issues a truncate #1401095 : log the error when locking the brick directory fails #1401218 : Fix compound fops memory leaks #1401404 : [Arbiter] IO's Halted and heal info command hung #1401777 : atime becomes zero when truncating file via ganesha (or gluster-NFS) #1401801 : [RFE] Use Host UUID to find local nodes to spawn workers #1401812 : RFE: Make readdirp parallel in dht #1401822 : [GANESHA]Unable to export the ganesha volume after doing volume start and stop #1401836 : update documentation to readthedocs.io #1401921 : glusterfsd crashed while taking snapshot using scheduler #1402237 : Bad spacing in error message in cli #1402261 : cli: compile warnings (unused var) if building without bd xlator #1402369 : Getting the warning message while erasing the gluster \"glusterfs-server\" package. #1402710 : ls and move hung on disperse volume #1402730 : self-heal not happening, as self-heal info lists the same pending shards to be healed #1402828 : Snapshot: Snapshot create command fails when gluster-shared-storage volume is stopped #1402841 : Files remain unhealed forever if shd is disabled and re-enabled while healing is in progress. #1403130 : [GANESHA] Adding a node to cluster failed to allocate resource-agents to new node. #1403780 : Incorrect incrementation of volinfo refcnt during volume start #1404118 : Snapshot: After snapshot restore failure , snapshot goes into inconsistent state #1404168 : Upcall: Possible use after free when log level set to TRACE #1404181 : [Ganesha+SSL] : Ganesha crashes on all nodes on volume restarts #1404410 : [Perf] : pcs cluster resources went into stopped state during Multithreaded perf tests on RHGS layered over RHEL 6 #1404573 : tests/bugs/snapshot/bug-1316437.t test is causing spurious failure #1404678 : [geo-rep]: Config commands fail when the status is 'Created' #1404905 : DHT : file rename operation is successful but log has error 'key:trusted.glusterfs.dht.linkto error:File exists' , 'setting xattrs on failed (File exists)' #1405165 : Allow user to disable mem-pool #1405301 : Fix the failure in tests/basic/gfapi/bug1291259.t #1405478 : Keepalive should be set for IPv6 & IPv4 #1405554 : Fix spurious failure in bug-1402841.t-mt-dir-scan-race.t #1405775 : GlusterFS process crashed after add-brick #1405902 : Fix spurious failure in tests/bugs/replicate/bug-1402730.t #1406224 : VM pauses due to storage I/O error, when one of the data brick is down with arbiter/replica volume #1406249 : [GANESHA] Deleting a node from ganesha cluster deletes the volume entry from /etc/ganesha/ganesha.conf file #1406252 : Free xdr-allocated compound request and response arrays #1406348 : [Eventing]: POSIX_SAME_GFID event seen for .trashcan folder and .trashcan/internal_op #1406410 : [GANESHA] Adding node to ganesha cluster is not assigning the correct VIP to the new node #1406411 : Fail add-brick command if replica count changes #1406878 : ec prove tests fail in FB build environment. #1408115 : Remove-brick rebalance failed while rm -rf is in progress #1408131 : Remove tests/distaf #1408395 : [Arbiter] After Killing a brick writes drastically slow down #1408712 : with granular-entry-self-heal enabled i see that there is a gfid mismatch and vm goes to paused state after migrating to another host #1408755 : Remove tests/basic/rpm.t #1408757 : Fix failure of split-brain-favorite-child-policy.t in CentOS7 #1408758 : tests/bugs/glusterd/bug-913555.t fails spuriously #1409078 : RFE: Need a command to check op-version compatibility of clients #1409186 : Dict_t leak in dht_migration_complete_check_task and dht_rebalance_inprogress_task #1409202 : Warning messages throwing when EC volume offline brick comes up are difficult to understand for end user. #1409206 : Extra lookup/fstats are sent over the network when a brick is down. #1409727 : [ganesha + EC]posix compliance rename tests failed on EC volume with nfs-ganesha mount. #1409730 : [ganesha+ec]: Contents of original file are not seen when hardlink is created #1410071 : [Geo-rep] Geo replication status detail without master and slave volume args #1410313 : brick crashed on systemic setup #1410355 : Remove-brick rebalance failed while rm -rf is in progress #1410375 : [Mdcache] clients being served wrong information about a file, can lead to file inconsistency #1410777 : ganesha service crashed on all nodes of ganesha cluster on disperse volume when doing lookup while copying files remotely using scp #1410853 : glusterfs-server should depend on firewalld-filesystem #1411607 : [Geo-rep] If for some reason MKDIR failed to sync, it should not proceed further. #1411625 : Spurious split-brain error messages are seen in rebalance logs #1411999 : URL to Fedora distgit no longer uptodate #1412002 : Examples/getvolfile.py is not pep8 compliant #1412069 : No rollback of renames on succeeded subvols during failure #1412174 : Memory leak on mount/fuse when setxattr fails #1412467 : Remove tests/bugs/distribute/bug-1063230.t #1412489 : Upcall: Possible memleak if inode_ctx_set fails #1412689 : [Geo-rep] Slave mount log file is cluttered by logs of multiple active mounts #1412917 : OOM kill of glusterfsd during continuous add-bricks #1412918 : fuse: Resource leak in fuse-helper under GF_SOLARIS_HOST_OS #1413967 : geo-rep session faulty with ChangelogException \"No such file or directory\" #1415226 : packaging: python/python2(/python3) cleanup #1415245 : core: max op version #1415279 : libgfapi: remove/revert glfs_ipc() changes targeted for 4.0 #1415581 : RFE : Create trash directory only when its is enabled #1415915 : RFE: An administrator friendly way to determine rebalance completion time #1415918 : Cache security.ima xattrs as well #1416285 : EXPECT_WITHIN is taking too much time even if the result matches with expected value #1416416 : Improve output of \"gluster volume status detail\" #1417027 : option performance.parallel-readdir should honor cluster.readdir-optimize #1417028 : option performance.parallel-readdir can cause OOM in large volumes #1417042 : glusterd restart is starting the offline shd daemon on other node in the cluster #1417135 : [Stress] : SHD Logs flooded with \"Heal Failed\" messages,filling up \"/\" quickly #1417521 : [SNAPSHOT] With all USS plugin enable .snaps directory is not visible in cifs mount as well as windows mount #1417527 : glusterfind: After glusterfind pre command execution all temporary files and directories /usr/var/lib/misc/glusterfsd/glusterfind/ / / should be removed #1417804 : debug/trace: Print iatts of individual entries in readdirp callback for better debugging experience #1418091 : [RFE] Support multiple bricks in one process (multiplexing) #1418536 : Portmap allocates way too much memory (256KB) on stack #1418541 : [Ganesha+SSL] : Bonnie++ hangs during rewrites. #1418623 : client process crashed due to write behind translator #1418650 : Samba crash when mounting a distributed dispersed volume over CIFS #1418981 : Unable to take Statedump for gfapi applications #1419305 : disable client.io-threads on replica volume creation #1419306 : [RFE] Need to have group cli option to set all md-cache options using a single command #1419503 : [SAMBA-SSL] Volume Share hungs when multiple mount & unmount is performed over a windows client on a SSL enabled cluster #1419696 : Fix spurious failure of ec-background-heal.t and tests/bitrot/bug-1373520.t #1419824 : repeated operation failed warnings in gluster mount logs with disperse volume #1419825 : Sequential and Random Writes are off target by 12% and 22% respectively on EC backed volumes over FUSE #1419846 : removing warning related to enum, to let the build take place without errors for 3.10 #1419855 : [Remove-brick] Hardlink migration fails with \"lookup failed (No such file or directory)\" error messages in rebalance logs #1419868 : removing old tier commands under the rebalance commands #1420606 : glusterd is crashed at the time of stop volume #1420808 : Trash feature improperly disabled #1420810 : Massive xlator_t leak in graph-switch code #1420982 : Automatic split brain resolution must check for all the bricks to be up to avoiding serving of inconsistent data(visible on x3 or more) #1420987 : warning messages seen in glusterd logs while setting the volume option #1420989 : when server-quorum is enabled, volume get returns 0 value for server-quorum-ratio #1420991 : Modified volume options not synced once offline nodes comes up. #1421017 : CLI option \"--timeout\" is accepting non numeric and negative values. #1421956 : Disperse: Fallback to pre-compiled code execution when dynamic code generation fails #1422350 : glustershd process crashed on systemic setup #1422363 : [Replicate] \"RPC call decoding failed\" leading to IO hang & mount inaccessible #1422391 : Gluster NFS server crashing in __mnt3svc_umountall #1422766 : Entry heal messages in glustershd.log while no entries shown in heal info #1422777 : DHT doesn't evenly balance files on FreeBSD with ZFS #1422819 : [Geo-rep] Recreating geo-rep session with same slave after deleting with reset-sync-time fails to sync #1422942 : Prevent reverse heal from happening #1423063 : glusterfs-fuse RPM now depends on gfapi #1423070 : Bricks not coming up when ran with address sanitizer #1423385 : Crash in index xlator because of race in inode_ctx_set and inode_ref #1423406 : Need to improve remove-brick failure message when the brick process is down. #1423412 : Mount of older client fails #1423429 : unnecessary logging in rda_opendir #1424921 : dht_setxattr returns EINVAL when a file is deleted during the FOP #1424931 : [RFE] Include few more options in virt file #1424937 : multiple glusterfsd process crashed making the complete subvolume unavailable #1424973 : remove-brick status shows 0 rebalanced files #1425556 : glusterd log is flooded with stale disconnect rpc messages","title":"3.10.0"},{"location":"release-notes/3.10.0/#release-notes-for-gluster-3100","text":"This is a major Gluster release that includes some substantial changes. The features revolve around, better support in container environments, scaling to larger number of bricks per node, and a few usability and performance improvements, among other bug fixes. The most notable features and changes are documented on this page. A full list of bugs that has been addressed is included further below.","title":"Release notes for Gluster 3.10.0"},{"location":"release-notes/3.10.0/#major-changes-and-features","text":"","title":"Major changes and features"},{"location":"release-notes/3.10.0/#brick-multiplexing","text":"Notes for users: Multiplexing reduces both port and memory usage. It does not improve performance vs. non-multiplexing except when memory is the limiting factor, though there are other related changes that improve performance overall (e.g. compared to 3.9). Multiplexing is off by default. It can be enabled with # gluster volume set all cluster.brick-multiplex on Limitations: There are currently no tuning options for multiplexing - it's all or nothing. This will change in the near future. Known Issues: The only feature or combination of features known not to work with multiplexing is USS and SSL. Anyone using that combination should leave multiplexing off.","title":"Brick multiplexing"},{"location":"release-notes/3.10.0/#support-to-display-op-version-information-from-clients","text":"Notes for users: To get information on what op-version are supported by the clients, users can invoke the gluster volume status command for clients. Along with information on hostname, port, bytes read, bytes written and number of clients connected per brick, we now also get the op-version on which the respective clients operate. Following is the example usage: # gluster volume status <VOLNAME|all> clients Limitations: Known Issues:","title":"Support to display op-version information from clients"},{"location":"release-notes/3.10.0/#support-to-get-maximum-op-version-in-a-heterogeneous-cluster","text":"Notes for users: A heterogeneous cluster operates on a common op-version that can be supported across all the nodes in the trusted storage pool. Upon upgrade of the nodes in the cluster, the cluster might support a higher op-version. Users can retrieve the maximum op-version to which the cluster could be bumped up to by invoking the gluster volume get command on the newly introduced global option, cluster.max-op-version . The usage is as follows: # gluster volume get all cluster.max-op-version Limitations: Known Issues:","title":"Support to get maximum op-version in a heterogeneous cluster"},{"location":"release-notes/3.10.0/#support-for-rebalance-time-to-completion-estimation","text":"Notes for users: Users can now see approximately how much time the rebalance operation will take to complete across all nodes. The estimated time left for rebalance to complete is displayed as part of the rebalance status. Use the command: # gluster volume rebalance <VOLNAME> status Limitations: The rebalance process calculates the time left based on the rate at while files are processed on the node and the total number of files on the brick which is determined using statfs. The limitations of this are: A single fs partition must host only one brick. Multiple bricks on the same fs partition will cause the statfs results to be invalid. The estimates are dynamic and are recalculated every time the rebalance status command is invoked.The estimates become more accurate over time so short running rebalance operations may not benefit. Known Issues: As glusterfs does not stored the number of files on the brick, we use statfs to guess the number. The .glusterfs directory contents can significantly skew this number and affect the calculated estimates.","title":"Support for rebalance time to completion estimation"},{"location":"release-notes/3.10.0/#separation-of-tier-as-its-own-service","text":"Notes for users: This change is to move the management of the tier daemon into the gluster service framework, thereby improving it stability and manageability by the service framework. This has no change to any of the tier commands or user facing interfaces and operations. Limitations: Known Issues:","title":"Separation of tier as its own service"},{"location":"release-notes/3.10.0/#statedump-support-for-gfapi-based-applications","text":"Notes for users: gfapi based applications now can dump state information for better trouble shooting of issues. A statedump can be triggered in two ways: by executing the following on one of the Gluster servers, # gluster volume statedump <VOLNAME> client <HOST>:<PID> <VOLNAME> should be replaced by the name of the volume <HOST> should be replaced by the hostname of the system running the gfapi application <PID> should be replaced by the PID of the gfapi application through calling glfs_sysrq(<FS>, GLFS_SYSRQ_STATEDUMP) within the application <FS> should be replaced by a pointer to a glfs_t structure All statedumps ( *.dump.* files) will be located at the usual location, on most distributions this would be /var/run/gluster/ . Limitations: It is not possible to trigger statedumps from the Gluster CLI when the gfapi application has lost its management connection to the GlusterD servers. GlusterFS 3.10 is the first release that contains support for the new glfs_sysrq() function. Applications that include features for debugging will need to be adapted to call this function. At the time of the release of 3.10, no applications are known to call glfs_sysrq() . Known Issues:","title":"Statedump support for gfapi based applications"},{"location":"release-notes/3.10.0/#disabled-creation-of-trash-directory-by-default","text":"Notes for users: From now onwards trash directory, namely .trashcan, will not be be created by default upon creation of new volumes unless and until the feature is turned ON and the restrictions on the same will be applicable as long as features.trash is set for a particular volume. Limitations: After upgrade for pre-existing volumes, trash directory will be still present at root of the volume. Those who are not interested in this feature may have to manually delete the directory from the mount point. Known Issues:","title":"Disabled creation of trash directory by default"},{"location":"release-notes/3.10.0/#implemented-parallel-readdirp-with-distribute-xlator","text":"Notes for users: Currently the directory listing gets slower as the number of bricks/nodes increases in a volume, though the file/directory numbers remain unchanged. With this feature, the performance of directory listing is made mostly independent of the number of nodes/bricks in the volume. Thus scale doesn't exponentially reduce the directory listing performance. (On a 2, 5, 10, 25 brick setup we saw ~5, 100, 400, 450% improvement consecutively) To enable this feature: # gluster volume set <VOLNAME> performance.readdir-ahead on # gluster volume set <VOLNAME> performance.parallel-readdir on To disable this feature: # gluster volume set <VOLNAME> performance.parallel-readdir off If there are more than 50 bricks in the volume it is good to increase the cache size to be more than 10Mb (default value): # gluster volume set <VOLNAME> performance.rda-cache-limit <CACHE SIZE> Limitations: Known Issues:","title":"Implemented parallel readdirp with distribute xlator"},{"location":"release-notes/3.10.0/#md-cache-can-optionally-ve-cache-securityima-xattr","text":"Notes for users: From kernel version 3.X or greater, creating of a file results in removexattr call on security.ima xattr. This xattr is not set on the file unless IMA feature is active. With this patch, removxattr call returns ENODATA if it is not found in the cache. The end benefit is faster create operations where IMA is not enabled. To cache this xattr use, # gluster volume set <VOLNAME> performance.cache-ima-xattrs on The above option is on by default. Limitations: Known Issues:","title":"md-cache can optionally -ve cache security.ima xattr"},{"location":"release-notes/3.10.0/#added-support-for-cpu-extensions-in-disperse-computations","text":"Notes for users: To improve disperse computations, a new way of generating dynamic code targeting specific CPU extensions like SSE and AVX on Intel processors is implemented. The available extensions are detected on run time. This can roughly double encoding and decoding speeds (or halve CPU usage). This change is 100% compatible with the old method. No change is needed if an existing volume is upgraded. You can control which extensions to use or disable them with the following command: # gluster volume set <VOLNAME> disperse.cpu-extensions <type> Valid values are: none: Completely disable dynamic code generation auto: Automatically detect available extensions and use the best one x64: Use dynamic code generation using standard 64 bits instructions sse: Use dynamic code generation using SSE extensions (128 bits) avx: Use dynamic code generation using AVX extensions (256 bits) The default value is 'auto'. If a value is specified that is not detected on run-time, it will automatically fall back to the next available option. Limitations: Known Issues: To solve a conflict between the dynamic code generator and SELinux, it has been necessary to create a dynamic file on runtime in the directory /usr/libexec/glusterfs. This directory only exists if the server package is installed. On nodes with only the client package installed, this directory won't exist and the dynamic code won't be used. It also needs root privileges to create the file there, so any gfapi application not running as root won't be able to use dynamic code generation. In these cases, disperse volumes will continue working normally but using the old implementation (equivalent to setting disperse.cpu-extensions to none). More information and a discussion on how to solve this can be found here: https://bugzilla.redhat.com/1421649","title":"Added support for CPU extensions in disperse computations"},{"location":"release-notes/3.10.0/#bugs-addressed","text":"Bugs addressed since release-3.9 are listed below. #789278 : Issues reported by Coverity static analysis tool #1198849 : Minor improvements and cleanup for the build system #1211863 : RFE: Support in md-cache to use upcall notifications to invalidate its cache #1231224 : Misleading error messages on brick logs while creating directory (mkdir) on fuse mount #1234054 : `gluster volume heal split-brain' does not heal if data/metadata/entry self-heal options are turned off #1289922 : Implement SIMD support on EC #1290304 : [RFE]Reducing number of network round trips #1297182 : Mounting with \"-o noatime\" or \"-o noexec\" causes \"nosuid,nodev\" to be set as well #1313838 : Tiering as separate process and in v status moving tier task to tier process #1316873 : EC: Set/unset dirty flag for all the update operations #1325531 : Statedump: Add per xlator ref counting for inode #1325792 : \"gluster vol heal test statistics heal-count replica\" seems doesn't work #1330604 : out-of-tree builds generate XDR headers and source files in the original directory #1336371 : Sequential volume start&stop is failing with SSL enabled setup. #1341948 : DHT: Rebalance- Misleading log messages from __dht_check_free_space function #1344714 : removal of file from nfs mount crashs ganesha server #1349385 : [FEAT]jbr: Add rollbacking of failed fops #1355956 : RFE : move ganesha related configuration into shared storage #1356076 : DHT doesn't evenly balance files on FreeBSD with ZFS #1356960 : OOM Kill on client when heal is in progress on 1*(2+1) arbiter volume #1357753 : JSON output for all Events CLI commands #1357754 : Delayed Events if any one Webhook is slow #1358296 : tier: breaking down the monolith processing function tier_migrate_using_query_file() #1359612 : [RFE] Geo-replication Logging Improvements #1360670 : Add output option --xml to man page of gluster #1363595 : Node remains in stopped state in pcs status with \"/usr/lib/ocf/resource.d/heartbeat/ganesha_mon: line 137: [: too many arguments ]\" messages in logs. #1363965 : geo-replication *changes.log does not respect the log-level configured #1364420 : [RFE] History Crawl performance improvement #1365395 : Support for rc.d and init for Service management #1365740 : dht: Update stbuf from servers having layout #1365791 : Geo-rep worker Faulty with OSError: [Errno 21] Is a directory #1365822 : [RFE] cli command to get max supported cluster.op-version #1366494 : Rebalance is not considering the brick sizes while fixing the layout #1366495 : 1 mkdir generates tons of log messages from dht xlator #1366648 : [GSS] A hot tier brick becomes full, causing the entire volume to have issues and returns stale file handle and input/output error. #1366815 : spurious heal info as pending heal entries never end on an EC volume while IOs are going on #1368012 : gluster fails to propagate permissions on the root of a gluster export when adding bricks #1368138 : Crash of glusterd when using long username with geo-replication #1368312 : Value of `replica.split-brain-status' attribute of a directory in metadata split-brain in a dist-rep volume reads that it is not in split-brain #1368336 : [RFE] Tier Events #1369077 : The directories get renamed when data bricks are offline in 4*(2+1) volume #1369124 : fix unused variable warnings from out-of-tree builds generate XDR headers and source files i... #1369397 : segment fault in changelog_cleanup_dispatchers #1369403 : [RFE]: events from protocol server #1369523 : worm: variable reten_mode is invalid to be free by mem_put in fini() #1370410 : [granular entry sh] - Provide a CLI to enable/disable the feature that checks that there are no heals pending before allowing the operation #1370567 : [RFE] Provide snapshot events for the new eventing framework #1370931 : glfs_realpath() should not return malloc()'d allocated memory #1371353 : posix: Integrate important events with events framework #1371470 : disperse: Integrate important events with events framework #1371485 : [RFE]: AFR events #1371539 : Quota version not changing in the quota.conf after upgrading to 3.7.1 from 3.6.1 #1371540 : Spurious regression in tests/basic/gfapi/bug1291259.t #1371874 : [RFE] DHT Events #1372193 : [geo-rep]: AttributeError: 'Popen' object has no attribute 'elines' #1372211 : write-behind: flush stuck by former failed write #1372356 : glusterd experiencing repeated connect/disconnect messages when shd is down #1372553 : \"gluster vol status all clients --xml\" doesn't generate xml if there is a failure in between #1372584 : Fix the test case http://review.gluster.org/#/c/15385/ #1373072 : Event pushed even if Answer is No in the Volume Stop and Delete prompt #1373373 : Worker crashes with EINVAL errors #1373520 : [Bitrot]: Recovery fails of a corrupted hardlink (and the corresponding parent file) in a disperse volume #1373741 : [geo-replication]: geo-rep Status is not showing bricks from one of the nodes #1374093 : glusterfs: create a directory with 0464 mode return EIO error #1374286 : [geo-rep]: defunct tar process while using tar+ssh sync #1374584 : Detach tier commit is allowed when detach tier start goes into failed state #1374587 : gf_event python fails with ImportError #1374993 : bug-963541.t spurious failure #1375181 : /var/tmp/rpm-tmp.KPCugR: line 2: /bin/systemctl: No such file or directory #1375431 : [RFE] enable sharding and strict-o-direct with virt profile - /var/lib/glusterd/groups/virt #1375526 : Kill rpc.statd on Linux machines #1375532 : Rpm installation fails with conflicts error for eventsconfig.json file #1376671 : Rebalance fails to start if a brick is down #1376693 : RFE: Provide a prompt when enabling gluster-NFS #1377097 : The GlusterFS Callback RPC-calls always use RPC/XID 42 #1377341 : out-of-tree builds generate XDR headers and source files in the original directory #1377427 : incorrect fuse dumping for WRITE #1377556 : Files not being opened with o_direct flag during random read operation (Glusterfs 3.8.2) #1377584 : memory leak problems are found in daemon:glusterd, server:glusterfsd and client:glusterfs #1377607 : Volume restart couldn't re-export the volume exported via ganesha. #1377864 : Creation of files on hot tier volume taking very long time #1378057 : glusterd fails to start without installing glusterfs-events package #1378072 : Modifications to AFR Events #1378305 : DHT: remove unused structure members #1378436 : build: python-ctypes no longer exists in Fedora Rawhide #1378492 : warning messages seen in glusterd logs for each 'gluster volume status' command #1378684 : Poor smallfile read performance on Arbiter volume compared to Replica 3 volume #1378778 : Add a test script for compound fops changes in AFR #1378842 : [RFE] 'gluster volume get' should implement the way to retrieve volume options using the volume name 'all' #1379223 : \"nfs.disable: on\" is not showing in Vol info by default for the 3.7.x volumes after updating to 3.9.0 #1379285 : gfapi: Fix fd ref leaks #1379328 : Boolean attributes are published as string #1379330 : eventsapi/georep: Events are not available for Checkpoint and Status Change #1379511 : Fix spurious failures in open-behind.t #1379655 : Recording (ffmpeg) processes on FUSE get hung #1379720 : errors appear in brick and nfs logs and getting stale files on NFS clients #1379769 : GlusterFS fails to build on old Linux distros with linux/oom.h missing #1380249 : Huge memory usage of FUSE client #1380275 : client ID should logged when SSL connection fails #1381115 : Polling failure errors getting when volume is started&stopped with SSL enabled setup. #1381421 : afr fix shd log message error #1381830 : Regression caused by enabling client-io-threads by default #1382236 : glusterfind pre session hangs indefinitely #1382258 : RFE: Support to update NFS-Ganesha export options dynamically #1382266 : md-cache: Invalidate cache entry in case of OPEN with O_TRUNC #1384142 : crypt: changes needed for openssl-1.1 (coming in Fedora 26) #1384297 : glusterfs can't self heal character dev file for invalid dev_t parameters #1384906 : arbiter volume write performance is bad with sharding #1385104 : invalid argument warning messages seen in fuse client logs 2016-09-30 06:34:58.938667] W [dict.c:418ict_set] (-->/usr/lib64/glusterfs/3.8.4/xlator/cluster/replicate.so(+0x58722) 0-dict: !this || !value for key=link-count [Invalid argument] #1385575 : pmap_signin event fails to update brickinfo->signed_in flag #1385593 : Fix some spelling mistakes in comments and log messages #1385839 : Incorrect volume type in the \"glusterd_state\" file generated using CLI \"gluster get-state\" #1386088 : Memory Leaks in snapshot code path #1386097 : 4 of 8 bricks (2 dht subvols) crashed on systemic setup #1386123 : geo-replica slave node goes faulty for non-root user session due to fail to locate gluster binary #1386141 : Error and warning message getting while removing glusterfs-events package #1386188 : Asynchronous Unsplit-brain still causes Input/Output Error on system calls #1386200 : Log all published events #1386247 : [Eventing]: 'gluster volume tier start force' does not generate a TIER_START event #1386450 : Continuous warning messages getting when one of the cluster node is down on SSL setup. #1386516 : [Eventing]: UUID is showing zeros in the event message for the peer probe operation. #1386626 : fuse mount point not accessible #1386766 : trashcan max file limit cannot go beyond 1GB #1387160 : clone creation with older names in a system fails #1387207 : [Eventing]: Random VOLUME_SET events seen when no operation is done on the gluster cluster #1387241 : Pass proper permission to acl_permit() in posix_acl_open() #1387652 : [Eventing]: BRICK_DISCONNECTED events seen when a tier volume is stopped #1387864 : [Eventing]: 'gluster vol bitrot scrub ondemand' does not produce an event #1388010 : [Eventing]: 'VOLUME_REBALANCE' event messages have an incorrect volume name #1388062 : throw warning to show that older tier commands are depricated and will be removed. #1388292 : performance.read-ahead on results in processes on client stuck in IO wait #1388348 : glusterd: Display proper error message and fail the command if S32gluster_enable_shared_storage.sh hook script is not present during gluster volume set all cluster.enable-shared-storage command #1388401 : Labelled geo-rep checkpoints hide geo-replication status #1388861 : build: python on Debian-based dists use .../lib/python2.7/dist-packages instead of .../site-packages #1388862 : [Eventing]: Events not seen when command is triggered from one of the peer nodes #1388877 : Continuous errors getting in the mount log when the volume mount server glusterd is down. #1389293 : build: incorrect Requires: for portblock resource agent #1389481 : glusterfind fails to list files from tiered volume #1389697 : Remove-brick status output is showing status of fix-layout instead of original remove-brick status output #1389746 : Refresh config fails while exporting subdirectories within a volume #1390050 : Elasticsearch get CorruptIndexException errors when running with GlusterFS persistent storage #1391086 : gfapi clients crash while using async calls due to double fd_unref #1391387 : The FUSE client log is filling up with posix_acl_default and posix_acl_access messages #1392167 : SMB[md-cache Private Build]:Error messages in brick logs related to upcall_cache_invalidate gf_uuid_is_null #1392445 : Hosted Engine VM paused post replace-brick operation #1392713 : inconsistent file permissions b/w write permission and sticky bits(---------T ) displayed when IOs are going on with md-cache enabled (and within the invalidation cycle) #1392772 : [setxattr_cbk] \"Permission denied\" warning messages are seen in logs while running pjd-fstest suite #1392865 : Better logging when reporting failures of the kind \" Failing MKNOD as quorum is not met\" #1393259 : stat of file is hung with possible deadlock #1393678 : Worker restarts on log-rsync-performance config update #1394131 : [md-cache]: All bricks crashed while performing symlink and rename from client at the same time #1394224 : \"nfs-grace-monitor\" timed out messages observed #1394548 : Make debugging EACCES errors easier to debug #1394719 : libgfapi core dumps #1394881 : Failed to enable nfs-ganesha after disabling nfs-ganesha cluster #1395261 : Seeing error messages [snapview-client.c:283:gf_svc_lookup_cbk] and [dht-helper.c:1666ht_inode_ctx_time_update] (-->/usr/lib64/glusterfs/3.8.4/xlator/cluster/replicate.so(+0x5d75c) #1395648 : ganesha-ha.conf --status should validate if the VIPs are assigned to right nodes #1395660 : Checkpoint completed event missing master node detail #1395687 : Client side IObuff leaks at a high pace consumes complete client memory and hence making gluster volume inaccessible #1395993 : heal info --xml when bricks are down in a systemic environment is not displaying anything even after more than 30minutes #1396038 : refresh-config fails and crashes ganesha when mdcache is enabled on the volume. #1396048 : A hard link is lost during rebalance+lookup #1396062 : [geo-rep]: Worker crashes seen while renaming directories in loop #1396081 : Wrong value in Last Synced column during Hybrid Crawl #1396364 : Scheduler : Scheduler should not depend on glusterfs-events package #1396793 : [Ganesha] : Ganesha crashes intermittently during nfs-ganesha restarts. #1396807 : capture volume tunables in get-state dump #1396952 : I/O errors on FUSE mount point when reading and writing from 2 clients #1397052 : OOM kill of nfs-ganesha on one node while fs-sanity test suite is executed. #1397177 : memory leak when using libgfapi #1397419 : glusterfs_ctx_defaults_init is re-initializing ctx->locks #1397424 : PEER_REJECT, EVENT_BRICKPATH_RESOLVE_FAILED, EVENT_COMPARE_FRIEND_VOLUME_FAILED are not seen #1397754 : [SAMBA-CIFS] : IO hungs in cifs mount while graph switch on & off #1397795 : NFS-Ganesha:Volume reset for any option causes reset of ganesha enable option and bring down the ganesha services #1398076 : SEEK_HOLE/ SEEK_DATA doesn't return the correct offset #1398226 : With compound fops on, client process crashes when a replica is brought down while IO is in progress #1398566 : self-heal info command hangs after triggering self-heal #1399031 : build: add systemd dependency to glusterfs sub-package #1399072 : [Disperse] healing should not start if only data bricks are UP #1399134 : GlusterFS client crashes during remove-brick operation #1399154 : After ganesha node reboot/shutdown, portblock process goes to FAILED state #1399186 : [GANESHA] Export ID changed during volume start and stop with message \"lookup_export failed with Export id not found\" in ganesha.log #1399578 : [compound FOPs]: Memory leak while doing FOPs with brick down #1399592 : Memory leak when self healing daemon queue is full #1399780 : Use standard refcounting for structures where possible #1399995 : Dump volume specific options in get-state output in a more parseable manner #1400013 : [USS,SSL] .snaps directory is not reachable when I/O encryption (SSL) is enabled #1400026 : Duplicate value assigned to GD_MSG_DAEMON_STATE_REQ_RCVD and GD_MSG_BRICK_CLEANUP_SUCCESS messages #1400237 : Ganesha services are not stopped when pacemaker quorum is lost #1400613 : [GANESHA] failed to create directory of hostname of new node in var/lib/nfs/ganesha/ in already existing cluster nodes #1400818 : possible memory leak on client when writing to a file while another client issues a truncate #1401095 : log the error when locking the brick directory fails #1401218 : Fix compound fops memory leaks #1401404 : [Arbiter] IO's Halted and heal info command hung #1401777 : atime becomes zero when truncating file via ganesha (or gluster-NFS) #1401801 : [RFE] Use Host UUID to find local nodes to spawn workers #1401812 : RFE: Make readdirp parallel in dht #1401822 : [GANESHA]Unable to export the ganesha volume after doing volume start and stop #1401836 : update documentation to readthedocs.io #1401921 : glusterfsd crashed while taking snapshot using scheduler #1402237 : Bad spacing in error message in cli #1402261 : cli: compile warnings (unused var) if building without bd xlator #1402369 : Getting the warning message while erasing the gluster \"glusterfs-server\" package. #1402710 : ls and move hung on disperse volume #1402730 : self-heal not happening, as self-heal info lists the same pending shards to be healed #1402828 : Snapshot: Snapshot create command fails when gluster-shared-storage volume is stopped #1402841 : Files remain unhealed forever if shd is disabled and re-enabled while healing is in progress. #1403130 : [GANESHA] Adding a node to cluster failed to allocate resource-agents to new node. #1403780 : Incorrect incrementation of volinfo refcnt during volume start #1404118 : Snapshot: After snapshot restore failure , snapshot goes into inconsistent state #1404168 : Upcall: Possible use after free when log level set to TRACE #1404181 : [Ganesha+SSL] : Ganesha crashes on all nodes on volume restarts #1404410 : [Perf] : pcs cluster resources went into stopped state during Multithreaded perf tests on RHGS layered over RHEL 6 #1404573 : tests/bugs/snapshot/bug-1316437.t test is causing spurious failure #1404678 : [geo-rep]: Config commands fail when the status is 'Created' #1404905 : DHT : file rename operation is successful but log has error 'key:trusted.glusterfs.dht.linkto error:File exists' , 'setting xattrs on failed (File exists)' #1405165 : Allow user to disable mem-pool #1405301 : Fix the failure in tests/basic/gfapi/bug1291259.t #1405478 : Keepalive should be set for IPv6 & IPv4 #1405554 : Fix spurious failure in bug-1402841.t-mt-dir-scan-race.t #1405775 : GlusterFS process crashed after add-brick #1405902 : Fix spurious failure in tests/bugs/replicate/bug-1402730.t #1406224 : VM pauses due to storage I/O error, when one of the data brick is down with arbiter/replica volume #1406249 : [GANESHA] Deleting a node from ganesha cluster deletes the volume entry from /etc/ganesha/ganesha.conf file #1406252 : Free xdr-allocated compound request and response arrays #1406348 : [Eventing]: POSIX_SAME_GFID event seen for .trashcan folder and .trashcan/internal_op #1406410 : [GANESHA] Adding node to ganesha cluster is not assigning the correct VIP to the new node #1406411 : Fail add-brick command if replica count changes #1406878 : ec prove tests fail in FB build environment. #1408115 : Remove-brick rebalance failed while rm -rf is in progress #1408131 : Remove tests/distaf #1408395 : [Arbiter] After Killing a brick writes drastically slow down #1408712 : with granular-entry-self-heal enabled i see that there is a gfid mismatch and vm goes to paused state after migrating to another host #1408755 : Remove tests/basic/rpm.t #1408757 : Fix failure of split-brain-favorite-child-policy.t in CentOS7 #1408758 : tests/bugs/glusterd/bug-913555.t fails spuriously #1409078 : RFE: Need a command to check op-version compatibility of clients #1409186 : Dict_t leak in dht_migration_complete_check_task and dht_rebalance_inprogress_task #1409202 : Warning messages throwing when EC volume offline brick comes up are difficult to understand for end user. #1409206 : Extra lookup/fstats are sent over the network when a brick is down. #1409727 : [ganesha + EC]posix compliance rename tests failed on EC volume with nfs-ganesha mount. #1409730 : [ganesha+ec]: Contents of original file are not seen when hardlink is created #1410071 : [Geo-rep] Geo replication status detail without master and slave volume args #1410313 : brick crashed on systemic setup #1410355 : Remove-brick rebalance failed while rm -rf is in progress #1410375 : [Mdcache] clients being served wrong information about a file, can lead to file inconsistency #1410777 : ganesha service crashed on all nodes of ganesha cluster on disperse volume when doing lookup while copying files remotely using scp #1410853 : glusterfs-server should depend on firewalld-filesystem #1411607 : [Geo-rep] If for some reason MKDIR failed to sync, it should not proceed further. #1411625 : Spurious split-brain error messages are seen in rebalance logs #1411999 : URL to Fedora distgit no longer uptodate #1412002 : Examples/getvolfile.py is not pep8 compliant #1412069 : No rollback of renames on succeeded subvols during failure #1412174 : Memory leak on mount/fuse when setxattr fails #1412467 : Remove tests/bugs/distribute/bug-1063230.t #1412489 : Upcall: Possible memleak if inode_ctx_set fails #1412689 : [Geo-rep] Slave mount log file is cluttered by logs of multiple active mounts #1412917 : OOM kill of glusterfsd during continuous add-bricks #1412918 : fuse: Resource leak in fuse-helper under GF_SOLARIS_HOST_OS #1413967 : geo-rep session faulty with ChangelogException \"No such file or directory\" #1415226 : packaging: python/python2(/python3) cleanup #1415245 : core: max op version #1415279 : libgfapi: remove/revert glfs_ipc() changes targeted for 4.0 #1415581 : RFE : Create trash directory only when its is enabled #1415915 : RFE: An administrator friendly way to determine rebalance completion time #1415918 : Cache security.ima xattrs as well #1416285 : EXPECT_WITHIN is taking too much time even if the result matches with expected value #1416416 : Improve output of \"gluster volume status detail\" #1417027 : option performance.parallel-readdir should honor cluster.readdir-optimize #1417028 : option performance.parallel-readdir can cause OOM in large volumes #1417042 : glusterd restart is starting the offline shd daemon on other node in the cluster #1417135 : [Stress] : SHD Logs flooded with \"Heal Failed\" messages,filling up \"/\" quickly #1417521 : [SNAPSHOT] With all USS plugin enable .snaps directory is not visible in cifs mount as well as windows mount #1417527 : glusterfind: After glusterfind pre command execution all temporary files and directories /usr/var/lib/misc/glusterfsd/glusterfind/ / / should be removed #1417804 : debug/trace: Print iatts of individual entries in readdirp callback for better debugging experience #1418091 : [RFE] Support multiple bricks in one process (multiplexing) #1418536 : Portmap allocates way too much memory (256KB) on stack #1418541 : [Ganesha+SSL] : Bonnie++ hangs during rewrites. #1418623 : client process crashed due to write behind translator #1418650 : Samba crash when mounting a distributed dispersed volume over CIFS #1418981 : Unable to take Statedump for gfapi applications #1419305 : disable client.io-threads on replica volume creation #1419306 : [RFE] Need to have group cli option to set all md-cache options using a single command #1419503 : [SAMBA-SSL] Volume Share hungs when multiple mount & unmount is performed over a windows client on a SSL enabled cluster #1419696 : Fix spurious failure of ec-background-heal.t and tests/bitrot/bug-1373520.t #1419824 : repeated operation failed warnings in gluster mount logs with disperse volume #1419825 : Sequential and Random Writes are off target by 12% and 22% respectively on EC backed volumes over FUSE #1419846 : removing warning related to enum, to let the build take place without errors for 3.10 #1419855 : [Remove-brick] Hardlink migration fails with \"lookup failed (No such file or directory)\" error messages in rebalance logs #1419868 : removing old tier commands under the rebalance commands #1420606 : glusterd is crashed at the time of stop volume #1420808 : Trash feature improperly disabled #1420810 : Massive xlator_t leak in graph-switch code #1420982 : Automatic split brain resolution must check for all the bricks to be up to avoiding serving of inconsistent data(visible on x3 or more) #1420987 : warning messages seen in glusterd logs while setting the volume option #1420989 : when server-quorum is enabled, volume get returns 0 value for server-quorum-ratio #1420991 : Modified volume options not synced once offline nodes comes up. #1421017 : CLI option \"--timeout\" is accepting non numeric and negative values. #1421956 : Disperse: Fallback to pre-compiled code execution when dynamic code generation fails #1422350 : glustershd process crashed on systemic setup #1422363 : [Replicate] \"RPC call decoding failed\" leading to IO hang & mount inaccessible #1422391 : Gluster NFS server crashing in __mnt3svc_umountall #1422766 : Entry heal messages in glustershd.log while no entries shown in heal info #1422777 : DHT doesn't evenly balance files on FreeBSD with ZFS #1422819 : [Geo-rep] Recreating geo-rep session with same slave after deleting with reset-sync-time fails to sync #1422942 : Prevent reverse heal from happening #1423063 : glusterfs-fuse RPM now depends on gfapi #1423070 : Bricks not coming up when ran with address sanitizer #1423385 : Crash in index xlator because of race in inode_ctx_set and inode_ref #1423406 : Need to improve remove-brick failure message when the brick process is down. #1423412 : Mount of older client fails #1423429 : unnecessary logging in rda_opendir #1424921 : dht_setxattr returns EINVAL when a file is deleted during the FOP #1424931 : [RFE] Include few more options in virt file #1424937 : multiple glusterfsd process crashed making the complete subvolume unavailable #1424973 : remove-brick status shows 0 rebalanced files #1425556 : glusterd log is flooded with stale disconnect rpc messages","title":"Bugs addressed"},{"location":"release-notes/3.10.1/","text":"Release notes for Gluster 3.10.1 This is a bugfix release. The release notes for 3.10.0 , contains a listing of all the new features that were added and bugs in the GlusterFS 3.10 stable release. Major changes, features and limitations addressed in this release auth-allow setting was broken with 3.10 release and is now fixed (#1429117) Major issues Expanding a gluster volume that is sharded may cause file corruption Sharded volumes are typically used for VM images, if such volumes are expanded or possibly contracted (i.e add/remove bricks and rebalance) there are reports of VM images getting corrupted. If you are using sharded volumes, DO NOT rebalance them till this is fixed Status of this bug can be tracked here, #1426508 Bugs addressed A total of 31 patches have been merged, addressing 26 bugs: #1419824 : repeated operation failed warnings in gluster mount logs with disperse volume #1422769 : brick process crashes when glusterd is restarted #1422781 : Transport endpoint not connected error seen on client when glusterd is restarted #1426222 : build: fixes to build 3.9.0rc2 on Debian (jessie) #1426323 : common-ha: no need to remove nodes one-by-one in teardown #1426329 : [Ganesha] : Add comment to Ganesha HA config file ,about cluster name's length limitation #1427387 : systemic testing: seeing lot of ping time outs which would lead to splitbrains #1427399 : [RFE] capture portmap details in glusterd's statedump #1427461 : Bricks take up new ports upon volume restart after add-brick op with brick mux enabled #1428670 : Disconnects in nfs mount leads to IO hang and mount inaccessible #1428739 : Fix crash in dht resulting from tests/features/nuke.t #1429117 : auth failure after upgrade to GlusterFS 3.10 #1429402 : Restore atime/mtime for symlinks and other non-regular files. #1429773 : disallow increasing replica count for arbiter volumes #1430512 : /libgfxdr.so.0.0.1: undefined symbol: __gf_free #1430844 : build/packaging: Debian and Ubuntu don't have /usr/libexec/; results in bad packages #1431175 : volume start command hangs #1431176 : USS is broken when multiplexing is on #1431591 : memory leak in features/locks xlator #1434296 : [Disperse] Metadata version is not healing when a brick is down #1434303 : Move spit-brain msg in read txn to debug #1434399 : glusterd crashes when peering an IP where the address is more than acceptable range (>255) OR with random hostnames #1435946 : When parallel readdir is enabled and there are simultaneous readdir and disconnects, then it results in crash #1436203 : Undo pending xattrs only on the up bricks #1436411 : Unrecognized filesystems (i.e. btrfs, zfs) log many errors about \"getinode size\" #1437326 : Sharding: Fix a performance bug","title":"3.10.1"},{"location":"release-notes/3.10.1/#release-notes-for-gluster-3101","text":"This is a bugfix release. The release notes for 3.10.0 , contains a listing of all the new features that were added and bugs in the GlusterFS 3.10 stable release.","title":"Release notes for Gluster 3.10.1"},{"location":"release-notes/3.10.1/#major-changes-features-and-limitations-addressed-in-this-release","text":"auth-allow setting was broken with 3.10 release and is now fixed (#1429117)","title":"Major changes, features and limitations addressed in this release"},{"location":"release-notes/3.10.1/#major-issues","text":"Expanding a gluster volume that is sharded may cause file corruption Sharded volumes are typically used for VM images, if such volumes are expanded or possibly contracted (i.e add/remove bricks and rebalance) there are reports of VM images getting corrupted. If you are using sharded volumes, DO NOT rebalance them till this is fixed Status of this bug can be tracked here, #1426508","title":"Major issues"},{"location":"release-notes/3.10.1/#bugs-addressed","text":"A total of 31 patches have been merged, addressing 26 bugs: #1419824 : repeated operation failed warnings in gluster mount logs with disperse volume #1422769 : brick process crashes when glusterd is restarted #1422781 : Transport endpoint not connected error seen on client when glusterd is restarted #1426222 : build: fixes to build 3.9.0rc2 on Debian (jessie) #1426323 : common-ha: no need to remove nodes one-by-one in teardown #1426329 : [Ganesha] : Add comment to Ganesha HA config file ,about cluster name's length limitation #1427387 : systemic testing: seeing lot of ping time outs which would lead to splitbrains #1427399 : [RFE] capture portmap details in glusterd's statedump #1427461 : Bricks take up new ports upon volume restart after add-brick op with brick mux enabled #1428670 : Disconnects in nfs mount leads to IO hang and mount inaccessible #1428739 : Fix crash in dht resulting from tests/features/nuke.t #1429117 : auth failure after upgrade to GlusterFS 3.10 #1429402 : Restore atime/mtime for symlinks and other non-regular files. #1429773 : disallow increasing replica count for arbiter volumes #1430512 : /libgfxdr.so.0.0.1: undefined symbol: __gf_free #1430844 : build/packaging: Debian and Ubuntu don't have /usr/libexec/; results in bad packages #1431175 : volume start command hangs #1431176 : USS is broken when multiplexing is on #1431591 : memory leak in features/locks xlator #1434296 : [Disperse] Metadata version is not healing when a brick is down #1434303 : Move spit-brain msg in read txn to debug #1434399 : glusterd crashes when peering an IP where the address is more than acceptable range (>255) OR with random hostnames #1435946 : When parallel readdir is enabled and there are simultaneous readdir and disconnects, then it results in crash #1436203 : Undo pending xattrs only on the up bricks #1436411 : Unrecognized filesystems (i.e. btrfs, zfs) log many errors about \"getinode size\" #1437326 : Sharding: Fix a performance bug","title":"Bugs addressed"},{"location":"release-notes/3.10.10/","text":"Release notes for Gluster 3.10.10 This is a bugfix release. The release notes for 3.10.0 , 3.10.1 , 3.10.2 , 3.10.3 , 3.10.4 , 3.10.5 , 3.10.6 , 3.10.7 , 3.10.8 and 3.10.9 contain a listing of all the new features that were added and bugs fixed in the GlusterFS 3.10 stable release. Major changes, features and limitations addressed in this release No Major changes Major issues Brick multiplexing is being tested and fixed aggressively but we still have a few crashes and memory leaks to fix. Bugs addressed Bugs addressed since release-3.10.9 are listed below. #1498081 : dht_(f)xattrop does not implement migration checks #1534848 : entries not getting cleared post healing of softlinks (stale entries showing up in heal info)","title":"3.10.10"},{"location":"release-notes/3.10.10/#release-notes-for-gluster-31010","text":"This is a bugfix release. The release notes for 3.10.0 , 3.10.1 , 3.10.2 , 3.10.3 , 3.10.4 , 3.10.5 , 3.10.6 , 3.10.7 , 3.10.8 and 3.10.9 contain a listing of all the new features that were added and bugs fixed in the GlusterFS 3.10 stable release.","title":"Release notes for Gluster 3.10.10"},{"location":"release-notes/3.10.10/#major-changes-features-and-limitations-addressed-in-this-release","text":"No Major changes","title":"Major changes, features and limitations addressed in this release"},{"location":"release-notes/3.10.10/#major-issues","text":"Brick multiplexing is being tested and fixed aggressively but we still have a few crashes and memory leaks to fix.","title":"Major issues"},{"location":"release-notes/3.10.10/#bugs-addressed","text":"Bugs addressed since release-3.10.9 are listed below. #1498081 : dht_(f)xattrop does not implement migration checks #1534848 : entries not getting cleared post healing of softlinks (stale entries showing up in heal info)","title":"Bugs addressed"},{"location":"release-notes/3.10.11/","text":"Release notes for Gluster 3.10.11 This is a bugfix release. The release notes for 3.10.0 , 3.10.1 , 3.10.2 , 3.10.3 , 3.10.4 , 3.10.5 , 3.10.6 , 3.10.7 , 3.10.8 , 3.10.9 and 3.10.10 contain a listing of all the new features that were added and bugs fixed in the GlusterFS 3.10 stable release. Major changes, features and limitations addressed in this release No Major changes Major issues Brick multiplexing is being tested and fixed aggressively but we still have a few crashes and memory leaks to fix. Bugs addressed Bugs addressed since release-3.10.10 are listed below. #1486542 : \"ganesha.so cannot open\" warning message in glusterd log in non ganesha setup. #1544461 : 3.8 -> 3.10 rolling upgrade fails (same for 3.12 or 3.13) on Ubuntu 14 #1544787 : tests/bugs/cli/bug-1169302.t fails spuriously #1546912 : tests/bugs/posix/bug-990028.t fails in release-3.10 branch #1549482 : Quota: After deleting directory from mount point on which quota was configured, quota list command output is blank","title":"3.10.11"},{"location":"release-notes/3.10.11/#release-notes-for-gluster-31011","text":"This is a bugfix release. The release notes for 3.10.0 , 3.10.1 , 3.10.2 , 3.10.3 , 3.10.4 , 3.10.5 , 3.10.6 , 3.10.7 , 3.10.8 , 3.10.9 and 3.10.10 contain a listing of all the new features that were added and bugs fixed in the GlusterFS 3.10 stable release.","title":"Release notes for Gluster 3.10.11"},{"location":"release-notes/3.10.11/#major-changes-features-and-limitations-addressed-in-this-release","text":"No Major changes","title":"Major changes, features and limitations addressed in this release"},{"location":"release-notes/3.10.11/#major-issues","text":"Brick multiplexing is being tested and fixed aggressively but we still have a few crashes and memory leaks to fix.","title":"Major issues"},{"location":"release-notes/3.10.11/#bugs-addressed","text":"Bugs addressed since release-3.10.10 are listed below. #1486542 : \"ganesha.so cannot open\" warning message in glusterd log in non ganesha setup. #1544461 : 3.8 -> 3.10 rolling upgrade fails (same for 3.12 or 3.13) on Ubuntu 14 #1544787 : tests/bugs/cli/bug-1169302.t fails spuriously #1546912 : tests/bugs/posix/bug-990028.t fails in release-3.10 branch #1549482 : Quota: After deleting directory from mount point on which quota was configured, quota list command output is blank","title":"Bugs addressed"},{"location":"release-notes/3.10.12/","text":"Release notes for Gluster 3.10.12 This is a bugfix release. The release notes for 3.10.0 , 3.10.1 , 3.10.2 , 3.10.3 , 3.10.4 , 3.10.5 , 3.10.6 , 3.10.7 , 3.10.8 , 3.10.9 , 3.10.10 and 3.10.11 contain a listing of all the new features that were added and bugs fixed in the GlusterFS 3.10 stable release. Major changes, features and limitations addressed in this release This release contains a fix for a security vulerability in Gluster as follows, - http://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2018-1088 - https://nvd.nist.gov/vuln/detail/CVE-2018-1088 Installing the updated packages and restarting gluster services, will update the Gluster shared storage volume volfiles, that are more secure than the defaults currently in place. Further, for increased security, the Gluster shared storage volume can be TLS enabled, and access to the same restricted using the auth.ssl-allow option. See, this guide for more details. Major issues Brick multiplexing is being tested and fixed aggressively but we still have a few crashes and memory leaks to fix. Bugs addressed Bugs addressed since release-3.10.11 are listed below. #1553777 : /var/log/glusterfs/bricks/export_vdb.log flooded with this error message \"Not able to add to index [Too many links]\" #1555195 : [Ganesha] Duplicate volume export entries in ganesha.conf causing volume unexport to fail #1555203 : After a replace brick command, self-heal takes some time to start healing files on disperse volumes #1557304 : [Glusterd] Volume operations fail on a (tiered) volume because of a stale lock held by one of the nodes #1559352 : [Ganesha] : Ganesha crashes while cluster enters failover/failback mode #1561732 : Rebalance failures on a dispersed volume with lookup-optimize enabled #1563500 : nfs-ganesha: in case pcs cluster setup fails then nfs-ganesha process should not start #1569409 : EIO errors on some operations when volume has mixed brick versions on a disperse volume #1570428 : CVE-2018-1088 glusterfs: Privilege escalation via gluster_shared_storage when snapshot scheduling is enabled [fedora-all]","title":"3.10.12"},{"location":"release-notes/3.10.12/#release-notes-for-gluster-31012","text":"This is a bugfix release. The release notes for 3.10.0 , 3.10.1 , 3.10.2 , 3.10.3 , 3.10.4 , 3.10.5 , 3.10.6 , 3.10.7 , 3.10.8 , 3.10.9 , 3.10.10 and 3.10.11 contain a listing of all the new features that were added and bugs fixed in the GlusterFS 3.10 stable release.","title":"Release notes for Gluster 3.10.12"},{"location":"release-notes/3.10.12/#major-changes-features-and-limitations-addressed-in-this-release","text":"This release contains a fix for a security vulerability in Gluster as follows, - http://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2018-1088 - https://nvd.nist.gov/vuln/detail/CVE-2018-1088 Installing the updated packages and restarting gluster services, will update the Gluster shared storage volume volfiles, that are more secure than the defaults currently in place. Further, for increased security, the Gluster shared storage volume can be TLS enabled, and access to the same restricted using the auth.ssl-allow option. See, this guide for more details.","title":"Major changes, features and limitations addressed in this release"},{"location":"release-notes/3.10.12/#major-issues","text":"Brick multiplexing is being tested and fixed aggressively but we still have a few crashes and memory leaks to fix.","title":"Major issues"},{"location":"release-notes/3.10.12/#bugs-addressed","text":"Bugs addressed since release-3.10.11 are listed below. #1553777 : /var/log/glusterfs/bricks/export_vdb.log flooded with this error message \"Not able to add to index [Too many links]\" #1555195 : [Ganesha] Duplicate volume export entries in ganesha.conf causing volume unexport to fail #1555203 : After a replace brick command, self-heal takes some time to start healing files on disperse volumes #1557304 : [Glusterd] Volume operations fail on a (tiered) volume because of a stale lock held by one of the nodes #1559352 : [Ganesha] : Ganesha crashes while cluster enters failover/failback mode #1561732 : Rebalance failures on a dispersed volume with lookup-optimize enabled #1563500 : nfs-ganesha: in case pcs cluster setup fails then nfs-ganesha process should not start #1569409 : EIO errors on some operations when volume has mixed brick versions on a disperse volume #1570428 : CVE-2018-1088 glusterfs: Privilege escalation via gluster_shared_storage when snapshot scheduling is enabled [fedora-all]","title":"Bugs addressed"},{"location":"release-notes/3.10.2/","text":"Release notes for Gluster 3.10.2 This is a bugfix release. The release notes for 3.10.0 and 3.10.1 contains a listing of all the new features that were added and bugs in the GlusterFS 3.10 stable release. Major changes, features and limitations addressed in this release Many bugs brick multiplexing and nfs-ganesha+ha bugs have been addressed. Rebalance and remove brick operations have been disabled for sharded volumes to prevent data corruption. Major issues Expanding a gluster volume that is sharded may cause file corruption Sharded volumes are typically used for VM images, if such volumes are expanded or possibly contracted (i.e add/remove bricks and rebalance) there are reports of VM images getting corrupted. Status of this bug can be tracked here, #1426508 Bugs addressed A total of 63 patches have been merged, addressing 46 bugs: #1437854 : Spellcheck issues reported during Debian build #1425726 : Stale export entries in ganesha.conf after executing \"gluster nfs-ganesha disable\" #1427079 : [Ganesha] : unexport fails if export configuration file is not present #1440148 : common-ha (debian/ubuntu): ganesha-ha.sh has a hard-coded /usr/libexec/ganesha... #1443478 : RFE: Support to update NFS-Ganesha export options dynamically #1443490 : [Nfs-ganesha] Refresh config fails when ganesha cluster is in failover mode. #1441474 : synclocks don't work correctly under contention #1449002 : [Brick Multiplexing] : Bricks for multiple volumes going down after glusterd restart and not coming back up after volume start force #1438813 : Segmentation fault when creating a qcow2 with qemu-img #1438423 : [Ganesha + EC] : Input/Output Error while creating LOTS of smallfiles #1444540 : rm -rf \\<dir> returns ENOTEMPTY even though ls on the mount point returns no files #1446227 : Incorrect and redundant logs in the DHT rmdir code path #1447608 : Don't allow rebalance/fix-layout operation on sharding enabled volumes till dht+sharding bugs are fixed #1448864 : Seeing error \"Failed to get the total number of files. Unable to estimate time to complete rebalance\" in rebalance logs #1443349 : [Eventing]: Unrelated error message displayed when path specified during a 'webhook-test/add' is missing a schema #1441576 : [geo-rep]: rsync should not try to sync internal xattrs #1441927 : [geo-rep]: Worker crashes with [Errno 16] Device or resource busy: '.gfid/00000000-0000-0000-0000-000000000001/dir.166 while renaming directories #1401877 : [GANESHA] Symlinks from /etc/ganesha/ganesha.conf to shared_storage are created on the non-ganesha nodes in 8 node gluster having 4 node ganesha cluster #1425723 : nfs-ganesha volume export file remains stale in shared_storage_volume when volume is deleted #1427759 : nfs-ganesha: Incorrect error message returned when disable fails #1438325 : Need to improve remove-brick failure message when the brick process is down. #1438338 : glusterd is setting replicate volume property over disperse volume or vice versa #1438340 : glusterd is not validating for allowed values while setting \"cluster.brick-multiplex\" property #1441476 : Glusterd crashes when restarted with many volumes #1444128 : [BrickMultiplex] gluster command not responding and .snaps directory is not visible after executing snapshot related command #1445260 : [GANESHA] Volume start and stop having ganesha enable on it,turns off cache-invalidation on volume #1445408 : gluster volume stop hangs #1449934 : Brick Multiplexing :- resetting a brick bring down other bricks with same PID #1435779 : Inode ref leak on anonymous reads and writes #1440278 : [GSS] NFS Sub-directory mount not working on solaris10 client #1450378 : GNFS crashed while taking lock on a file from 2 different clients having same volume mounted from 2 different servers #1449779 : quota: limit-usage command failed with error \" Failed to start aux mount\" #1450564 : glfsheal: crashed(segfault) with disperse volume in RDMA #1443501 : Don't wind post-op on a brick where the fop phase failed. #1444892 : When either killing or restarting a brick with performance.stat-prefetch on, stat sometimes returns a bad st_size value. #1449169 : Multiple bricks WILL crash after TCP port probing #1440805 : Update rfc.sh to check Change-Id consistency for backports #1443010 : snapshot: snapshots appear to be failing with respect to secure geo-rep slave #1445209 : snapshot: Unable to take snapshot on a geo-replicated volume, even after stopping the session #1444773 : explicitly specify executor to be bash for tests #1445407 : remove bug-1421590-brick-mux-reuse-ports.t #1440742 : Test files clean up for tier during 3.10 #1448790 : [Tiering]: High and low watermark values when set to the same level, is allowed #1435942 : Enabling parallel-readdir causes dht linkto files to be visible on the mount, #1437763 : File-level WORM allows ftruncate() on read-only files #1439148 : Parallel readdir on Gluster NFS displays less number of dentries","title":"3.10.2"},{"location":"release-notes/3.10.2/#release-notes-for-gluster-3102","text":"This is a bugfix release. The release notes for 3.10.0 and 3.10.1 contains a listing of all the new features that were added and bugs in the GlusterFS 3.10 stable release.","title":"Release notes for Gluster 3.10.2"},{"location":"release-notes/3.10.2/#major-changes-features-and-limitations-addressed-in-this-release","text":"Many bugs brick multiplexing and nfs-ganesha+ha bugs have been addressed. Rebalance and remove brick operations have been disabled for sharded volumes to prevent data corruption.","title":"Major changes, features and limitations addressed in this release"},{"location":"release-notes/3.10.2/#major-issues","text":"Expanding a gluster volume that is sharded may cause file corruption Sharded volumes are typically used for VM images, if such volumes are expanded or possibly contracted (i.e add/remove bricks and rebalance) there are reports of VM images getting corrupted. Status of this bug can be tracked here, #1426508","title":"Major issues"},{"location":"release-notes/3.10.2/#bugs-addressed","text":"A total of 63 patches have been merged, addressing 46 bugs: #1437854 : Spellcheck issues reported during Debian build #1425726 : Stale export entries in ganesha.conf after executing \"gluster nfs-ganesha disable\" #1427079 : [Ganesha] : unexport fails if export configuration file is not present #1440148 : common-ha (debian/ubuntu): ganesha-ha.sh has a hard-coded /usr/libexec/ganesha... #1443478 : RFE: Support to update NFS-Ganesha export options dynamically #1443490 : [Nfs-ganesha] Refresh config fails when ganesha cluster is in failover mode. #1441474 : synclocks don't work correctly under contention #1449002 : [Brick Multiplexing] : Bricks for multiple volumes going down after glusterd restart and not coming back up after volume start force #1438813 : Segmentation fault when creating a qcow2 with qemu-img #1438423 : [Ganesha + EC] : Input/Output Error while creating LOTS of smallfiles #1444540 : rm -rf \\<dir> returns ENOTEMPTY even though ls on the mount point returns no files #1446227 : Incorrect and redundant logs in the DHT rmdir code path #1447608 : Don't allow rebalance/fix-layout operation on sharding enabled volumes till dht+sharding bugs are fixed #1448864 : Seeing error \"Failed to get the total number of files. Unable to estimate time to complete rebalance\" in rebalance logs #1443349 : [Eventing]: Unrelated error message displayed when path specified during a 'webhook-test/add' is missing a schema #1441576 : [geo-rep]: rsync should not try to sync internal xattrs #1441927 : [geo-rep]: Worker crashes with [Errno 16] Device or resource busy: '.gfid/00000000-0000-0000-0000-000000000001/dir.166 while renaming directories #1401877 : [GANESHA] Symlinks from /etc/ganesha/ganesha.conf to shared_storage are created on the non-ganesha nodes in 8 node gluster having 4 node ganesha cluster #1425723 : nfs-ganesha volume export file remains stale in shared_storage_volume when volume is deleted #1427759 : nfs-ganesha: Incorrect error message returned when disable fails #1438325 : Need to improve remove-brick failure message when the brick process is down. #1438338 : glusterd is setting replicate volume property over disperse volume or vice versa #1438340 : glusterd is not validating for allowed values while setting \"cluster.brick-multiplex\" property #1441476 : Glusterd crashes when restarted with many volumes #1444128 : [BrickMultiplex] gluster command not responding and .snaps directory is not visible after executing snapshot related command #1445260 : [GANESHA] Volume start and stop having ganesha enable on it,turns off cache-invalidation on volume #1445408 : gluster volume stop hangs #1449934 : Brick Multiplexing :- resetting a brick bring down other bricks with same PID #1435779 : Inode ref leak on anonymous reads and writes #1440278 : [GSS] NFS Sub-directory mount not working on solaris10 client #1450378 : GNFS crashed while taking lock on a file from 2 different clients having same volume mounted from 2 different servers #1449779 : quota: limit-usage command failed with error \" Failed to start aux mount\" #1450564 : glfsheal: crashed(segfault) with disperse volume in RDMA #1443501 : Don't wind post-op on a brick where the fop phase failed. #1444892 : When either killing or restarting a brick with performance.stat-prefetch on, stat sometimes returns a bad st_size value. #1449169 : Multiple bricks WILL crash after TCP port probing #1440805 : Update rfc.sh to check Change-Id consistency for backports #1443010 : snapshot: snapshots appear to be failing with respect to secure geo-rep slave #1445209 : snapshot: Unable to take snapshot on a geo-replicated volume, even after stopping the session #1444773 : explicitly specify executor to be bash for tests #1445407 : remove bug-1421590-brick-mux-reuse-ports.t #1440742 : Test files clean up for tier during 3.10 #1448790 : [Tiering]: High and low watermark values when set to the same level, is allowed #1435942 : Enabling parallel-readdir causes dht linkto files to be visible on the mount, #1437763 : File-level WORM allows ftruncate() on read-only files #1439148 : Parallel readdir on Gluster NFS displays less number of dentries","title":"Bugs addressed"},{"location":"release-notes/3.10.3/","text":"Release notes for Gluster 3.10.3 This is a bugfix release. The release notes for 3.10.0 , 3.10.1 and 3.10.2 contain a listing of all the new features that were added and bugs in the GlusterFS 3.10 stable release. Major changes, features and limitations addressed in this release No Major changes Major issues Expanding a gluster volume that is sharded may cause file corruption Sharded volumes are typically used for VM images, if such volumes are expanded or possibly contracted (i.e add/remove bricks and rebalance) there are reports of VM images getting corrupted. Status of this bug can be tracked here, #1426508 Brick multiplexing is being tested and fixed aggressively but we still have a few crashes and memory leaks to fix. Bugs addressed A total of 18 patches have been merged, addressing 13 bugs: #1450053 : [GANESHA] Adding a node to existing cluster failed to start pacemaker service on new node #1450773 : Quota: After upgrade from 3.7 to higher version , gluster quota list command shows \"No quota configured on volume repvol\" #1450934 : [New] - Replacing an arbiter brick while I/O happens causes vm pause #1450947 : Autoconf leaves unexpanded variables in path names of non-shell-scripttext files #1451371 : crash in dht_rmdir_do #1451561 : AFR returns the node uuid of the same node for every file in the replica #1451587 : cli xml status of detach tier broken #1451977 : Add logs to identify whether disconnects are voluntary or due to network problems #1451995 : Log message shows error code as success even when rpc fails to connect #1453056 : [DHt] : segfault in dht_selfheal_dir_setattr while running regressions #1453087 : Brick Multiplexing: On reboot of a node Brick multiplexing feature lost on that node as multiple brick processes get spawned #1456682 : tierd listens to a port. #1457054 : glusterfs client crash on io-cache.so(__ioc_page_wakeup+0x44)","title":"3.10.3"},{"location":"release-notes/3.10.3/#release-notes-for-gluster-3103","text":"This is a bugfix release. The release notes for 3.10.0 , 3.10.1 and 3.10.2 contain a listing of all the new features that were added and bugs in the GlusterFS 3.10 stable release.","title":"Release notes for Gluster 3.10.3"},{"location":"release-notes/3.10.3/#major-changes-features-and-limitations-addressed-in-this-release","text":"No Major changes","title":"Major changes, features and limitations addressed in this release"},{"location":"release-notes/3.10.3/#major-issues","text":"Expanding a gluster volume that is sharded may cause file corruption Sharded volumes are typically used for VM images, if such volumes are expanded or possibly contracted (i.e add/remove bricks and rebalance) there are reports of VM images getting corrupted. Status of this bug can be tracked here, #1426508 Brick multiplexing is being tested and fixed aggressively but we still have a few crashes and memory leaks to fix.","title":"Major issues"},{"location":"release-notes/3.10.3/#bugs-addressed","text":"A total of 18 patches have been merged, addressing 13 bugs: #1450053 : [GANESHA] Adding a node to existing cluster failed to start pacemaker service on new node #1450773 : Quota: After upgrade from 3.7 to higher version , gluster quota list command shows \"No quota configured on volume repvol\" #1450934 : [New] - Replacing an arbiter brick while I/O happens causes vm pause #1450947 : Autoconf leaves unexpanded variables in path names of non-shell-scripttext files #1451371 : crash in dht_rmdir_do #1451561 : AFR returns the node uuid of the same node for every file in the replica #1451587 : cli xml status of detach tier broken #1451977 : Add logs to identify whether disconnects are voluntary or due to network problems #1451995 : Log message shows error code as success even when rpc fails to connect #1453056 : [DHt] : segfault in dht_selfheal_dir_setattr while running regressions #1453087 : Brick Multiplexing: On reboot of a node Brick multiplexing feature lost on that node as multiple brick processes get spawned #1456682 : tierd listens to a port. #1457054 : glusterfs client crash on io-cache.so(__ioc_page_wakeup+0x44)","title":"Bugs addressed"},{"location":"release-notes/3.10.4/","text":"Release notes for Gluster 3.10.4 This is a bugfix release. The release notes for 3.10.0 , 3.10.1 , 3.10.2 and 3.10.3 contain a listing of all the new features that were added and bugs fixed in the GlusterFS 3.10 stable release. Major changes, features and limitations addressed in this release No Major changes Major issues Expanding a gluster volume that is sharded may cause file corruption Sharded volumes are typically used for VM images, if such volumes are expanded or possibly contracted (i.e add/remove bricks and rebalance) there are reports of VM images getting corrupted. Status of this bug can be tracked here, #1426508 Brick multiplexing is being tested and fixed aggressively but we still have a few crashes and memory leaks to fix. Another rebalance related bug is being worked upon #1467010 Bugs addressed A total of 18 patches have been merged, addressing 13 bugs: #1457732 : \"split-brain observed [Input/output error]\" error messages in samba logs during parallel rm -rf #1459760 : Glusterd segmentation fault in ' _Unwind_Backtrace' while running peer probe #1460649 : posix-acl: Whitelist virtual ACL xattrs #1460914 : Rebalance estimate time sometimes shows negative values #1460993 : Revert CLI restrictions on running rebalance in VM store use case #1461019 : [Ganesha] : Grace period is not being adhered to on RHEL 7.4; Clients continue running IO even during grace. #1462080 : [Bitrot]: Inconsistency seen with 'scrub ondemand' - fails to trigger scrub #1463623 : [Ganesha]Bricks got crashed while running posix compliance test suit on V4 mount #1463641 : [Ganesha] Ganesha service failed to start on new node added in existing ganeshacluster #1464078 : with AFR now making both nodes to return UUID for a file will result in georep consuming more resources #1466852 : assorted typos and spelling mistakes from Debian lintian #1466863 : dht_rename_lock_cbk crashes in upstream regression test #1467269 : Heal info shows incorrect status","title":"3.10.4"},{"location":"release-notes/3.10.4/#release-notes-for-gluster-3104","text":"This is a bugfix release. The release notes for 3.10.0 , 3.10.1 , 3.10.2 and 3.10.3 contain a listing of all the new features that were added and bugs fixed in the GlusterFS 3.10 stable release.","title":"Release notes for Gluster 3.10.4"},{"location":"release-notes/3.10.4/#major-changes-features-and-limitations-addressed-in-this-release","text":"No Major changes","title":"Major changes, features and limitations addressed in this release"},{"location":"release-notes/3.10.4/#major-issues","text":"Expanding a gluster volume that is sharded may cause file corruption Sharded volumes are typically used for VM images, if such volumes are expanded or possibly contracted (i.e add/remove bricks and rebalance) there are reports of VM images getting corrupted. Status of this bug can be tracked here, #1426508 Brick multiplexing is being tested and fixed aggressively but we still have a few crashes and memory leaks to fix. Another rebalance related bug is being worked upon #1467010","title":"Major issues"},{"location":"release-notes/3.10.4/#bugs-addressed","text":"A total of 18 patches have been merged, addressing 13 bugs: #1457732 : \"split-brain observed [Input/output error]\" error messages in samba logs during parallel rm -rf #1459760 : Glusterd segmentation fault in ' _Unwind_Backtrace' while running peer probe #1460649 : posix-acl: Whitelist virtual ACL xattrs #1460914 : Rebalance estimate time sometimes shows negative values #1460993 : Revert CLI restrictions on running rebalance in VM store use case #1461019 : [Ganesha] : Grace period is not being adhered to on RHEL 7.4; Clients continue running IO even during grace. #1462080 : [Bitrot]: Inconsistency seen with 'scrub ondemand' - fails to trigger scrub #1463623 : [Ganesha]Bricks got crashed while running posix compliance test suit on V4 mount #1463641 : [Ganesha] Ganesha service failed to start on new node added in existing ganeshacluster #1464078 : with AFR now making both nodes to return UUID for a file will result in georep consuming more resources #1466852 : assorted typos and spelling mistakes from Debian lintian #1466863 : dht_rename_lock_cbk crashes in upstream regression test #1467269 : Heal info shows incorrect status","title":"Bugs addressed"},{"location":"release-notes/3.10.5/","text":"Release notes for Gluster 3.10.5 This is a bugfix release. The release notes for 3.10.0 , 3.10.1 , 3.10.2 , 3.10.3 and 3.10.4 contain a listing of all the new features that were added and bugs fixed in the GlusterFS 3.10 stable release. Major changes, features and limitations addressed in this release No Major changes Major issues Expanding a gluster volume that is sharded may cause file corruption Sharded volumes are typically used for VM images, if such volumes are expanded or possibly contracted (i.e add/remove bricks and rebalance) there are reports of VM images getting corrupted. The last known cause for corruption #1467010 has a fix with this release. As further testing is still in progress, the issue is retained as a major issue. Brick multiplexing is being tested and fixed aggressively but we still have a few crashes and memory leaks to fix. Bugs addressed Bugs addressed since release-3.10.4 are listed below. #1467010 : Fd based fops fail with EBADF on file migration #1468126 : disperse seek does not correctly handle the end of file #1468198 : [Geo-rep]: entry failed to sync to slave with ENOENT errror #1470040 : packaging: Upgrade glusterfs-ganesha sometimes fails to semanage ganesha_use_fusefs #1470488 : gluster volume status --xml fails when there are 100 volumes #1471028 : glusterfs process leaking memory when error occurs #1471612 : metadata heal not happening despite having an active sink #1471870 : cthon04 can cause segfault in gNFS/NLM #1471917 : [GANESHA] Ganesha setup creation fails due to selinux blocking some services required for setup creation #1472446 : packaging: save ganesha config files in (/var)/run/gluster/shared_storage/nfs-ganesha #1473129 : dht/rebalance: Improve rebalance crawl performance #1473132 : dht/cluster: rebalance/remove-brick should honor min-free-disk #1473133 : dht/cluster: rebalance/remove-brick should honor min-free-disk #1473134 : The rebal-throttle setting does not work as expected #1473136 : rebalance: Allow admin to change thread count for rebalance #1473137 : dht: Make throttle option \"normal\" value uniform across dht_init and dht_reconfigure #1473140 : Fix on demand file migration from client #1473141 : cluster/dht: Fix hardlink migration failures #1475638 : [Scale] : Client logs flooded with \"inode context is NULL\" error messages #1476212 : [geo-rep]: few of the self healed hardlinks on master did not sync to slave #1478498 : scripts: invalid test in S32gluster_enable_shared_storage.sh #1478499 : packaging: /var/lib/glusterd/options should be %config(noreplace) #1480594 : nfs process crashed in \"nfs3_getattr\"","title":"3.10.5"},{"location":"release-notes/3.10.5/#release-notes-for-gluster-3105","text":"This is a bugfix release. The release notes for 3.10.0 , 3.10.1 , 3.10.2 , 3.10.3 and 3.10.4 contain a listing of all the new features that were added and bugs fixed in the GlusterFS 3.10 stable release.","title":"Release notes for Gluster 3.10.5"},{"location":"release-notes/3.10.5/#major-changes-features-and-limitations-addressed-in-this-release","text":"No Major changes","title":"Major changes, features and limitations addressed in this release"},{"location":"release-notes/3.10.5/#major-issues","text":"Expanding a gluster volume that is sharded may cause file corruption Sharded volumes are typically used for VM images, if such volumes are expanded or possibly contracted (i.e add/remove bricks and rebalance) there are reports of VM images getting corrupted. The last known cause for corruption #1467010 has a fix with this release. As further testing is still in progress, the issue is retained as a major issue. Brick multiplexing is being tested and fixed aggressively but we still have a few crashes and memory leaks to fix.","title":"Major issues"},{"location":"release-notes/3.10.5/#bugs-addressed","text":"Bugs addressed since release-3.10.4 are listed below. #1467010 : Fd based fops fail with EBADF on file migration #1468126 : disperse seek does not correctly handle the end of file #1468198 : [Geo-rep]: entry failed to sync to slave with ENOENT errror #1470040 : packaging: Upgrade glusterfs-ganesha sometimes fails to semanage ganesha_use_fusefs #1470488 : gluster volume status --xml fails when there are 100 volumes #1471028 : glusterfs process leaking memory when error occurs #1471612 : metadata heal not happening despite having an active sink #1471870 : cthon04 can cause segfault in gNFS/NLM #1471917 : [GANESHA] Ganesha setup creation fails due to selinux blocking some services required for setup creation #1472446 : packaging: save ganesha config files in (/var)/run/gluster/shared_storage/nfs-ganesha #1473129 : dht/rebalance: Improve rebalance crawl performance #1473132 : dht/cluster: rebalance/remove-brick should honor min-free-disk #1473133 : dht/cluster: rebalance/remove-brick should honor min-free-disk #1473134 : The rebal-throttle setting does not work as expected #1473136 : rebalance: Allow admin to change thread count for rebalance #1473137 : dht: Make throttle option \"normal\" value uniform across dht_init and dht_reconfigure #1473140 : Fix on demand file migration from client #1473141 : cluster/dht: Fix hardlink migration failures #1475638 : [Scale] : Client logs flooded with \"inode context is NULL\" error messages #1476212 : [geo-rep]: few of the self healed hardlinks on master did not sync to slave #1478498 : scripts: invalid test in S32gluster_enable_shared_storage.sh #1478499 : packaging: /var/lib/glusterd/options should be %config(noreplace) #1480594 : nfs process crashed in \"nfs3_getattr\"","title":"Bugs addressed"},{"location":"release-notes/3.10.6/","text":"Release notes for Gluster 3.10.6 This is a bugfix release. The release notes for 3.10.0 , 3.10.1 , 3.10.2 , 3.10.3 , 3.10.4 and 3.10.5 contain a listing of all the new features that were added and bugs fixed in the GlusterFS 3.10 stable release. Major changes, features and limitations addressed in this release No Major changes Major issues Expanding a gluster volume that is sharded may cause file corruption Sharded volumes are typically used for VM images, if such volumes are expanded or possibly contracted (i.e add/remove bricks and rebalance) there are reports of VM images getting corrupted. The last known cause for corruption #1498081 is still pending, and not yet a part of this release. Brick multiplexing is being tested and fixed aggressively but we still have a few crashes and memory leaks to fix. Bugs addressed Bugs addressed since release-3.10.5 are listed below. #1467010 : Fd based fops fail with EBADF on file migration #1481394 : libgfapi: memory leak in glfs_h_acl_get #1482857 : glusterd fails to start #1483997 : packaging: use rdma-core(-devel) instead of ibverbs, rdmacm; disable rdma on armv7hl #1484443 : packaging: /run and /var/run; prefer /run #1486542 : \"ganesha.so cannot open\" warning message in glusterd log in non ganesha setup. #1487042 : AFR returns the node uuid of the same node for every file in the replica #1487647 : with AFR now making both nodes to return UUID for a file will result in georep consuming more resources #1488391 : gluster-blockd process crashed and core generated #1488719 : [RHHI] cannot boot vms created from template when disk format = qcow2 #1490909 : [Ganesha] : Unable to bring up a Ganesha HA cluster on SELinux disabled machines on latest gluster bits. #1491166 : GlusterD returns a bad memory pointer in glusterd_get_args_from_dict() #1491691 : rpc: TLSv1_2_method() is deprecated in OpenSSL-1.1 #1491966 : AFR entry self heal removes a directory's .glusterfs symlink. #1491985 : Add NULL gfid checks before creating file #1491995 : afr: check op_ret value in __afr_selfheal_name_impunge #1492010 : Launch metadata heal in discover code path. #1495430 : Make event-history feature configurable and have it disabled by default #1496321 : [afr] split-brain observed on T files post hardlink and rename in x3 volume #1497122 : Crash in dht_check_and_open_fd_on_subvol_task()","title":"3.10.6"},{"location":"release-notes/3.10.6/#release-notes-for-gluster-3106","text":"This is a bugfix release. The release notes for 3.10.0 , 3.10.1 , 3.10.2 , 3.10.3 , 3.10.4 and 3.10.5 contain a listing of all the new features that were added and bugs fixed in the GlusterFS 3.10 stable release.","title":"Release notes for Gluster 3.10.6"},{"location":"release-notes/3.10.6/#major-changes-features-and-limitations-addressed-in-this-release","text":"No Major changes","title":"Major changes, features and limitations addressed in this release"},{"location":"release-notes/3.10.6/#major-issues","text":"Expanding a gluster volume that is sharded may cause file corruption Sharded volumes are typically used for VM images, if such volumes are expanded or possibly contracted (i.e add/remove bricks and rebalance) there are reports of VM images getting corrupted. The last known cause for corruption #1498081 is still pending, and not yet a part of this release. Brick multiplexing is being tested and fixed aggressively but we still have a few crashes and memory leaks to fix.","title":"Major issues"},{"location":"release-notes/3.10.6/#bugs-addressed","text":"Bugs addressed since release-3.10.5 are listed below. #1467010 : Fd based fops fail with EBADF on file migration #1481394 : libgfapi: memory leak in glfs_h_acl_get #1482857 : glusterd fails to start #1483997 : packaging: use rdma-core(-devel) instead of ibverbs, rdmacm; disable rdma on armv7hl #1484443 : packaging: /run and /var/run; prefer /run #1486542 : \"ganesha.so cannot open\" warning message in glusterd log in non ganesha setup. #1487042 : AFR returns the node uuid of the same node for every file in the replica #1487647 : with AFR now making both nodes to return UUID for a file will result in georep consuming more resources #1488391 : gluster-blockd process crashed and core generated #1488719 : [RHHI] cannot boot vms created from template when disk format = qcow2 #1490909 : [Ganesha] : Unable to bring up a Ganesha HA cluster on SELinux disabled machines on latest gluster bits. #1491166 : GlusterD returns a bad memory pointer in glusterd_get_args_from_dict() #1491691 : rpc: TLSv1_2_method() is deprecated in OpenSSL-1.1 #1491966 : AFR entry self heal removes a directory's .glusterfs symlink. #1491985 : Add NULL gfid checks before creating file #1491995 : afr: check op_ret value in __afr_selfheal_name_impunge #1492010 : Launch metadata heal in discover code path. #1495430 : Make event-history feature configurable and have it disabled by default #1496321 : [afr] split-brain observed on T files post hardlink and rename in x3 volume #1497122 : Crash in dht_check_and_open_fd_on_subvol_task()","title":"Bugs addressed"},{"location":"release-notes/3.10.7/","text":"Release notes for Gluster 3.10.7 This is a bugfix release. The release notes for 3.10.0 , 3.10.1 , 3.10.2 , 3.10.3 , 3.10.4 , 3.10.5 and 3.10.6 contain a listing of all the new features that were added and bugs fixed in the GlusterFS 3.10 stable release. Major changes, features and limitations addressed in this release No Major changes Major issues Expanding a gluster volume that is sharded may cause file corruption Sharded volumes are typically used for VM images, if such volumes are expanded or possibly contracted (i.e add/remove bricks and rebalance) there are reports of VM images getting corrupted. The last known cause for corruption #1498081 is still pending, and not yet a part of this release. Brick multiplexing is being tested and fixed aggressively but we still have a few crashes and memory leaks to fix. Bugs addressed Bugs addressed since release-3.10.6 are listed below. #1480788 : File-level WORM allows mv over read-only files #1491059 : PID File handling: brick pid file leaves stale pid and brick fails to start when glusterd is started #1496321 : [afr] split-brain observed on T files post hardlink and rename in x3 volume #1497990 : Gluster 3.10.x Packages require manual systemctl daemon reload after install #1499890 : md-cache uses incorrect xattr keynames for GF_POSIX_ACL keys #1499893 : md-cache: xattr values should not be checked with string functions #1501955 : gfapi: API needed to set lk_owner #1502928 : Mishandling null check at send_brick_req of glusterfsd/src/gf_attach.c #1503405 : Potential use of NULL this variable before it gets initialized","title":"3.10.7"},{"location":"release-notes/3.10.7/#release-notes-for-gluster-3107","text":"This is a bugfix release. The release notes for 3.10.0 , 3.10.1 , 3.10.2 , 3.10.3 , 3.10.4 , 3.10.5 and 3.10.6 contain a listing of all the new features that were added and bugs fixed in the GlusterFS 3.10 stable release.","title":"Release notes for Gluster 3.10.7"},{"location":"release-notes/3.10.7/#major-changes-features-and-limitations-addressed-in-this-release","text":"No Major changes","title":"Major changes, features and limitations addressed in this release"},{"location":"release-notes/3.10.7/#major-issues","text":"Expanding a gluster volume that is sharded may cause file corruption Sharded volumes are typically used for VM images, if such volumes are expanded or possibly contracted (i.e add/remove bricks and rebalance) there are reports of VM images getting corrupted. The last known cause for corruption #1498081 is still pending, and not yet a part of this release. Brick multiplexing is being tested and fixed aggressively but we still have a few crashes and memory leaks to fix.","title":"Major issues"},{"location":"release-notes/3.10.7/#bugs-addressed","text":"Bugs addressed since release-3.10.6 are listed below. #1480788 : File-level WORM allows mv over read-only files #1491059 : PID File handling: brick pid file leaves stale pid and brick fails to start when glusterd is started #1496321 : [afr] split-brain observed on T files post hardlink and rename in x3 volume #1497990 : Gluster 3.10.x Packages require manual systemctl daemon reload after install #1499890 : md-cache uses incorrect xattr keynames for GF_POSIX_ACL keys #1499893 : md-cache: xattr values should not be checked with string functions #1501955 : gfapi: API needed to set lk_owner #1502928 : Mishandling null check at send_brick_req of glusterfsd/src/gf_attach.c #1503405 : Potential use of NULL this variable before it gets initialized","title":"Bugs addressed"},{"location":"release-notes/3.10.8/","text":"Release notes for Gluster 3.10.8 This is a bugfix release. The release notes for 3.10.0 , 3.10.1 , 3.10.2 , 3.10.3 , 3.10.4 , 3.10.5 , 3.10.6 and 3.10.7 contain a listing of all the new features that were added and bugs fixed in the GlusterFS 3.10 stable release. Major changes, features and limitations addressed in this release No Major changes Major issues Expanding a gluster volume that is sharded may cause file corruption Sharded volumes are typically used for VM images, if such volumes are expanded or possibly contracted (i.e add/remove bricks and rebalance) there are reports of VM images getting corrupted. The last known cause for corruption #1498081 is still pending, and not yet a part of this release. Brick multiplexing is being tested and fixed aggressively but we still have a few crashes and memory leaks to fix. Bugs addressed Bugs addressed since release-3.10.7 are listed below. #1507749 : clean up port map on brick disconnect #1507752 : Brick port mismatch #1507880 : reset-brick commit force failed with glusterd_volume_brickinfo_get Returning -1 #1508036 : Address lstat usage in glusterd-snapshot.c code #1514388 : default timeout of 5min not honored for analyzing split-brain files post setfattr replica.split-brain-heal-finalize #1514424 : gluster volume splitbrain info needs to display output of each brick in a stream fashion instead of buffering and dumping at the end #1517682 : Memory leak in locks xlator","title":"3.10.8"},{"location":"release-notes/3.10.8/#release-notes-for-gluster-3108","text":"This is a bugfix release. The release notes for 3.10.0 , 3.10.1 , 3.10.2 , 3.10.3 , 3.10.4 , 3.10.5 , 3.10.6 and 3.10.7 contain a listing of all the new features that were added and bugs fixed in the GlusterFS 3.10 stable release.","title":"Release notes for Gluster 3.10.8"},{"location":"release-notes/3.10.8/#major-changes-features-and-limitations-addressed-in-this-release","text":"No Major changes","title":"Major changes, features and limitations addressed in this release"},{"location":"release-notes/3.10.8/#major-issues","text":"Expanding a gluster volume that is sharded may cause file corruption Sharded volumes are typically used for VM images, if such volumes are expanded or possibly contracted (i.e add/remove bricks and rebalance) there are reports of VM images getting corrupted. The last known cause for corruption #1498081 is still pending, and not yet a part of this release. Brick multiplexing is being tested and fixed aggressively but we still have a few crashes and memory leaks to fix.","title":"Major issues"},{"location":"release-notes/3.10.8/#bugs-addressed","text":"Bugs addressed since release-3.10.7 are listed below. #1507749 : clean up port map on brick disconnect #1507752 : Brick port mismatch #1507880 : reset-brick commit force failed with glusterd_volume_brickinfo_get Returning -1 #1508036 : Address lstat usage in glusterd-snapshot.c code #1514388 : default timeout of 5min not honored for analyzing split-brain files post setfattr replica.split-brain-heal-finalize #1514424 : gluster volume splitbrain info needs to display output of each brick in a stream fashion instead of buffering and dumping at the end #1517682 : Memory leak in locks xlator","title":"Bugs addressed"},{"location":"release-notes/3.10.9/","text":"Release notes for Gluster 3.10.9 This is a bugfix release. The release notes for 3.10.0 , 3.10.1 , 3.10.2 , 3.10.3 , 3.10.4 , 3.10.5 , 3.10.6 , 3.10.7 and 3.10.8 contain a listing of all the new features that were added and bugs fixed in the GlusterFS 3.10 stable release. Major changes, features and limitations addressed in this release No Major changes Major issues Expanding a gluster volume that is sharded may cause file corruption Sharded volumes are typically used for VM images, if such volumes are expanded or possibly contracted (i.e add/remove bricks and rebalance) there are reports of VM images getting corrupted. The last known cause for corruption #1498081 is still pending, and not yet a part of this release. Brick multiplexing is being tested and fixed aggressively but we still have a few crashes and memory leaks to fix. Bugs addressed Bugs addressed since release-3.10.8 are listed below. #1523050 : glusterd consuming high memory #1529086 : fstat returns ENOENT/ESTALE #1529089 : opening a file that is destination of rename results in ENOENT errors #1529096 : /usr/sbin/glusterfs crashing on Red Hat OpenShift Container Platform node #1530341 : [snapshot cifs]ls on .snaps directory is throwing input/output error over cifs mount #1530450 : glustershd fails to start on a volume force start after a brick is down","title":"3.10.9"},{"location":"release-notes/3.10.9/#release-notes-for-gluster-3109","text":"This is a bugfix release. The release notes for 3.10.0 , 3.10.1 , 3.10.2 , 3.10.3 , 3.10.4 , 3.10.5 , 3.10.6 , 3.10.7 and 3.10.8 contain a listing of all the new features that were added and bugs fixed in the GlusterFS 3.10 stable release.","title":"Release notes for Gluster 3.10.9"},{"location":"release-notes/3.10.9/#major-changes-features-and-limitations-addressed-in-this-release","text":"No Major changes","title":"Major changes, features and limitations addressed in this release"},{"location":"release-notes/3.10.9/#major-issues","text":"Expanding a gluster volume that is sharded may cause file corruption Sharded volumes are typically used for VM images, if such volumes are expanded or possibly contracted (i.e add/remove bricks and rebalance) there are reports of VM images getting corrupted. The last known cause for corruption #1498081 is still pending, and not yet a part of this release. Brick multiplexing is being tested and fixed aggressively but we still have a few crashes and memory leaks to fix.","title":"Major issues"},{"location":"release-notes/3.10.9/#bugs-addressed","text":"Bugs addressed since release-3.10.8 are listed below. #1523050 : glusterd consuming high memory #1529086 : fstat returns ENOENT/ESTALE #1529089 : opening a file that is destination of rename results in ENOENT errors #1529096 : /usr/sbin/glusterfs crashing on Red Hat OpenShift Container Platform node #1530341 : [snapshot cifs]ls on .snaps directory is throwing input/output error over cifs mount #1530450 : glustershd fails to start on a volume force start after a brick is down","title":"Bugs addressed"},{"location":"release-notes/3.11.0/","text":"Release notes for Gluster 3.11.0 This is a major Gluster release that includes some substantial changes. The features revolve around, improvements to small file workloads, SE Linux support, Halo replication enhancement from Facebook, some usability and performance improvements, among other bug fixes. The most notable features and changes are documented on this page. A full list of bugs that have been addressed is included further below. Major changes and features Switched to storhaug for ganesha and samba high availability Notes for users: High Availability (HA) support for NFS-Ganesha (NFS) and Samba (SMB) is managed by Storhaug. Like the old HA implementation, Storhaug uses Pacemaker and Corosync to manage Virtual (floating) IP addresses (VIPs) and fencing. See https://github.com/linux-ha-storage/storhaug. Storhaug packages are available in Fedora and for several popular Linux distributions from https://download.gluster.org/pub/gluster/storhaug/ Note: Storhaug does not dictate which fencing solution should be used. There are many to choose from in most popular Linux distributions. Choose the one the best fits your environment and use it. Added SELinux support for Gluster Volumes Notes for users: A new xlator has been introduced ( features/selinux ) to allow setting the extended attribute ( security.selinux ) that is needed to support SELinux on Gluster volumes. The current ability to enforce the SELinux policy on the Gluster Storage servers prevents setting the extended attribute for use on the client side. The new translator converts the client-side SELinux extended attribute to a Gluster internal representation (the trusted.glusterfs.selinux extended attribute) to prevent problems. This feature is intended to be the base for implementing Labelled-NFS in NFS-Ganesha and SELinux support for FUSE mounts in the Linux kernel. Limitations: - The Linux kernel does not support mounting of FUSE filesystems with SELinux support, yet. - NFS-Ganesha does not support Labelled-NFS, yet. Known Issues: - There has been limited testing, because other projects can not consume the functionality yet without being part of a release. So far, no problems have been observed, but this might change when other projects start to seriously use this. Several memory leaks are fixed in gfapi during graph switches Notes for users: Gluster API (or gfapi), has had a few memory leak issues arising specifically during changes to volume graphs (volume topology or options). A few of these are addressed in this release, and more work towards ironing out the pending leaks are in the works across the next few releases. Limitations: - There are still a few leaks to be addressed when graph switches occur get-state CLI is enhanced to provide client and brick capacity related information Notes for users: The get-state CLI output now optionally accommodates client related information corresponding locally running bricks as obtained from gluster volume status <volname>|all clients . Getting the client details is a is a relatively more costly operation and these details will only be added to the output if the get-state command is invoked with the 'detail' option. The following is the updated usage for the get-state command: # gluster get-state [<daemon>] [[odir </path/to/output/dir/>] [file <filename>]] [detail] Other than client details, capacity related information for respective local bricks as obtained from gluster volume status <volname>|all detail has also been added to the get-state output. Limitations: - Information for non-local bricks and clients connected to non-local bricks won't be available. This is a known limitation of the get-state command, since get-state command doesn't provide information on non-local bricks. Ability to serve negative lookups from cache has been added Notes for users: Before creating / renaming any file, lookups (around, 5-6 when using the SMB protocol) are sent to verify if the file already exists. The negative lookup cache, serves these lookups from the cache when possible, thus increasing the create/rename performance when using SMB based access to a gluster volume. Execute the following commands to enable negative-lookup cache: # gluster volume set <volname> features.cache-invalidation on # gluster volume set <volname> features.cache-invalidation-timeout 600 # gluster volume set <VOLNAME> nl-cache on Limitations - This feature is supported only for SMB access, for this release New xlator to help developers detecting resource leaks has been added Notes for users: This is intended as a developer feature, and hence there is no direct user impact. For developers, the sink xlator provides ways to help detect memory leaks in gfapi and any xlator in between the API and the sink xlator. More details can be found in this thread on the gluster-devel lists Feature for metadata-caching/small file performance is production ready Notes for users: Over the course of releases several fixes and enhancements have been made to the mdcache xlator, to improve performance of small file workloads. As a result, with this release we are announcing this feature to be production ready. In order to improve the performance of directory operations of Gluster volumes, the maximum metadata (stat, xattr) caching time on the client side is increased to 10 minutes, without compromising on the consistency of the cache. Significant performance improvements can be achieved in the following workloads on FUSE and SMB access, by enabling metadata caching: Listing of directories (recursive) Creating files Deleting files Renaming files To enable metadata caching execute the following commands: # gluster volume set group metadata-cache # gluster volume set network.inode-lru-limit <n> \\<n>, is set to 50000 by default. It should be increased if the number of concurrently accessed files in the volume is very high. Increasing this number increases the memory footprint of the brick processes. \"Parallel Readdir\" feature introduced in 3.10.0 is production ready Notes for users: This feature was introduced in 3.10 and was experimental in nature. Over the course of 3.10 minor releases and 3.11.0 release, this feature has been stabilized and is ready for use in production environments. For further details refer: 3.10.0 release notes Object versioning is enabled only if bitrot is enabled Notes for users: Object versioning was turned on by default on brick processes by the bitrot xlator. This caused, setting and looking up of additional extended attributes on the backed file system for every object, even when not actively using bitrot. This at times caused high CPU utilization on the brick processes. To fix this, object versioning is disabled by default, and is only enabled as a part of enabling the bitrot option. Distribute layer provides more robust transactions during directory namespace operations Notes for users: Distribute layer in Gluster, creates and maintains directories in all subvolumes and as a result operations involving creation/manipulation/deletion of these directories needed better transaction support to ensure consistency of the file system. This transaction support is now implemented in the distribute layer, thus ensuring better consistency of the file system as a whole, when dealing with racing operations, operating on the same directory object. gfapi extended readdirplus API has been added Notes for users: An extended readdirplus API glfs_xreaddirplus is added to get extra information along with readdirplus results on demand. This is useful for the applications (like NFS-Ganesha which needs handles) to retrieve more information along with stat in a single call, thus improving performance of work-loads involving directory listing. The API syntax and usage can be found in glfs.h header file. Limitations: - This API currently has support to only return stat and handles ( glfs_object ) for each dirent of the directory, but can be extended in the future. Improved adoption of standard refcounting functions across the code Notes for users: This change does not impact users, it is an internal code cleanup activity that ensures that we ref count in a standard manner, thus avoiding unwanted bugs due to different implementations of the same. Known Issues: - This standardization started with this release and is expected to continue across releases. Performance improvements to rebalance have been made Notes for users: Both crawling and migration improvement has been done in rebalance. The crawler is optimized now to split the migration load across replica and ec nodes. Prior to this change, in case the replicating bricks are distributed over two nodes, then only one node used to do the migration. With the new optimization both the nodes divide the load among each other giving boost to migration performance. And also there have been some optimization to avoid redundant network operations (or RPC calls) in the process of migrating a file. Further, file migration now avoids syncop framework and is managed entirely by rebalance threads giving performance boost. Also, There is a change to throttle settings in rebalance. Earlier user could set three values to rebalance which were \"lazy\", \"normal\", \"aggressive\", which was not flexible enough. To overcome that we have introduced number based throttle settings. User now can set numbers which is an indication of the number of threads rebalance process will work with, thereby translating to the number of files being migrated in parallel. Halo Replication feature in AFR has been introduced Notes for users: Halo Geo-replication is a feature which allows Gluster or NFS clients to write locally to their region (as defined by a latency \"halo\" or threshold if you like), and have their writes asynchronously propagate from their origin to the rest of the cluster. Clients can also write synchronously to the cluster simply by specifying a halo-latency which is very large (e.g. 10seconds) which will include all bricks. To enable halo feature execute the following commands: # gluster volume set cluster.halo-enabled yes You may have to set the following following options to change defaults. cluster.halo-shd-latency : The threshold below which self-heal daemons will consider children (bricks) connected. cluster.halo-nfsd-latency : The threshold below which NFS daemons will consider children (bricks) connected. cluster.halo-latency : The threshold below which all other clients will consider children (bricks) connected. cluster.halo-min-replicas : The minimum number of replicas which are to be enforced regardless of latency specified in the above 3 options. If the number of children falls below this threshold the next best (chosen by latency) shall be swapped in. FALLOCATE support with EC Notes for users Support for FALLOCATE file operation on EC volume is added with this release. EC volumes can now support basic FALLOCATE functionality. Self-heal window-size control option for EC Notes for users Support to control the maximum size of read/write operation carried out during self-heal process has been added with this release. User has to tune 'disperse.self-heal-window-size' option on disperse volume to adjust the size. Major issues Expanding a gluster volume that is sharded may cause file corruption Sharded volumes are typically used for VM images, if such volumes are expanded or possibly contracted (i.e add/remove bricks and rebalance) there are reports of VM images getting corrupted. Status of this bug can be tracked here, #1426508 Latest series of fixes for the issue (which are present in this release as well) are not showing the previous corruption, and hence the fixes look good, but this is maintained on the watch list nevetheness. Bugs addressed Bugs addressed since release-3.10.0 are listed below. #1169302 : Unable to take Statedump for gfapi applications #1197308 : do not depend on \"killall\", use \"pkill\" instead #1198849 : Minor improvements and cleanup for the build system #1257792 : bug-1238706-daemons-stop-on-peer-cleanup.t fails occasionally #1261689 : geo-replication faulty #1264849 : RFE : Create trash directory only when its is enabled #1297182 : Mounting with \"-o noatime\" or \"-o noexec\" causes \"nosuid,nodev\" to be set as well #1318100 : RFE : SELinux translator to support setting SELinux contexts on files in a glusterfs volume #1321578 : auth.allow and auth.reject not working host mentioned with hostnames/FQDN #1322145 : Glusterd fails to restart after replacing a failed GlusterFS node and a volume has a snapshot #1326219 : Make Gluster/NFS an optional component #1328342 : [tiering]: gluster v reset of watermark levels can allow low watermark level to have a higher value than hi watermark level #1353952 : [geo-rep]: rsync should not try to sync internal xattrs #1356076 : DHT doesn't evenly balance files on FreeBSD with ZFS #1359599 : BitRot :- bit-rot.signature and bit-rot.version xattr should not be set if bitrot is not enabled on volume #1369393 : dead loop in changelog_rpc_server_destroy #1383893 : glusterd restart is starting the offline shd daemon on other node in the cluster #1384989 : libglusterfs : update correct memory segments in glfs-message-id #1385758 : [RFE] Support multiple bricks in one process (multiplexing) #1386578 : mounting with rdma protocol fails for tcp,rdma volumes #1389127 : build: fixes to build 3.9.0rc2 on Debian (jessie) #1390050 : Elasticsearch get CorruptIndexException errors when running with GlusterFS persistent storage #1393338 : Rebalance should skip the file if the file has hardlinks instead of failing #1395643 : [SELinux] [Scheduler]: Unable to create Snapshots on RHEL-7.1 using Scheduler #1396004 : RFE: An administrator friendly way to determine rebalance completion time #1399196 : use attribute(format(printf)) to catch format string errors at compile time #1399593 : Obvious typo in cleanup code in rpc_clnt_notify #1401571 : bitrot quarantine dir misspelled #1401812 : RFE: Make readdirp parallel in dht #1401877 : [GANESHA] Symlinks from /etc/ganesha/ganesha.conf to shared_storage are created on the non-ganesha nodes in 8 node gluster having 4 node ganesha cluster #1402254 : compile warning unused variable #1402661 : Samba crash when mounting a distributed dispersed volume over CIFS #1404424 : The data-self-heal option is not honored in AFR #1405628 : Socket search code at startup is slow #1408809 : [Perf] : significant Performance regression seen with disperse volume when compared with 3.1.3 #1409191 : Sequential and Random Writes are off target by 12% and 22% respectively on EC backed volumes over FUSE #1410425 : [GNFS+EC] Cthon failures/issues with Lock/Special Test cases on disperse volume with GNFS mount #1410701 : [SAMBA-SSL] Volume Share hungs when multiple mount & unmount is performed over a windows client on a SSL enabled cluster #1411228 : remove-brick status shows 0 rebalanced files #1411334 : Improve output of \"gluster volume status detail\" #1412135 : rename of the same file from multiple clients with caching enabled may result in duplicate files #1412549 : EXPECT_WITHIN is taking too much time even if the result matches with expected value #1413526 : glusterfind: After glusterfind pre command execution all temporary files and directories /usr/var/lib/misc/glusterfsd/glusterfind/ / / should be removed #1413971 : Bonnie test suite failed with \"Can't open file\" error #1414287 : repeated operation failed warnings in gluster mount logs with disperse volume #1414346 : Quota: After upgrade from 3.7 to higher version , gluster quota list command shows \"No quota configured on volume repvol\" #1414645 : Typo in glusterfs code comments #1414782 : Add logs to selfheal code path to be helpful for debug #1414902 : packaging: python/python2(/python3) cleanup #1415115 : client process crashed due to write behind translator #1415590 : removing old tier commands under the rebalance commands #1415761 : [Remove-brick] Hardlink migration fails with \"lookup failed (No such file or directory)\" error messages in rebalance logs #1416251 : [SNAPSHOT] With all USS plugin enable .snaps directory is not visible in cifs mount as well as windows mount #1416520 : Missing FOPs in the io-stats xlator #1416689 : Fix spurious failure of ec-background-heal.t #1416889 : Simplify refcount API for free'ing function #1417050 : [Stress] : SHD Logs flooded with \"Heal Failed\" messages,filling up \"/\" quickly #1417466 : Prevent reverse heal from happening #1417522 : Automatic split brain resolution must check for all the bricks to be up to avoiding serving of inconsistent data(visible on x3 or more) #1417540 : Mark tests/bitrot/bug-1373520.t bad #1417588 : glusterd is setting replicate volume property over disperse volume or vice versa #1417913 : Hangs on 32 bit systems since 3.9.0 #1418014 : disable client.io-threads on replica volume creation #1418095 : Portmap allocates way too much memory (256KB) on stack #1418213 : [Ganesha+SSL] : Bonnie++ hangs during rewrites. #1418249 : [RFE] Need to have group cli option to set all md-cache options using a single command #1418259 : Quota: After deleting directory from mount point on which quota was configured, quota list command output is blank #1418417 : packaging: remove glusterfs-ganesha subpackage #1418629 : glustershd process crashed on systemic setup #1418900 : [RFE] Include few more options in virt file #1418973 : removing warning related to enum, to let the build take place without errors for 3.10 #1420166 : The rebal-throttle setting does not work as expected #1420202 : glusterd is crashed at the time of stop volume #1420434 : Trash feature improperly disabled #1420571 : Massive xlator_t leak in graph-switch code #1420611 : when server-quorum is enabled, volume get returns 0 value for server-quorum-ratio #1420614 : warning messages seen in glusterd logs while setting the volume option #1420619 : Entry heal messages in glustershd.log while no entries shown in heal info #1420623 : [RHV-RHGS]: Application VM paused after add brick operation and VM didn't comeup after power cycle. #1420637 : Modified volume options not synced once offline nodes comes up. #1420697 : CLI option \"--timeout\" is accepting non numeric and negative values. #1420713 : glusterd: storhaug, remove all vestiges ganesha #1421023 : Binary file gf_attach generated during build process should be git ignored #1421590 : Bricks take up new ports upon volume restart after add-brick op with brick mux enabled #1421600 : Test files clean up for tier during 3.10 #1421607 : Getting error messages in glusterd.log when peer detach is done #1421653 : dht_setxattr returns EINVAL when a file is deleted during the FOP #1421721 : volume start command hangs #1421724 : glusterd log is flooded with stale disconnect rpc messages #1421759 : Gluster NFS server crashing in __mnt3svc_umountall #1421937 : [Replicate] \"RPC call decoding failed\" leading to IO hang & mount inaccessible #1421938 : systemic testing: seeing lot of ping time outs which would lead to splitbrains #1421955 : Disperse: Fallback to pre-compiled code execution when dynamic code generation fails #1422074 : GlusterFS truncates nanoseconds to microseconds when setting mtime #1422152 : Bricks not coming up when ran with address sanitizer #1422624 : Need to improve remove-brick failure message when the brick process is down. #1422760 : [Geo-rep] Recreating geo-rep session with same slave after deleting with reset-sync-time fails to sync #1422776 : multiple glusterfsd process crashed making the complete subvolume unavailable #1423369 : unnecessary logging in rda_opendir #1423373 : Crash in index xlator because of race in inode_ctx_set and inode_ref #1423410 : Mount of older client fails #1423413 : Self-heal fail an WORMed-Files #1423448 : glusterfs-fuse RPM now depends on gfapi #1424764 : Coverty scan return false positive regarding crypto #1424791 : Coverty scan detect a potential free on uninitialised pointer in error code path #1424793 : Missing verification of fcntl return code #1424796 : Remove deadcode found by coverty in glusterd-utils.c #1424802 : Missing call to va_end in xlators/cluster/dht/src/dht-common.c #1424809 : Fix another coverty error for useless goto #1424815 : Fix erronous comparaison of flags resulting in UUID always sent #1424894 : Some switches don't have breaks causing unintended fall throughs. #1424905 : Coverity: Memory issues and dead code #1425288 : glusterd is not validating for allowed values while setting \"cluster.brick-multiplex\" property #1425515 : tests: quota-anon-fd-nfs.t needs to check if nfs mount is avialable before mounting #1425623 : Free all xlator specific resources when xlator->fini() gets called #1425676 : gfids are not populated in release/releasedir requests #1425703 : [Disperse] Metadata version is not healing when a brick is down #1425743 : Tier ./tests/bugs/glusterd/bug-1303028-Rebalance-glusterd-rpc-connection-issue.t #1426032 : Log message shows error code as success even when rpc fails to connect #1426052 : \u2018state\u2019 set but not used error when readline and/or ncurses is not installed #1426059 : gluster fuse client losing connection to gluster volume frequently #1426125 : Add logs to identify whether disconnects are voluntary or due to network problems #1426509 : include volume name in rebalance stage error log #1426667 : [GSS] NFS Sub-directory mount not working on solaris10 client #1426891 : script to resolve function name and line number from backtrace #1426948 : [RFE] capture portmap details in glusterd's statedump #1427012 : Disconnects in nfs mount leads to IO hang and mount inaccessible #1427018 : [RFE] - Need a way to reduce the logging of messages \"Peer CN\" and \"SSL verification suceeded messages\" in glusterd.log file #1427404 : Move tests/bitrot/bug-1373520.t to bad tests and fix the underlying issue in posix #1428036 : Update rfc.sh to check/request issue # when a commit is an \u201crfc\u201d #1428047 : Require a Jenkins job to validate Change-ID on commits to branches in glusterfs repository #1428055 : dht/rebalance: Increase maximum read block size from 128 KB to 1 MB #1428058 : tests: Fix tests/bugs/distribute/bug-1161311.t #1428064 : nfs: Check for null buf, and set op_errno to EIO not 0 #1428068 : nfs: Tear down transports for requests that arrive before the volume is initialized #1428073 : nfs: Fix compiler warning when calling svc_getcaller #1428093 : protocol/server: Fix crash bug in unlink flow #1428510 : memory leak in features/locks xlator #1429198 : Restore atime/mtime for symlinks and other non-regular files. #1429200 : disallow increasing replica count for arbiter volumes #1429330 : [crawler]: auxiliary mount remains even after crawler finishes #1429696 : ldd libgfxdr.so.0.0.1: undefined symbol: __gf_free #1430042 : Transport endpoint not connected error seen on client when glusterd is restarted #1430148 : USS is broken when multiplexing is on #1430608 : [RFE] Pass slave volume in geo-rep as read-only #1430719 : gfid split brains not getting resolved with automatic splitbrain resolution #1430841 : build/packaging: Debian and Ubuntu don't have /usr/libexec/; results in bad packages #1430860 : brick process crashes when glusterd is restarted #1431183 : [RFE] Gluster get state command should provide connected client related information #1431192 : [RFE] Gluster get state command should provide volume and cluster utilization related information #1431908 : Enabling parallel-readdir causes dht linkto files to be visible on the mount, #1431963 : Warn CLI while creating replica 2 volumes #1432542 : Glusterd crashes when restarted with many volumes #1433405 : GF_REF_PUT() should return 0 when the structure becomes invalid #1433425 : Unrecognized filesystems (i.e. btrfs, zfs) log many errors about \"getinode size\" #1433506 : [Geo-rep] Master and slave mounts are not accessible to take client profile info #1433571 : Undo pending xattrs only on the up bricks #1433578 : glusterd crashes when peering an IP where the address is more than acceptable range (>255) OR with random hostnames #1433815 : auth failure after upgrade to GlusterFS 3.10 #1433838 : Move spit-brain msg in read txn to debug #1434018 : [geo-rep]: Worker crashes with [Errno 16] Device or resource busy: '.gfid/00000000-0000-0000-0000-000000000001/dir.166 while renaming directories #1434062 : synclocks don't work correctly under contention #1434274 : BZ for some bugs found while going through synctask code #1435943 : When parallel readdir is enabled and there are simultaneous readdir and disconnects, then it results in crash #1436086 : Parallel readdir on Gluster NFS displays less number of dentries #1436090 : When parallel readdir is enabled, linked to file resolution fails #1436739 : Sharding: Fix a performance bug #1436936 : parameter state->size is wrong in server3_3_writev #1437037 : Standardize atomic increment/decrement calling conventions #1437494 : Brick Multiplexing:Volume status still shows the PID even after killing the process #1437748 : Spacing issue in fix-layout status output #1437780 : don't send lookup in fuse_getattr() #1437853 : Spellcheck issues reported during Debian build #1438255 : Don't wind post-op on a brick where the fop phase failed. #1438370 : rebalance: Allow admin to change thread count for rebalance #1438411 : [Ganesha + EC] : Input/Output Error while creating LOTS of smallfiles #1438738 : Inode ref leak on anonymous reads and writes #1438772 : build: clang/llvm has __builtin_ffs() and __builtin_popcount() #1438810 : File-level WORM allows ftruncate() on read-only files #1438858 : explicitly specify executor to be bash for tests #1439527 : [disperse] Don't count healing brick as healthy brick #1439571 : dht/rebalance: Improve rebalance crawl performance #1439640 : [Parallel Readdir] : No bound-checks/CLI validation for parallel readdir tunables #1440051 : Application VMs with their disk images on sharded-replica 3 volume are unable to boot after performing rebalance #1441035 : remove bug-1421590-brick-mux-reuse-ports.t #1441106 : [Geo-rep]: Unnecessary unlink call while processing rmdir #1441491 : The data-self-heal option is not honored in EC #1441508 : dht/cluster: rebalance/remove-brick should honor min-free-disk #1441910 : gluster volume stop hangs #1441945 : [Eventing]: Unrelated error message displayed when path specified during a 'webhook-test/add' is missing a schema #1442145 : split-brain-favorite-child-policy.t depends on \"bc\" #1442411 : meta xlator leaks memory when unloaded #1442569 : Implement Negative lookup cache feature to improve create performance #1442724 : rm -rf returns ENOTEMPTY even though ls on the mount point returns no files #1442760 : snapshot: snapshots appear to be failing with respect to secure geo-rep slave #1443373 : mkdir/rmdir loop causes gfid-mismatch on a 6 brick distribute volume #1443896 : [BrickMultiplex] gluster command not responding and .snaps directory is not visible after executing snapshot related command #1443959 : packaging: no firewalld-filesystem before el 7.3 #1443977 : Unable to take snapshot on a geo-replicated volume, even after stopping the session #1444023 : io-stats xlator leaks memory when fini() is called #1444228 : Autoconf leaves unexpanded variables in path names of non-shell-script text files #1444941 : bogus date in %changelog #1445569 : Provide a correct way to save the statedump generated by gfapi application #1445590 : Incorrect and redundant logs in the DHT rmdir code path #1446126 : S30samba-start.sh throws 'unary operator expected' warning during independent execution #1446273 : Some functions are exported incorrectly for Mac OS X with the GFAPI_PUBLIC macro #1447543 : Revert experimental and 4.0 features to prepare for 3.11 release #1447571 : RFE: Enhance handleops readdirplus operation to return handles along with dirents #1447597 : RFE : SELinux translator to support setting SELinux contexts on files in a glusterfs volume #1447604 : volume set fails if nfs.so is not installed #1447607 : Don't allow rebalance/fix-layout operation on sharding enabled volumes till dht+sharding bugs are fixed #1448345 : Segmentation fault when creating a qcow2 with qemu-img #1448416 : Halo Replication feature for AFR translator #1449004 : [Brick Multiplexing] : Bricks for multiple volumes going down after glusterd restart and not coming back up after volume start force #1449191 : Multiple bricks WILL crash after TCP port probing #1449311 : [whql][virtio-block+glusterfs]\"Disk Stress\" and \"Disk Verification\" job always failed on win7-32/win2012/win2k8R2 guest #1449775 : quota: limit-usage command failed with error \" Failed to start aux mount\" #1449921 : afr: include quorum type and count when dumping afr priv #1449924 : When either killing or restarting a brick with performance.stat-prefetch on, stat sometimes returns a bad st_size value. #1449933 : Brick Multiplexing :- resetting a brick bring down other bricks with same PID #1450267 : nl-cache xlator leaks timer wheel and other memory #1450377 : GNFS crashed while taking lock on a file from 2 different clients having same volume mounted from 2 different servers #1450565 : glfsheal: crashed(segfault) with disperse volume in RDMA #1450729 : Brick Multiplexing: seeing Input/Output Error for .trashcan #1450933 : [New] - Replacing an arbiter brick while I/O happens causes vm pause #1451033 : contrib: timer-wheel 32-bit bug, use builtin_fls, license, etc #1451573 : AFR returns the node uuid of the same node for every file in the replica #1451586 : crash in dht_rmdir_do #1451591 : cli xml status of detach tier broken #1451887 : Add tests/basic/afr/gfid-mismatch-resolution-with-fav-child-policy.t to bad tests #1452000 : Spacing issue in fix-layout status output #1453050 : [DHt] : segfault in dht_selfheal_dir_setattr while running regressions #1453086 : Brick Multiplexing: On reboot of a node Brick multiplexing feature lost on that node as multiple brick processes get spawned #1453152 : [Parallel Readdir] : Mounts fail when performance.parallel-readdir is set to \"off\" #1454533 : lock_revocation.t Marked as bad in 3.11 for CentOS as well #1454569 : [geo-rep + nl]: Multiple crashes observed on slave with \"nlc_lookup_cbk\" #1454597 : [Tiering]: High and low watermark values when set to the same level, is allowed #1454612 : glusterd on a node crashed after running volume profile command #1454686 : Implement FALLOCATE FOP for EC #1454853 : Seeing error \"Failed to get the total number of files. Unable to estimate time to complete rebalance\" in rebalance logs #1455177 : ignore incorrect uuid validation in gd_validate_mgmt_hndsk_req #1455423 : dht: dht self heal fails with no hashed subvol error #1455907 : heal info shows the status of the bricks as \"Transport endpoint is not connected\" though bricks are up #1456224 : [gluster-block]:Need a volume group profile option for gluster-block volume to add necessary options to be added. #1456225 : gluster-block is not working as expected when shard is enabled #1456331 : [Bitrot]: Brick process crash observed while trying to recover a bad file in disperse volume","title":"3.11.0"},{"location":"release-notes/3.11.0/#release-notes-for-gluster-3110","text":"This is a major Gluster release that includes some substantial changes. The features revolve around, improvements to small file workloads, SE Linux support, Halo replication enhancement from Facebook, some usability and performance improvements, among other bug fixes. The most notable features and changes are documented on this page. A full list of bugs that have been addressed is included further below.","title":"Release notes for Gluster 3.11.0"},{"location":"release-notes/3.11.0/#major-changes-and-features","text":"","title":"Major changes and features"},{"location":"release-notes/3.11.0/#switched-to-storhaug-for-ganesha-and-samba-high-availability","text":"Notes for users: High Availability (HA) support for NFS-Ganesha (NFS) and Samba (SMB) is managed by Storhaug. Like the old HA implementation, Storhaug uses Pacemaker and Corosync to manage Virtual (floating) IP addresses (VIPs) and fencing. See https://github.com/linux-ha-storage/storhaug. Storhaug packages are available in Fedora and for several popular Linux distributions from https://download.gluster.org/pub/gluster/storhaug/ Note: Storhaug does not dictate which fencing solution should be used. There are many to choose from in most popular Linux distributions. Choose the one the best fits your environment and use it.","title":"Switched to storhaug for ganesha and samba high availability"},{"location":"release-notes/3.11.0/#added-selinux-support-for-gluster-volumes","text":"Notes for users: A new xlator has been introduced ( features/selinux ) to allow setting the extended attribute ( security.selinux ) that is needed to support SELinux on Gluster volumes. The current ability to enforce the SELinux policy on the Gluster Storage servers prevents setting the extended attribute for use on the client side. The new translator converts the client-side SELinux extended attribute to a Gluster internal representation (the trusted.glusterfs.selinux extended attribute) to prevent problems. This feature is intended to be the base for implementing Labelled-NFS in NFS-Ganesha and SELinux support for FUSE mounts in the Linux kernel. Limitations: - The Linux kernel does not support mounting of FUSE filesystems with SELinux support, yet. - NFS-Ganesha does not support Labelled-NFS, yet. Known Issues: - There has been limited testing, because other projects can not consume the functionality yet without being part of a release. So far, no problems have been observed, but this might change when other projects start to seriously use this.","title":"Added SELinux support for Gluster Volumes"},{"location":"release-notes/3.11.0/#several-memory-leaks-are-fixed-in-gfapi-during-graph-switches","text":"Notes for users: Gluster API (or gfapi), has had a few memory leak issues arising specifically during changes to volume graphs (volume topology or options). A few of these are addressed in this release, and more work towards ironing out the pending leaks are in the works across the next few releases. Limitations: - There are still a few leaks to be addressed when graph switches occur","title":"Several memory leaks are fixed in gfapi during graph switches"},{"location":"release-notes/3.11.0/#get-state-cli-is-enhanced-to-provide-client-and-brick-capacity-related-information","text":"Notes for users: The get-state CLI output now optionally accommodates client related information corresponding locally running bricks as obtained from gluster volume status <volname>|all clients . Getting the client details is a is a relatively more costly operation and these details will only be added to the output if the get-state command is invoked with the 'detail' option. The following is the updated usage for the get-state command: # gluster get-state [<daemon>] [[odir </path/to/output/dir/>] [file <filename>]] [detail] Other than client details, capacity related information for respective local bricks as obtained from gluster volume status <volname>|all detail has also been added to the get-state output. Limitations: - Information for non-local bricks and clients connected to non-local bricks won't be available. This is a known limitation of the get-state command, since get-state command doesn't provide information on non-local bricks.","title":"get-state CLI is enhanced to provide client and brick capacity related information"},{"location":"release-notes/3.11.0/#ability-to-serve-negative-lookups-from-cache-has-been-added","text":"Notes for users: Before creating / renaming any file, lookups (around, 5-6 when using the SMB protocol) are sent to verify if the file already exists. The negative lookup cache, serves these lookups from the cache when possible, thus increasing the create/rename performance when using SMB based access to a gluster volume. Execute the following commands to enable negative-lookup cache: # gluster volume set <volname> features.cache-invalidation on # gluster volume set <volname> features.cache-invalidation-timeout 600 # gluster volume set <VOLNAME> nl-cache on Limitations - This feature is supported only for SMB access, for this release","title":"Ability to serve negative lookups from cache has been added"},{"location":"release-notes/3.11.0/#new-xlator-to-help-developers-detecting-resource-leaks-has-been-added","text":"Notes for users: This is intended as a developer feature, and hence there is no direct user impact. For developers, the sink xlator provides ways to help detect memory leaks in gfapi and any xlator in between the API and the sink xlator. More details can be found in this thread on the gluster-devel lists","title":"New xlator to help developers detecting resource leaks has been added"},{"location":"release-notes/3.11.0/#feature-for-metadata-cachingsmall-file-performance-is-production-ready","text":"Notes for users: Over the course of releases several fixes and enhancements have been made to the mdcache xlator, to improve performance of small file workloads. As a result, with this release we are announcing this feature to be production ready. In order to improve the performance of directory operations of Gluster volumes, the maximum metadata (stat, xattr) caching time on the client side is increased to 10 minutes, without compromising on the consistency of the cache. Significant performance improvements can be achieved in the following workloads on FUSE and SMB access, by enabling metadata caching: Listing of directories (recursive) Creating files Deleting files Renaming files To enable metadata caching execute the following commands: # gluster volume set group metadata-cache # gluster volume set network.inode-lru-limit <n> \\<n>, is set to 50000 by default. It should be increased if the number of concurrently accessed files in the volume is very high. Increasing this number increases the memory footprint of the brick processes.","title":"Feature for metadata-caching/small file performance is production ready"},{"location":"release-notes/3.11.0/#parallel-readdir-feature-introduced-in-3100-is-production-ready","text":"Notes for users: This feature was introduced in 3.10 and was experimental in nature. Over the course of 3.10 minor releases and 3.11.0 release, this feature has been stabilized and is ready for use in production environments. For further details refer: 3.10.0 release notes","title":"\"Parallel Readdir\" feature introduced in 3.10.0 is production ready"},{"location":"release-notes/3.11.0/#object-versioning-is-enabled-only-if-bitrot-is-enabled","text":"Notes for users: Object versioning was turned on by default on brick processes by the bitrot xlator. This caused, setting and looking up of additional extended attributes on the backed file system for every object, even when not actively using bitrot. This at times caused high CPU utilization on the brick processes. To fix this, object versioning is disabled by default, and is only enabled as a part of enabling the bitrot option.","title":"Object versioning is enabled only if bitrot is enabled"},{"location":"release-notes/3.11.0/#distribute-layer-provides-more-robust-transactions-during-directory-namespace-operations","text":"Notes for users: Distribute layer in Gluster, creates and maintains directories in all subvolumes and as a result operations involving creation/manipulation/deletion of these directories needed better transaction support to ensure consistency of the file system. This transaction support is now implemented in the distribute layer, thus ensuring better consistency of the file system as a whole, when dealing with racing operations, operating on the same directory object.","title":"Distribute layer provides more robust transactions during directory namespace operations"},{"location":"release-notes/3.11.0/#gfapi-extended-readdirplus-api-has-been-added","text":"Notes for users: An extended readdirplus API glfs_xreaddirplus is added to get extra information along with readdirplus results on demand. This is useful for the applications (like NFS-Ganesha which needs handles) to retrieve more information along with stat in a single call, thus improving performance of work-loads involving directory listing. The API syntax and usage can be found in glfs.h header file. Limitations: - This API currently has support to only return stat and handles ( glfs_object ) for each dirent of the directory, but can be extended in the future.","title":"gfapi extended readdirplus API has been added"},{"location":"release-notes/3.11.0/#improved-adoption-of-standard-refcounting-functions-across-the-code","text":"Notes for users: This change does not impact users, it is an internal code cleanup activity that ensures that we ref count in a standard manner, thus avoiding unwanted bugs due to different implementations of the same. Known Issues: - This standardization started with this release and is expected to continue across releases.","title":"Improved adoption of standard refcounting functions across the code"},{"location":"release-notes/3.11.0/#performance-improvements-to-rebalance-have-been-made","text":"Notes for users: Both crawling and migration improvement has been done in rebalance. The crawler is optimized now to split the migration load across replica and ec nodes. Prior to this change, in case the replicating bricks are distributed over two nodes, then only one node used to do the migration. With the new optimization both the nodes divide the load among each other giving boost to migration performance. And also there have been some optimization to avoid redundant network operations (or RPC calls) in the process of migrating a file. Further, file migration now avoids syncop framework and is managed entirely by rebalance threads giving performance boost. Also, There is a change to throttle settings in rebalance. Earlier user could set three values to rebalance which were \"lazy\", \"normal\", \"aggressive\", which was not flexible enough. To overcome that we have introduced number based throttle settings. User now can set numbers which is an indication of the number of threads rebalance process will work with, thereby translating to the number of files being migrated in parallel.","title":"Performance improvements to rebalance have been made"},{"location":"release-notes/3.11.0/#halo-replication-feature-in-afr-has-been-introduced","text":"Notes for users: Halo Geo-replication is a feature which allows Gluster or NFS clients to write locally to their region (as defined by a latency \"halo\" or threshold if you like), and have their writes asynchronously propagate from their origin to the rest of the cluster. Clients can also write synchronously to the cluster simply by specifying a halo-latency which is very large (e.g. 10seconds) which will include all bricks. To enable halo feature execute the following commands: # gluster volume set cluster.halo-enabled yes You may have to set the following following options to change defaults. cluster.halo-shd-latency : The threshold below which self-heal daemons will consider children (bricks) connected. cluster.halo-nfsd-latency : The threshold below which NFS daemons will consider children (bricks) connected. cluster.halo-latency : The threshold below which all other clients will consider children (bricks) connected. cluster.halo-min-replicas : The minimum number of replicas which are to be enforced regardless of latency specified in the above 3 options. If the number of children falls below this threshold the next best (chosen by latency) shall be swapped in.","title":"Halo Replication feature in AFR has been introduced"},{"location":"release-notes/3.11.0/#fallocate-support-with-ec","text":"Notes for users Support for FALLOCATE file operation on EC volume is added with this release. EC volumes can now support basic FALLOCATE functionality.","title":"FALLOCATE support with EC"},{"location":"release-notes/3.11.0/#self-heal-window-size-control-option-for-ec","text":"Notes for users Support to control the maximum size of read/write operation carried out during self-heal process has been added with this release. User has to tune 'disperse.self-heal-window-size' option on disperse volume to adjust the size.","title":"Self-heal window-size control option for EC"},{"location":"release-notes/3.11.0/#major-issues","text":"Expanding a gluster volume that is sharded may cause file corruption Sharded volumes are typically used for VM images, if such volumes are expanded or possibly contracted (i.e add/remove bricks and rebalance) there are reports of VM images getting corrupted. Status of this bug can be tracked here, #1426508 Latest series of fixes for the issue (which are present in this release as well) are not showing the previous corruption, and hence the fixes look good, but this is maintained on the watch list nevetheness.","title":"Major issues"},{"location":"release-notes/3.11.0/#bugs-addressed","text":"Bugs addressed since release-3.10.0 are listed below. #1169302 : Unable to take Statedump for gfapi applications #1197308 : do not depend on \"killall\", use \"pkill\" instead #1198849 : Minor improvements and cleanup for the build system #1257792 : bug-1238706-daemons-stop-on-peer-cleanup.t fails occasionally #1261689 : geo-replication faulty #1264849 : RFE : Create trash directory only when its is enabled #1297182 : Mounting with \"-o noatime\" or \"-o noexec\" causes \"nosuid,nodev\" to be set as well #1318100 : RFE : SELinux translator to support setting SELinux contexts on files in a glusterfs volume #1321578 : auth.allow and auth.reject not working host mentioned with hostnames/FQDN #1322145 : Glusterd fails to restart after replacing a failed GlusterFS node and a volume has a snapshot #1326219 : Make Gluster/NFS an optional component #1328342 : [tiering]: gluster v reset of watermark levels can allow low watermark level to have a higher value than hi watermark level #1353952 : [geo-rep]: rsync should not try to sync internal xattrs #1356076 : DHT doesn't evenly balance files on FreeBSD with ZFS #1359599 : BitRot :- bit-rot.signature and bit-rot.version xattr should not be set if bitrot is not enabled on volume #1369393 : dead loop in changelog_rpc_server_destroy #1383893 : glusterd restart is starting the offline shd daemon on other node in the cluster #1384989 : libglusterfs : update correct memory segments in glfs-message-id #1385758 : [RFE] Support multiple bricks in one process (multiplexing) #1386578 : mounting with rdma protocol fails for tcp,rdma volumes #1389127 : build: fixes to build 3.9.0rc2 on Debian (jessie) #1390050 : Elasticsearch get CorruptIndexException errors when running with GlusterFS persistent storage #1393338 : Rebalance should skip the file if the file has hardlinks instead of failing #1395643 : [SELinux] [Scheduler]: Unable to create Snapshots on RHEL-7.1 using Scheduler #1396004 : RFE: An administrator friendly way to determine rebalance completion time #1399196 : use attribute(format(printf)) to catch format string errors at compile time #1399593 : Obvious typo in cleanup code in rpc_clnt_notify #1401571 : bitrot quarantine dir misspelled #1401812 : RFE: Make readdirp parallel in dht #1401877 : [GANESHA] Symlinks from /etc/ganesha/ganesha.conf to shared_storage are created on the non-ganesha nodes in 8 node gluster having 4 node ganesha cluster #1402254 : compile warning unused variable #1402661 : Samba crash when mounting a distributed dispersed volume over CIFS #1404424 : The data-self-heal option is not honored in AFR #1405628 : Socket search code at startup is slow #1408809 : [Perf] : significant Performance regression seen with disperse volume when compared with 3.1.3 #1409191 : Sequential and Random Writes are off target by 12% and 22% respectively on EC backed volumes over FUSE #1410425 : [GNFS+EC] Cthon failures/issues with Lock/Special Test cases on disperse volume with GNFS mount #1410701 : [SAMBA-SSL] Volume Share hungs when multiple mount & unmount is performed over a windows client on a SSL enabled cluster #1411228 : remove-brick status shows 0 rebalanced files #1411334 : Improve output of \"gluster volume status detail\" #1412135 : rename of the same file from multiple clients with caching enabled may result in duplicate files #1412549 : EXPECT_WITHIN is taking too much time even if the result matches with expected value #1413526 : glusterfind: After glusterfind pre command execution all temporary files and directories /usr/var/lib/misc/glusterfsd/glusterfind/ / / should be removed #1413971 : Bonnie test suite failed with \"Can't open file\" error #1414287 : repeated operation failed warnings in gluster mount logs with disperse volume #1414346 : Quota: After upgrade from 3.7 to higher version , gluster quota list command shows \"No quota configured on volume repvol\" #1414645 : Typo in glusterfs code comments #1414782 : Add logs to selfheal code path to be helpful for debug #1414902 : packaging: python/python2(/python3) cleanup #1415115 : client process crashed due to write behind translator #1415590 : removing old tier commands under the rebalance commands #1415761 : [Remove-brick] Hardlink migration fails with \"lookup failed (No such file or directory)\" error messages in rebalance logs #1416251 : [SNAPSHOT] With all USS plugin enable .snaps directory is not visible in cifs mount as well as windows mount #1416520 : Missing FOPs in the io-stats xlator #1416689 : Fix spurious failure of ec-background-heal.t #1416889 : Simplify refcount API for free'ing function #1417050 : [Stress] : SHD Logs flooded with \"Heal Failed\" messages,filling up \"/\" quickly #1417466 : Prevent reverse heal from happening #1417522 : Automatic split brain resolution must check for all the bricks to be up to avoiding serving of inconsistent data(visible on x3 or more) #1417540 : Mark tests/bitrot/bug-1373520.t bad #1417588 : glusterd is setting replicate volume property over disperse volume or vice versa #1417913 : Hangs on 32 bit systems since 3.9.0 #1418014 : disable client.io-threads on replica volume creation #1418095 : Portmap allocates way too much memory (256KB) on stack #1418213 : [Ganesha+SSL] : Bonnie++ hangs during rewrites. #1418249 : [RFE] Need to have group cli option to set all md-cache options using a single command #1418259 : Quota: After deleting directory from mount point on which quota was configured, quota list command output is blank #1418417 : packaging: remove glusterfs-ganesha subpackage #1418629 : glustershd process crashed on systemic setup #1418900 : [RFE] Include few more options in virt file #1418973 : removing warning related to enum, to let the build take place without errors for 3.10 #1420166 : The rebal-throttle setting does not work as expected #1420202 : glusterd is crashed at the time of stop volume #1420434 : Trash feature improperly disabled #1420571 : Massive xlator_t leak in graph-switch code #1420611 : when server-quorum is enabled, volume get returns 0 value for server-quorum-ratio #1420614 : warning messages seen in glusterd logs while setting the volume option #1420619 : Entry heal messages in glustershd.log while no entries shown in heal info #1420623 : [RHV-RHGS]: Application VM paused after add brick operation and VM didn't comeup after power cycle. #1420637 : Modified volume options not synced once offline nodes comes up. #1420697 : CLI option \"--timeout\" is accepting non numeric and negative values. #1420713 : glusterd: storhaug, remove all vestiges ganesha #1421023 : Binary file gf_attach generated during build process should be git ignored #1421590 : Bricks take up new ports upon volume restart after add-brick op with brick mux enabled #1421600 : Test files clean up for tier during 3.10 #1421607 : Getting error messages in glusterd.log when peer detach is done #1421653 : dht_setxattr returns EINVAL when a file is deleted during the FOP #1421721 : volume start command hangs #1421724 : glusterd log is flooded with stale disconnect rpc messages #1421759 : Gluster NFS server crashing in __mnt3svc_umountall #1421937 : [Replicate] \"RPC call decoding failed\" leading to IO hang & mount inaccessible #1421938 : systemic testing: seeing lot of ping time outs which would lead to splitbrains #1421955 : Disperse: Fallback to pre-compiled code execution when dynamic code generation fails #1422074 : GlusterFS truncates nanoseconds to microseconds when setting mtime #1422152 : Bricks not coming up when ran with address sanitizer #1422624 : Need to improve remove-brick failure message when the brick process is down. #1422760 : [Geo-rep] Recreating geo-rep session with same slave after deleting with reset-sync-time fails to sync #1422776 : multiple glusterfsd process crashed making the complete subvolume unavailable #1423369 : unnecessary logging in rda_opendir #1423373 : Crash in index xlator because of race in inode_ctx_set and inode_ref #1423410 : Mount of older client fails #1423413 : Self-heal fail an WORMed-Files #1423448 : glusterfs-fuse RPM now depends on gfapi #1424764 : Coverty scan return false positive regarding crypto #1424791 : Coverty scan detect a potential free on uninitialised pointer in error code path #1424793 : Missing verification of fcntl return code #1424796 : Remove deadcode found by coverty in glusterd-utils.c #1424802 : Missing call to va_end in xlators/cluster/dht/src/dht-common.c #1424809 : Fix another coverty error for useless goto #1424815 : Fix erronous comparaison of flags resulting in UUID always sent #1424894 : Some switches don't have breaks causing unintended fall throughs. #1424905 : Coverity: Memory issues and dead code #1425288 : glusterd is not validating for allowed values while setting \"cluster.brick-multiplex\" property #1425515 : tests: quota-anon-fd-nfs.t needs to check if nfs mount is avialable before mounting #1425623 : Free all xlator specific resources when xlator->fini() gets called #1425676 : gfids are not populated in release/releasedir requests #1425703 : [Disperse] Metadata version is not healing when a brick is down #1425743 : Tier ./tests/bugs/glusterd/bug-1303028-Rebalance-glusterd-rpc-connection-issue.t #1426032 : Log message shows error code as success even when rpc fails to connect #1426052 : \u2018state\u2019 set but not used error when readline and/or ncurses is not installed #1426059 : gluster fuse client losing connection to gluster volume frequently #1426125 : Add logs to identify whether disconnects are voluntary or due to network problems #1426509 : include volume name in rebalance stage error log #1426667 : [GSS] NFS Sub-directory mount not working on solaris10 client #1426891 : script to resolve function name and line number from backtrace #1426948 : [RFE] capture portmap details in glusterd's statedump #1427012 : Disconnects in nfs mount leads to IO hang and mount inaccessible #1427018 : [RFE] - Need a way to reduce the logging of messages \"Peer CN\" and \"SSL verification suceeded messages\" in glusterd.log file #1427404 : Move tests/bitrot/bug-1373520.t to bad tests and fix the underlying issue in posix #1428036 : Update rfc.sh to check/request issue # when a commit is an \u201crfc\u201d #1428047 : Require a Jenkins job to validate Change-ID on commits to branches in glusterfs repository #1428055 : dht/rebalance: Increase maximum read block size from 128 KB to 1 MB #1428058 : tests: Fix tests/bugs/distribute/bug-1161311.t #1428064 : nfs: Check for null buf, and set op_errno to EIO not 0 #1428068 : nfs: Tear down transports for requests that arrive before the volume is initialized #1428073 : nfs: Fix compiler warning when calling svc_getcaller #1428093 : protocol/server: Fix crash bug in unlink flow #1428510 : memory leak in features/locks xlator #1429198 : Restore atime/mtime for symlinks and other non-regular files. #1429200 : disallow increasing replica count for arbiter volumes #1429330 : [crawler]: auxiliary mount remains even after crawler finishes #1429696 : ldd libgfxdr.so.0.0.1: undefined symbol: __gf_free #1430042 : Transport endpoint not connected error seen on client when glusterd is restarted #1430148 : USS is broken when multiplexing is on #1430608 : [RFE] Pass slave volume in geo-rep as read-only #1430719 : gfid split brains not getting resolved with automatic splitbrain resolution #1430841 : build/packaging: Debian and Ubuntu don't have /usr/libexec/; results in bad packages #1430860 : brick process crashes when glusterd is restarted #1431183 : [RFE] Gluster get state command should provide connected client related information #1431192 : [RFE] Gluster get state command should provide volume and cluster utilization related information #1431908 : Enabling parallel-readdir causes dht linkto files to be visible on the mount, #1431963 : Warn CLI while creating replica 2 volumes #1432542 : Glusterd crashes when restarted with many volumes #1433405 : GF_REF_PUT() should return 0 when the structure becomes invalid #1433425 : Unrecognized filesystems (i.e. btrfs, zfs) log many errors about \"getinode size\" #1433506 : [Geo-rep] Master and slave mounts are not accessible to take client profile info #1433571 : Undo pending xattrs only on the up bricks #1433578 : glusterd crashes when peering an IP where the address is more than acceptable range (>255) OR with random hostnames #1433815 : auth failure after upgrade to GlusterFS 3.10 #1433838 : Move spit-brain msg in read txn to debug #1434018 : [geo-rep]: Worker crashes with [Errno 16] Device or resource busy: '.gfid/00000000-0000-0000-0000-000000000001/dir.166 while renaming directories #1434062 : synclocks don't work correctly under contention #1434274 : BZ for some bugs found while going through synctask code #1435943 : When parallel readdir is enabled and there are simultaneous readdir and disconnects, then it results in crash #1436086 : Parallel readdir on Gluster NFS displays less number of dentries #1436090 : When parallel readdir is enabled, linked to file resolution fails #1436739 : Sharding: Fix a performance bug #1436936 : parameter state->size is wrong in server3_3_writev #1437037 : Standardize atomic increment/decrement calling conventions #1437494 : Brick Multiplexing:Volume status still shows the PID even after killing the process #1437748 : Spacing issue in fix-layout status output #1437780 : don't send lookup in fuse_getattr() #1437853 : Spellcheck issues reported during Debian build #1438255 : Don't wind post-op on a brick where the fop phase failed. #1438370 : rebalance: Allow admin to change thread count for rebalance #1438411 : [Ganesha + EC] : Input/Output Error while creating LOTS of smallfiles #1438738 : Inode ref leak on anonymous reads and writes #1438772 : build: clang/llvm has __builtin_ffs() and __builtin_popcount() #1438810 : File-level WORM allows ftruncate() on read-only files #1438858 : explicitly specify executor to be bash for tests #1439527 : [disperse] Don't count healing brick as healthy brick #1439571 : dht/rebalance: Improve rebalance crawl performance #1439640 : [Parallel Readdir] : No bound-checks/CLI validation for parallel readdir tunables #1440051 : Application VMs with their disk images on sharded-replica 3 volume are unable to boot after performing rebalance #1441035 : remove bug-1421590-brick-mux-reuse-ports.t #1441106 : [Geo-rep]: Unnecessary unlink call while processing rmdir #1441491 : The data-self-heal option is not honored in EC #1441508 : dht/cluster: rebalance/remove-brick should honor min-free-disk #1441910 : gluster volume stop hangs #1441945 : [Eventing]: Unrelated error message displayed when path specified during a 'webhook-test/add' is missing a schema #1442145 : split-brain-favorite-child-policy.t depends on \"bc\" #1442411 : meta xlator leaks memory when unloaded #1442569 : Implement Negative lookup cache feature to improve create performance #1442724 : rm -rf returns ENOTEMPTY even though ls on the mount point returns no files #1442760 : snapshot: snapshots appear to be failing with respect to secure geo-rep slave #1443373 : mkdir/rmdir loop causes gfid-mismatch on a 6 brick distribute volume #1443896 : [BrickMultiplex] gluster command not responding and .snaps directory is not visible after executing snapshot related command #1443959 : packaging: no firewalld-filesystem before el 7.3 #1443977 : Unable to take snapshot on a geo-replicated volume, even after stopping the session #1444023 : io-stats xlator leaks memory when fini() is called #1444228 : Autoconf leaves unexpanded variables in path names of non-shell-script text files #1444941 : bogus date in %changelog #1445569 : Provide a correct way to save the statedump generated by gfapi application #1445590 : Incorrect and redundant logs in the DHT rmdir code path #1446126 : S30samba-start.sh throws 'unary operator expected' warning during independent execution #1446273 : Some functions are exported incorrectly for Mac OS X with the GFAPI_PUBLIC macro #1447543 : Revert experimental and 4.0 features to prepare for 3.11 release #1447571 : RFE: Enhance handleops readdirplus operation to return handles along with dirents #1447597 : RFE : SELinux translator to support setting SELinux contexts on files in a glusterfs volume #1447604 : volume set fails if nfs.so is not installed #1447607 : Don't allow rebalance/fix-layout operation on sharding enabled volumes till dht+sharding bugs are fixed #1448345 : Segmentation fault when creating a qcow2 with qemu-img #1448416 : Halo Replication feature for AFR translator #1449004 : [Brick Multiplexing] : Bricks for multiple volumes going down after glusterd restart and not coming back up after volume start force #1449191 : Multiple bricks WILL crash after TCP port probing #1449311 : [whql][virtio-block+glusterfs]\"Disk Stress\" and \"Disk Verification\" job always failed on win7-32/win2012/win2k8R2 guest #1449775 : quota: limit-usage command failed with error \" Failed to start aux mount\" #1449921 : afr: include quorum type and count when dumping afr priv #1449924 : When either killing or restarting a brick with performance.stat-prefetch on, stat sometimes returns a bad st_size value. #1449933 : Brick Multiplexing :- resetting a brick bring down other bricks with same PID #1450267 : nl-cache xlator leaks timer wheel and other memory #1450377 : GNFS crashed while taking lock on a file from 2 different clients having same volume mounted from 2 different servers #1450565 : glfsheal: crashed(segfault) with disperse volume in RDMA #1450729 : Brick Multiplexing: seeing Input/Output Error for .trashcan #1450933 : [New] - Replacing an arbiter brick while I/O happens causes vm pause #1451033 : contrib: timer-wheel 32-bit bug, use builtin_fls, license, etc #1451573 : AFR returns the node uuid of the same node for every file in the replica #1451586 : crash in dht_rmdir_do #1451591 : cli xml status of detach tier broken #1451887 : Add tests/basic/afr/gfid-mismatch-resolution-with-fav-child-policy.t to bad tests #1452000 : Spacing issue in fix-layout status output #1453050 : [DHt] : segfault in dht_selfheal_dir_setattr while running regressions #1453086 : Brick Multiplexing: On reboot of a node Brick multiplexing feature lost on that node as multiple brick processes get spawned #1453152 : [Parallel Readdir] : Mounts fail when performance.parallel-readdir is set to \"off\" #1454533 : lock_revocation.t Marked as bad in 3.11 for CentOS as well #1454569 : [geo-rep + nl]: Multiple crashes observed on slave with \"nlc_lookup_cbk\" #1454597 : [Tiering]: High and low watermark values when set to the same level, is allowed #1454612 : glusterd on a node crashed after running volume profile command #1454686 : Implement FALLOCATE FOP for EC #1454853 : Seeing error \"Failed to get the total number of files. Unable to estimate time to complete rebalance\" in rebalance logs #1455177 : ignore incorrect uuid validation in gd_validate_mgmt_hndsk_req #1455423 : dht: dht self heal fails with no hashed subvol error #1455907 : heal info shows the status of the bricks as \"Transport endpoint is not connected\" though bricks are up #1456224 : [gluster-block]:Need a volume group profile option for gluster-block volume to add necessary options to be added. #1456225 : gluster-block is not working as expected when shard is enabled #1456331 : [Bitrot]: Brick process crash observed while trying to recover a bad file in disperse volume","title":"Bugs addressed"},{"location":"release-notes/3.11.1/","text":"Release notes for Gluster 3.11.1 This is a bugfix release. The release notes for 3.11.0 , contains a listing of all the new features that were added and bugs fixed, in the GlusterFS 3.11 stable release. Major changes, features and limitations addressed in this release Improved disperse performance Fix for bug #1456259 changes the way messages are read and processed from the socket layers on the Gluster client. This has shown performance improvements on disperse volumes, and is applicable to other volume types as well, where there maybe multiple applications or users accessing the same mount point. Group settings for enabling negative lookup caching provided Ability to serve negative lookups from cache was added in 3.11.0 and with this release, a group volume set option is added for ease in enabling this feature. See group-nl-cache for more details. Gluster fuse now implements \"-oauto_unmount\" feature. libfuse has an auto_unmount option which, if enabled, ensures that the file system is unmounted at FUSE server termination by running a separate monitor process that performs the unmount when that occurs. This release implements that option and behavior for glusterfs. Note that \"auto unmount\" (robust or not) is a leaky abstraction, as the kernel cannot guarantee that at the path where the FUSE fs is mounted is actually the toplevel mount at the time of the umount(2) call, for multiple reasons, among others, see: - fuse-devel: \"fuse: feasible to distinguish between umount and abort?\" - https://github.com/libfuse/libfuse/issues/122 Major issues Expanding a gluster volume that is sharded may cause file corruption Sharded volumes are typically used for VM images, if such volumes are expanded or possibly contracted (i.e add/remove bricks and rebalance) there are reports of VM images getting corrupted. Status of this bug can be tracked here, #1465123 Bugs addressed Bugs addressed since release-3.11.0 are listed below. #1456259 : limited throughput with disperse volume over small number of bricks #1457058 : glusterfs client crash on io-cache.so(__ioc_page_wakeup+0x44) #1457289 : tierd listens to a port. #1457339 : DHT: slow readdirp performance #1457616 : \"split-brain observed [Input/output error]\" error messages in samba logs during parallel rm -rf #1457901 : nlc_lookup_cbk floods logs #1458570 : [brick multiplexing] detach a brick if posix health check thread complaints about underlying brick #1458664 : [Geo-rep]: METADATA errors are seen even though everything is in sync #1459090 : all: spelling errors (debian package maintainer) #1459095 : extras/hook-scripts: non-portable shell syntax (debian package maintainer) #1459392 : possible repeatedly recursive healing of same file with background heal not happening when IO is going on #1459759 : Glusterd segmentation fault in ' _Unwind_Backtrace' while running peer probe #1460647 : posix-acl: Whitelist virtual ACL xattrs #1460894 : Rebalance estimate time sometimes shows negative values #1460895 : Upcall missing invalidations #1460896 : [Negative Lookup Cache]Need a single group set command for enabling all required nl cache options #1460898 : Enabling parallel-readdir causes dht linkto files to be visible on the mount, #1462121 : [GNFS+EC] Unable to release the lock when the other client tries to acquire the lock on the same file #1462127 : [Bitrot]: Inconsistency seen with 'scrub ondemand' - fails to trigger scrub #1462636 : Use of force with volume start, creates brick directory even it is not present #1462661 : lk fop succeeds even when lock is not acquired on at least quorum number of bricks #1463250 : with AFR now making both nodes to return UUID for a file will result in georep consuming more resources","title":"3.11.1"},{"location":"release-notes/3.11.1/#release-notes-for-gluster-3111","text":"This is a bugfix release. The release notes for 3.11.0 , contains a listing of all the new features that were added and bugs fixed, in the GlusterFS 3.11 stable release.","title":"Release notes for Gluster 3.11.1"},{"location":"release-notes/3.11.1/#major-changes-features-and-limitations-addressed-in-this-release","text":"","title":"Major changes, features and limitations addressed in this release"},{"location":"release-notes/3.11.1/#improved-disperse-performance","text":"Fix for bug #1456259 changes the way messages are read and processed from the socket layers on the Gluster client. This has shown performance improvements on disperse volumes, and is applicable to other volume types as well, where there maybe multiple applications or users accessing the same mount point.","title":"Improved disperse performance"},{"location":"release-notes/3.11.1/#group-settings-for-enabling-negative-lookup-caching-provided","text":"Ability to serve negative lookups from cache was added in 3.11.0 and with this release, a group volume set option is added for ease in enabling this feature. See group-nl-cache for more details.","title":"Group settings for enabling negative lookup caching provided"},{"location":"release-notes/3.11.1/#gluster-fuse-now-implements-oauto_unmount-feature","text":"libfuse has an auto_unmount option which, if enabled, ensures that the file system is unmounted at FUSE server termination by running a separate monitor process that performs the unmount when that occurs. This release implements that option and behavior for glusterfs. Note that \"auto unmount\" (robust or not) is a leaky abstraction, as the kernel cannot guarantee that at the path where the FUSE fs is mounted is actually the toplevel mount at the time of the umount(2) call, for multiple reasons, among others, see: - fuse-devel: \"fuse: feasible to distinguish between umount and abort?\" - https://github.com/libfuse/libfuse/issues/122","title":"Gluster fuse now implements \"-oauto_unmount\" feature."},{"location":"release-notes/3.11.1/#major-issues","text":"Expanding a gluster volume that is sharded may cause file corruption Sharded volumes are typically used for VM images, if such volumes are expanded or possibly contracted (i.e add/remove bricks and rebalance) there are reports of VM images getting corrupted. Status of this bug can be tracked here, #1465123","title":"Major issues"},{"location":"release-notes/3.11.1/#bugs-addressed","text":"Bugs addressed since release-3.11.0 are listed below. #1456259 : limited throughput with disperse volume over small number of bricks #1457058 : glusterfs client crash on io-cache.so(__ioc_page_wakeup+0x44) #1457289 : tierd listens to a port. #1457339 : DHT: slow readdirp performance #1457616 : \"split-brain observed [Input/output error]\" error messages in samba logs during parallel rm -rf #1457901 : nlc_lookup_cbk floods logs #1458570 : [brick multiplexing] detach a brick if posix health check thread complaints about underlying brick #1458664 : [Geo-rep]: METADATA errors are seen even though everything is in sync #1459090 : all: spelling errors (debian package maintainer) #1459095 : extras/hook-scripts: non-portable shell syntax (debian package maintainer) #1459392 : possible repeatedly recursive healing of same file with background heal not happening when IO is going on #1459759 : Glusterd segmentation fault in ' _Unwind_Backtrace' while running peer probe #1460647 : posix-acl: Whitelist virtual ACL xattrs #1460894 : Rebalance estimate time sometimes shows negative values #1460895 : Upcall missing invalidations #1460896 : [Negative Lookup Cache]Need a single group set command for enabling all required nl cache options #1460898 : Enabling parallel-readdir causes dht linkto files to be visible on the mount, #1462121 : [GNFS+EC] Unable to release the lock when the other client tries to acquire the lock on the same file #1462127 : [Bitrot]: Inconsistency seen with 'scrub ondemand' - fails to trigger scrub #1462636 : Use of force with volume start, creates brick directory even it is not present #1462661 : lk fop succeeds even when lock is not acquired on at least quorum number of bricks #1463250 : with AFR now making both nodes to return UUID for a file will result in georep consuming more resources","title":"Bugs addressed"},{"location":"release-notes/3.11.2/","text":"Release notes for Gluster 3.11.2 This is a bugfix release. The release notes for 3.11.1 , 3.11.0 , contains a listing of all the new features that were added and bugs fixed, in the GlusterFS 3.11 stable release. Major changes, features and limitations addressed in this release There are no major features or changes made in this release. Major issues Expanding a gluster volume that is sharded may cause file corruption Sharded volumes are typically used for VM images, if such volumes are expanded or possibly contracted (i.e add/remove bricks and rebalance) there are reports of VM images getting corrupted. The last known cause for corruption (Bug #1465123) has a fix with this release. As further testing is still in progress, the issue is retained as a major issue. Status of this bug can be tracked here, #1465123 Bugs addressed Bugs addressed since release-3.11.0 are listed below. #1463512 : USS: stale snap entries are seen when activation/deactivation performed during one of the glusterd's unavailability #1463513 : [geo-rep]: extended attributes are not synced if the entry and extended attributes are done within changelog roleover/or entry sync #1463517 : Brick Multiplexing:dmesg shows request_sock_TCP: Possible SYN flooding on port 49152 and memory related backtraces #1463528 : [Perf] 35% drop in small file creates on smbv3 on *2 #1463626 : [Ganesha]Bricks got crashed while running posix compliance test suit on V4 mount #1464316 : DHT: Pass errno as an argument to gf_msg #1465123 : Fd based fops fail with EBADF on file migration #1465854 : Regression: Heal info takes longer time when a brick is down #1466801 : assorted typos and spelling mistakes from Debian lintian #1466859 : dht_rename_lock_cbk crashes in upstream regression test #1467268 : Heal info shows incorrect status #1468118 : disperse seek does not correctly handle the end of file #1468200 : [Geo-rep]: entry failed to sync to slave with ENOENT errror #1468457 : selfheal deamon cpu consumption not reducing when IOs are going on and all redundant bricks are brought down one after another #1469459 : Rebalance hangs on remove-brick if the target volume changes #1470938 : Regression: non-disruptive(in-service) upgrade on EC volume fails #1471025 : glusterfs process leaking memory when error occurs #1471611 : metadata heal not happening despite having an active sink #1471869 : cthon04 can cause segfault in gNFS/NLM #1472794 : Test script failing with brick multiplexing enabled","title":"3.11.2"},{"location":"release-notes/3.11.2/#release-notes-for-gluster-3112","text":"This is a bugfix release. The release notes for 3.11.1 , 3.11.0 , contains a listing of all the new features that were added and bugs fixed, in the GlusterFS 3.11 stable release.","title":"Release notes for Gluster 3.11.2"},{"location":"release-notes/3.11.2/#major-changes-features-and-limitations-addressed-in-this-release","text":"There are no major features or changes made in this release.","title":"Major changes, features and limitations addressed in this release"},{"location":"release-notes/3.11.2/#major-issues","text":"Expanding a gluster volume that is sharded may cause file corruption Sharded volumes are typically used for VM images, if such volumes are expanded or possibly contracted (i.e add/remove bricks and rebalance) there are reports of VM images getting corrupted. The last known cause for corruption (Bug #1465123) has a fix with this release. As further testing is still in progress, the issue is retained as a major issue. Status of this bug can be tracked here, #1465123","title":"Major issues"},{"location":"release-notes/3.11.2/#bugs-addressed","text":"Bugs addressed since release-3.11.0 are listed below. #1463512 : USS: stale snap entries are seen when activation/deactivation performed during one of the glusterd's unavailability #1463513 : [geo-rep]: extended attributes are not synced if the entry and extended attributes are done within changelog roleover/or entry sync #1463517 : Brick Multiplexing:dmesg shows request_sock_TCP: Possible SYN flooding on port 49152 and memory related backtraces #1463528 : [Perf] 35% drop in small file creates on smbv3 on *2 #1463626 : [Ganesha]Bricks got crashed while running posix compliance test suit on V4 mount #1464316 : DHT: Pass errno as an argument to gf_msg #1465123 : Fd based fops fail with EBADF on file migration #1465854 : Regression: Heal info takes longer time when a brick is down #1466801 : assorted typos and spelling mistakes from Debian lintian #1466859 : dht_rename_lock_cbk crashes in upstream regression test #1467268 : Heal info shows incorrect status #1468118 : disperse seek does not correctly handle the end of file #1468200 : [Geo-rep]: entry failed to sync to slave with ENOENT errror #1468457 : selfheal deamon cpu consumption not reducing when IOs are going on and all redundant bricks are brought down one after another #1469459 : Rebalance hangs on remove-brick if the target volume changes #1470938 : Regression: non-disruptive(in-service) upgrade on EC volume fails #1471025 : glusterfs process leaking memory when error occurs #1471611 : metadata heal not happening despite having an active sink #1471869 : cthon04 can cause segfault in gNFS/NLM #1472794 : Test script failing with brick multiplexing enabled","title":"Bugs addressed"},{"location":"release-notes/3.11.3/","text":"Release notes for Gluster 3.11.3 This is a bugfix release. The release notes for 3.11.2 , 3.11.1 , and 3.11.0 contain a listing of all the new features that were added and bugs fixed, in the GlusterFS 3.11 stable release. This is possibly the last bugfix release for 3.11, as 3.12 is expected to be released around end of August, 2017, which will hence EOL the 3.11 release, as it is a short term maintenence release (see release status ). Major changes, features and limitations addressed in this release There are no major features or changes made in this release. Major issues Expanding a gluster volume that is sharded may cause file corruption Sharded volumes are typically used for VM images, if such volumes are expanded or possibly contracted (i.e add/remove bricks and rebalance) there are reports of VM images getting corrupted. The last known cause for corruption (Bug #1465123) has a fix with the 3.11.2 release. As further testing is still in progress, the issue is retained as a major issue. Status of this bug can be tracked here, #1465123 Bugs addressed Bugs addressed since release-3.11.2 are listed below. #1475637 : [Scale] : Client logs flooded with \"inode context is NULL\" error messages #1476822 : scripts: invalid test in S32gluster_enable_shared_storage.sh #1476870 : [EC]: md5sum mismatches every time for a file from the fuse client on EC volume #1476873 : packaging: /var/lib/glusterd/options should be %config(noreplace) #1479656 : Permission denied errors when appending files after readdir #1479692 : Running sysbench on vm disk from plain distribute gluster volume causes disk corruption","title":"3.11.3"},{"location":"release-notes/3.11.3/#release-notes-for-gluster-3113","text":"This is a bugfix release. The release notes for 3.11.2 , 3.11.1 , and 3.11.0 contain a listing of all the new features that were added and bugs fixed, in the GlusterFS 3.11 stable release. This is possibly the last bugfix release for 3.11, as 3.12 is expected to be released around end of August, 2017, which will hence EOL the 3.11 release, as it is a short term maintenence release (see release status ).","title":"Release notes for Gluster 3.11.3"},{"location":"release-notes/3.11.3/#major-changes-features-and-limitations-addressed-in-this-release","text":"There are no major features or changes made in this release.","title":"Major changes, features and limitations addressed in this release"},{"location":"release-notes/3.11.3/#major-issues","text":"Expanding a gluster volume that is sharded may cause file corruption Sharded volumes are typically used for VM images, if such volumes are expanded or possibly contracted (i.e add/remove bricks and rebalance) there are reports of VM images getting corrupted. The last known cause for corruption (Bug #1465123) has a fix with the 3.11.2 release. As further testing is still in progress, the issue is retained as a major issue. Status of this bug can be tracked here, #1465123","title":"Major issues"},{"location":"release-notes/3.11.3/#bugs-addressed","text":"Bugs addressed since release-3.11.2 are listed below. #1475637 : [Scale] : Client logs flooded with \"inode context is NULL\" error messages #1476822 : scripts: invalid test in S32gluster_enable_shared_storage.sh #1476870 : [EC]: md5sum mismatches every time for a file from the fuse client on EC volume #1476873 : packaging: /var/lib/glusterd/options should be %config(noreplace) #1479656 : Permission denied errors when appending files after readdir #1479692 : Running sysbench on vm disk from plain distribute gluster volume causes disk corruption","title":"Bugs addressed"},{"location":"release-notes/3.12.0/","text":"Release notes for Gluster 3.12.0 This is a major Gluster release that includes, ability to mount sub-directories using the Gluster native protocol (FUSE), further brick multiplexing enhancements that help scale to larger brick counts per node, enhancements to gluster get-state CLI enabling better understanding of various bricks and nodes participation/roles in the cluster, ability to resolve GFID split-brain using existing CLI, easier GFID to real path mapping thus enabling easier diagnostics and correction for reported GFID issues (healing among other uses where GFID is the only available source for identifying a file), and other changes and fixes. The most notable features and changes are documented on this page. A full list of bugs that have been addressed is included further below. Further, as 3.11 release is a short term maintenance release, features included in that release are available with 3.12 as well, and could be of interest to users upgrading to 3.12 from older than 3.11 releases. The 3.11 release notes captures the list of features that were introduced with 3.11. Major changes and features Ability to mount sub-directories using the Gluster FUSE protocol Notes for users: With this release, it is possible define sub-directories to be mounted by specific clients and additional granularity in the form of clients to mount only that portion of the volume for access. Until recently, Gluster FUSE mounts enabled mounting the entire volume on the client. This feature helps sharing a volume among the multiple consumers along with enabling restricting access to the sub-directory of choice. Option controlling sub-directory allow/deny rules can be set as follows: # gluster volume set <volname> auth.allow \"/subdir1(192.168.1.*),/(192.168.10.*),/subdir2(192.168.8.*)\" How to mount from the client: # mount -t glusterfs <hostname>:/<volname>/<subdir> /<mount_point> Or, # mount -t glusterfs <hostname>:/<volname> -osubdir_mount=<subdir> /<mount_point> Limitations: There are no throttling or QoS support for this feature. The feature will just provide the namespace isolation for the different clients. Known Issues: Once we cross more than 1000s of subdirs in 'auth.allow' option, the performance of reconnect / authentication would be impacted. GFID to path conversion is enabled by default Notes for users: Prior to this feature, only when quota was enabled, did the on disk data have pointers back from GFID to their respective filenames. As a result, if there were a need to locate the path given a GFID, quota had to be enabled. The change brought in by this feature, is to enable this on disk data to be present, for all cases, than just quota. Further, enhancements here have been to improve the manner of storing this information on disk as extended attributes. The internal on disk xattr that is now stored to reference the filename and parent for a GFID is, trusted.gfid2path.<xxhash> This feature is enabled by default with this release. Limitations: None Known Issues: None Various enhancements have been made to the output of get-state CLI command Notes for users: The command #gluster get-state has been enhanced to output more information as below, - Arbiter bricks are marked more clearly in a volume that has the feature enabled - Ability to get all volume options (both set and defaults) in the get-state output - Rebalance time estimates, for ongoing rebalance, is captured in the get-state output - If geo-replication is configured, then get-state now captures the session details of the same Limitations: None Known Issues: None Provided an option to set a limit on number of bricks multiplexed in a processes Notes for users: This release includes a global option to be switched on only if brick multiplexing is enabled for the cluster. The introduction of this option allows the user to control the number of bricks that are multiplexed in a process on a node. If the limit set by this option is insufficient for a single process, more processes are spawned for the subsequent bricks. Usage: #gluster volume set all cluster.max-bricks-per-process <value> Provided an option to use localtime timestamps in log entries Limitations: Gluster defaults to UTC timestamps. glusterd, glusterfsd, and server-side glusterfs daemons will use UTC until one of, 1. command line option is processed, 2. gluster config (/var/lib/glusterd/options) is loaded, 3. admin manually sets localtime-logging (cluster.localtime-logging, e.g. #gluster volume set all cluster.localtime-logging enable ). There is no mount option to make the FUSE client enable localtime logging. There is no option in gfapi to enable localtime logging. Enhanced the option to export statfs data for bricks sharing the same backend filesystem Notes for users: In the past 'storage/posix' xlator had an option called option export-statfs-size , which, when set to 'no', exports zero as values for few fields in struct statvfs . These are typically reflected in an output of df command, from a user perspective. When backend bricks are shared between multiple brick processes, the values of these variables have been corrected to reflect field_value / number-of-bricks-at-node . Thus enabling better usage reporting and also enhancing the ability for file placement in the distribute translator when used with the option min-free-disk . Provided a means to resolve GFID split-brain using the gluster CLI Notes for users: The existing CLI commands to heal files under split-brain did not handle cases where there was a GFID mismatch between the files. With the provided enhancement the same CLI commands can now address GFID split-brain situations based on the choices provided. The CLI options that are enhanced to help with this situation are, volume heal <VOLNAME> split-brain {bigger-file <FILE> | latest-mtime <FILE> | source-brick <HOSTNAME:BRICKNAME> [<FILE>]} Limitations: None Known Issues: None Developer related: Added a 'site.h' for more vendor/company specific defaults Notes for developers: NOTE : Also relevant for users building from sources and needing different defaults for some options Most people consume Gluster in one of two ways: * From packages provided by their OS/distribution vendor * By building themselves from source For the first group it doesn't matter whether configuration is done in a configure script, via command-line options to that configure script, or in a header file. All of these end up as edits to some file under the packager's control, which is then run through their tools and process (e.g. rpmbuild) to create the packages that users will install. For the second group, convenience matters. Such users might not even have a script wrapped around the configure process, and editing one line in a header file is a lot easier than editing several in the configure script. This also prevents a messy profusion of configure options, dozens of which might need to be added to support a single such user's preferences. This comes back around as greater simplicity for packagers as well. This patch defines site.h as the header file for options and parameters that someone building the code for themselves might want to tweak. The project ships one version to reflect the developers' guess at the best defaults for most users, and sophisticated users with unusual needs can override many options at once just by maintaining their own version of that file. Further guidelines for how to determine whether an option should go in configure.ac or site.h are explained within site.h itself. Developer related: Added xxhash library to libglusterfs for required use Notes for developers: Function gf_xxh64_wrapper has been added as a wrapper into libglusterfs for consumption by interested developers. Reference to code can be found here Developer related: glfs_ipc API in libgfapi is removed as a public interface Notes for users: glfs_ipc API was maintained as a public API in the GFAPI libraries. This has been removed as a public interface, from this release onwards. Any application, written directly to consume gfapi as a means of interfacing with Gluster, using the mentioned API, would need to be modified to adapt to this change. NOTE: As of this release there are no known public consumers of this API Major issues Expanding a gluster volume that is sharded may cause file corruption Sharded volumes are typically used for VM images, if such volumes are expanded or possibly contracted (i.e add/remove bricks and rebalance) there are reports of VM images getting corrupted. The last known cause for corruption (Bug #1465123) has a fix with this release. As further testing is still in progress, the issue is retained as a major issue. Status of this bug can be tracked here, #1465123 Bugs addressed Bugs addressed since release-3.11.0 are listed below. #1047975 : glusterfs/extras: add a convenience script to label (selinux) gluster bricks #1254002 : [RFE] Have named pthreads for easier debugging #1318100 : RFE : SELinux translator to support setting SELinux contexts on files in a glusterfs volume #1318895 : Heal info shows incorrect status #1326219 : Make Gluster/NFS an optional component #1356453 : DHT: slow readdirp performance #1366817 : AFR returns the node uuid of the same node for every file in the replica #1381970 : GlusterFS Daemon stops working after a longer runtime and higher file workload due to design flaws? #1400924 : [RFE] Rsync flags for performance improvements #1402406 : Client stale file handle error in dht-linkfile.c under SPEC SFS 2014 VDA workload #1414242 : [whql][virtio-block+glusterfs]\"Disk Stress\" and \"Disk Verification\" job always failed on win7-32/win2012/win2k8R2 guest #1421938 : systemic testing: seeing lot of ping time outs which would lead to splitbrains #1424817 : Fix wrong operators, found by coverty #1428061 : Halo Replication feature for AFR translator #1428673 : possible repeatedly recursive healing of same file with background heal not happening when IO is going on #1430608 : [RFE] Pass slave volume in geo-rep as read-only #1431908 : Enabling parallel-readdir causes dht linkto files to be visible on the mount, #1433906 : quota: limit-usage command failed with error \" Failed to start aux mount\" #1437748 : Spacing issue in fix-layout status output #1438966 : Multiple bricks WILL crash after TCP port probing #1439068 : Segmentation fault when creating a qcow2 with qemu-img #1442569 : Implement Negative lookup cache feature to improve create performance #1442788 : Cleanup timer wheel in glfs_fini() #1442950 : RFE: Enhance handleops readdirplus operation to return handles along with dirents #1444596 : [Brick Multiplexing] : Bricks for multiple volumes going down after glusterd restart and not coming back up after volume start force #1445609 : [perf-xlators/write-behind] write-behind-window-size could be set greater than its allowed MAX value 1073741824 #1446172 : Brick Multiplexing :- resetting a brick bring down other bricks with same PID #1446362 : cli xml status of detach tier broken #1446412 : error-gen don't need to convert error string to int in every fop #1446516 : [Parallel Readdir] : Mounts fail when performance.parallel-readdir is set to \"off\" #1447116 : gfapi exports non-existing glfs_upcall_inode_get_event symbol #1447266 : [snapshot cifs]ls on .snaps directory is throwing input/output error over cifs mount #1447389 : Brick Multiplexing: seeing Input/Output Error for .trashcan #1447609 : server: fd should be refed before put into fdtable #1447630 : Don't allow rebalance/fix-layout operation on sharding enabled volumes till dht+sharding bugs are fixed #1447826 : potential endless loop in function glusterfs_graph_validate_options #1447828 : Should use dict_set_uint64 to set fd->pid when dump fd's info to dict #1447953 : Remove inadvertently merged IPv6 code #1447960 : [Tiering]: High and low watermark values when set to the same level, is allowed #1447966 : 'make cscope' fails on a clean tree due to missing generated XDR files #1448150 : USS: stale snap entries are seen when activation/deactivation performed during one of the glusterd's unavailability #1448265 : use common function iov_length to instead of duplicate code #1448293 : Implement FALLOCATE FOP for EC #1448299 : Mismatch in checksum of the image file after copying to a new image file #1448364 : limited throughput with disperse volume over small number of bricks #1448640 : Seeing error \"Failed to get the total number of files. Unable to estimate time to complete rebalance\" in rebalance logs #1448692 : use GF_ATOMIC to generate callid #1448804 : afr: include quorum type and count when dumping afr priv #1448914 : [geo-rep]: extended attributes are not synced if the entry and extended attributes are done within changelog roleover/or entry sync #1449008 : remove useless options from glusterd's volume set table #1449232 : race condition between client_ctx_get and client_ctx_set #1449329 : When either killing or restarting a brick with performance.stat-prefetch on, stat sometimes returns a bad st_size value. #1449348 : disperse seek does not correctly handle the end of file #1449495 : glfsheal: crashed(segfault) with disperse volume in RDMA #1449610 : [New] - Replacing an arbiter brick while I/O happens causes vm pause #1450010 : [gluster-block]:Need a volume group profile option for gluster-block volume to add necessary options to be added. #1450559 : Error 0-socket.management: socket_poller XX.XX.XX.XX:YYY failed (Input/output error) during any volume operation #1450630 : [brick multiplexing] detach a brick if posix health check thread complaints about underlying brick #1450730 : Add tests/basic/afr/gfid-mismatch-resolution-with-fav-child-policy.t to bad tests #1450975 : Fix on demand file migration from client #1451083 : crash in dht_rmdir_do #1451162 : dht: Make throttle option \"normal\" value uniform across dht_init and dht_reconfigure #1451248 : Brick Multiplexing: On reboot of a node Brick multiplexing feature lost on that node as multiple brick processes get spawned #1451588 : [geo-rep + nl]: Multiple crashes observed on slave with \"nlc_lookup_cbk\" #1451724 : glusterfind pre crashes with \"UnicodeDecodeError: 'utf8' codec can't decode\" error when the --no-encode is used #1452006 : tierd listens to a port. #1452084 : [Ganesha] : Stale linkto files after unsuccessfuly hardlinks #1452102 : [DHt] : segfault in dht_selfheal_dir_setattr while running regressions #1452378 : Cleanup unnecessary logs in fix_quorum_options #1452527 : Shared volume doesn't get mounted on few nodes after rebooting all nodes in cluster. #1452956 : glusterd on a node crashed after running volume profile command #1453151 : [RFE] glusterfind: add --end-time and --field-separator options #1453977 : Brick Multiplexing: Deleting brick directories of the base volume must gracefully detach from glusterfsd without impacting other volumes IO(currently seeing transport end point error) #1454317 : [Bitrot]: Brick process crash observed while trying to recover a bad file in disperse volume #1454375 : ignore incorrect uuid validation in gd_validate_mgmt_hndsk_req #1454418 : Glusterd segmentation fault in ' _Unwind_Backtrace' while running peer probe #1454701 : DHT: Pass errno as an argument to gf_msg #1454865 : [Brick Multiplexing] heal info shows the status of the bricks as \"Transport endpoint is not connected\" though bricks are up #1454872 : [Geo-rep]: Make changelog batch size configurable #1455049 : [GNFS+EC] Unable to release the lock when the other client tries to acquire the lock on the same file #1455104 : dht: dht self heal fails with no hashed subvol error #1455179 : [Geo-rep]: Log time taken to sync entry ops, metadata ops and data ops for each batch #1455301 : gluster-block is not working as expected when shard is enabled #1455559 : [Geo-rep]: METADATA errors are seen even though everything is in sync #1455831 : libglusterfs: updates old comment for 'arena_size' #1456361 : DHT : for many operation directory/file path is '(null)' in brick log #1456385 : glusterfs client crash on io-cache.so(__ioc_page_wakeup+0x44) #1456405 : Brick Multiplexing:dmesg shows request_sock_TCP: Possible SYN flooding on port 49152 and memory related backtraces #1456582 : \"split-brain observed [Input/output error]\" error messages in samba logs during parallel rm -rf #1456653 : nlc_lookup_cbk floods logs #1456898 : Regression test for add-brick failing with brick multiplexing enabled #1457202 : Use of force with volume start, creates brick directory even it is not present #1457808 : all: spelling errors (debian package maintainer) #1457812 : extras/hook-scripts: non-portable shell syntax (debian package maintainer) #1457981 : client fails to connect to the brick due to an incorrect port reported back by glusterd #1457985 : Rebalance estimate time sometimes shows negative values #1458127 : Upcall missing invalidations #1458193 : Implement seek() fop in trace translator #1458197 : io-stats usability/performance statistics enhancements #1458539 : [Negative Lookup]: negative lookup features doesn't seem to work on restart of volume #1458582 : add all as volume option in gluster volume get usage #1458768 : [Perf] 35% drop in small file creates on smbv3 on *2 #1459402 : brick process crashes while running bug-1432542-mpx-restart-crash.t in a loop #1459530 : [RFE] Need a way to resolve gfid split brains #1459620 : [geo-rep]: Worker crashed with TypeError: expected string or buffer #1459781 : Brick Multiplexing:Even clean Deleting of the brick directories of base volume is resulting in posix health check errors(just as we see in ungraceful delete methods) #1459971 : posix-acl: Whitelist virtual ACL xattrs #1460225 : Not cleaning up stale socket file is resulting in spamming glusterd logs with warnings of \"got disconnect from stale rpc\" #1460514 : [Ganesha] : Ganesha crashes while cluster enters failover/failback mode #1460585 : Revert CLI restrictions on running rebalance in VM store use case #1460638 : ec-data-heal.t fails with brick mux enabled #1460659 : Avoid one extra call of l(get|list)xattr system call after use buffer in posix_getxattr #1461129 : malformed cluster.server-quorum-ratio setting can lead to split brain #1461648 : Update GlusterFS README #1461655 : glusterd crashes when statedump is taken #1461792 : lk fop succeeds even when lock is not acquired on at least quorum number of bricks #1461845 : [Bitrot]: Inconsistency seen with 'scrub ondemand' - fails to trigger scrub #1462200 : glusterd status showing failed when it's stopped in RHEL7 #1462241 : glusterfind: syntax error due to uninitialized variable 'end' #1462790 : with AFR now making both nodes to return UUID for a file will result in georep consuming more resources #1463178 : [Ganesha]Bricks got crashed while running posix compliance test suit on V4 mount #1463365 : Changes for Maintainers 2.0 #1463648 : Use GF_XATTR_LIST_NODE_UUIDS_KEY to figure out local subvols #1464072 : cns-brick-multiplexing: brick process fails to restart after gluster pod failure #1464091 : Regression: Heal info takes longer time when a brick is down #1464110 : [Scale] : Rebalance ETA (towards the end) may be inaccurate,even on a moderately large data set. #1464327 : glusterfs client crashes when reading large directory #1464359 : selfheal deamon cpu consumption not reducing when IOs are going on and all redundant bricks are brought down one after another #1465024 : glusterfind: DELETE path needs to be unquoted before further processing #1465075 : Fd based fops fail with EBADF on file migration #1465214 : build failed with GF_DISABLE_MEMPOOL #1465559 : multiple brick processes seen on gluster(fs)d restart in brick multiplexing #1466037 : Fuse mount crashed with continuous dd on a file and reading the file in parallel #1466110 : dht_rename_lock_cbk crashes in upstream regression test #1466188 : Add scripts to analyze quota xattr in backend and identify accounting issues #1466785 : assorted typos and spelling mistakes from Debian lintian #1467209 : [Scale] : Rebalance ETA shows the initial estimate to be ~140 days,finishes within 18 hours though. #1467277 : [GSS] [RFE] add documentation on --xml and --mode=script options to gluster interactive help and man pages #1467313 : cthon04 can cause segfault in gNFS/NLM #1467513 : CIFS:[USS]: .snaps is not accessible from the CIFS client after volume stop/start #1467718 : [Geo-rep]: entry failed to sync to slave with ENOENT errror #1467841 : gluster volume status --xml fails when there are 100 volumes #1467986 : possible memory leak in glusterfsd with multiplexing #1468191 : Enable stat-prefetch in group virt #1468261 : Regression: non-disruptive(in-service) upgrade on EC volume fails #1468279 : metadata heal not happening despite having an active sink #1468291 : NFS Sub directory is getting mounted on solaris 10 even when the permission is restricted in nfs.export-dir volume option #1468432 : tests: fix stats-dump.t failure #1468433 : rpc: include current second in timed out frame cleanup on client #1468863 : Assert in mem_pools_fini during libgfapi-fini-hang.t on NetBSD #1469029 : Rebalance hangs on remove-brick if the target volume changes #1469179 : invoke checkpatch.pl with strict #1469964 : cluster/dht: Fix hardlink migration failures #1470170 : mem-pool: mem_pool_fini() doesn't release entire memory allocated #1470220 : glusterfs process leaking memory when error occurs #1470489 : bulk removexattr shouldn't allow removal of trusted.gfid/trusted.glusterfs.volume-id #1470533 : Brick Mux Setup: brick processes(glusterfsd) crash after a restart of volume which was preceded with some actions #1470768 : file /usr/lib64/glusterfs/3.12dev/xlator is not owned by any package #1471790 : [Brick Multiplexing] : cluster.brick-multiplex has no description. #1472094 : Test script failing with brick multiplexing enabled #1472250 : Remove fop_enum_to_string, get_fop_int usage in libglusterfs #1472417 : No clear method to multiplex all bricks to one process(glusterfsd) with cluster.max-bricks-per-process option #1472949 : [distribute] crashes seen upon rmdirs #1475181 : dht remove-brick status does not indicate failures files not migrated because of a lack of space #1475192 : [Scale] : Rebalance ETA shows the initial estimate to be ~140 days,finishes within 18 hours though. #1475258 : [Geo-rep]: Geo-rep hangs in changelog mode #1475399 : Rebalance estimate time sometimes shows negative values #1475635 : [Scale] : Client logs flooded with \"inode context is NULL\" error messages #1475641 : gluster core dump due to assert failed GF_ASSERT (brick_index < wordcount); #1475662 : [Scale] : Rebalance Logs are bulky. #1476109 : Brick Multiplexing: Brick process crashed at changetimerecorder(ctr) translator when restarting volumes #1476208 : [geo-rep]: few of the self healed hardlinks on master did not sync to slave #1476653 : cassandra fails on gluster-block with both replicate and ec volumes #1476654 : gluster-block default shard-size should be 64MB #1476819 : scripts: invalid test in S32gluster_enable_shared_storage.sh #1476863 : packaging: /var/lib/glusterd/options should be %config(noreplace) #1476868 : [EC]: md5sum mismatches every time for a file from the fuse client on EC volume #1477152 : [Remove-brick] Few files are getting migrated eventhough the bricks crossed cluster.min-free-disk value #1477190 : [GNFS] GNFS got crashed while mounting volume on solaris client #1477381 : Revert experimental and 4.0 features to prepare for 3.12 release #1477405 : eager-lock should be off for cassandra to work at the moment #1477994 : [Ganesha] : Ganesha crashes while cluster enters failover/failback mode #1478276 : separating attach tier and add brick #1479118 : AFR entry self heal removes a directory's .glusterfs symlink. #1479263 : nfs process crashed in \"nfs3svc_getattr\" #1479303 : [Perf] : Large file sequential reads are off target by ~38% on FUSE/Ganesha #1479474 : Add NULL gfid checks before creating file #1479655 : Permission denied errors when appending files after readdir #1479662 : when gluster pod is restarted, bricks from the restarted pod fails to connect to fuse, self-heal etc #1479717 : Running sysbench on vm disk from plain distribute gluster volume causes disk corruption #1480448 : More useful error - replace 'not optimal' #1480459 : Gluster puts PID files in wrong place #1481931 : [Scale] : I/O errors on multiple gNFS mounts with \"Stale file handle\" during rebalance of an erasure coded volume. #1482804 : Negative Test: glusterd crashes for some of the volume options if set at cluster level #1482835 : glusterd fails to start #1483402 : DHT: readdirp fails to read some directories. #1483996 : packaging: use rdma-core(-devel) instead of ibverbs, rdmacm; disable rdma on armv7hl #1484440 : packaging: /run and /var/run; prefer /run #1484885 : [rpc]: EPOLLERR - disconnecting now messages every 3 secs after completing rebalance #1486107 : /var/lib/glusterd/peers File had a blank line, Stopped Glusterd from starting #1486110 : [quorum]: Replace brick is happened when Quorum not met. #1486120 : symlinks trigger faulty geo-replication state (rsnapshot usecase) #1486122 : gluster-block profile needs to have strict-o-direct","title":"3.12.0"},{"location":"release-notes/3.12.0/#release-notes-for-gluster-3120","text":"This is a major Gluster release that includes, ability to mount sub-directories using the Gluster native protocol (FUSE), further brick multiplexing enhancements that help scale to larger brick counts per node, enhancements to gluster get-state CLI enabling better understanding of various bricks and nodes participation/roles in the cluster, ability to resolve GFID split-brain using existing CLI, easier GFID to real path mapping thus enabling easier diagnostics and correction for reported GFID issues (healing among other uses where GFID is the only available source for identifying a file), and other changes and fixes. The most notable features and changes are documented on this page. A full list of bugs that have been addressed is included further below. Further, as 3.11 release is a short term maintenance release, features included in that release are available with 3.12 as well, and could be of interest to users upgrading to 3.12 from older than 3.11 releases. The 3.11 release notes captures the list of features that were introduced with 3.11.","title":"Release notes for Gluster 3.12.0"},{"location":"release-notes/3.12.0/#major-changes-and-features","text":"","title":"Major changes and features"},{"location":"release-notes/3.12.0/#ability-to-mount-sub-directories-using-the-gluster-fuse-protocol","text":"Notes for users: With this release, it is possible define sub-directories to be mounted by specific clients and additional granularity in the form of clients to mount only that portion of the volume for access. Until recently, Gluster FUSE mounts enabled mounting the entire volume on the client. This feature helps sharing a volume among the multiple consumers along with enabling restricting access to the sub-directory of choice. Option controlling sub-directory allow/deny rules can be set as follows: # gluster volume set <volname> auth.allow \"/subdir1(192.168.1.*),/(192.168.10.*),/subdir2(192.168.8.*)\" How to mount from the client: # mount -t glusterfs <hostname>:/<volname>/<subdir> /<mount_point> Or, # mount -t glusterfs <hostname>:/<volname> -osubdir_mount=<subdir> /<mount_point> Limitations: There are no throttling or QoS support for this feature. The feature will just provide the namespace isolation for the different clients. Known Issues: Once we cross more than 1000s of subdirs in 'auth.allow' option, the performance of reconnect / authentication would be impacted.","title":"Ability to mount sub-directories using the Gluster FUSE protocol"},{"location":"release-notes/3.12.0/#gfid-to-path-conversion-is-enabled-by-default","text":"Notes for users: Prior to this feature, only when quota was enabled, did the on disk data have pointers back from GFID to their respective filenames. As a result, if there were a need to locate the path given a GFID, quota had to be enabled. The change brought in by this feature, is to enable this on disk data to be present, for all cases, than just quota. Further, enhancements here have been to improve the manner of storing this information on disk as extended attributes. The internal on disk xattr that is now stored to reference the filename and parent for a GFID is, trusted.gfid2path.<xxhash> This feature is enabled by default with this release. Limitations: None Known Issues: None","title":"GFID to path conversion is enabled by default"},{"location":"release-notes/3.12.0/#various-enhancements-have-been-made-to-the-output-of-get-state-cli-command","text":"Notes for users: The command #gluster get-state has been enhanced to output more information as below, - Arbiter bricks are marked more clearly in a volume that has the feature enabled - Ability to get all volume options (both set and defaults) in the get-state output - Rebalance time estimates, for ongoing rebalance, is captured in the get-state output - If geo-replication is configured, then get-state now captures the session details of the same Limitations: None Known Issues: None","title":"Various enhancements have been made to the output of get-state CLI command"},{"location":"release-notes/3.12.0/#provided-an-option-to-set-a-limit-on-number-of-bricks-multiplexed-in-a-processes","text":"Notes for users: This release includes a global option to be switched on only if brick multiplexing is enabled for the cluster. The introduction of this option allows the user to control the number of bricks that are multiplexed in a process on a node. If the limit set by this option is insufficient for a single process, more processes are spawned for the subsequent bricks. Usage: #gluster volume set all cluster.max-bricks-per-process <value>","title":"Provided an option to set a limit on number of bricks multiplexed in a processes"},{"location":"release-notes/3.12.0/#provided-an-option-to-use-localtime-timestamps-in-log-entries","text":"Limitations: Gluster defaults to UTC timestamps. glusterd, glusterfsd, and server-side glusterfs daemons will use UTC until one of, 1. command line option is processed, 2. gluster config (/var/lib/glusterd/options) is loaded, 3. admin manually sets localtime-logging (cluster.localtime-logging, e.g. #gluster volume set all cluster.localtime-logging enable ). There is no mount option to make the FUSE client enable localtime logging. There is no option in gfapi to enable localtime logging.","title":"Provided an option to use localtime timestamps in log entries"},{"location":"release-notes/3.12.0/#enhanced-the-option-to-export-statfs-data-for-bricks-sharing-the-same-backend-filesystem","text":"Notes for users: In the past 'storage/posix' xlator had an option called option export-statfs-size , which, when set to 'no', exports zero as values for few fields in struct statvfs . These are typically reflected in an output of df command, from a user perspective. When backend bricks are shared between multiple brick processes, the values of these variables have been corrected to reflect field_value / number-of-bricks-at-node . Thus enabling better usage reporting and also enhancing the ability for file placement in the distribute translator when used with the option min-free-disk .","title":"Enhanced the option to export statfs data for bricks sharing the same backend filesystem"},{"location":"release-notes/3.12.0/#provided-a-means-to-resolve-gfid-split-brain-using-the-gluster-cli","text":"Notes for users: The existing CLI commands to heal files under split-brain did not handle cases where there was a GFID mismatch between the files. With the provided enhancement the same CLI commands can now address GFID split-brain situations based on the choices provided. The CLI options that are enhanced to help with this situation are, volume heal <VOLNAME> split-brain {bigger-file <FILE> | latest-mtime <FILE> | source-brick <HOSTNAME:BRICKNAME> [<FILE>]} Limitations: None Known Issues: None","title":"Provided a means to resolve GFID split-brain using the gluster CLI"},{"location":"release-notes/3.12.0/#developer-related-added-a-siteh-for-more-vendorcompany-specific-defaults","text":"Notes for developers: NOTE : Also relevant for users building from sources and needing different defaults for some options Most people consume Gluster in one of two ways: * From packages provided by their OS/distribution vendor * By building themselves from source For the first group it doesn't matter whether configuration is done in a configure script, via command-line options to that configure script, or in a header file. All of these end up as edits to some file under the packager's control, which is then run through their tools and process (e.g. rpmbuild) to create the packages that users will install. For the second group, convenience matters. Such users might not even have a script wrapped around the configure process, and editing one line in a header file is a lot easier than editing several in the configure script. This also prevents a messy profusion of configure options, dozens of which might need to be added to support a single such user's preferences. This comes back around as greater simplicity for packagers as well. This patch defines site.h as the header file for options and parameters that someone building the code for themselves might want to tweak. The project ships one version to reflect the developers' guess at the best defaults for most users, and sophisticated users with unusual needs can override many options at once just by maintaining their own version of that file. Further guidelines for how to determine whether an option should go in configure.ac or site.h are explained within site.h itself.","title":"Developer related: Added a 'site.h' for more vendor/company specific defaults"},{"location":"release-notes/3.12.0/#developer-related-added-xxhash-library-to-libglusterfs-for-required-use","text":"Notes for developers: Function gf_xxh64_wrapper has been added as a wrapper into libglusterfs for consumption by interested developers. Reference to code can be found here","title":"Developer related: Added xxhash library to libglusterfs for required use"},{"location":"release-notes/3.12.0/#developer-related-glfs_ipc-api-in-libgfapi-is-removed-as-a-public-interface","text":"Notes for users: glfs_ipc API was maintained as a public API in the GFAPI libraries. This has been removed as a public interface, from this release onwards. Any application, written directly to consume gfapi as a means of interfacing with Gluster, using the mentioned API, would need to be modified to adapt to this change. NOTE: As of this release there are no known public consumers of this API","title":"Developer related: glfs_ipc API in libgfapi is removed as a public interface"},{"location":"release-notes/3.12.0/#major-issues","text":"Expanding a gluster volume that is sharded may cause file corruption Sharded volumes are typically used for VM images, if such volumes are expanded or possibly contracted (i.e add/remove bricks and rebalance) there are reports of VM images getting corrupted. The last known cause for corruption (Bug #1465123) has a fix with this release. As further testing is still in progress, the issue is retained as a major issue. Status of this bug can be tracked here, #1465123","title":"Major issues"},{"location":"release-notes/3.12.0/#bugs-addressed","text":"Bugs addressed since release-3.11.0 are listed below. #1047975 : glusterfs/extras: add a convenience script to label (selinux) gluster bricks #1254002 : [RFE] Have named pthreads for easier debugging #1318100 : RFE : SELinux translator to support setting SELinux contexts on files in a glusterfs volume #1318895 : Heal info shows incorrect status #1326219 : Make Gluster/NFS an optional component #1356453 : DHT: slow readdirp performance #1366817 : AFR returns the node uuid of the same node for every file in the replica #1381970 : GlusterFS Daemon stops working after a longer runtime and higher file workload due to design flaws? #1400924 : [RFE] Rsync flags for performance improvements #1402406 : Client stale file handle error in dht-linkfile.c under SPEC SFS 2014 VDA workload #1414242 : [whql][virtio-block+glusterfs]\"Disk Stress\" and \"Disk Verification\" job always failed on win7-32/win2012/win2k8R2 guest #1421938 : systemic testing: seeing lot of ping time outs which would lead to splitbrains #1424817 : Fix wrong operators, found by coverty #1428061 : Halo Replication feature for AFR translator #1428673 : possible repeatedly recursive healing of same file with background heal not happening when IO is going on #1430608 : [RFE] Pass slave volume in geo-rep as read-only #1431908 : Enabling parallel-readdir causes dht linkto files to be visible on the mount, #1433906 : quota: limit-usage command failed with error \" Failed to start aux mount\" #1437748 : Spacing issue in fix-layout status output #1438966 : Multiple bricks WILL crash after TCP port probing #1439068 : Segmentation fault when creating a qcow2 with qemu-img #1442569 : Implement Negative lookup cache feature to improve create performance #1442788 : Cleanup timer wheel in glfs_fini() #1442950 : RFE: Enhance handleops readdirplus operation to return handles along with dirents #1444596 : [Brick Multiplexing] : Bricks for multiple volumes going down after glusterd restart and not coming back up after volume start force #1445609 : [perf-xlators/write-behind] write-behind-window-size could be set greater than its allowed MAX value 1073741824 #1446172 : Brick Multiplexing :- resetting a brick bring down other bricks with same PID #1446362 : cli xml status of detach tier broken #1446412 : error-gen don't need to convert error string to int in every fop #1446516 : [Parallel Readdir] : Mounts fail when performance.parallel-readdir is set to \"off\" #1447116 : gfapi exports non-existing glfs_upcall_inode_get_event symbol #1447266 : [snapshot cifs]ls on .snaps directory is throwing input/output error over cifs mount #1447389 : Brick Multiplexing: seeing Input/Output Error for .trashcan #1447609 : server: fd should be refed before put into fdtable #1447630 : Don't allow rebalance/fix-layout operation on sharding enabled volumes till dht+sharding bugs are fixed #1447826 : potential endless loop in function glusterfs_graph_validate_options #1447828 : Should use dict_set_uint64 to set fd->pid when dump fd's info to dict #1447953 : Remove inadvertently merged IPv6 code #1447960 : [Tiering]: High and low watermark values when set to the same level, is allowed #1447966 : 'make cscope' fails on a clean tree due to missing generated XDR files #1448150 : USS: stale snap entries are seen when activation/deactivation performed during one of the glusterd's unavailability #1448265 : use common function iov_length to instead of duplicate code #1448293 : Implement FALLOCATE FOP for EC #1448299 : Mismatch in checksum of the image file after copying to a new image file #1448364 : limited throughput with disperse volume over small number of bricks #1448640 : Seeing error \"Failed to get the total number of files. Unable to estimate time to complete rebalance\" in rebalance logs #1448692 : use GF_ATOMIC to generate callid #1448804 : afr: include quorum type and count when dumping afr priv #1448914 : [geo-rep]: extended attributes are not synced if the entry and extended attributes are done within changelog roleover/or entry sync #1449008 : remove useless options from glusterd's volume set table #1449232 : race condition between client_ctx_get and client_ctx_set #1449329 : When either killing or restarting a brick with performance.stat-prefetch on, stat sometimes returns a bad st_size value. #1449348 : disperse seek does not correctly handle the end of file #1449495 : glfsheal: crashed(segfault) with disperse volume in RDMA #1449610 : [New] - Replacing an arbiter brick while I/O happens causes vm pause #1450010 : [gluster-block]:Need a volume group profile option for gluster-block volume to add necessary options to be added. #1450559 : Error 0-socket.management: socket_poller XX.XX.XX.XX:YYY failed (Input/output error) during any volume operation #1450630 : [brick multiplexing] detach a brick if posix health check thread complaints about underlying brick #1450730 : Add tests/basic/afr/gfid-mismatch-resolution-with-fav-child-policy.t to bad tests #1450975 : Fix on demand file migration from client #1451083 : crash in dht_rmdir_do #1451162 : dht: Make throttle option \"normal\" value uniform across dht_init and dht_reconfigure #1451248 : Brick Multiplexing: On reboot of a node Brick multiplexing feature lost on that node as multiple brick processes get spawned #1451588 : [geo-rep + nl]: Multiple crashes observed on slave with \"nlc_lookup_cbk\" #1451724 : glusterfind pre crashes with \"UnicodeDecodeError: 'utf8' codec can't decode\" error when the --no-encode is used #1452006 : tierd listens to a port. #1452084 : [Ganesha] : Stale linkto files after unsuccessfuly hardlinks #1452102 : [DHt] : segfault in dht_selfheal_dir_setattr while running regressions #1452378 : Cleanup unnecessary logs in fix_quorum_options #1452527 : Shared volume doesn't get mounted on few nodes after rebooting all nodes in cluster. #1452956 : glusterd on a node crashed after running volume profile command #1453151 : [RFE] glusterfind: add --end-time and --field-separator options #1453977 : Brick Multiplexing: Deleting brick directories of the base volume must gracefully detach from glusterfsd without impacting other volumes IO(currently seeing transport end point error) #1454317 : [Bitrot]: Brick process crash observed while trying to recover a bad file in disperse volume #1454375 : ignore incorrect uuid validation in gd_validate_mgmt_hndsk_req #1454418 : Glusterd segmentation fault in ' _Unwind_Backtrace' while running peer probe #1454701 : DHT: Pass errno as an argument to gf_msg #1454865 : [Brick Multiplexing] heal info shows the status of the bricks as \"Transport endpoint is not connected\" though bricks are up #1454872 : [Geo-rep]: Make changelog batch size configurable #1455049 : [GNFS+EC] Unable to release the lock when the other client tries to acquire the lock on the same file #1455104 : dht: dht self heal fails with no hashed subvol error #1455179 : [Geo-rep]: Log time taken to sync entry ops, metadata ops and data ops for each batch #1455301 : gluster-block is not working as expected when shard is enabled #1455559 : [Geo-rep]: METADATA errors are seen even though everything is in sync #1455831 : libglusterfs: updates old comment for 'arena_size' #1456361 : DHT : for many operation directory/file path is '(null)' in brick log #1456385 : glusterfs client crash on io-cache.so(__ioc_page_wakeup+0x44) #1456405 : Brick Multiplexing:dmesg shows request_sock_TCP: Possible SYN flooding on port 49152 and memory related backtraces #1456582 : \"split-brain observed [Input/output error]\" error messages in samba logs during parallel rm -rf #1456653 : nlc_lookup_cbk floods logs #1456898 : Regression test for add-brick failing with brick multiplexing enabled #1457202 : Use of force with volume start, creates brick directory even it is not present #1457808 : all: spelling errors (debian package maintainer) #1457812 : extras/hook-scripts: non-portable shell syntax (debian package maintainer) #1457981 : client fails to connect to the brick due to an incorrect port reported back by glusterd #1457985 : Rebalance estimate time sometimes shows negative values #1458127 : Upcall missing invalidations #1458193 : Implement seek() fop in trace translator #1458197 : io-stats usability/performance statistics enhancements #1458539 : [Negative Lookup]: negative lookup features doesn't seem to work on restart of volume #1458582 : add all as volume option in gluster volume get usage #1458768 : [Perf] 35% drop in small file creates on smbv3 on *2 #1459402 : brick process crashes while running bug-1432542-mpx-restart-crash.t in a loop #1459530 : [RFE] Need a way to resolve gfid split brains #1459620 : [geo-rep]: Worker crashed with TypeError: expected string or buffer #1459781 : Brick Multiplexing:Even clean Deleting of the brick directories of base volume is resulting in posix health check errors(just as we see in ungraceful delete methods) #1459971 : posix-acl: Whitelist virtual ACL xattrs #1460225 : Not cleaning up stale socket file is resulting in spamming glusterd logs with warnings of \"got disconnect from stale rpc\" #1460514 : [Ganesha] : Ganesha crashes while cluster enters failover/failback mode #1460585 : Revert CLI restrictions on running rebalance in VM store use case #1460638 : ec-data-heal.t fails with brick mux enabled #1460659 : Avoid one extra call of l(get|list)xattr system call after use buffer in posix_getxattr #1461129 : malformed cluster.server-quorum-ratio setting can lead to split brain #1461648 : Update GlusterFS README #1461655 : glusterd crashes when statedump is taken #1461792 : lk fop succeeds even when lock is not acquired on at least quorum number of bricks #1461845 : [Bitrot]: Inconsistency seen with 'scrub ondemand' - fails to trigger scrub #1462200 : glusterd status showing failed when it's stopped in RHEL7 #1462241 : glusterfind: syntax error due to uninitialized variable 'end' #1462790 : with AFR now making both nodes to return UUID for a file will result in georep consuming more resources #1463178 : [Ganesha]Bricks got crashed while running posix compliance test suit on V4 mount #1463365 : Changes for Maintainers 2.0 #1463648 : Use GF_XATTR_LIST_NODE_UUIDS_KEY to figure out local subvols #1464072 : cns-brick-multiplexing: brick process fails to restart after gluster pod failure #1464091 : Regression: Heal info takes longer time when a brick is down #1464110 : [Scale] : Rebalance ETA (towards the end) may be inaccurate,even on a moderately large data set. #1464327 : glusterfs client crashes when reading large directory #1464359 : selfheal deamon cpu consumption not reducing when IOs are going on and all redundant bricks are brought down one after another #1465024 : glusterfind: DELETE path needs to be unquoted before further processing #1465075 : Fd based fops fail with EBADF on file migration #1465214 : build failed with GF_DISABLE_MEMPOOL #1465559 : multiple brick processes seen on gluster(fs)d restart in brick multiplexing #1466037 : Fuse mount crashed with continuous dd on a file and reading the file in parallel #1466110 : dht_rename_lock_cbk crashes in upstream regression test #1466188 : Add scripts to analyze quota xattr in backend and identify accounting issues #1466785 : assorted typos and spelling mistakes from Debian lintian #1467209 : [Scale] : Rebalance ETA shows the initial estimate to be ~140 days,finishes within 18 hours though. #1467277 : [GSS] [RFE] add documentation on --xml and --mode=script options to gluster interactive help and man pages #1467313 : cthon04 can cause segfault in gNFS/NLM #1467513 : CIFS:[USS]: .snaps is not accessible from the CIFS client after volume stop/start #1467718 : [Geo-rep]: entry failed to sync to slave with ENOENT errror #1467841 : gluster volume status --xml fails when there are 100 volumes #1467986 : possible memory leak in glusterfsd with multiplexing #1468191 : Enable stat-prefetch in group virt #1468261 : Regression: non-disruptive(in-service) upgrade on EC volume fails #1468279 : metadata heal not happening despite having an active sink #1468291 : NFS Sub directory is getting mounted on solaris 10 even when the permission is restricted in nfs.export-dir volume option #1468432 : tests: fix stats-dump.t failure #1468433 : rpc: include current second in timed out frame cleanup on client #1468863 : Assert in mem_pools_fini during libgfapi-fini-hang.t on NetBSD #1469029 : Rebalance hangs on remove-brick if the target volume changes #1469179 : invoke checkpatch.pl with strict #1469964 : cluster/dht: Fix hardlink migration failures #1470170 : mem-pool: mem_pool_fini() doesn't release entire memory allocated #1470220 : glusterfs process leaking memory when error occurs #1470489 : bulk removexattr shouldn't allow removal of trusted.gfid/trusted.glusterfs.volume-id #1470533 : Brick Mux Setup: brick processes(glusterfsd) crash after a restart of volume which was preceded with some actions #1470768 : file /usr/lib64/glusterfs/3.12dev/xlator is not owned by any package #1471790 : [Brick Multiplexing] : cluster.brick-multiplex has no description. #1472094 : Test script failing with brick multiplexing enabled #1472250 : Remove fop_enum_to_string, get_fop_int usage in libglusterfs #1472417 : No clear method to multiplex all bricks to one process(glusterfsd) with cluster.max-bricks-per-process option #1472949 : [distribute] crashes seen upon rmdirs #1475181 : dht remove-brick status does not indicate failures files not migrated because of a lack of space #1475192 : [Scale] : Rebalance ETA shows the initial estimate to be ~140 days,finishes within 18 hours though. #1475258 : [Geo-rep]: Geo-rep hangs in changelog mode #1475399 : Rebalance estimate time sometimes shows negative values #1475635 : [Scale] : Client logs flooded with \"inode context is NULL\" error messages #1475641 : gluster core dump due to assert failed GF_ASSERT (brick_index < wordcount); #1475662 : [Scale] : Rebalance Logs are bulky. #1476109 : Brick Multiplexing: Brick process crashed at changetimerecorder(ctr) translator when restarting volumes #1476208 : [geo-rep]: few of the self healed hardlinks on master did not sync to slave #1476653 : cassandra fails on gluster-block with both replicate and ec volumes #1476654 : gluster-block default shard-size should be 64MB #1476819 : scripts: invalid test in S32gluster_enable_shared_storage.sh #1476863 : packaging: /var/lib/glusterd/options should be %config(noreplace) #1476868 : [EC]: md5sum mismatches every time for a file from the fuse client on EC volume #1477152 : [Remove-brick] Few files are getting migrated eventhough the bricks crossed cluster.min-free-disk value #1477190 : [GNFS] GNFS got crashed while mounting volume on solaris client #1477381 : Revert experimental and 4.0 features to prepare for 3.12 release #1477405 : eager-lock should be off for cassandra to work at the moment #1477994 : [Ganesha] : Ganesha crashes while cluster enters failover/failback mode #1478276 : separating attach tier and add brick #1479118 : AFR entry self heal removes a directory's .glusterfs symlink. #1479263 : nfs process crashed in \"nfs3svc_getattr\" #1479303 : [Perf] : Large file sequential reads are off target by ~38% on FUSE/Ganesha #1479474 : Add NULL gfid checks before creating file #1479655 : Permission denied errors when appending files after readdir #1479662 : when gluster pod is restarted, bricks from the restarted pod fails to connect to fuse, self-heal etc #1479717 : Running sysbench on vm disk from plain distribute gluster volume causes disk corruption #1480448 : More useful error - replace 'not optimal' #1480459 : Gluster puts PID files in wrong place #1481931 : [Scale] : I/O errors on multiple gNFS mounts with \"Stale file handle\" during rebalance of an erasure coded volume. #1482804 : Negative Test: glusterd crashes for some of the volume options if set at cluster level #1482835 : glusterd fails to start #1483402 : DHT: readdirp fails to read some directories. #1483996 : packaging: use rdma-core(-devel) instead of ibverbs, rdmacm; disable rdma on armv7hl #1484440 : packaging: /run and /var/run; prefer /run #1484885 : [rpc]: EPOLLERR - disconnecting now messages every 3 secs after completing rebalance #1486107 : /var/lib/glusterd/peers File had a blank line, Stopped Glusterd from starting #1486110 : [quorum]: Replace brick is happened when Quorum not met. #1486120 : symlinks trigger faulty geo-replication state (rsnapshot usecase) #1486122 : gluster-block profile needs to have strict-o-direct","title":"Bugs addressed"},{"location":"release-notes/3.12.1/","text":"Release notes for Gluster 3.12.1 This is a bugfix release. The Release Notes for 3.12.0 , 3.12.1 contain a listing of all the new features that were added and bugs fixed in the GlusterFS 3.12 stable release. Major changes, features and limitations addressed in this release No Major changes Major issues Expanding a gluster volume that is sharded may cause file corruption Sharded volumes are typically used for VM images, if such volumes are expanded or possibly contracted (i.e add/remove bricks and rebalance) there are reports of VM images getting corrupted. The last known cause for corruption (Bug #1465123) has a fix with this release. As further testing is still in progress, the issue is retained as a major issue. Status of this bug can be tracked here, #1465123 Bugs addressed A total of 12 patches have been merged, addressing 11 bugs #1486538 : [geo-rep+qr]: Crashes observed at slave from qr_lookup_sbk during rename/hardlink/rebalance #1486557 : Log entry of files skipped/failed during rebalance operation #1487033 : rpc: client_t and related objects leaked due to incorrect ref counts #1487319 : afr: check op_ret value in __afr_selfheal_name_impunge #1488119 : scripts: mount.glusterfs contains non-portable bashisms #1488168 : Launch metadata heal in discover code path. #1488387 : gluster-blockd process crashed and core generated #1488718 : [RHHI] cannot boot vms created from template when disk format = qcow2 #1489260 : Crash in dht_check_and_open_fd_on_subvol_task() #1489296 : glusterfsd (brick) process crashed #1489511 : return ENOSYS for 'non readable' FOPs","title":"3.12.1"},{"location":"release-notes/3.12.1/#release-notes-for-gluster-3121","text":"This is a bugfix release. The Release Notes for 3.12.0 , 3.12.1 contain a listing of all the new features that were added and bugs fixed in the GlusterFS 3.12 stable release.","title":"Release notes for Gluster 3.12.1"},{"location":"release-notes/3.12.1/#major-changes-features-and-limitations-addressed-in-this-release","text":"No Major changes","title":"Major changes, features and limitations addressed in this release"},{"location":"release-notes/3.12.1/#major-issues","text":"Expanding a gluster volume that is sharded may cause file corruption Sharded volumes are typically used for VM images, if such volumes are expanded or possibly contracted (i.e add/remove bricks and rebalance) there are reports of VM images getting corrupted. The last known cause for corruption (Bug #1465123) has a fix with this release. As further testing is still in progress, the issue is retained as a major issue. Status of this bug can be tracked here, #1465123","title":"Major issues"},{"location":"release-notes/3.12.1/#bugs-addressed","text":"A total of 12 patches have been merged, addressing 11 bugs #1486538 : [geo-rep+qr]: Crashes observed at slave from qr_lookup_sbk during rename/hardlink/rebalance #1486557 : Log entry of files skipped/failed during rebalance operation #1487033 : rpc: client_t and related objects leaked due to incorrect ref counts #1487319 : afr: check op_ret value in __afr_selfheal_name_impunge #1488119 : scripts: mount.glusterfs contains non-portable bashisms #1488168 : Launch metadata heal in discover code path. #1488387 : gluster-blockd process crashed and core generated #1488718 : [RHHI] cannot boot vms created from template when disk format = qcow2 #1489260 : Crash in dht_check_and_open_fd_on_subvol_task() #1489296 : glusterfsd (brick) process crashed #1489511 : return ENOSYS for 'non readable' FOPs","title":"Bugs addressed"},{"location":"release-notes/3.12.10/","text":"Release notes for Gluster 3.12.10 This is a bugfix release. The release notes for 3.12.0 , 3.12.1 , 3.12.2 , 3.12.3 , 3.12.4 , 3.12.5 , 3.12.6 , 3.12.7 , 3.12.8 and 3.12.9 contain a listing of all the new features that were added and bugs fixed in the GlusterFS 3.12 stable release. Major changes, features and limitations addressed in this release None Major issues None Bugs addressed Bugs addressed since release-3.12.9 are listed below . - #1570475 : Rebalance on few nodes doesn't seem to complete - stuck at FUTEX_WAIT - #1576816 : GlusterFS can be improved - #1577164 : gfapi: broken symbol versions - #1577845 : Geo-rep: faulty session due to OSError: [Errno 95] Operation not supported - #1577862 : [geo-rep]: Upgrade fails, session in FAULTY state - #1577868 : Glusterd crashed on a few (master) nodes - #1577871 : [geo-rep]: Geo-rep scheduler fails - #1580519 : the regression test \"tests/bugs/posix/bug-990028.t\" fails - #1581746 : bug-1309462.t is failing reliably due to changes in security.capability changes in the kernel - #1590133 : xdata is leaking in server3_3_seek","title":"3.12.10"},{"location":"release-notes/3.12.10/#release-notes-for-gluster-31210","text":"This is a bugfix release. The release notes for 3.12.0 , 3.12.1 , 3.12.2 , 3.12.3 , 3.12.4 , 3.12.5 , 3.12.6 , 3.12.7 , 3.12.8 and 3.12.9 contain a listing of all the new features that were added and bugs fixed in the GlusterFS 3.12 stable release.","title":"Release notes for Gluster 3.12.10"},{"location":"release-notes/3.12.10/#major-changes-features-and-limitations-addressed-in-this-release","text":"None","title":"Major changes, features and limitations addressed in this release"},{"location":"release-notes/3.12.10/#major-issues","text":"None","title":"Major issues"},{"location":"release-notes/3.12.10/#bugs-addressed","text":"Bugs addressed since release-3.12.9 are listed below . - #1570475 : Rebalance on few nodes doesn't seem to complete - stuck at FUTEX_WAIT - #1576816 : GlusterFS can be improved - #1577164 : gfapi: broken symbol versions - #1577845 : Geo-rep: faulty session due to OSError: [Errno 95] Operation not supported - #1577862 : [geo-rep]: Upgrade fails, session in FAULTY state - #1577868 : Glusterd crashed on a few (master) nodes - #1577871 : [geo-rep]: Geo-rep scheduler fails - #1580519 : the regression test \"tests/bugs/posix/bug-990028.t\" fails - #1581746 : bug-1309462.t is failing reliably due to changes in security.capability changes in the kernel - #1590133 : xdata is leaking in server3_3_seek","title":"Bugs addressed"},{"location":"release-notes/3.12.11/","text":"Release notes for Gluster 3.12.11 This is a bugfix release. The release notes for 3.12.0 , 3.12.1 , 3.12.2 , 3.12.3 , 3.12.4 , 3.12.5 , 3.12.6 , 3.12.7 , 3.12.8 , 3.12.9 , and 3.12.10 contain a listing of all the new features that were added and bugs fixed in the GlusterFS 3.12 stable release. Major changes, features and limitations addressed in this release This release contains a fix for a security vulerability in Gluster as follows, - http://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2018-10841 - https://nvd.nist.gov/vuln/detail/CVE-2018-10841 Installing the updated packages and restarting gluster services on gluster brick hosts, will help prevent the security issue. Major issues None Bugs addressed Bugs addressed since release-3.12.10 are listed below. #1559829 : snap/gcron.py: ABRT report for package glusterfs has reached 100 occurrences #1591187 : Gluster Block PVC fails to mount on Jenkins pod #1593526 : CVE-2018-10841 glusterfs: access trusted peer group via remote-host command [glusterfs upstream]","title":"3.12.11"},{"location":"release-notes/3.12.11/#release-notes-for-gluster-31211","text":"This is a bugfix release. The release notes for 3.12.0 , 3.12.1 , 3.12.2 , 3.12.3 , 3.12.4 , 3.12.5 , 3.12.6 , 3.12.7 , 3.12.8 , 3.12.9 , and 3.12.10 contain a listing of all the new features that were added and bugs fixed in the GlusterFS 3.12 stable release.","title":"Release notes for Gluster 3.12.11"},{"location":"release-notes/3.12.11/#major-changes-features-and-limitations-addressed-in-this-release","text":"This release contains a fix for a security vulerability in Gluster as follows, - http://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2018-10841 - https://nvd.nist.gov/vuln/detail/CVE-2018-10841 Installing the updated packages and restarting gluster services on gluster brick hosts, will help prevent the security issue.","title":"Major changes, features and limitations addressed in this release"},{"location":"release-notes/3.12.11/#major-issues","text":"None","title":"Major issues"},{"location":"release-notes/3.12.11/#bugs-addressed","text":"Bugs addressed since release-3.12.10 are listed below. #1559829 : snap/gcron.py: ABRT report for package glusterfs has reached 100 occurrences #1591187 : Gluster Block PVC fails to mount on Jenkins pod #1593526 : CVE-2018-10841 glusterfs: access trusted peer group via remote-host command [glusterfs upstream]","title":"Bugs addressed"},{"location":"release-notes/3.12.12/","text":"Release notes for Gluster 3.12.12 This is a bugfix release. The release notes for 3.12.0 , 3.12.1 , 3.12.2 , 3.12.3 , 3.12.4 , 3.12.5 , 3.12.6 , 3.12.7 , 3.12.8 , 3.12.9 , 3.12.10 and 3.12.11 contain a listing of all the new features that were added and bugs fixed in the GlusterFS 3.12 stable release. Major changes, features and limitations addressed in this release None Major issues None Bugs addressed Bugs addressed since release-3.12.12 are listed below - #1579673 : Remove EIO from the dht_inode_missing macro - #1595528 : rmdir is leaking softlinks to directories in .glusterfs - #1597120 : Add quorum checks in post-op - #1597123 : Changes to self-heal logic w.r.t. detecting of split-brains - #1597154 : When storage reserve limit is reached, appending data to an existing file throws EROFS error - #1597230 : glustershd crashes when index heal is launched before graph is initialized. - #1598121 : lookup not assigning gfid if file is not present in all bricks of replica - #1598720 : afr: fix bug-1363721.t failure - #1599247 : afr: don't update readables if inode refresh failed on all children","title":"3.12.12"},{"location":"release-notes/3.12.12/#release-notes-for-gluster-31212","text":"This is a bugfix release. The release notes for 3.12.0 , 3.12.1 , 3.12.2 , 3.12.3 , 3.12.4 , 3.12.5 , 3.12.6 , 3.12.7 , 3.12.8 , 3.12.9 , 3.12.10 and 3.12.11 contain a listing of all the new features that were added and bugs fixed in the GlusterFS 3.12 stable release.","title":"Release notes for Gluster 3.12.12"},{"location":"release-notes/3.12.12/#major-changes-features-and-limitations-addressed-in-this-release","text":"None","title":"Major changes, features and limitations addressed in this release"},{"location":"release-notes/3.12.12/#major-issues","text":"None","title":"Major issues"},{"location":"release-notes/3.12.12/#bugs-addressed","text":"Bugs addressed since release-3.12.12 are listed below - #1579673 : Remove EIO from the dht_inode_missing macro - #1595528 : rmdir is leaking softlinks to directories in .glusterfs - #1597120 : Add quorum checks in post-op - #1597123 : Changes to self-heal logic w.r.t. detecting of split-brains - #1597154 : When storage reserve limit is reached, appending data to an existing file throws EROFS error - #1597230 : glustershd crashes when index heal is launched before graph is initialized. - #1598121 : lookup not assigning gfid if file is not present in all bricks of replica - #1598720 : afr: fix bug-1363721.t failure - #1599247 : afr: don't update readables if inode refresh failed on all children","title":"Bugs addressed"},{"location":"release-notes/3.12.13/","text":"Release notes for Gluster 3.12.13 This is a bugfix release. The release notes for 3.12.0 , 3.12.1 , 3.12.2 , 3.12.3 , 3.12.4 , 3.12.5 , 3.12.6 , 3.12.7 , 3.12.8 , 3.12.9 , 3.12.10 , 3.12.11 and 3.12.12 contain a listing of all the new features that were added and bugs fixed in the GlusterFS 3.12 stable release. Major changes, features and limitations addressed in this release None Major issues None Bugs addressed Bugs addressed in release-3.12.13 are listed below - #1599788 : _is_prefix should return false for 0-length strings - #1603093 : directories are invisible on client side - #1613512 : Backport glusterfs-client memory leak fix to 3.12.x - #1618838 : gluster bash completion leaks TOP=0 into the environment - #1618348 : [Ganesha] Ganesha crashed in mdcache_alloc_and_check_handle while running bonnie and untars with parallel lookups","title":"3.12.13"},{"location":"release-notes/3.12.13/#release-notes-for-gluster-31213","text":"This is a bugfix release. The release notes for 3.12.0 , 3.12.1 , 3.12.2 , 3.12.3 , 3.12.4 , 3.12.5 , 3.12.6 , 3.12.7 , 3.12.8 , 3.12.9 , 3.12.10 , 3.12.11 and 3.12.12 contain a listing of all the new features that were added and bugs fixed in the GlusterFS 3.12 stable release.","title":"Release notes for Gluster 3.12.13"},{"location":"release-notes/3.12.13/#major-changes-features-and-limitations-addressed-in-this-release","text":"None","title":"Major changes, features and limitations addressed in this release"},{"location":"release-notes/3.12.13/#major-issues","text":"None","title":"Major issues"},{"location":"release-notes/3.12.13/#bugs-addressed","text":"Bugs addressed in release-3.12.13 are listed below - #1599788 : _is_prefix should return false for 0-length strings - #1603093 : directories are invisible on client side - #1613512 : Backport glusterfs-client memory leak fix to 3.12.x - #1618838 : gluster bash completion leaks TOP=0 into the environment - #1618348 : [Ganesha] Ganesha crashed in mdcache_alloc_and_check_handle while running bonnie and untars with parallel lookups","title":"Bugs addressed"},{"location":"release-notes/3.12.14/","text":"Release notes for Gluster 3.12.14 This is a bugfix release. The release notes for 3.12.0 , 3.12.1 , 3.12.2 , 3.12.3 , 3.12.4 , 3.12.5 , 3.12.6 , 3.12.7 , 3.12.8 , 3.12.9 , 3.12.10 , 3.12.11 , 3.12.12 and 3.12.13 contain a listing of all the new features that were added and bugs fixed in the GlusterFS 3.12 stable release. Major changes, features and limitations addressed in this release This release contains fix for following security vulnerabilities, https://nvd.nist.gov/vuln/detail/CVE-2018-10904 https://nvd.nist.gov/vuln/detail/CVE-2018-10907 https://nvd.nist.gov/vuln/detail/CVE-2018-10911 https://nvd.nist.gov/vuln/detail/CVE-2018-10913 https://nvd.nist.gov/vuln/detail/CVE-2018-10914 https://nvd.nist.gov/vuln/detail/CVE-2018-10923 https://nvd.nist.gov/vuln/detail/CVE-2018-10926 https://nvd.nist.gov/vuln/detail/CVE-2018-10927 https://nvd.nist.gov/vuln/detail/CVE-2018-10928 https://nvd.nist.gov/vuln/detail/CVE-2018-10929 https://nvd.nist.gov/vuln/detail/CVE-2018-10930 To resolve the security vulnerabilities following limitations were made in GlusterFS open,read,write on special files like char and block are no longer permitted io-stat xlator can dump stat info only to /var/run/gluster directory Addressed an issue that affected copying a file over SSL/TLS in a volume Installing the updated packages and restarting gluster services on gluster brick hosts, will fix the security issues. Major issues None Bugs addressed Bugs addressed since release-3.12.14 are listed below. #1622405 : Problem with SSL/TLS encryption on Gluster 4.0 & 4.1 #1625286 : Information Exposure in posix_get_file_contents function in posix-helpers.c #1625648 : I/O to arbitrary devices on storage server #1625654 : Stack-based buffer overflow in server-rpc-fops.c allows remote attackers to execute arbitrary code #1625656 : Improper deserialization in dict.c:dict_unserialize() can allow attackers to read arbitrary memory #1625660 : Unsanitized file names in debug/io-stats translator can allow remote attackers to execute arbitrary code #1625664 : Files can be renamed outside volume","title":"3.12.14"},{"location":"release-notes/3.12.14/#release-notes-for-gluster-31214","text":"This is a bugfix release. The release notes for 3.12.0 , 3.12.1 , 3.12.2 , 3.12.3 , 3.12.4 , 3.12.5 , 3.12.6 , 3.12.7 , 3.12.8 , 3.12.9 , 3.12.10 , 3.12.11 , 3.12.12 and 3.12.13 contain a listing of all the new features that were added and bugs fixed in the GlusterFS 3.12 stable release.","title":"Release notes for Gluster 3.12.14"},{"location":"release-notes/3.12.14/#major-changes-features-and-limitations-addressed-in-this-release","text":"This release contains fix for following security vulnerabilities, https://nvd.nist.gov/vuln/detail/CVE-2018-10904 https://nvd.nist.gov/vuln/detail/CVE-2018-10907 https://nvd.nist.gov/vuln/detail/CVE-2018-10911 https://nvd.nist.gov/vuln/detail/CVE-2018-10913 https://nvd.nist.gov/vuln/detail/CVE-2018-10914 https://nvd.nist.gov/vuln/detail/CVE-2018-10923 https://nvd.nist.gov/vuln/detail/CVE-2018-10926 https://nvd.nist.gov/vuln/detail/CVE-2018-10927 https://nvd.nist.gov/vuln/detail/CVE-2018-10928 https://nvd.nist.gov/vuln/detail/CVE-2018-10929 https://nvd.nist.gov/vuln/detail/CVE-2018-10930 To resolve the security vulnerabilities following limitations were made in GlusterFS open,read,write on special files like char and block are no longer permitted io-stat xlator can dump stat info only to /var/run/gluster directory Addressed an issue that affected copying a file over SSL/TLS in a volume Installing the updated packages and restarting gluster services on gluster brick hosts, will fix the security issues.","title":"Major changes, features and limitations addressed in this release"},{"location":"release-notes/3.12.14/#major-issues","text":"None","title":"Major issues"},{"location":"release-notes/3.12.14/#bugs-addressed","text":"Bugs addressed since release-3.12.14 are listed below. #1622405 : Problem with SSL/TLS encryption on Gluster 4.0 & 4.1 #1625286 : Information Exposure in posix_get_file_contents function in posix-helpers.c #1625648 : I/O to arbitrary devices on storage server #1625654 : Stack-based buffer overflow in server-rpc-fops.c allows remote attackers to execute arbitrary code #1625656 : Improper deserialization in dict.c:dict_unserialize() can allow attackers to read arbitrary memory #1625660 : Unsanitized file names in debug/io-stats translator can allow remote attackers to execute arbitrary code #1625664 : Files can be renamed outside volume","title":"Bugs addressed"},{"location":"release-notes/3.12.15/","text":"Release notes for Gluster 3.12.15 This is a bugfix release. The release notes for 3.12.0 , 3.12.1 , 3.12.2 , 3.12.3 , 3.12.4 , 3.12.5 , 3.12.6 , 3.12.7 , 3.12.8 , 3.12.9 , 3.12.10 , 3.12.11 3.12.12 , 3.12.13 and 3.12.14 contain a listing of all the new features that were added and bugs fixed in the GlusterFS 3.12 stable release. Major changes, features and limitations addressed in this release None Major issues None Bugs addressed Bugs addressed since release-3.12.15 are listed below. #1569336 : Volume status inode is broken with brickmux #1625588 : Prevent hangs while increasing replica-count/replace-brick for directory hierarchy #1497989 : Gluster 3.12.1 Packages require manual systemctl daemon reload after install #1512371 : parallel-readdir = TRUE prevents directories listing #1633625 : split-brain observed on parent dir #1637989 : data-self-heal in arbiter volume results in stale locks.","title":"3.12.15"},{"location":"release-notes/3.12.15/#release-notes-for-gluster-31215","text":"This is a bugfix release. The release notes for 3.12.0 , 3.12.1 , 3.12.2 , 3.12.3 , 3.12.4 , 3.12.5 , 3.12.6 , 3.12.7 , 3.12.8 , 3.12.9 , 3.12.10 , 3.12.11 3.12.12 , 3.12.13 and 3.12.14 contain a listing of all the new features that were added and bugs fixed in the GlusterFS 3.12 stable release.","title":"Release notes for Gluster 3.12.15"},{"location":"release-notes/3.12.15/#major-changes-features-and-limitations-addressed-in-this-release","text":"None","title":"Major changes, features and limitations addressed in this release"},{"location":"release-notes/3.12.15/#major-issues","text":"None","title":"Major issues"},{"location":"release-notes/3.12.15/#bugs-addressed","text":"Bugs addressed since release-3.12.15 are listed below. #1569336 : Volume status inode is broken with brickmux #1625588 : Prevent hangs while increasing replica-count/replace-brick for directory hierarchy #1497989 : Gluster 3.12.1 Packages require manual systemctl daemon reload after install #1512371 : parallel-readdir = TRUE prevents directories listing #1633625 : split-brain observed on parent dir #1637989 : data-self-heal in arbiter volume results in stale locks.","title":"Bugs addressed"},{"location":"release-notes/3.12.2/","text":"Release notes for Gluster 3.12.2 This is a bugfix release. The release notes for 3.12.0 , 3.12.1 , 3.12.2 contain a listing of all the new features that were added and bugs fixed in the GlusterFS 3.12 stable release. Major changes, features and limitations addressed in this release 1.) In a pure distribute volume there is no source to heal the replaced brick from and hence would cause a loss of data that was present in the replaced brick. The CLI has been enhanced to prevent a user from inadvertently using replace brick in a pure distribute volume. It is advised to use add/remove brick to migrate from an existing brick in a pure distribute volume. Major issues Expanding a gluster volume that is sharded may cause file corruption Sharded volumes are typically used for VM images, if such volumes are expanded or possibly contracted (i.e add/remove bricks and rebalance) there are reports of VM images getting corrupted. The last known cause for corruption #1465123 is still pending, and not yet part of this release. Gluster volume restarts fail if the sub directory export feature is in use. Status of this issue can be tracked here, #1501315 Mounting a gluster snapshot will fail, when attempting a FUSE based mount of the snapshot. So for the current users, it is recommend to only access snapshot via \".snaps\" directory on a mounted gluster volume. Status of this issue can be tracked here, #1501378 Bugs addressed A total of 31 patches have been merged, addressing 28 bugs #1490493 : Sub-directory mount details are incorrect in /proc/mounts #1491178 : GlusterD returns a bad memory pointer in glusterd_get_args_from_dict() #1491292 : Provide brick list as part of VOLUME_CREATE event. #1491690 : rpc: TLSv1_2_method() is deprecated in OpenSSL-1.1 #1492026 : set the shard-block-size to 64MB in virt profile #1492061 : CLIENT_CONNECT event not being raised #1492066 : AFR_SUBVOL_UP and AFR_SUBVOLS_DOWN events not working #1493975 : disallow replace brick operation on plain distribute volume #1494523 : Spelling errors in 3.12.1 #1495162 : glusterd ends up with multiple uuids for the same node #1495397 : Make event-history feature configurable and have it disabled by default #1495858 : gluster volume create asks for confirmation for replica-2 volume even with force #1496238 : [geo-rep]: Scheduler help needs correction for description of --no-color #1496317 : [afr] split-brain observed on T files post hardlink and rename in x3 volume #1496326 : [GNFS+EC] lock is being granted to 2 different client for the same data range at a time after performing lock acquire/release from the clients1 #1497084 : glusterfs process consume huge memory on both server and client node #1499123 : Readdirp is considerably slower than readdir on acl clients #1499150 : Improve performance with xattrop update. #1499158 : client-io-threads option not working for replicated volumes #1499202 : self-heal daemon stuck #1499392 : [geo-rep]: Improve the output message to reflect the real failure with schedule_georep script #1500396 : [geo-rep]: Observed \"Operation not supported\" error with traceback on slave log #1500472 : Use a bitmap to store local node info instead of conf->local_nodeuuids[i].uuids #1500662 : gluster volume heal info \"healed\" and \"heal-failed\" showing wrong information #1500835 : [geo-rep]: Status shows ACTIVE for most workers in EC before it becomes the PASSIVE #1500841 : [geo-rep]: Worker crashes with OSError: [Errno 61] No data available #1500845 : [geo-rep] master worker crash with interrupted system call #1500853 : [geo-rep]: Incorrect last sync \"0\" during hystory crawl after upgrade/stop-start #1501022 : Make choose-local configurable through volume-set command #1501154 : Brick Multiplexing: Gluster volume start force complains with command \"Error : Request timed out\" when there are multiple volumes","title":"3.12.2"},{"location":"release-notes/3.12.2/#release-notes-for-gluster-3122","text":"This is a bugfix release. The release notes for 3.12.0 , 3.12.1 , 3.12.2 contain a listing of all the new features that were added and bugs fixed in the GlusterFS 3.12 stable release.","title":"Release notes for Gluster 3.12.2"},{"location":"release-notes/3.12.2/#major-changes-features-and-limitations-addressed-in-this-release","text":"1.) In a pure distribute volume there is no source to heal the replaced brick from and hence would cause a loss of data that was present in the replaced brick. The CLI has been enhanced to prevent a user from inadvertently using replace brick in a pure distribute volume. It is advised to use add/remove brick to migrate from an existing brick in a pure distribute volume.","title":"Major changes, features and limitations addressed in this release"},{"location":"release-notes/3.12.2/#major-issues","text":"Expanding a gluster volume that is sharded may cause file corruption Sharded volumes are typically used for VM images, if such volumes are expanded or possibly contracted (i.e add/remove bricks and rebalance) there are reports of VM images getting corrupted. The last known cause for corruption #1465123 is still pending, and not yet part of this release. Gluster volume restarts fail if the sub directory export feature is in use. Status of this issue can be tracked here, #1501315 Mounting a gluster snapshot will fail, when attempting a FUSE based mount of the snapshot. So for the current users, it is recommend to only access snapshot via \".snaps\" directory on a mounted gluster volume. Status of this issue can be tracked here, #1501378","title":"Major issues"},{"location":"release-notes/3.12.2/#bugs-addressed","text":"A total of 31 patches have been merged, addressing 28 bugs #1490493 : Sub-directory mount details are incorrect in /proc/mounts #1491178 : GlusterD returns a bad memory pointer in glusterd_get_args_from_dict() #1491292 : Provide brick list as part of VOLUME_CREATE event. #1491690 : rpc: TLSv1_2_method() is deprecated in OpenSSL-1.1 #1492026 : set the shard-block-size to 64MB in virt profile #1492061 : CLIENT_CONNECT event not being raised #1492066 : AFR_SUBVOL_UP and AFR_SUBVOLS_DOWN events not working #1493975 : disallow replace brick operation on plain distribute volume #1494523 : Spelling errors in 3.12.1 #1495162 : glusterd ends up with multiple uuids for the same node #1495397 : Make event-history feature configurable and have it disabled by default #1495858 : gluster volume create asks for confirmation for replica-2 volume even with force #1496238 : [geo-rep]: Scheduler help needs correction for description of --no-color #1496317 : [afr] split-brain observed on T files post hardlink and rename in x3 volume #1496326 : [GNFS+EC] lock is being granted to 2 different client for the same data range at a time after performing lock acquire/release from the clients1 #1497084 : glusterfs process consume huge memory on both server and client node #1499123 : Readdirp is considerably slower than readdir on acl clients #1499150 : Improve performance with xattrop update. #1499158 : client-io-threads option not working for replicated volumes #1499202 : self-heal daemon stuck #1499392 : [geo-rep]: Improve the output message to reflect the real failure with schedule_georep script #1500396 : [geo-rep]: Observed \"Operation not supported\" error with traceback on slave log #1500472 : Use a bitmap to store local node info instead of conf->local_nodeuuids[i].uuids #1500662 : gluster volume heal info \"healed\" and \"heal-failed\" showing wrong information #1500835 : [geo-rep]: Status shows ACTIVE for most workers in EC before it becomes the PASSIVE #1500841 : [geo-rep]: Worker crashes with OSError: [Errno 61] No data available #1500845 : [geo-rep] master worker crash with interrupted system call #1500853 : [geo-rep]: Incorrect last sync \"0\" during hystory crawl after upgrade/stop-start #1501022 : Make choose-local configurable through volume-set command #1501154 : Brick Multiplexing: Gluster volume start force complains with command \"Error : Request timed out\" when there are multiple volumes","title":"Bugs addressed"},{"location":"release-notes/3.12.3/","text":"Release notes for Gluster 3.12.3 This is a bugfix release. The release notes for 3.12.0 , 3.12.1 , 3.12.2 , 3.12.3 contain a listing of all the new features that were added and bugs fixed in the GlusterFS 3.12 stable release. Major changes, features and limitations addressed in this release The two regression related to with subdir mount got fixed - gluster volume restart failure (#1465123) - mounting gluster snapshot via fuse (#1501378) Improvements for \"help\" command with in gluster cli (#1509786) Introduction of new api glfs_fd_set_lkowner() to set lock owner Major issues Expanding a gluster volume that is sharded may cause file corruption Sharded volumes are typically used for VM images, if such volumes are expanded or possibly contracted (i.e add/remove bricks and rebalance) there are reports of VM images getting corrupted. The last known cause for corruption #1465123 is still pending, and not yet part of this release. Bugs addressed A total of 25 patches have been merged, addressing 25 bugs #1484489 : File-level WORM allows mv over read-only files #1494527 : glusterfs fails to build twice in a row #1499889 : md-cache uses incorrect xattr keynames for GF_POSIX_ACL keys #1499892 : md-cache: xattr values should not be checked with string functions #1501238 : [SNAPSHOT] Unable to mount a snapshot on client #1501315 : Gluster Volume restart fail after exporting fuse sub-dir #1501864 : Add generated HMAC token in header for webhook calls #1501956 : gfapi: API needed to set lk_owner #1502104 : [geo-rep]: RSYNC throwing internal errors #1503239 : [Glusterd] Volume operations fail on a (tiered) volume because of a stale lock held by one of the nodes #1505221 : glusterfs client crash when removing directories #1505323 : When sub-dir is mounted on Fuse client,adding bricks to the same volume unmounts the subdir from fuse client #1505370 : Mishandling null check at send_brick_req of glusterfsd/src/gf_attach.c #1505373 : server.allow-insecure should be visible in \"gluster volume set help\" #1505527 : Posix compliance rename test fails on fuse subdir mount #1505846 : [GSS] gluster volume status command is missing in man page #1505856 : Potential use of NULL this variable before it gets initialized #1507747 : clean up port map on brick disconnect #1507748 : Brick port mismatch #1507877 : reset-brick commit force failed with glusterd_volume_brickinfo_get Returning -1 #1508283 : stale brick processes getting created and volume status shows brick as down(pkill glusterfsd glusterfs ,glusterd restart) #1509200 : Event webhook should work with HTTPS urls #1509786 : The output of the \"gluster help\" command is difficult to read #1511271 : Rebalance estimate(ETA) shows wrong details(as intial message of 10min wait reappears) when still in progress #1511301 : In distribute volume after glusterd restart, brick goes offline","title":"3.12.3"},{"location":"release-notes/3.12.3/#release-notes-for-gluster-3123","text":"This is a bugfix release. The release notes for 3.12.0 , 3.12.1 , 3.12.2 , 3.12.3 contain a listing of all the new features that were added and bugs fixed in the GlusterFS 3.12 stable release.","title":"Release notes for Gluster 3.12.3"},{"location":"release-notes/3.12.3/#major-changes-features-and-limitations-addressed-in-this-release","text":"The two regression related to with subdir mount got fixed - gluster volume restart failure (#1465123) - mounting gluster snapshot via fuse (#1501378) Improvements for \"help\" command with in gluster cli (#1509786) Introduction of new api glfs_fd_set_lkowner() to set lock owner","title":"Major changes, features and limitations addressed in this release"},{"location":"release-notes/3.12.3/#major-issues","text":"Expanding a gluster volume that is sharded may cause file corruption Sharded volumes are typically used for VM images, if such volumes are expanded or possibly contracted (i.e add/remove bricks and rebalance) there are reports of VM images getting corrupted. The last known cause for corruption #1465123 is still pending, and not yet part of this release.","title":"Major issues"},{"location":"release-notes/3.12.3/#bugs-addressed","text":"A total of 25 patches have been merged, addressing 25 bugs #1484489 : File-level WORM allows mv over read-only files #1494527 : glusterfs fails to build twice in a row #1499889 : md-cache uses incorrect xattr keynames for GF_POSIX_ACL keys #1499892 : md-cache: xattr values should not be checked with string functions #1501238 : [SNAPSHOT] Unable to mount a snapshot on client #1501315 : Gluster Volume restart fail after exporting fuse sub-dir #1501864 : Add generated HMAC token in header for webhook calls #1501956 : gfapi: API needed to set lk_owner #1502104 : [geo-rep]: RSYNC throwing internal errors #1503239 : [Glusterd] Volume operations fail on a (tiered) volume because of a stale lock held by one of the nodes #1505221 : glusterfs client crash when removing directories #1505323 : When sub-dir is mounted on Fuse client,adding bricks to the same volume unmounts the subdir from fuse client #1505370 : Mishandling null check at send_brick_req of glusterfsd/src/gf_attach.c #1505373 : server.allow-insecure should be visible in \"gluster volume set help\" #1505527 : Posix compliance rename test fails on fuse subdir mount #1505846 : [GSS] gluster volume status command is missing in man page #1505856 : Potential use of NULL this variable before it gets initialized #1507747 : clean up port map on brick disconnect #1507748 : Brick port mismatch #1507877 : reset-brick commit force failed with glusterd_volume_brickinfo_get Returning -1 #1508283 : stale brick processes getting created and volume status shows brick as down(pkill glusterfsd glusterfs ,glusterd restart) #1509200 : Event webhook should work with HTTPS urls #1509786 : The output of the \"gluster help\" command is difficult to read #1511271 : Rebalance estimate(ETA) shows wrong details(as intial message of 10min wait reappears) when still in progress #1511301 : In distribute volume after glusterd restart, brick goes offline","title":"Bugs addressed"},{"location":"release-notes/3.12.4/","text":"Release notes for Gluster 3.12.4 This is a bugfix release. The release notes for 3.12.0 , 3.12.1 , 3.12.2 , 3.12.3 , 3.12.4 contain a listing of all the new features that were added and bugs fixed in the GlusterFS 3.12 stable release. Major issues Expanding a gluster volume that is sharded may cause file corruption Sharded volumes are typically used for VM images, if such volumes are expanded or possibly contracted (i.e add/remove bricks and rebalance) there are reports of VM images getting corrupted. The last known cause for corruption #1465123 is still pending, and not yet part of this release. Bugs addressed A total of 13 patches have been merged, addressing 12 bugs #1478411 : Directory listings on fuse mount are very slow due to small number of getdents() entries #1511782 : In Replica volume 2*2 when quorum is set, after glusterd restart nfs server is coming up instead of self-heal daemon #1512432 : Test bug-1483058-replace-brick-quorum-validation.t fails inconsistently #1513258 : NetBSD port #1514380 : default timeout of 5min not honored for analyzing split-brain files post setfattr replica.split-brain-heal-finalize #1514420 : gluster volume splitbrain info needs to display output of each brick in a stream fashion instead of buffering and dumping at the end #1515042 : bug-1247563.t is failing on master #1516691 : Rebalance fails on NetBSD because fallocate is not implemented #1517689 : Memory leak in locks xlator #1518061 : Remove 'summary' option from 'gluster vol heal..' CLI #1523048 : glusterd consuming high memory #1523455 : Store allocated objects in the mem_acct","title":"3.12.4"},{"location":"release-notes/3.12.4/#release-notes-for-gluster-3124","text":"This is a bugfix release. The release notes for 3.12.0 , 3.12.1 , 3.12.2 , 3.12.3 , 3.12.4 contain a listing of all the new features that were added and bugs fixed in the GlusterFS 3.12 stable release.","title":"Release notes for Gluster 3.12.4"},{"location":"release-notes/3.12.4/#major-issues","text":"Expanding a gluster volume that is sharded may cause file corruption Sharded volumes are typically used for VM images, if such volumes are expanded or possibly contracted (i.e add/remove bricks and rebalance) there are reports of VM images getting corrupted. The last known cause for corruption #1465123 is still pending, and not yet part of this release.","title":"Major issues"},{"location":"release-notes/3.12.4/#bugs-addressed","text":"A total of 13 patches have been merged, addressing 12 bugs #1478411 : Directory listings on fuse mount are very slow due to small number of getdents() entries #1511782 : In Replica volume 2*2 when quorum is set, after glusterd restart nfs server is coming up instead of self-heal daemon #1512432 : Test bug-1483058-replace-brick-quorum-validation.t fails inconsistently #1513258 : NetBSD port #1514380 : default timeout of 5min not honored for analyzing split-brain files post setfattr replica.split-brain-heal-finalize #1514420 : gluster volume splitbrain info needs to display output of each brick in a stream fashion instead of buffering and dumping at the end #1515042 : bug-1247563.t is failing on master #1516691 : Rebalance fails on NetBSD because fallocate is not implemented #1517689 : Memory leak in locks xlator #1518061 : Remove 'summary' option from 'gluster vol heal..' CLI #1523048 : glusterd consuming high memory #1523455 : Store allocated objects in the mem_acct","title":"Bugs addressed"},{"location":"release-notes/3.12.5/","text":"Release notes for Gluster 3.12.5 This is a bugfix release. The release notes for 3.12.0 , 3.12.1 , 3.12.2 , 3.12.3 , 3.12.4 , 3.12.5 contain a listing of all the new features that were added and bugs fixed in the GlusterFS 3.12 stable release. Major issues Expanding a gluster volume that is sharded may cause file corruption Sharded volumes are typically used for VM images, if such volumes are expanded or possibly contracted (i.e add/remove bricks and rebalance) there are reports of VM images getting corrupted. The last known cause for corruption #1465123 is still pending, and not yet part of this release. Bugs addressed A total of 12 patches have been merged, addressing 11 bugs #1489043 : The number of bytes of the quota specified in version 3.7 or later is incorrect #1511301 : In distribute volume after glusterd restart, brick goes offline #1525850 : rdma transport may access an obsolete item in gf_rdma_device_t->all_mr, and causes glusterfsd/glusterfs process crash. #1527276 : feature/bitrot: remove internal xattrs from lookup cbk #1529085 : fstat returns ENOENT/ESTALE #1529088 : opening a file that is destination of rename results in ENOENT errors #1529095 : /usr/sbin/glusterfs crashing on Red Hat OpenShift Container Platform node #1529539 : JWT support without external dependency #1530448 : glustershd fails to start on a volume force start after a brick is down #1530455 : Files are not rebalanced if destination brick(available size) is of smaller size than source brick(available size) #1531372 : Use after free in cli_cmd_volume_create_cbk","title":"3.12.5"},{"location":"release-notes/3.12.5/#release-notes-for-gluster-3125","text":"This is a bugfix release. The release notes for 3.12.0 , 3.12.1 , 3.12.2 , 3.12.3 , 3.12.4 , 3.12.5 contain a listing of all the new features that were added and bugs fixed in the GlusterFS 3.12 stable release.","title":"Release notes for Gluster 3.12.5"},{"location":"release-notes/3.12.5/#major-issues","text":"Expanding a gluster volume that is sharded may cause file corruption Sharded volumes are typically used for VM images, if such volumes are expanded or possibly contracted (i.e add/remove bricks and rebalance) there are reports of VM images getting corrupted. The last known cause for corruption #1465123 is still pending, and not yet part of this release.","title":"Major issues"},{"location":"release-notes/3.12.5/#bugs-addressed","text":"A total of 12 patches have been merged, addressing 11 bugs #1489043 : The number of bytes of the quota specified in version 3.7 or later is incorrect #1511301 : In distribute volume after glusterd restart, brick goes offline #1525850 : rdma transport may access an obsolete item in gf_rdma_device_t->all_mr, and causes glusterfsd/glusterfs process crash. #1527276 : feature/bitrot: remove internal xattrs from lookup cbk #1529085 : fstat returns ENOENT/ESTALE #1529088 : opening a file that is destination of rename results in ENOENT errors #1529095 : /usr/sbin/glusterfs crashing on Red Hat OpenShift Container Platform node #1529539 : JWT support without external dependency #1530448 : glustershd fails to start on a volume force start after a brick is down #1530455 : Files are not rebalanced if destination brick(available size) is of smaller size than source brick(available size) #1531372 : Use after free in cli_cmd_volume_create_cbk","title":"Bugs addressed"},{"location":"release-notes/3.12.6/","text":"Release notes for Gluster 3.12.6 This is a bugfix release. The release notes for 3.12.0 , 3.12.1 , 3.12.2 , 3.12.3 , 3.12.4 , 3.12.5 , 3.12.5 contain a listing of all the new features that were added and bugs fixed in the GlusterFS 3.12 stable release. Major issues Expanding a gluster volume that is sharded may cause file corruption Sharded volumes are typically used for VM images, if such volumes are expanded or possibly contracted (i.e add/remove bricks and rebalance) there are reports of VM images getting corrupted. The last known cause for corruption #1465123 is still pending, and not yet part of this release. Bugs addressed A total of 16 patches have been merged, addressing 16 bugs #1510342 : Not all files synced using geo-replication #1533269 : Random GlusterFSD process dies during rebalance #1534847 : entries not getting cleared post healing of softlinks (stale entries showing up in heal info) #1536334 : [Disperse] Implement open fd heal for disperse volume #1537346 : glustershd/glusterd is not using right port when connecting to glusterfsd process #1539516 : DHT log messages: Found anomalies in (null) (gfid = 00000000-0000-0000-0000-000000000000). Holes=1 overlaps=0 #1540224 : dht_(f)xattrop does not implement migration checks #1541267 : dht_layout_t leak in dht_populate_inode_for_dentry #1541930 : A down brick is incorrectly considered to be online and makes the volume to be started without any brick available #1542054 : tests/bugs/cli/bug-1169302.t fails spuriously #1542475 : Random failures in tests/bugs/nfs/bug-974972.t #1542601 : The used space in the volume increases when the volume is expanded #1542615 : tests/bugs/core/multiplex-limit-issue-151.t fails sometimes in upstream master #1542826 : Mark tests/bugs/posix/bug-990028.t bad on release-3.12 #1542934 : Seeing timer errors in the rebalance logs #1543016 : dht_lookup_unlink_of_false_linkto_cbk fails with \"Permission denied\" #1544637 : 3.8 -> 3.10 rolling upgrade fails (same for 3.12 or 3.13) on Ubuntu 14","title":"3.12.6"},{"location":"release-notes/3.12.6/#release-notes-for-gluster-3126","text":"This is a bugfix release. The release notes for 3.12.0 , 3.12.1 , 3.12.2 , 3.12.3 , 3.12.4 , 3.12.5 , 3.12.5 contain a listing of all the new features that were added and bugs fixed in the GlusterFS 3.12 stable release.","title":"Release notes for Gluster 3.12.6"},{"location":"release-notes/3.12.6/#major-issues","text":"Expanding a gluster volume that is sharded may cause file corruption Sharded volumes are typically used for VM images, if such volumes are expanded or possibly contracted (i.e add/remove bricks and rebalance) there are reports of VM images getting corrupted. The last known cause for corruption #1465123 is still pending, and not yet part of this release.","title":"Major issues"},{"location":"release-notes/3.12.6/#bugs-addressed","text":"A total of 16 patches have been merged, addressing 16 bugs #1510342 : Not all files synced using geo-replication #1533269 : Random GlusterFSD process dies during rebalance #1534847 : entries not getting cleared post healing of softlinks (stale entries showing up in heal info) #1536334 : [Disperse] Implement open fd heal for disperse volume #1537346 : glustershd/glusterd is not using right port when connecting to glusterfsd process #1539516 : DHT log messages: Found anomalies in (null) (gfid = 00000000-0000-0000-0000-000000000000). Holes=1 overlaps=0 #1540224 : dht_(f)xattrop does not implement migration checks #1541267 : dht_layout_t leak in dht_populate_inode_for_dentry #1541930 : A down brick is incorrectly considered to be online and makes the volume to be started without any brick available #1542054 : tests/bugs/cli/bug-1169302.t fails spuriously #1542475 : Random failures in tests/bugs/nfs/bug-974972.t #1542601 : The used space in the volume increases when the volume is expanded #1542615 : tests/bugs/core/multiplex-limit-issue-151.t fails sometimes in upstream master #1542826 : Mark tests/bugs/posix/bug-990028.t bad on release-3.12 #1542934 : Seeing timer errors in the rebalance logs #1543016 : dht_lookup_unlink_of_false_linkto_cbk fails with \"Permission denied\" #1544637 : 3.8 -> 3.10 rolling upgrade fails (same for 3.12 or 3.13) on Ubuntu 14","title":"Bugs addressed"},{"location":"release-notes/3.12.7/","text":"Release notes for Gluster 3.12.7 This is a bugfix release. The release notes for 3.12.0 , 3.12.1 , 3.12.2 , 3.12.3 , 3.12.4 , 3.12.5 , 3.12.6 contain a listing of all the new features that were added and bugs fixed in the GlusterFS 3.12 stable release. Bugs addressed Major issues Consider a case in which one of the nodes goes down in gluster cluster with brick multiplexing enabled, if volume operations are performed then post when the node comes back, brick processes will fail to come up. The issue is tracked in #1543708 and will be fixed by next release. A total of 8 patches have been merged, addressing 8 bugs #1517260 : Volume wrong size #1543709 : Optimize glusterd_import_friend_volume code path #1544635 : Though files are in split-brain able to perform writes to the file #1547841 : Typo error in __dht_check_free_space function log message #1548078 : [Rebalance] \"Migrate file failed: : failed to get xattr [No data available]\" warnings in rebalance logs #1548270 : DHT calls dht_lookup_everywhere for 1xn volumes #1549505 : Backport patch to reduce duplicate code in server-rpc-fops.c","title":"3.12.7"},{"location":"release-notes/3.12.7/#release-notes-for-gluster-3127","text":"This is a bugfix release. The release notes for 3.12.0 , 3.12.1 , 3.12.2 , 3.12.3 , 3.12.4 , 3.12.5 , 3.12.6 contain a listing of all the new features that were added and bugs fixed in the GlusterFS 3.12 stable release.","title":"Release notes for Gluster 3.12.7"},{"location":"release-notes/3.12.7/#bugs-addressed","text":"","title":"Bugs addressed"},{"location":"release-notes/3.12.7/#major-issues","text":"Consider a case in which one of the nodes goes down in gluster cluster with brick multiplexing enabled, if volume operations are performed then post when the node comes back, brick processes will fail to come up. The issue is tracked in #1543708 and will be fixed by next release. A total of 8 patches have been merged, addressing 8 bugs #1517260 : Volume wrong size #1543709 : Optimize glusterd_import_friend_volume code path #1544635 : Though files are in split-brain able to perform writes to the file #1547841 : Typo error in __dht_check_free_space function log message #1548078 : [Rebalance] \"Migrate file failed: : failed to get xattr [No data available]\" warnings in rebalance logs #1548270 : DHT calls dht_lookup_everywhere for 1xn volumes #1549505 : Backport patch to reduce duplicate code in server-rpc-fops.c","title":"Major issues"},{"location":"release-notes/3.12.8/","text":"Release notes for Gluster 3.12.8 This is a bugfix release. The release notes for 3.12.0 , 3.12.1 , 3.12.2 , 3.12.3 , 3.12.4 , 3.12.5 , 3.12.6 , 3.12.7 contain a listing of all the new features that were added and bugs fixed in the GlusterFS 3.12 stable release. Bugs addressed A total of 9 patches have been merged, addressing 9 bugs #1543708 : glusterd fails to attach brick during restart of the node #1546627 : Syntactical errors in hook scripts for managing SELinux context on bricks #1549473 : possible memleak in glusterfsd process with brick multiplexing on #1555161 : [Rebalance] ENOSPC errors on few files in rebalance logs #1555201 : After a replace brick command, self-heal takes some time to start healing files on disperse volumes #1558352 : [EC] Read performance of EC volume exported over gNFS is significantly lower than write performance #1561731 : Rebalance failures on a dispersed volume with lookup-optimize enabled #1562723 : SHD is not healing entries in halo replication #1565590 : timer: Possible race condition between gf_timer_* routines","title":"3.12.8"},{"location":"release-notes/3.12.8/#release-notes-for-gluster-3128","text":"This is a bugfix release. The release notes for 3.12.0 , 3.12.1 , 3.12.2 , 3.12.3 , 3.12.4 , 3.12.5 , 3.12.6 , 3.12.7 contain a listing of all the new features that were added and bugs fixed in the GlusterFS 3.12 stable release.","title":"Release notes for Gluster 3.12.8"},{"location":"release-notes/3.12.8/#bugs-addressed","text":"A total of 9 patches have been merged, addressing 9 bugs #1543708 : glusterd fails to attach brick during restart of the node #1546627 : Syntactical errors in hook scripts for managing SELinux context on bricks #1549473 : possible memleak in glusterfsd process with brick multiplexing on #1555161 : [Rebalance] ENOSPC errors on few files in rebalance logs #1555201 : After a replace brick command, self-heal takes some time to start healing files on disperse volumes #1558352 : [EC] Read performance of EC volume exported over gNFS is significantly lower than write performance #1561731 : Rebalance failures on a dispersed volume with lookup-optimize enabled #1562723 : SHD is not healing entries in halo replication #1565590 : timer: Possible race condition between gf_timer_* routines","title":"Bugs addressed"},{"location":"release-notes/3.12.9/","text":"Release notes for Gluster 3.12.9 This is a bugfix release. The release notes for 3.12.0 , 3.12.1 , 3.12.2 , 3.12.3 , 3.12.4 , 3.12.5 , 3.12.6 , 3.12.7 , and 3.12.8 contain a listing of all the new features that were added and bugs fixed in the GlusterFS 3.12 stable release. Major changes, features and limitations addressed in this release This release contains a fix for a security vulerability in Gluster as follows, - http://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2018-1088 - https://nvd.nist.gov/vuln/detail/CVE-2018-1088 Installing the updated packages and restarting gluster services, will update the Gluster shared storage volume volfiles, that are more secure than the defaults currently in place. Further, for increased security, the Gluster shared storage volume can be TLS enabled, and access to the same restricted using the auth.ssl-allow option. See, this guide for more details. Major issues None Bugs addressed Bugs addressed since release-3.12.8 are listed below. #1566131 : Bringing down data bricks in cyclic order results in arbiter brick becoming the source for heal. #1566820 : [Remove-brick] Many files were not migrated from the decommissioned bricks; commit results in data loss #1569407 : EIO errors on some operations when volume has mixed brick versions on a disperse volume #1570430 : CVE-2018-1088 glusterfs: Privilege escalation via gluster_shared_storage when snapshot scheduling is enabled [fedora-all]","title":"3.12.9"},{"location":"release-notes/3.12.9/#release-notes-for-gluster-3129","text":"This is a bugfix release. The release notes for 3.12.0 , 3.12.1 , 3.12.2 , 3.12.3 , 3.12.4 , 3.12.5 , 3.12.6 , 3.12.7 , and 3.12.8 contain a listing of all the new features that were added and bugs fixed in the GlusterFS 3.12 stable release.","title":"Release notes for Gluster 3.12.9"},{"location":"release-notes/3.12.9/#major-changes-features-and-limitations-addressed-in-this-release","text":"This release contains a fix for a security vulerability in Gluster as follows, - http://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2018-1088 - https://nvd.nist.gov/vuln/detail/CVE-2018-1088 Installing the updated packages and restarting gluster services, will update the Gluster shared storage volume volfiles, that are more secure than the defaults currently in place. Further, for increased security, the Gluster shared storage volume can be TLS enabled, and access to the same restricted using the auth.ssl-allow option. See, this guide for more details.","title":"Major changes, features and limitations addressed in this release"},{"location":"release-notes/3.12.9/#major-issues","text":"None","title":"Major issues"},{"location":"release-notes/3.12.9/#bugs-addressed","text":"Bugs addressed since release-3.12.8 are listed below. #1566131 : Bringing down data bricks in cyclic order results in arbiter brick becoming the source for heal. #1566820 : [Remove-brick] Many files were not migrated from the decommissioned bricks; commit results in data loss #1569407 : EIO errors on some operations when volume has mixed brick versions on a disperse volume #1570430 : CVE-2018-1088 glusterfs: Privilege escalation via gluster_shared_storage when snapshot scheduling is enabled [fedora-all]","title":"Bugs addressed"},{"location":"release-notes/3.13.0/","text":"Release notes for Gluster 3.13.0 This is a major release that includes a range of features enhancing usability; enhancements to GFAPI for developers and a set of bug fixes. The most notable features and changes are documented on this page. A full list of bugs that have been addressed is included further below. Major changes and features Addition of summary option to the heal info CLI Notes for users: The Gluster heal info CLI now has a 'summary' option displaying the statistics of entries pending heal, in split-brain and currently being healed, per brick. Usage: # gluster volume heal <volname> info summary Sample output: Brick <brickname> Status: Connected Total Number of entries: 3 Number of entries in heal pending: 2 Number of entries in split-brain: 1 Number of entries possibly healing: 0 Brick <brickname> Status: Connected Total Number of entries: 4 Number of entries in heal pending: 3 Number of entries in split-brain: 1 Number of entries possibly healing: 0 Using the --xml option with the CLI results in the output in XML format. NOTE: Summary information is obtained in a similar fashion to detailed information, thus time taken for the command to complete would still be the same, and not faster. Addition of checks for allowing lookups in AFR and removal of 'cluster.quorum-reads' volume option. Notes for users: Previously, AFR has never failed lookup unless there is a gfid mismatch. This behavior is being changed with this release, as a part of fixing Bug#1515572 . Lookups in replica-3 and arbiter volumes will now succeed only if there is quorum and there is a good copy of a file. I.e. the lookup has to succeed on quorum #bricks and at least one of them has to be a good copy. If these conditions are not met, the operation will fail with the ENOTCONN error. As a part of this change the cluster.quorum-reads volume option is removed, as lookup failure will result in all subsequent operations (including reads) failing, which makes this option redundant. Ensuring this strictness also helps prevent a long standing rename-leading-to-dataloss Bug#1366818 , by disallowing lookups (and thus renames) when a good copy is not available. Note: These checks do not affect replica 2 volumes where lookups works as before, even when only 1 brick is online. Further reference: mailing list discussions on topic Support for max-port range in glusterd.vol Notes for users: Glusterd configuration provides an option to control number of ports that can be used by gluster daemons on a node. The option is named \"max-port\" and can be set in the glusterd.vol file per-node to the desired maximum. Prevention of other processes accessing the mounted brick snapshots Notes for users: Snapshot of gluster bricks are now only mounted when the snapshot is active, or when these are restored. Prior to this snapshots of gluster volumes were mounted by default across the entire life-cycle of the snapshot. This behavior is transparent to users and managed by the gluster processes. Enabling thin client Notes for users: Gluster client stack encompasses the cluster translators (like distribution and replication or disperse). This is in addition to the usual caching translators on the client stacks. In certain cases this makes the client footprint larger than sustainable and also incurs frequent client updates. The thin client feature, moves the clustering translators (like distribute and other translators below it) and a few caching translators to a managed protocol endpoint (called gfproxy) on the gluster server nodes, thus thinning the client stack. Usage: # gluster volume set <volname> config.gfproxyd enable The above enables the gfproxy protocol service on the server nodes. To mount a client that interacts with this end point, use the --thin-client mount option. Example: # glusterfs --thin-client --volfile-id=<volname> --volfile-server=<host> <mountpoint> Limitations: This feature is a technical preview in the 3.13.0 release, and will be improved in the upcoming releases. Ability to reserve back-end storage space Notes for users: Posix translator is enhanced with an option that enables reserving disk space on the bricks. This reserved space is not used by the client mounts thus preventing disk full scenarios, as disk expansion or cluster expansion is more tedious to achieve when back-end bricks are full. When the bricks have free space equal to or lesser than the reserved space, mount points using the brick would get ENOSPC errors. The default value for the option is 1(%) of the brick size. If set to 0(%) this feature is disabled. The option takes a numeric percentage value, that reserves up to that percentage of disk space. Usage: # gluster volume set <volname> storage.reserve <number> List all the connected clients for a brick and also exported bricks/snapshots from each brick process Notes for users: Gluster CLI is enhanced with an option to list all connected clients to a volume (or all volumes) and also the list of exported bricks and snapshots for the volume. Usage: # gluster volume status <volname/all> client-list Improved write performance with Disperse xlator Notes for users: Disperse translator has been enhanced to support parallel writes, that hence improves the performance of write operations when using disperse volumes. This feature is enabled by default, and can be toggled using the boolean option, 'disperse.parallel-writes' Disperse xlator now supports discard operations Notes for users: This feature enables users to punch hole in files created on disperse volumes. Usage: # fallocate -p -o <offset> -l <len> <file_name> Included details about memory pools in statedumps Notes for users: For troubleshooting purposes it sometimes is useful to verify the memory allocations done by Gluster. A previous release of Gluster included a rewrite of the memory pool internals. Since these changes, statedump s did not include details about the memory pools anymore. This version of Gluster adds details about the used memory pools in the statedump . Troubleshooting memory consumption problems is much more efficient again. Limitations: There are currently no statistics included in the statedump about the actual behavior of the memory pools. This means that the efficiency of the memory pools can not be verified. Gluster APIs added to register callback functions for upcalls Notes for developers: New APIs have been added to allow gfapi applications to register and unregister for upcall events. Along with the list of events interested, applications now have to register callback function. This routine shall be invoked asynchronously, in gluster thread context, in case of any upcalls sent by the backend server. int glfs_upcall_register (struct glfs *fs, uint32_t event_list, glfs_upcall_cbk cbk, void *data); int glfs_upcall_unregister (struct glfs *fs, uint32_t event_list); libgfapi header files include the complete synopsis about these APIs definition and their usage. Limitations: An application can register only a single callback function for all the upcall events it is interested in. Known Issues: Bug#1515748 GlusterFS server should be able to identify the clients which registered for upcalls and notify only those clients in case of such events Gluster API added with a glfs_mem_header for exported memory Notes for developers: Memory allocations done in libgfapi that return a structure to the calling application should use GLFS_CALLOC() and friends. Applications can then correctly free the memory by calling glfs_free() . This is implemented with a new glfs_mem_header similar to how the memory allocations are done with GF_CALLOC() etc. The new header includes a release() function pointer that gets called to free the resource when the application calls glfs_free() . The change is a major improvement for allocating and free'ing resources in a standardized way that is transparent to the libgfapi applications. Provided a new xlator to delay fops, to aid slow brick response simulation and debugging Notes for developers: Like error-gen translator, a new translator that introduces delays for FOPs is added to the code base. This can help determine issues around slow(er) client responses and enable better qualification of the translator stacks. For usage refer to this test case . Major issues Expanding a gluster volume that is sharded may cause file corruption Sharded volumes are typically used for VM images, if such volumes are expanded or possibly contracted (i.e add/remove bricks and rebalance) there are reports of VM images getting corrupted. The last known cause for corruption (Bug #1515434) has a fix with this release. As further testing is still in progress, the issue is retained as a major issue. Status of this bug can be tracked here, #1515434 Bugs addressed Bugs addressed since release-3.12.0 are listed below. #1248393 : DHT: readdirp fails to read some directories. #1258561 : Gluster puts PID files in wrong place #1261463 : AFR : [RFE] Improvements needed in \"gluster volume heal info\" commands #1294051 : Though files are in split-brain able to perform writes to the file #1328994 : When a feature fails needing a higher opversion, the message should state what version it needs. #1335251 : mgmt/glusterd: clang compile warnings in glusterd-snapshot.c #1350406 : [storage/posix] - posix_do_futimes function not implemented #1365683 : Fix crash bug when mnt3_resolve_subdir_cbk fails #1371806 : DHT :- inconsistent 'custom extended attributes',uid and gid, Access permission (for directories) if User set/modifies it after bringing one or more sub-volume down #1376326 : separating attach tier and add brick #1388509 : gluster volume heal info \"healed\" and \"heal-failed\" showing wrong information #1395492 : trace/error-gen be turned on together while use 'volume set' command to set one of them #1396327 : gluster core dump due to assert failed GF_ASSERT (brick_index < wordcount); #1406898 : Need build time option to default to IPv6 #1428063 : gfproxy: Introduce new server-side daemon called GFProxy #1432046 : symlinks trigger faulty geo-replication state (rsnapshot usecase) #1443145 : Free runtime allocated resources upon graph switch or glfs_fini() #1445663 : Improve performance with xattrop update. #1451434 : Use a bitmap to store local node info instead of conf->local_nodeuuids[i].uuids #1454590 : run.c demo mode broken #1457985 : Rebalance estimate time sometimes shows negative values #1460514 : [Ganesha] : Ganesha crashes while cluster enters failover/failback mode #1461018 : Implement DISCARD FOP for EC #1462969 : Peer-file parsing is too fragile #1467209 : [Scale] : Rebalance ETA shows the initial estimate to be ~140 days,finishes within 18 hours though. #1467614 : Gluster read/write performance improvements on NVMe backend #1468291 : NFS Sub directory is getting mounted on solaris 10 even when the permission is restricted in nfs.export-dir volume option #1471366 : Posix xlator needs to reserve disk space to prevent the brick from getting full. #1472267 : glusterd fails to start #1472609 : Root path xattr does not heal correctly in certain cases when volume is in stopped state #1472758 : Running sysbench on vm disk from plain distribute gluster volume causes disk corruption #1472961 : [GNFS+EC] lock is being granted to 2 different client for the same data range at a time after performing lock acquire/release from the clients1 #1473026 : replace-brick failure leaves glusterd in inconsistent state #1473636 : Launch metadata heal in discover code path. #1474180 : [Scale] : Client logs flooded with \"inode context is NULL\" error messages #1474190 : cassandra fails on gluster-block with both replicate and ec volumes #1474309 : Disperse: Coverity issue #1474318 : dht remove-brick status does not indicate failures files not migrated because of a lack of space #1474639 : [Scale] : Rebalance Logs are bulky. #1475255 : [Geo-rep]: Geo-rep hangs in changelog mode #1475282 : [Remove-brick] Few files are getting migrated eventhough the bricks crossed cluster.min-free-disk value #1475300 : implementation of fallocate call in read-only xlator #1475308 : [geo-rep]: few of the self healed hardlinks on master did not sync to slave #1475605 : gluster-block default shard-size should be 64MB #1475632 : Brick Multiplexing: Brick process crashed at changetimerecorder(ctr) translator when restarting volumes #1476205 : [EC]: md5sum mismatches every time for a file from the fuse client on EC volume #1476295 : md-cache uses incorrect xattr keynames for GF_POSIX_ACL keys #1476324 : md-cache: xattr values should not be checked with string functions #1476410 : glusterd: code lacks clarity of logic in glusterd_get_quorum_cluster_counts() #1476665 : [Perf] : Large file sequential reads are off target by ~38% on FUSE/Ganesha #1476668 : [Disperse] : Improve heal info command to handle obvious cases #1476719 : glusterd: flow in glusterd_validate_quorum() could be streamlined #1476785 : scripts: invalid test in S32gluster_enable_shared_storage.sh #1476861 : packaging: /var/lib/glusterd/options should be %config(noreplace) #1476957 : peer-parsing.t fails on NetBSD #1477169 : AFR entry self heal removes a directory's .glusterfs symlink. #1477404 : eager-lock should be off for cassandra to work at the moment #1477488 : Permission denied errors when appending files after readdir #1478297 : Add NULL gfid checks before creating file #1478710 : when gluster pod is restarted, bricks from the restarted pod fails to connect to fuse, self-heal etc #1479030 : nfs process crashed in \"nfs3_getattr\" #1480099 : More useful error - replace 'not optimal' #1480445 : Log entry of files skipped/failed during rebalance operation #1480525 : Make choose-local configurable through volume-set command #1480591 : [Scale] : I/O errors on multiple gNFS mounts with \"Stale file handle\" during rebalance of an erasure coded volume. #1481199 : mempool: run-time crash when built with --disable-mempool #1481600 : rpc: client_t and related objects leaked due to incorrect ref counts #1482023 : snpashots issues with other processes accessing the mounted brick snapshots #1482344 : Negative Test: glusterd crashes for some of the volume options if set at cluster level #1482906 : /var/lib/glusterd/peers File had a blank line, Stopped Glusterd from starting #1482923 : afr: check op_ret value in __afr_selfheal_name_impunge #1483058 : [quorum]: Replace brick is happened when Quorum not met. #1483995 : packaging: use rdma-core(-devel) instead of ibverbs, rdmacm; disable rdma on armv7hl #1484215 : Add Deepshika has CI Peer #1484225 : [rpc]: EPOLLERR - disconnecting now messages every 3 secs after completing rebalance #1484246 : [PATCH] incorrect xattr list handling on FreeBSD #1484490 : File-level WORM allows mv over read-only files #1484709 : [geo-rep+qr]: Crashes observed at slave from qr_lookup_sbk during rename/hardlink/rebalance cases #1484722 : return ENOSYS for 'non readable' FOPs #1485962 : gluster-block profile needs to have strict-o-direct #1486134 : glusterfsd (brick) process crashed #1487644 : Fix reference to readthedocs.io in source code and elsewhere #1487830 : scripts: mount.glusterfs contains non-portable bashisms #1487840 : glusterd: spelling errors reported by Debian maintainer #1488354 : gluster-blockd process crashed and core generated #1488399 : Crash in dht_check_and_open_fd_on_subvol_task() #1488546 : [RHHI] cannot boot vms created from template when disk format = qcow2 #1488808 : Warning on FreeBSD regarding -Wformat-extra-args #1488829 : Fix unused variable when TCP_USER_TIMEOUT is undefined #1488840 : Fix guard define on nl-cache #1488906 : Fix clagn/gcc warning for umountd #1488909 : Fix the type of 'len' in posix.c, clang is showing a warning #1488913 : Sub-directory mount details are incorrect in /proc/mounts #1489432 : disallow replace brick operation on plain distribute volume #1489823 : set the shard-block-size to 64MB in virt profile #1490642 : glusterfs client crash when removing directories #1490897 : GlusterD returns a bad memory pointer in glusterd_get_args_from_dict() #1491025 : rpc: TLSv1_2_method() is deprecated in OpenSSL-1.1 #1491670 : [afr] split-brain observed on T files post hardlink and rename in x3 volume #1492109 : Provide brick list as part of VOLUME_CREATE event. #1492542 : Gluster v status client-list prints wrong output for multiplexed volume. #1492849 : xlator/tier: flood of -Wformat-truncation warnings with gcc-7. #1492851 : xlator/bitrot: flood of -Wformat-truncation warnings with gcc-7. #1492968 : CLIENT_CONNECT event is not notified by eventsapi #1492996 : Readdirp is considerably slower than readdir on acl clients #1493133 : GlusterFS failed to build while running make #1493415 : self-heal daemon stuck #1493539 : AFR_SUBVOL_UP and AFR_SUBVOLS_DOWN events not working #1493893 : gluster volume asks for confirmation for disperse volume even with force #1493967 : glusterd ends up with multiple uuids for the same node #1495384 : Gluster 3.12.1 Packages require manual systemctl daemon reload after install #1495436 : [geo-rep]: Scheduler help needs correction for description of --no-color #1496363 : Add generated HMAC token in header for webhook calls #1496379 : glusterfs process consume huge memory on both server and client node #1496675 : Verify pool pointer before destroying it #1498570 : client-io-threads option not working for replicated volumes #1499004 : [Glusterd] Volume operations fail on a (tiered) volume because of a stale lock held by one of the nodes #1499159 : [geo-rep]: Improve the output message to reflect the real failure with schedule_georep script #1499180 : [geo-rep]: Observed \"Operation not supported\" error with traceback on slave log #1499391 : [geo-rep]: Worker crashes with OSError: [Errno 61] No data available #1499393 : [geo-rep] master worker crash with interrupted system call #1499509 : Brick Multiplexing: Gluster volume start force complains with command \"Error : Request timed out\" when there are multiple volumes #1499641 : gfapi: API needed to set lk_owner #1499663 : Mark test case ./tests/bugs/bug-1371806_1.t as a bad test case. #1499933 : md-cache: Add additional samba and macOS specific EAs to mdcache #1500269 : opening a file that is destination of rename results in ENOENT errors #1500284 : [geo-rep]: Status shows ACTIVE for most workers in EC before it becomes the PASSIVE #1500346 : [geo-rep]: Incorrect last sync \"0\" during hystory crawl after upgrade/stop-start #1500433 : [geo-rep]: RSYNC throwing internal errors #1500649 : Shellcheck errors in hook scripts #1501235 : [SNAPSHOT] Unable to mount a snapshot on client #1501317 : glusterfs fails to build twice in a row #1501390 : Intermittent failure in tests/basic/afr/gfid-mismatch-resolution-with-fav-child-policy.t on NetBSD #1502253 : snapshot_scheduler crashes when SELinux is absent on the system #1503246 : clean up port map on brick disconnect #1503394 : Mishandling null check at send_brick_req of glusterfsd/src/gf_attach.c #1503424 : server.allow-insecure should be visible in \"gluster volume set help\" #1503510 : [BitRot] man page of gluster needs to be updated for scrub-frequency #1503519 : default timeout of 5min not honored for analyzing split-brain files post setfattr replica.split-brain-heal-finalize #1503983 : Wrong usage of getopt shell command in hook-scripts #1505253 : Update .t test files to use the new tier commands #1505323 : When sub-dir is mounted on Fuse client,adding bricks to the same volume unmounts the subdir from fuse client #1505325 : Potential use of NULL this variable before it gets initialized #1505527 : Posix compliance rename test fails on fuse subdir mount #1505663 : [GSS] gluster volume status command is missing in man page #1505807 : files are appendable on file-based worm volume #1506083 : Ignore disk space reserve check for internal FOPS #1506513 : stale brick processes getting created and volume status shows brick as down(pkill glusterfsd glusterfs ,glusterd restart) #1506589 : Brick port mismatch #1506903 : Event webhook should work with HTTPS urls #1507466 : reset-brick commit force failed with glusterd_volume_brickinfo_get Returning -1 #1508898 : Add new configuration option to manage deletion of Worm files #1509789 : The output of the \"gluster help\" command is difficult to read #1510012 : GlusterFS 3.13.0 tracker #1510019 : Change default versions of certain features to 3.13 from 4.0 #1510022 : Revert experimental and 4.0 features to prepare for 3.13 release #1511274 : Rebalance estimate(ETA) shows wrong details(as intial message of 10min wait reappears) when still in progress #1511293 : In distribute volume after glusterd restart, brick goes offline #1511768 : In Replica volume 2*2 when quorum is set, after glusterd restart nfs server is coming up instead of self-heal daemon #1512435 : Test bug-1483058-replace-brick-quorum-validation.t fails inconsistently #1512460 : disperse eager-lock degrades performance for file create workloads #1513259 : NetBSD port #1514419 : gluster volume splitbrain info needs to display output of each brick in a stream fashion instead of buffering and dumping at the end #1515045 : bug-1247563.t is failing on master #1515572 : Accessing a file when source brick is down results in that FOP being hung #1516313 : Bringing down data bricks in cyclic order results in arbiter brick becoming the source for heal. #1517692 : Memory leak in locks xlator #1518257 : EC DISCARD doesn't punch hole properly #1518512 : Change GD_OP_VERSION to 3_13_0 from 3_12_0 for RFE https://bugzilla.redhat.com/show_bug.cgi?id=1464350 #1518744 : Add release notes about DISCARD on EC volume","title":"3.13.0"},{"location":"release-notes/3.13.0/#release-notes-for-gluster-3130","text":"This is a major release that includes a range of features enhancing usability; enhancements to GFAPI for developers and a set of bug fixes. The most notable features and changes are documented on this page. A full list of bugs that have been addressed is included further below.","title":"Release notes for Gluster 3.13.0"},{"location":"release-notes/3.13.0/#major-changes-and-features","text":"","title":"Major changes and features"},{"location":"release-notes/3.13.0/#addition-of-summary-option-to-the-heal-info-cli","text":"Notes for users: The Gluster heal info CLI now has a 'summary' option displaying the statistics of entries pending heal, in split-brain and currently being healed, per brick. Usage: # gluster volume heal <volname> info summary Sample output: Brick <brickname> Status: Connected Total Number of entries: 3 Number of entries in heal pending: 2 Number of entries in split-brain: 1 Number of entries possibly healing: 0 Brick <brickname> Status: Connected Total Number of entries: 4 Number of entries in heal pending: 3 Number of entries in split-brain: 1 Number of entries possibly healing: 0 Using the --xml option with the CLI results in the output in XML format. NOTE: Summary information is obtained in a similar fashion to detailed information, thus time taken for the command to complete would still be the same, and not faster.","title":"Addition of summary option to the heal info CLI"},{"location":"release-notes/3.13.0/#addition-of-checks-for-allowing-lookups-in-afr-and-removal-of-clusterquorum-reads-volume-option","text":"Notes for users: Previously, AFR has never failed lookup unless there is a gfid mismatch. This behavior is being changed with this release, as a part of fixing Bug#1515572 . Lookups in replica-3 and arbiter volumes will now succeed only if there is quorum and there is a good copy of a file. I.e. the lookup has to succeed on quorum #bricks and at least one of them has to be a good copy. If these conditions are not met, the operation will fail with the ENOTCONN error. As a part of this change the cluster.quorum-reads volume option is removed, as lookup failure will result in all subsequent operations (including reads) failing, which makes this option redundant. Ensuring this strictness also helps prevent a long standing rename-leading-to-dataloss Bug#1366818 , by disallowing lookups (and thus renames) when a good copy is not available. Note: These checks do not affect replica 2 volumes where lookups works as before, even when only 1 brick is online. Further reference: mailing list discussions on topic","title":"Addition of checks for allowing lookups in AFR and removal of 'cluster.quorum-reads' volume option."},{"location":"release-notes/3.13.0/#support-for-max-port-range-in-glusterdvol","text":"Notes for users: Glusterd configuration provides an option to control number of ports that can be used by gluster daemons on a node. The option is named \"max-port\" and can be set in the glusterd.vol file per-node to the desired maximum.","title":"Support for max-port range in glusterd.vol"},{"location":"release-notes/3.13.0/#prevention-of-other-processes-accessing-the-mounted-brick-snapshots","text":"Notes for users: Snapshot of gluster bricks are now only mounted when the snapshot is active, or when these are restored. Prior to this snapshots of gluster volumes were mounted by default across the entire life-cycle of the snapshot. This behavior is transparent to users and managed by the gluster processes.","title":"Prevention of other processes accessing the mounted brick snapshots"},{"location":"release-notes/3.13.0/#enabling-thin-client","text":"Notes for users: Gluster client stack encompasses the cluster translators (like distribution and replication or disperse). This is in addition to the usual caching translators on the client stacks. In certain cases this makes the client footprint larger than sustainable and also incurs frequent client updates. The thin client feature, moves the clustering translators (like distribute and other translators below it) and a few caching translators to a managed protocol endpoint (called gfproxy) on the gluster server nodes, thus thinning the client stack. Usage: # gluster volume set <volname> config.gfproxyd enable The above enables the gfproxy protocol service on the server nodes. To mount a client that interacts with this end point, use the --thin-client mount option. Example: # glusterfs --thin-client --volfile-id=<volname> --volfile-server=<host> <mountpoint> Limitations: This feature is a technical preview in the 3.13.0 release, and will be improved in the upcoming releases.","title":"Enabling thin client"},{"location":"release-notes/3.13.0/#ability-to-reserve-back-end-storage-space","text":"Notes for users: Posix translator is enhanced with an option that enables reserving disk space on the bricks. This reserved space is not used by the client mounts thus preventing disk full scenarios, as disk expansion or cluster expansion is more tedious to achieve when back-end bricks are full. When the bricks have free space equal to or lesser than the reserved space, mount points using the brick would get ENOSPC errors. The default value for the option is 1(%) of the brick size. If set to 0(%) this feature is disabled. The option takes a numeric percentage value, that reserves up to that percentage of disk space. Usage: # gluster volume set <volname> storage.reserve <number>","title":"Ability to reserve back-end storage space"},{"location":"release-notes/3.13.0/#list-all-the-connected-clients-for-a-brick-and-also-exported-brickssnapshots-from-each-brick-process","text":"Notes for users: Gluster CLI is enhanced with an option to list all connected clients to a volume (or all volumes) and also the list of exported bricks and snapshots for the volume. Usage: # gluster volume status <volname/all> client-list","title":"List all the connected clients for a brick and also exported bricks/snapshots from each brick process"},{"location":"release-notes/3.13.0/#improved-write-performance-with-disperse-xlator","text":"Notes for users: Disperse translator has been enhanced to support parallel writes, that hence improves the performance of write operations when using disperse volumes. This feature is enabled by default, and can be toggled using the boolean option, 'disperse.parallel-writes'","title":"Improved write performance with Disperse xlator"},{"location":"release-notes/3.13.0/#disperse-xlator-now-supports-discard-operations","text":"Notes for users: This feature enables users to punch hole in files created on disperse volumes. Usage: # fallocate -p -o <offset> -l <len> <file_name>","title":"Disperse xlator now supports discard operations"},{"location":"release-notes/3.13.0/#included-details-about-memory-pools-in-statedumps","text":"Notes for users: For troubleshooting purposes it sometimes is useful to verify the memory allocations done by Gluster. A previous release of Gluster included a rewrite of the memory pool internals. Since these changes, statedump s did not include details about the memory pools anymore. This version of Gluster adds details about the used memory pools in the statedump . Troubleshooting memory consumption problems is much more efficient again. Limitations: There are currently no statistics included in the statedump about the actual behavior of the memory pools. This means that the efficiency of the memory pools can not be verified.","title":"Included details about memory pools in statedumps"},{"location":"release-notes/3.13.0/#gluster-apis-added-to-register-callback-functions-for-upcalls","text":"Notes for developers: New APIs have been added to allow gfapi applications to register and unregister for upcall events. Along with the list of events interested, applications now have to register callback function. This routine shall be invoked asynchronously, in gluster thread context, in case of any upcalls sent by the backend server. int glfs_upcall_register (struct glfs *fs, uint32_t event_list, glfs_upcall_cbk cbk, void *data); int glfs_upcall_unregister (struct glfs *fs, uint32_t event_list); libgfapi header files include the complete synopsis about these APIs definition and their usage. Limitations: An application can register only a single callback function for all the upcall events it is interested in. Known Issues: Bug#1515748 GlusterFS server should be able to identify the clients which registered for upcalls and notify only those clients in case of such events","title":"Gluster APIs added to register callback functions for upcalls"},{"location":"release-notes/3.13.0/#gluster-api-added-with-a-glfs_mem_header-for-exported-memory","text":"Notes for developers: Memory allocations done in libgfapi that return a structure to the calling application should use GLFS_CALLOC() and friends. Applications can then correctly free the memory by calling glfs_free() . This is implemented with a new glfs_mem_header similar to how the memory allocations are done with GF_CALLOC() etc. The new header includes a release() function pointer that gets called to free the resource when the application calls glfs_free() . The change is a major improvement for allocating and free'ing resources in a standardized way that is transparent to the libgfapi applications.","title":"Gluster API added with a glfs_mem_header for exported memory"},{"location":"release-notes/3.13.0/#provided-a-new-xlator-to-delay-fops-to-aid-slow-brick-response-simulation-and-debugging","text":"Notes for developers: Like error-gen translator, a new translator that introduces delays for FOPs is added to the code base. This can help determine issues around slow(er) client responses and enable better qualification of the translator stacks. For usage refer to this test case .","title":"Provided a new xlator to delay fops, to aid slow brick response simulation and debugging"},{"location":"release-notes/3.13.0/#major-issues","text":"Expanding a gluster volume that is sharded may cause file corruption Sharded volumes are typically used for VM images, if such volumes are expanded or possibly contracted (i.e add/remove bricks and rebalance) there are reports of VM images getting corrupted. The last known cause for corruption (Bug #1515434) has a fix with this release. As further testing is still in progress, the issue is retained as a major issue. Status of this bug can be tracked here, #1515434","title":"Major issues"},{"location":"release-notes/3.13.0/#bugs-addressed","text":"Bugs addressed since release-3.12.0 are listed below. #1248393 : DHT: readdirp fails to read some directories. #1258561 : Gluster puts PID files in wrong place #1261463 : AFR : [RFE] Improvements needed in \"gluster volume heal info\" commands #1294051 : Though files are in split-brain able to perform writes to the file #1328994 : When a feature fails needing a higher opversion, the message should state what version it needs. #1335251 : mgmt/glusterd: clang compile warnings in glusterd-snapshot.c #1350406 : [storage/posix] - posix_do_futimes function not implemented #1365683 : Fix crash bug when mnt3_resolve_subdir_cbk fails #1371806 : DHT :- inconsistent 'custom extended attributes',uid and gid, Access permission (for directories) if User set/modifies it after bringing one or more sub-volume down #1376326 : separating attach tier and add brick #1388509 : gluster volume heal info \"healed\" and \"heal-failed\" showing wrong information #1395492 : trace/error-gen be turned on together while use 'volume set' command to set one of them #1396327 : gluster core dump due to assert failed GF_ASSERT (brick_index < wordcount); #1406898 : Need build time option to default to IPv6 #1428063 : gfproxy: Introduce new server-side daemon called GFProxy #1432046 : symlinks trigger faulty geo-replication state (rsnapshot usecase) #1443145 : Free runtime allocated resources upon graph switch or glfs_fini() #1445663 : Improve performance with xattrop update. #1451434 : Use a bitmap to store local node info instead of conf->local_nodeuuids[i].uuids #1454590 : run.c demo mode broken #1457985 : Rebalance estimate time sometimes shows negative values #1460514 : [Ganesha] : Ganesha crashes while cluster enters failover/failback mode #1461018 : Implement DISCARD FOP for EC #1462969 : Peer-file parsing is too fragile #1467209 : [Scale] : Rebalance ETA shows the initial estimate to be ~140 days,finishes within 18 hours though. #1467614 : Gluster read/write performance improvements on NVMe backend #1468291 : NFS Sub directory is getting mounted on solaris 10 even when the permission is restricted in nfs.export-dir volume option #1471366 : Posix xlator needs to reserve disk space to prevent the brick from getting full. #1472267 : glusterd fails to start #1472609 : Root path xattr does not heal correctly in certain cases when volume is in stopped state #1472758 : Running sysbench on vm disk from plain distribute gluster volume causes disk corruption #1472961 : [GNFS+EC] lock is being granted to 2 different client for the same data range at a time after performing lock acquire/release from the clients1 #1473026 : replace-brick failure leaves glusterd in inconsistent state #1473636 : Launch metadata heal in discover code path. #1474180 : [Scale] : Client logs flooded with \"inode context is NULL\" error messages #1474190 : cassandra fails on gluster-block with both replicate and ec volumes #1474309 : Disperse: Coverity issue #1474318 : dht remove-brick status does not indicate failures files not migrated because of a lack of space #1474639 : [Scale] : Rebalance Logs are bulky. #1475255 : [Geo-rep]: Geo-rep hangs in changelog mode #1475282 : [Remove-brick] Few files are getting migrated eventhough the bricks crossed cluster.min-free-disk value #1475300 : implementation of fallocate call in read-only xlator #1475308 : [geo-rep]: few of the self healed hardlinks on master did not sync to slave #1475605 : gluster-block default shard-size should be 64MB #1475632 : Brick Multiplexing: Brick process crashed at changetimerecorder(ctr) translator when restarting volumes #1476205 : [EC]: md5sum mismatches every time for a file from the fuse client on EC volume #1476295 : md-cache uses incorrect xattr keynames for GF_POSIX_ACL keys #1476324 : md-cache: xattr values should not be checked with string functions #1476410 : glusterd: code lacks clarity of logic in glusterd_get_quorum_cluster_counts() #1476665 : [Perf] : Large file sequential reads are off target by ~38% on FUSE/Ganesha #1476668 : [Disperse] : Improve heal info command to handle obvious cases #1476719 : glusterd: flow in glusterd_validate_quorum() could be streamlined #1476785 : scripts: invalid test in S32gluster_enable_shared_storage.sh #1476861 : packaging: /var/lib/glusterd/options should be %config(noreplace) #1476957 : peer-parsing.t fails on NetBSD #1477169 : AFR entry self heal removes a directory's .glusterfs symlink. #1477404 : eager-lock should be off for cassandra to work at the moment #1477488 : Permission denied errors when appending files after readdir #1478297 : Add NULL gfid checks before creating file #1478710 : when gluster pod is restarted, bricks from the restarted pod fails to connect to fuse, self-heal etc #1479030 : nfs process crashed in \"nfs3_getattr\" #1480099 : More useful error - replace 'not optimal' #1480445 : Log entry of files skipped/failed during rebalance operation #1480525 : Make choose-local configurable through volume-set command #1480591 : [Scale] : I/O errors on multiple gNFS mounts with \"Stale file handle\" during rebalance of an erasure coded volume. #1481199 : mempool: run-time crash when built with --disable-mempool #1481600 : rpc: client_t and related objects leaked due to incorrect ref counts #1482023 : snpashots issues with other processes accessing the mounted brick snapshots #1482344 : Negative Test: glusterd crashes for some of the volume options if set at cluster level #1482906 : /var/lib/glusterd/peers File had a blank line, Stopped Glusterd from starting #1482923 : afr: check op_ret value in __afr_selfheal_name_impunge #1483058 : [quorum]: Replace brick is happened when Quorum not met. #1483995 : packaging: use rdma-core(-devel) instead of ibverbs, rdmacm; disable rdma on armv7hl #1484215 : Add Deepshika has CI Peer #1484225 : [rpc]: EPOLLERR - disconnecting now messages every 3 secs after completing rebalance #1484246 : [PATCH] incorrect xattr list handling on FreeBSD #1484490 : File-level WORM allows mv over read-only files #1484709 : [geo-rep+qr]: Crashes observed at slave from qr_lookup_sbk during rename/hardlink/rebalance cases #1484722 : return ENOSYS for 'non readable' FOPs #1485962 : gluster-block profile needs to have strict-o-direct #1486134 : glusterfsd (brick) process crashed #1487644 : Fix reference to readthedocs.io in source code and elsewhere #1487830 : scripts: mount.glusterfs contains non-portable bashisms #1487840 : glusterd: spelling errors reported by Debian maintainer #1488354 : gluster-blockd process crashed and core generated #1488399 : Crash in dht_check_and_open_fd_on_subvol_task() #1488546 : [RHHI] cannot boot vms created from template when disk format = qcow2 #1488808 : Warning on FreeBSD regarding -Wformat-extra-args #1488829 : Fix unused variable when TCP_USER_TIMEOUT is undefined #1488840 : Fix guard define on nl-cache #1488906 : Fix clagn/gcc warning for umountd #1488909 : Fix the type of 'len' in posix.c, clang is showing a warning #1488913 : Sub-directory mount details are incorrect in /proc/mounts #1489432 : disallow replace brick operation on plain distribute volume #1489823 : set the shard-block-size to 64MB in virt profile #1490642 : glusterfs client crash when removing directories #1490897 : GlusterD returns a bad memory pointer in glusterd_get_args_from_dict() #1491025 : rpc: TLSv1_2_method() is deprecated in OpenSSL-1.1 #1491670 : [afr] split-brain observed on T files post hardlink and rename in x3 volume #1492109 : Provide brick list as part of VOLUME_CREATE event. #1492542 : Gluster v status client-list prints wrong output for multiplexed volume. #1492849 : xlator/tier: flood of -Wformat-truncation warnings with gcc-7. #1492851 : xlator/bitrot: flood of -Wformat-truncation warnings with gcc-7. #1492968 : CLIENT_CONNECT event is not notified by eventsapi #1492996 : Readdirp is considerably slower than readdir on acl clients #1493133 : GlusterFS failed to build while running make #1493415 : self-heal daemon stuck #1493539 : AFR_SUBVOL_UP and AFR_SUBVOLS_DOWN events not working #1493893 : gluster volume asks for confirmation for disperse volume even with force #1493967 : glusterd ends up with multiple uuids for the same node #1495384 : Gluster 3.12.1 Packages require manual systemctl daemon reload after install #1495436 : [geo-rep]: Scheduler help needs correction for description of --no-color #1496363 : Add generated HMAC token in header for webhook calls #1496379 : glusterfs process consume huge memory on both server and client node #1496675 : Verify pool pointer before destroying it #1498570 : client-io-threads option not working for replicated volumes #1499004 : [Glusterd] Volume operations fail on a (tiered) volume because of a stale lock held by one of the nodes #1499159 : [geo-rep]: Improve the output message to reflect the real failure with schedule_georep script #1499180 : [geo-rep]: Observed \"Operation not supported\" error with traceback on slave log #1499391 : [geo-rep]: Worker crashes with OSError: [Errno 61] No data available #1499393 : [geo-rep] master worker crash with interrupted system call #1499509 : Brick Multiplexing: Gluster volume start force complains with command \"Error : Request timed out\" when there are multiple volumes #1499641 : gfapi: API needed to set lk_owner #1499663 : Mark test case ./tests/bugs/bug-1371806_1.t as a bad test case. #1499933 : md-cache: Add additional samba and macOS specific EAs to mdcache #1500269 : opening a file that is destination of rename results in ENOENT errors #1500284 : [geo-rep]: Status shows ACTIVE for most workers in EC before it becomes the PASSIVE #1500346 : [geo-rep]: Incorrect last sync \"0\" during hystory crawl after upgrade/stop-start #1500433 : [geo-rep]: RSYNC throwing internal errors #1500649 : Shellcheck errors in hook scripts #1501235 : [SNAPSHOT] Unable to mount a snapshot on client #1501317 : glusterfs fails to build twice in a row #1501390 : Intermittent failure in tests/basic/afr/gfid-mismatch-resolution-with-fav-child-policy.t on NetBSD #1502253 : snapshot_scheduler crashes when SELinux is absent on the system #1503246 : clean up port map on brick disconnect #1503394 : Mishandling null check at send_brick_req of glusterfsd/src/gf_attach.c #1503424 : server.allow-insecure should be visible in \"gluster volume set help\" #1503510 : [BitRot] man page of gluster needs to be updated for scrub-frequency #1503519 : default timeout of 5min not honored for analyzing split-brain files post setfattr replica.split-brain-heal-finalize #1503983 : Wrong usage of getopt shell command in hook-scripts #1505253 : Update .t test files to use the new tier commands #1505323 : When sub-dir is mounted on Fuse client,adding bricks to the same volume unmounts the subdir from fuse client #1505325 : Potential use of NULL this variable before it gets initialized #1505527 : Posix compliance rename test fails on fuse subdir mount #1505663 : [GSS] gluster volume status command is missing in man page #1505807 : files are appendable on file-based worm volume #1506083 : Ignore disk space reserve check for internal FOPS #1506513 : stale brick processes getting created and volume status shows brick as down(pkill glusterfsd glusterfs ,glusterd restart) #1506589 : Brick port mismatch #1506903 : Event webhook should work with HTTPS urls #1507466 : reset-brick commit force failed with glusterd_volume_brickinfo_get Returning -1 #1508898 : Add new configuration option to manage deletion of Worm files #1509789 : The output of the \"gluster help\" command is difficult to read #1510012 : GlusterFS 3.13.0 tracker #1510019 : Change default versions of certain features to 3.13 from 4.0 #1510022 : Revert experimental and 4.0 features to prepare for 3.13 release #1511274 : Rebalance estimate(ETA) shows wrong details(as intial message of 10min wait reappears) when still in progress #1511293 : In distribute volume after glusterd restart, brick goes offline #1511768 : In Replica volume 2*2 when quorum is set, after glusterd restart nfs server is coming up instead of self-heal daemon #1512435 : Test bug-1483058-replace-brick-quorum-validation.t fails inconsistently #1512460 : disperse eager-lock degrades performance for file create workloads #1513259 : NetBSD port #1514419 : gluster volume splitbrain info needs to display output of each brick in a stream fashion instead of buffering and dumping at the end #1515045 : bug-1247563.t is failing on master #1515572 : Accessing a file when source brick is down results in that FOP being hung #1516313 : Bringing down data bricks in cyclic order results in arbiter brick becoming the source for heal. #1517692 : Memory leak in locks xlator #1518257 : EC DISCARD doesn't punch hole properly #1518512 : Change GD_OP_VERSION to 3_13_0 from 3_12_0 for RFE https://bugzilla.redhat.com/show_bug.cgi?id=1464350 #1518744 : Add release notes about DISCARD on EC volume","title":"Bugs addressed"},{"location":"release-notes/3.13.1/","text":"Release notes for Gluster 3.13.1 This is a bugfix release. The release notes for 3.13.0 , contain a listing of all the new features that were added and bugs fixed in the GlusterFS 3.13 stable release. Major changes, features and limitations addressed in this release No Major changes Major issues Expanding a gluster volume that is sharded may cause file corruption Sharded volumes are typically used for VM images, if such volumes are expanded or possibly contracted (i.e add/remove bricks and rebalance) there are reports of VM images getting corrupted. The last known cause for corruption (Bug #1515434) is still under review. Status of this bug can be tracked here, #1515434 Bugs addressed Bugs addressed since release-3.13.0 are listed below. #1428060 : write-behind: Allow trickling-writes to be configurable, fix usage of page_size and window_size #1520232 : Rebalance fails on NetBSD because fallocate is not implemented #1522710 : Directory listings on fuse mount are very slow due to small number of getdents() entries #1523046 : glusterd consuming high memory #1523456 : Store allocated objects in the mem_acct #1527275 : feature/bitrot: remove internal xattrs from lookup cbk #1527699 : rdma transport may access an obsolete item in gf_rdma_device_t->all_mr, and causes glusterfsd/glusterfs process crash.","title":"3.13.1"},{"location":"release-notes/3.13.1/#release-notes-for-gluster-3131","text":"This is a bugfix release. The release notes for 3.13.0 , contain a listing of all the new features that were added and bugs fixed in the GlusterFS 3.13 stable release.","title":"Release notes for Gluster 3.13.1"},{"location":"release-notes/3.13.1/#major-changes-features-and-limitations-addressed-in-this-release","text":"No Major changes","title":"Major changes, features and limitations addressed in this release"},{"location":"release-notes/3.13.1/#major-issues","text":"Expanding a gluster volume that is sharded may cause file corruption Sharded volumes are typically used for VM images, if such volumes are expanded or possibly contracted (i.e add/remove bricks and rebalance) there are reports of VM images getting corrupted. The last known cause for corruption (Bug #1515434) is still under review. Status of this bug can be tracked here, #1515434","title":"Major issues"},{"location":"release-notes/3.13.1/#bugs-addressed","text":"Bugs addressed since release-3.13.0 are listed below. #1428060 : write-behind: Allow trickling-writes to be configurable, fix usage of page_size and window_size #1520232 : Rebalance fails on NetBSD because fallocate is not implemented #1522710 : Directory listings on fuse mount are very slow due to small number of getdents() entries #1523046 : glusterd consuming high memory #1523456 : Store allocated objects in the mem_acct #1527275 : feature/bitrot: remove internal xattrs from lookup cbk #1527699 : rdma transport may access an obsolete item in gf_rdma_device_t->all_mr, and causes glusterfsd/glusterfs process crash.","title":"Bugs addressed"},{"location":"release-notes/3.13.2/","text":"Release notes for Gluster 3.13.2 This is a bugfix release. The release notes for 3.13.0 and 3.13.1 , contain a listing of all the new features that were added and bugs fixed in the GlusterFS 3.13 stable release. Major changes, features and limitations addressed in this release No Major changes Major issues No Major iissues Bugs addressed Bugs addressed since release-3.13.1 are listed below. #1511293 : In distribute volume after glusterd restart, brick goes offline #1515434 : dht_(f)xattrop does not implement migration checks #1516313 : Bringing down data bricks in cyclic order results in arbiter brick becoming the source for heal. #1529055 : Test case ./tests/bugs/bug-1371806_1.t is failing #1529084 : fstat returns ENOENT/ESTALE #1529094 : /usr/sbin/glusterfs crashing on Red Hat OpenShift Container Platform node #1530449 : glustershd fails to start on a volume force start after a brick is down #1531371 : Use after free in cli_cmd_volume_create_cbk #1533023 : [Disperse] Implement open fd heal for disperse volume #1534842 : entries not getting cleared post healing of softlinks (stale entries showing up in heal info) #1535438 : Take full lock on files in 3 way replication #1536294 : Random GlusterFSD process dies during rebalance","title":"3.13.2"},{"location":"release-notes/3.13.2/#release-notes-for-gluster-3132","text":"This is a bugfix release. The release notes for 3.13.0 and 3.13.1 , contain a listing of all the new features that were added and bugs fixed in the GlusterFS 3.13 stable release.","title":"Release notes for Gluster 3.13.2"},{"location":"release-notes/3.13.2/#major-changes-features-and-limitations-addressed-in-this-release","text":"No Major changes","title":"Major changes, features and limitations addressed in this release"},{"location":"release-notes/3.13.2/#major-issues","text":"No Major iissues","title":"Major issues"},{"location":"release-notes/3.13.2/#bugs-addressed","text":"Bugs addressed since release-3.13.1 are listed below. #1511293 : In distribute volume after glusterd restart, brick goes offline #1515434 : dht_(f)xattrop does not implement migration checks #1516313 : Bringing down data bricks in cyclic order results in arbiter brick becoming the source for heal. #1529055 : Test case ./tests/bugs/bug-1371806_1.t is failing #1529084 : fstat returns ENOENT/ESTALE #1529094 : /usr/sbin/glusterfs crashing on Red Hat OpenShift Container Platform node #1530449 : glustershd fails to start on a volume force start after a brick is down #1531371 : Use after free in cli_cmd_volume_create_cbk #1533023 : [Disperse] Implement open fd heal for disperse volume #1534842 : entries not getting cleared post healing of softlinks (stale entries showing up in heal info) #1535438 : Take full lock on files in 3 way replication #1536294 : Random GlusterFSD process dies during rebalance","title":"Bugs addressed"},{"location":"release-notes/3.5.0/","text":"Major Changes and Features Documentation about major changes and features is also included in the doc/features/ directory of GlusterFS repository. AFR_CLI_enhancements The AFR reporting via CLI has been improved. This feature provides a coherent mechanism to present heal status,information and the logs associated. This makes the end user more aware of healing status and provides statistics. For more information refer here . File_Snapshot This feature provides ability to take snapshots of files in GlusterFS. File snapshot is supported on the files of QCOW2/QED format. This feature adds better integration with Openstack Cinder, and in general ability to take snapshots of files (typically VM images) For more information refer here . gfid-access This feature add a new translator which is designed to provide direct access to files in glusterfs using its GFID For more information refer here . Prevent NFS restart on Volume change Earlier any volume change (volume option, volume start, volume stop, volume delete,brick add, etc) required restarting NFS server. With this feature, it is no longer required to restart NFS server, thereby providing better usability with no disrupts in NFS connections Features/Quota_Scalability This feature provides support upto 65536 quota configurations per volume. readdir_ahead This feature provides read-ahead support for directories to improve sequential directory read performance. zerofill zerofill feature allows creation of pre-allocated and zeroed-out files on GlusterFS volumes by offloading the zeroing part to server and/or storage (storage offloads use SCSI WRITESAME), thereby achieves quick creation of pre-allocated and zeroed-out VM disk image by using server/storage off-loads. For more information refer here . Brick_Failure_Detection This feature attempts to identify storage/file system failures and disable the failed brick without disrupting the rest of the NODE operation. This adds a health-checker that periodically checks the status of the filesystem (implies checking of functional storage-hardware). For more information refer here . Changelog based distributed geo-replication New improved geo-replication which makes use of all the nodes in the master volume. Unlike previous version of geo-replication where all changes were detected and synced on a single node in master volume, now each node of master volume participates in the geo-replication. Change Detection - Now geo-rep makes use of changelog xlator to detect the set of files which needs to be synced. Changelog xlator runs per brick and when enabled, records each fops which modifies the files. geo-rep consumes these journal created by this xlator and syncs the files identified as 'changed' to slave. Distributed nature - Each of the nodes take the repsonsibility of syncing the data which is present in that node. In case of replicated volume, one of them will be 'Active'ly syncing the data, while the other one is 'Passive'. Syncing Method - Apart from the using rsync as the syncing method, now there tar+ssh syncing method, which can be leveraged by the workload where there is large amount of smallfiles. Improved block device translator This feature provides a translator to use logical volumes to store VM images and expose them as files to QEMU/KVM. The Volume group is represented as directory and logical volumes as files. Remove brick CLI Change remove-brick CLI earlier used to remove the brick forcefully ( without data migration ), when called without any arguments. This mode of 'remove-brick' cli, without any arguments has been deprecated. Experimental Features The following features are experimental with this release: RDMA-connection manager (RDMA-CM). support for NUFA translator. disk-encryption On-Wire Compression + Decompression [CDC] Minor Improvements: Old graphs are cleaned up by FUSE clients New command \"volume status tasks\" introduced to track asynchronous tasks like rebalance and remove-brick glfs_readdir(), glfs_readdirplus(), glfs_fallocate(), glfs_discard() APIs support added in libgfapi Per client RPC throttling added in rpc server Communication between cli and glusterd happens over unix domain socket Information on connected NFS clients is persistent across NFS restarts. Hardlink creation failures with SMB addressed Non-local clients function with nufa volumes Configurable option added to mount.glusterfs to use kernel-readdirp with fuse client AUTH support for exported nfs sub-directories added Known Issues: The following configuration changes are necessary for qemu and samba integration with libgfapi to work seamlessly: 1) gluster volume set <volname> server.allow-insecure on 2) Edit /etc/glusterfs/glusterd.vol to contain this line: option rpc-auth-allow-insecure on Post 1), restarting the volume would be necessary. Post 2), restarting glusterd would be necessary. RDMA connection manager needs IPoIB for connection establishment. More details can be found here . For Block Device translator based volumes open-behind translator at the client side needs to be disabled. libgfapi clients calling glfs_fini before a successfull glfs_init will cause the client to hang as reported here . The workaround is NOT to call glfs_fini for error cases encountered before a successfull glfs_init.","title":"3.5.0"},{"location":"release-notes/3.5.0/#major-changes-and-features","text":"Documentation about major changes and features is also included in the doc/features/ directory of GlusterFS repository.","title":"Major Changes and Features"},{"location":"release-notes/3.5.0/#afr_cli_enhancements","text":"The AFR reporting via CLI has been improved. This feature provides a coherent mechanism to present heal status,information and the logs associated. This makes the end user more aware of healing status and provides statistics. For more information refer here .","title":"AFR_CLI_enhancements"},{"location":"release-notes/3.5.0/#file_snapshot","text":"This feature provides ability to take snapshots of files in GlusterFS. File snapshot is supported on the files of QCOW2/QED format. This feature adds better integration with Openstack Cinder, and in general ability to take snapshots of files (typically VM images) For more information refer here .","title":"File_Snapshot"},{"location":"release-notes/3.5.0/#gfid-access","text":"This feature add a new translator which is designed to provide direct access to files in glusterfs using its GFID For more information refer here .","title":"gfid-access"},{"location":"release-notes/3.5.0/#prevent-nfs-restart-on-volume-change","text":"Earlier any volume change (volume option, volume start, volume stop, volume delete,brick add, etc) required restarting NFS server. With this feature, it is no longer required to restart NFS server, thereby providing better usability with no disrupts in NFS connections","title":"Prevent NFS restart on Volume change"},{"location":"release-notes/3.5.0/#featuresquota_scalability","text":"This feature provides support upto 65536 quota configurations per volume.","title":"Features/Quota_Scalability"},{"location":"release-notes/3.5.0/#readdir_ahead","text":"This feature provides read-ahead support for directories to improve sequential directory read performance.","title":"readdir_ahead"},{"location":"release-notes/3.5.0/#zerofill","text":"zerofill feature allows creation of pre-allocated and zeroed-out files on GlusterFS volumes by offloading the zeroing part to server and/or storage (storage offloads use SCSI WRITESAME), thereby achieves quick creation of pre-allocated and zeroed-out VM disk image by using server/storage off-loads. For more information refer here .","title":"zerofill"},{"location":"release-notes/3.5.0/#brick_failure_detection","text":"This feature attempts to identify storage/file system failures and disable the failed brick without disrupting the rest of the NODE operation. This adds a health-checker that periodically checks the status of the filesystem (implies checking of functional storage-hardware). For more information refer here .","title":"Brick_Failure_Detection"},{"location":"release-notes/3.5.0/#changelog-based-distributed-geo-replication","text":"New improved geo-replication which makes use of all the nodes in the master volume. Unlike previous version of geo-replication where all changes were detected and synced on a single node in master volume, now each node of master volume participates in the geo-replication. Change Detection - Now geo-rep makes use of changelog xlator to detect the set of files which needs to be synced. Changelog xlator runs per brick and when enabled, records each fops which modifies the files. geo-rep consumes these journal created by this xlator and syncs the files identified as 'changed' to slave. Distributed nature - Each of the nodes take the repsonsibility of syncing the data which is present in that node. In case of replicated volume, one of them will be 'Active'ly syncing the data, while the other one is 'Passive'. Syncing Method - Apart from the using rsync as the syncing method, now there tar+ssh syncing method, which can be leveraged by the workload where there is large amount of smallfiles.","title":"Changelog based distributed geo-replication"},{"location":"release-notes/3.5.0/#improved-block-device-translator","text":"This feature provides a translator to use logical volumes to store VM images and expose them as files to QEMU/KVM. The Volume group is represented as directory and logical volumes as files.","title":"Improved block device translator"},{"location":"release-notes/3.5.0/#remove-brick-cli-change","text":"remove-brick CLI earlier used to remove the brick forcefully ( without data migration ), when called without any arguments. This mode of 'remove-brick' cli, without any arguments has been deprecated.","title":"Remove brick CLI Change"},{"location":"release-notes/3.5.0/#experimental-features","text":"The following features are experimental with this release: RDMA-connection manager (RDMA-CM). support for NUFA translator. disk-encryption On-Wire Compression + Decompression [CDC]","title":"Experimental Features"},{"location":"release-notes/3.5.0/#minor-improvements","text":"Old graphs are cleaned up by FUSE clients New command \"volume status tasks\" introduced to track asynchronous tasks like rebalance and remove-brick glfs_readdir(), glfs_readdirplus(), glfs_fallocate(), glfs_discard() APIs support added in libgfapi Per client RPC throttling added in rpc server Communication between cli and glusterd happens over unix domain socket Information on connected NFS clients is persistent across NFS restarts. Hardlink creation failures with SMB addressed Non-local clients function with nufa volumes Configurable option added to mount.glusterfs to use kernel-readdirp with fuse client AUTH support for exported nfs sub-directories added","title":"Minor Improvements:"},{"location":"release-notes/3.5.0/#known-issues","text":"The following configuration changes are necessary for qemu and samba integration with libgfapi to work seamlessly: 1) gluster volume set <volname> server.allow-insecure on 2) Edit /etc/glusterfs/glusterd.vol to contain this line: option rpc-auth-allow-insecure on Post 1), restarting the volume would be necessary. Post 2), restarting glusterd would be necessary. RDMA connection manager needs IPoIB for connection establishment. More details can be found here . For Block Device translator based volumes open-behind translator at the client side needs to be disabled. libgfapi clients calling glfs_fini before a successfull glfs_init will cause the client to hang as reported here . The workaround is NOT to call glfs_fini for error cases encountered before a successfull glfs_init.","title":"Known Issues:"},{"location":"release-notes/3.5.1/","text":"Release Notes for GlusterFS 3.5.1 This is mostly a bugfix release. The Release Notes for 3.5.0 contain a listing of all the new features that were added. There are two notable changes that are not only bug fixes, or documentation additions: a new volume option server.manage-gids has been added This option should be used when users of a volume are in more than approximately 93 groups (Bug 1096425 ) Duplicate Request Cache for NFS has now been disabled by default, this may reduce performance for certain workloads, but improves the overall stability and memory footprint for most users Bugs Fixed: 765202 : lgetxattr called with invalid keys on the bricks 833586 : inodelk hang from marker_rename_release_newp_lock 859581 : self-heal process can sometimes create directories instead of symlinks for the root gfid file in .glusterfs 986429 : Backupvolfile server option should work internal to GlusterFS framework 1039544 : [FEAT] \"gluster volume heal info\" should list the entries that actually required to be healed. 1046624 : Unable to heal symbolic Links 1046853 : AFR : For every file self-heal there are warning messages reported in glustershd.log file 1063190 : Volume was not accessible after server side quorum was met 1064096 : The old Python Translator code (not Glupy) should be removed 1066996 : Using sanlock on a gluster mount with replica 3 (quorum-type auto) leads to a split-brain 1071191 : [3.5.1] Sporadic SIGBUS with mmap() on a sparse file created with open(), seek(), write() 1078061 : Need ability to heal mismatching user extended attributes without any changelogs 1078365 : New xlators are linked as versioned .so files, creating .so.0.0.0 1086743 : Add documentation for the Feature: RDMA-connection manager (RDMA-CM) 1086748 : Add documentation for the Feature: AFR CLI enhancements 1086749 : Add documentation for the Feature: Exposing Volume Capabilities 1086750 : Add documentation for the Feature: File Snapshots in GlusterFS 1086751 : Add documentation for the Feature: gfid-access 1086752 : Add documentation for the Feature: On-Wire Compression/Decompression 1086754 : Add documentation for the Feature: Quota Scalability 1086755 : Add documentation for the Feature: readdir-ahead 1086756 : Add documentation for the Feature: zerofill API for GlusterFS 1086758 : Add documentation for the Feature: Changelog based parallel geo-replication 1086760 : Add documentation for the Feature: Write Once Read Many (WORM) volume 1086762 : Add documentation for the Feature: BD Xlator - Block Device translator 1086766 : Add documentation for the Feature: Libgfapi 1086774 : Add documentation for the Feature: Access Control List - Version 3 support for Gluster NFS 1086781 : Add documentation for the Feature: Eager locking 1086782 : Add documentation for the Feature: glusterfs and oVirt integration 1086783 : Add documentation for the Feature: qemu 1.3 - libgfapi integration 1088848 : Spelling errors in rpc/rpc-transport/rdma/src/rdma.c 1089054 : gf-error-codes.h is missing from source tarball 1089470 : SMB: Crash on brick process during compile kernel. 1089934 : list dir with more than N files results in Input/output error 1091340 : Doc: Add glfs_fini known issue to release notes 3.5 1091392 : glusterfs.spec.in: minor/nit changes to sync with Fedora spec 1095256 : Excessive logging from self-heal daemon, and bricks 1095595 : Stick to IANA standard while allocating brick ports 1095775 : Add support in libgfapi to fetch volume info from glusterd. 1095971 : Stopping/Starting a Gluster volume resets ownership 1096040 : AFR : self-heal-daemon not clearing the change-logs of all the sources after self-heal 1096425 : i/o error when one user tries to access RHS volume over NFS with 100+ GIDs 1099878 : Need support for handle based Ops to fetch/modify extended attributes of a file 1101647 : gluster volume heal volname statistics heal-count not giving desired output. 1102306 : license: xlators/features/glupy dual license GPLv2 and LGPLv3+ 1103413 : Failure in gf_log_init reopening stderr 1104592 : heal info may give Success instead of transport end point not connected when a brick is down. 1104915 : glusterfsd crashes while doing stress tests 1104919 : Fix memory leaks in gfid-access xlator. 1104959 : Dist-geo-rep : some of the files not accessible on slave after the geo-rep sync from master to slave. 1105188 : Two instances each, of brick processes, glusterfs-nfs and quotad seen after glusterd restart 1105524 : Disable nfs.drc by default 1107937 : quota-anon-fd-nfs.t fails spuriously 1109832 : I/O fails for for glusterfs 3.4 AFR clients accessing servers upgraded to glusterfs 3.5 1110777 : glusterfsd OOM - using all memory when quota is enabled Known Issues: The following configuration changes are necessary for qemu and samba integration with libgfapi to work seamlessly: gluster volume set server.allow-insecure on restarting the volume is necessary ~~~ gluster volume stop gluster volume start ~~~ Edit /etc/glusterfs/glusterd.vol to contain this line: ~~~ option rpc-auth-allow-insecure on ~~~ restarting glusterd is necessary ~~~ service glusterd restart ~~~ More details are also documented in the Gluster Wiki on the Libgfapi with qemu libvirt page. For Block Device translator based volumes open-behind translator at the client side needs to be disabled. libgfapi clients calling glfs_fini before a successfull glfs_init will cause the client to hang has been reported by QEMU developers . The workaround is NOT to call glfs_fini for error cases encountered before a successfull glfs_init . Follow Bug 1091335 to get informed when a release is made available that contains a final fix. After enabling server.manage-gids , the volume needs to be stopped and started again to have the option enabled in the brick processes gluster volume stop <volname> gluster volume start <volname>","title":"3.5.1"},{"location":"release-notes/3.5.1/#release-notes-for-glusterfs-351","text":"This is mostly a bugfix release. The Release Notes for 3.5.0 contain a listing of all the new features that were added. There are two notable changes that are not only bug fixes, or documentation additions: a new volume option server.manage-gids has been added This option should be used when users of a volume are in more than approximately 93 groups (Bug 1096425 ) Duplicate Request Cache for NFS has now been disabled by default, this may reduce performance for certain workloads, but improves the overall stability and memory footprint for most users","title":"Release Notes for GlusterFS 3.5.1"},{"location":"release-notes/3.5.1/#bugs-fixed","text":"765202 : lgetxattr called with invalid keys on the bricks 833586 : inodelk hang from marker_rename_release_newp_lock 859581 : self-heal process can sometimes create directories instead of symlinks for the root gfid file in .glusterfs 986429 : Backupvolfile server option should work internal to GlusterFS framework 1039544 : [FEAT] \"gluster volume heal info\" should list the entries that actually required to be healed. 1046624 : Unable to heal symbolic Links 1046853 : AFR : For every file self-heal there are warning messages reported in glustershd.log file 1063190 : Volume was not accessible after server side quorum was met 1064096 : The old Python Translator code (not Glupy) should be removed 1066996 : Using sanlock on a gluster mount with replica 3 (quorum-type auto) leads to a split-brain 1071191 : [3.5.1] Sporadic SIGBUS with mmap() on a sparse file created with open(), seek(), write() 1078061 : Need ability to heal mismatching user extended attributes without any changelogs 1078365 : New xlators are linked as versioned .so files, creating .so.0.0.0 1086743 : Add documentation for the Feature: RDMA-connection manager (RDMA-CM) 1086748 : Add documentation for the Feature: AFR CLI enhancements 1086749 : Add documentation for the Feature: Exposing Volume Capabilities 1086750 : Add documentation for the Feature: File Snapshots in GlusterFS 1086751 : Add documentation for the Feature: gfid-access 1086752 : Add documentation for the Feature: On-Wire Compression/Decompression 1086754 : Add documentation for the Feature: Quota Scalability 1086755 : Add documentation for the Feature: readdir-ahead 1086756 : Add documentation for the Feature: zerofill API for GlusterFS 1086758 : Add documentation for the Feature: Changelog based parallel geo-replication 1086760 : Add documentation for the Feature: Write Once Read Many (WORM) volume 1086762 : Add documentation for the Feature: BD Xlator - Block Device translator 1086766 : Add documentation for the Feature: Libgfapi 1086774 : Add documentation for the Feature: Access Control List - Version 3 support for Gluster NFS 1086781 : Add documentation for the Feature: Eager locking 1086782 : Add documentation for the Feature: glusterfs and oVirt integration 1086783 : Add documentation for the Feature: qemu 1.3 - libgfapi integration 1088848 : Spelling errors in rpc/rpc-transport/rdma/src/rdma.c 1089054 : gf-error-codes.h is missing from source tarball 1089470 : SMB: Crash on brick process during compile kernel. 1089934 : list dir with more than N files results in Input/output error 1091340 : Doc: Add glfs_fini known issue to release notes 3.5 1091392 : glusterfs.spec.in: minor/nit changes to sync with Fedora spec 1095256 : Excessive logging from self-heal daemon, and bricks 1095595 : Stick to IANA standard while allocating brick ports 1095775 : Add support in libgfapi to fetch volume info from glusterd. 1095971 : Stopping/Starting a Gluster volume resets ownership 1096040 : AFR : self-heal-daemon not clearing the change-logs of all the sources after self-heal 1096425 : i/o error when one user tries to access RHS volume over NFS with 100+ GIDs 1099878 : Need support for handle based Ops to fetch/modify extended attributes of a file 1101647 : gluster volume heal volname statistics heal-count not giving desired output. 1102306 : license: xlators/features/glupy dual license GPLv2 and LGPLv3+ 1103413 : Failure in gf_log_init reopening stderr 1104592 : heal info may give Success instead of transport end point not connected when a brick is down. 1104915 : glusterfsd crashes while doing stress tests 1104919 : Fix memory leaks in gfid-access xlator. 1104959 : Dist-geo-rep : some of the files not accessible on slave after the geo-rep sync from master to slave. 1105188 : Two instances each, of brick processes, glusterfs-nfs and quotad seen after glusterd restart 1105524 : Disable nfs.drc by default 1107937 : quota-anon-fd-nfs.t fails spuriously 1109832 : I/O fails for for glusterfs 3.4 AFR clients accessing servers upgraded to glusterfs 3.5 1110777 : glusterfsd OOM - using all memory when quota is enabled","title":"Bugs Fixed:"},{"location":"release-notes/3.5.1/#known-issues","text":"The following configuration changes are necessary for qemu and samba integration with libgfapi to work seamlessly: gluster volume set server.allow-insecure on restarting the volume is necessary ~~~ gluster volume stop gluster volume start ~~~ Edit /etc/glusterfs/glusterd.vol to contain this line: ~~~ option rpc-auth-allow-insecure on ~~~ restarting glusterd is necessary ~~~ service glusterd restart ~~~ More details are also documented in the Gluster Wiki on the Libgfapi with qemu libvirt page. For Block Device translator based volumes open-behind translator at the client side needs to be disabled. libgfapi clients calling glfs_fini before a successfull glfs_init will cause the client to hang has been reported by QEMU developers . The workaround is NOT to call glfs_fini for error cases encountered before a successfull glfs_init . Follow Bug 1091335 to get informed when a release is made available that contains a final fix. After enabling server.manage-gids , the volume needs to be stopped and started again to have the option enabled in the brick processes gluster volume stop <volname> gluster volume start <volname>","title":"Known Issues:"},{"location":"release-notes/3.5.2/","text":"Release Notes for GlusterFS 3.5.2 This is mostly a bugfix release. The Release Notes for 3.5.0 and 3.5.1 contain a listing of all the new features that were added and bugs fixed. Bugs Fixed: 1096020 : NFS server crashes in _socket_read_vectored_request 1100050 : Can't write to quota enable folder 1103050 : nfs: reset command does not alter the result for nfs options earlier set 1105891 : features/gfid-access: stat on .gfid virtual directory return EINVAL 1111454 : creating symlinks generates errors on stripe volume 1112111 : Self-heal errors with \"afr crawl failed for child 0 with ret -1\" while performing rolling upgrade. 1112348 : [AFR] I/O fails when one of the replica nodes go down 1112659 : Fix inode leaks in gfid-access xlator 1112980 : NFS subdir authentication doesn't correctly handle multi-(homed,protocol,etc) network addresses 1113007 : nfs-utils should be installed as dependency while installing glusterfs-server 1113403 : Excessive logging in quotad.log of the kind 'null client' 1113749 : client_t clienttable cliententries are never expanded when all entries are used 1113894 : AFR : self-heal of few files not happening when a AWS EC2 Instance is back online after a restart 1113959 : Spec %post server does not wait for the old glusterd to exit 1114501 : Dist-geo-rep : deletion of files on master, geo-rep fails to propagate to slaves. 1115369 : Allow the usage of the wildcard character '*' to the options \"nfs.rpc-auth-allow\" and \"nfs.rpc-auth-reject\" 1115950 : glfsheal: Improve the way in which we check the presence of replica volumes 1116672 : Resource cleanup doesn't happen for clients on servers after disconnect 1116997 : mounting a volume over NFS (TCP) with MOUNT over UDP fails 1117241 : backport 'gluster volume status --xml' issues 1120151 : Glustershd memory usage too high 1124728 : SMB: CIFS mount fails with the latest glusterfs rpm's Known Issues: The following configuration changes are necessary for 'qemu' and 'samba vfs plugin' integration with libgfapi to work seamlessly: gluster volume set server.allow-insecure on restarting the volume is necessary ~~~ gluster volume stop gluster volume start ~~~ Edit /etc/glusterfs/glusterd.vol to contain this line: ~~~ option rpc-auth-allow-insecure on ~~~ restarting glusterd is necessary ~~~ service glusterd restart ~~~ More details are also documented in the Gluster Wiki on the Libgfapi with qemu libvirt page. For Block Device translator based volumes open-behind translator at the client side needs to be disabled. gluster volume set <volname> performance.open-behind disabled libgfapi clients calling glfs_fini before a successfull glfs_init will cause the client to hang as reported here . The workaround is NOT to call glfs_fini for error cases encountered before a successfull glfs_init . If the /var/run/gluster directory does not exist enabling quota will likely fail ( Bug 1117888 ).","title":"3.5.2"},{"location":"release-notes/3.5.2/#release-notes-for-glusterfs-352","text":"This is mostly a bugfix release. The Release Notes for 3.5.0 and 3.5.1 contain a listing of all the new features that were added and bugs fixed.","title":"Release Notes for GlusterFS 3.5.2"},{"location":"release-notes/3.5.2/#bugs-fixed","text":"1096020 : NFS server crashes in _socket_read_vectored_request 1100050 : Can't write to quota enable folder 1103050 : nfs: reset command does not alter the result for nfs options earlier set 1105891 : features/gfid-access: stat on .gfid virtual directory return EINVAL 1111454 : creating symlinks generates errors on stripe volume 1112111 : Self-heal errors with \"afr crawl failed for child 0 with ret -1\" while performing rolling upgrade. 1112348 : [AFR] I/O fails when one of the replica nodes go down 1112659 : Fix inode leaks in gfid-access xlator 1112980 : NFS subdir authentication doesn't correctly handle multi-(homed,protocol,etc) network addresses 1113007 : nfs-utils should be installed as dependency while installing glusterfs-server 1113403 : Excessive logging in quotad.log of the kind 'null client' 1113749 : client_t clienttable cliententries are never expanded when all entries are used 1113894 : AFR : self-heal of few files not happening when a AWS EC2 Instance is back online after a restart 1113959 : Spec %post server does not wait for the old glusterd to exit 1114501 : Dist-geo-rep : deletion of files on master, geo-rep fails to propagate to slaves. 1115369 : Allow the usage of the wildcard character '*' to the options \"nfs.rpc-auth-allow\" and \"nfs.rpc-auth-reject\" 1115950 : glfsheal: Improve the way in which we check the presence of replica volumes 1116672 : Resource cleanup doesn't happen for clients on servers after disconnect 1116997 : mounting a volume over NFS (TCP) with MOUNT over UDP fails 1117241 : backport 'gluster volume status --xml' issues 1120151 : Glustershd memory usage too high 1124728 : SMB: CIFS mount fails with the latest glusterfs rpm's","title":"Bugs Fixed:"},{"location":"release-notes/3.5.2/#known-issues","text":"The following configuration changes are necessary for 'qemu' and 'samba vfs plugin' integration with libgfapi to work seamlessly: gluster volume set server.allow-insecure on restarting the volume is necessary ~~~ gluster volume stop gluster volume start ~~~ Edit /etc/glusterfs/glusterd.vol to contain this line: ~~~ option rpc-auth-allow-insecure on ~~~ restarting glusterd is necessary ~~~ service glusterd restart ~~~ More details are also documented in the Gluster Wiki on the Libgfapi with qemu libvirt page. For Block Device translator based volumes open-behind translator at the client side needs to be disabled. gluster volume set <volname> performance.open-behind disabled libgfapi clients calling glfs_fini before a successfull glfs_init will cause the client to hang as reported here . The workaround is NOT to call glfs_fini for error cases encountered before a successfull glfs_init . If the /var/run/gluster directory does not exist enabling quota will likely fail ( Bug 1117888 ).","title":"Known Issues:"},{"location":"release-notes/3.5.3/","text":"Release Notes for GlusterFS 3.5.3 This is a bugfix release. The Release Notes for 3.5.0 , 3.5.1 and 3.5.2 contain a listing of all the new features that were added and bugs fixed in the GlusterFS 3.5 stable release. Bugs Fixed: 1081016 : glusterd needs xfsprogs and e2fsprogs packages 1100204 : brick failure detection does not work for ext4 filesystems 1126801 : glusterfs logrotate config file pollutes global config 1129527 : DHT :- data loss - file is missing on renaming same file from multiple client at same time 1129541 : [DHT:REBALANCE]: Rebalance failures are seen with error message \" remote operation failed: File exists\" 1132391 : NFS interoperability problem: stripe-xlator removes EOF at end of READDIR 1133949 : Minor typo in afr logging 1136221 : The memories are exhausted quickly when handle the message which has multi fragments in a single record 1136835 : crash on fsync 1138922 : DHT + rebalance : rebalance process crashed + data loss + few Directories are present on sub-volumes but not visible on mount point + lookup is not healing directories 1139103 : DHT + Snapshot :- If snapshot is taken when Directory is created only on hashed sub-vol; On restoring that snapshot Directory is not listed on mount point and lookup on parent is not healing 1139170 : DHT :- rm -rf is not removing stale link file and because of that unable to create file having same name as stale link file 1139245 : vdsm invoked oom-killer during rebalance and Killed process 4305, UID 0, (glusterfs nfs process) 1140338 : rebalance is not resulting in the hash layout changes being available to nfs client 1140348 : Renaming file while rebalance is in progress causes data loss 1140549 : DHT: Rebalance process crash after add-brick and `rebalance start' operation 1140556 : Core: client crash while doing rename operations on the mount 1141558 : AFR : \"gluster volume heal info\" prints some random characters 1141733 : data loss when rebalance + renames are in progress and bricks from replica pairs goes down and comes back 1142052 : Very high memory usage during rebalance 1142614 : files with open fd's getting into split-brain when bricks goes offline and comes back online 1144315 : core: all brick processes crash when quota is enabled 1145000 : Spec %post server does not wait for the old glusterd to exit 1147156 : AFR client segmentation fault in afr_priv_destroy 1147243 : nfs: volume set help says the rmtab file is in \"/var/lib/glusterd/rmtab\" 1149857 : Option transport.socket.bind-address ignored 1153626 : Sizeof bug for allocation of memory in afr_lookup 1153629 : AFR : excessive logging of \"Non blocking entrylks failed\" in glfsheal log file. 1153900 : Enabling Quota on existing data won't create pgfid xattrs 1153904 : self heal info logs are filled with messages reporting ENOENT while self-heal is going on 1155073 : Excessive logging in the self-heal daemon after a replace-brick 1157661 : GlusterFS allows insecure SSL modes Known Issues: The following configuration changes are necessary for 'qemu' and 'samba vfs plugin' integration with libgfapi to work seamlessly: gluster volume set server.allow-insecure on restarting the volume is necessary ~~~ gluster volume stop gluster volume start ~~~ Edit /etc/glusterfs/glusterd.vol to contain this line: ~~~ option rpc-auth-allow-insecure on ~~~ restarting glusterd is necessary ~~~ service glusterd restart ~~~ More details are also documented in the Gluster Wiki on the Libgfapi with qemu libvirt page. For Block Device translator based volumes open-behind translator at the client side needs to be disabled. gluster volume set <volname> performance.open-behind disabled libgfapi clients calling glfs_fini before a successful glfs_init will cause the client to hang as reported here . The workaround is NOT to call glfs_fini for error cases encountered before a successful glfs_init . This is being tracked in Bug 1134050 for glusterfs-3.5 and Bug 1093594 for mainline. If the /var/run/gluster directory does not exist enabling quota will likely fail ( Bug 1117888 ).","title":"3.5.3"},{"location":"release-notes/3.5.3/#release-notes-for-glusterfs-353","text":"This is a bugfix release. The Release Notes for 3.5.0 , 3.5.1 and 3.5.2 contain a listing of all the new features that were added and bugs fixed in the GlusterFS 3.5 stable release.","title":"Release Notes for GlusterFS 3.5.3"},{"location":"release-notes/3.5.3/#bugs-fixed","text":"1081016 : glusterd needs xfsprogs and e2fsprogs packages 1100204 : brick failure detection does not work for ext4 filesystems 1126801 : glusterfs logrotate config file pollutes global config 1129527 : DHT :- data loss - file is missing on renaming same file from multiple client at same time 1129541 : [DHT:REBALANCE]: Rebalance failures are seen with error message \" remote operation failed: File exists\" 1132391 : NFS interoperability problem: stripe-xlator removes EOF at end of READDIR 1133949 : Minor typo in afr logging 1136221 : The memories are exhausted quickly when handle the message which has multi fragments in a single record 1136835 : crash on fsync 1138922 : DHT + rebalance : rebalance process crashed + data loss + few Directories are present on sub-volumes but not visible on mount point + lookup is not healing directories 1139103 : DHT + Snapshot :- If snapshot is taken when Directory is created only on hashed sub-vol; On restoring that snapshot Directory is not listed on mount point and lookup on parent is not healing 1139170 : DHT :- rm -rf is not removing stale link file and because of that unable to create file having same name as stale link file 1139245 : vdsm invoked oom-killer during rebalance and Killed process 4305, UID 0, (glusterfs nfs process) 1140338 : rebalance is not resulting in the hash layout changes being available to nfs client 1140348 : Renaming file while rebalance is in progress causes data loss 1140549 : DHT: Rebalance process crash after add-brick and `rebalance start' operation 1140556 : Core: client crash while doing rename operations on the mount 1141558 : AFR : \"gluster volume heal info\" prints some random characters 1141733 : data loss when rebalance + renames are in progress and bricks from replica pairs goes down and comes back 1142052 : Very high memory usage during rebalance 1142614 : files with open fd's getting into split-brain when bricks goes offline and comes back online 1144315 : core: all brick processes crash when quota is enabled 1145000 : Spec %post server does not wait for the old glusterd to exit 1147156 : AFR client segmentation fault in afr_priv_destroy 1147243 : nfs: volume set help says the rmtab file is in \"/var/lib/glusterd/rmtab\" 1149857 : Option transport.socket.bind-address ignored 1153626 : Sizeof bug for allocation of memory in afr_lookup 1153629 : AFR : excessive logging of \"Non blocking entrylks failed\" in glfsheal log file. 1153900 : Enabling Quota on existing data won't create pgfid xattrs 1153904 : self heal info logs are filled with messages reporting ENOENT while self-heal is going on 1155073 : Excessive logging in the self-heal daemon after a replace-brick 1157661 : GlusterFS allows insecure SSL modes","title":"Bugs Fixed:"},{"location":"release-notes/3.5.3/#known-issues","text":"The following configuration changes are necessary for 'qemu' and 'samba vfs plugin' integration with libgfapi to work seamlessly: gluster volume set server.allow-insecure on restarting the volume is necessary ~~~ gluster volume stop gluster volume start ~~~ Edit /etc/glusterfs/glusterd.vol to contain this line: ~~~ option rpc-auth-allow-insecure on ~~~ restarting glusterd is necessary ~~~ service glusterd restart ~~~ More details are also documented in the Gluster Wiki on the Libgfapi with qemu libvirt page. For Block Device translator based volumes open-behind translator at the client side needs to be disabled. gluster volume set <volname> performance.open-behind disabled libgfapi clients calling glfs_fini before a successful glfs_init will cause the client to hang as reported here . The workaround is NOT to call glfs_fini for error cases encountered before a successful glfs_init . This is being tracked in Bug 1134050 for glusterfs-3.5 and Bug 1093594 for mainline. If the /var/run/gluster directory does not exist enabling quota will likely fail ( Bug 1117888 ).","title":"Known Issues:"},{"location":"release-notes/3.5.4/","text":"Release Notes for GlusterFS 3.5.4 This is a bugfix release. The Release Notes for 3.5.0 , 3.5.1 , 3.5.2 and 3.5.3 contain a listing of all the new features that were added and bugs fixed in the GlusterFS 3.5 stable release. Bugs Fixed: 1092037 : Issues reported by Cppcheck static analysis tool 1101138 : meta-data split-brain prevents entry/data self-heal of dir/file respectively 1115197 : Directory quota does not apply on it's sub-directories 1159968 : glusterfs.spec.in: deprecate *.logrotate files in dist-git in favor of the upstream logrotate files 1160711 : libgfapi: use versioned symbols in libgfapi.so for compatibility 1161102 : self heal info logs are filled up with messages reporting split-brain 1162150 : AFR gives EROFS when fop fails on all subvolumes when client-quorum is enabled 1162226 : bulk remove xattr should not fail if removexattr fails with ENOATTR/ENODATA 1162230 : quota xattrs are exposed in lookup and getxattr 1162767 : DHT: Rebalance- Rebalance process crash after remove-brick 1166275 : Directory fd leaks in index translator 1168173 : Regression tests fail in quota-anon-fs-nfs.t 1173515 : [HC] - mount.glusterfs fails to check return of mount command. 1174250 : Glusterfs outputs a lot of warnings and errors when quota is enabled 1177339 : entry self-heal in 3.5 and 3.6 are not compatible 1177928 : Directories not visible anymore after add-brick, new brick dirs not part of old bricks 1184528 : Some newly created folders have root ownership although created by unprivileged user 1186121 : tar on a gluster directory gives message \"file changed as we read it\" even though no updates to file in progress 1190633 : self-heal-algorithm with option \"full\" doesn't heal sparse files correctly 1191006 : Building argp-standalone breaks nightly builds on Fedora Rawhide 1192832 : log files get flooded when removexattr() can't find a specified key or value 1200764 : [AFR] Core dump and crash observed during disk replacement case 1202675 : Perf: readdirp in replicated volumes causes performance degrade 1211841 : glusterfs-api.pc versioning breaks QEMU 1222150 : readdirp return 64bits inodes even if enable-ino32 is set Known Issues: The following configuration changes are necessary for 'qemu' and 'samba vfs plugin' integration with libgfapi to work seamlessly: gluster volume set server.allow-insecure on restarting the volume is necessary ~~~ gluster volume stop gluster volume start ~~~ Edit /etc/glusterfs/glusterd.vol to contain this line: ~~~ option rpc-auth-allow-insecure on ~~~ restarting glusterd is necessary ~~~ service glusterd restart ~~~ More details are also documented in the Gluster Wiki on the Libgfapi with qemu libvirt page. For Block Device translator based volumes open-behind translator at the client side needs to be disabled. gluster volume set <volname> performance.open-behind disabled libgfapi clients calling glfs_fini before a successful glfs_init will cause the client to hang as reported here . The workaround is NOT to call glfs_fini for error cases encountered before a successful glfs_init . This is being tracked in Bug 1134050 for glusterfs-3.5 and Bug 1093594 for mainline. If the /var/run/gluster directory does not exist enabling quota will likely fail ( Bug 1117888 ).","title":"3.5.4"},{"location":"release-notes/3.5.4/#release-notes-for-glusterfs-354","text":"This is a bugfix release. The Release Notes for 3.5.0 , 3.5.1 , 3.5.2 and 3.5.3 contain a listing of all the new features that were added and bugs fixed in the GlusterFS 3.5 stable release.","title":"Release Notes for GlusterFS 3.5.4"},{"location":"release-notes/3.5.4/#bugs-fixed","text":"1092037 : Issues reported by Cppcheck static analysis tool 1101138 : meta-data split-brain prevents entry/data self-heal of dir/file respectively 1115197 : Directory quota does not apply on it's sub-directories 1159968 : glusterfs.spec.in: deprecate *.logrotate files in dist-git in favor of the upstream logrotate files 1160711 : libgfapi: use versioned symbols in libgfapi.so for compatibility 1161102 : self heal info logs are filled up with messages reporting split-brain 1162150 : AFR gives EROFS when fop fails on all subvolumes when client-quorum is enabled 1162226 : bulk remove xattr should not fail if removexattr fails with ENOATTR/ENODATA 1162230 : quota xattrs are exposed in lookup and getxattr 1162767 : DHT: Rebalance- Rebalance process crash after remove-brick 1166275 : Directory fd leaks in index translator 1168173 : Regression tests fail in quota-anon-fs-nfs.t 1173515 : [HC] - mount.glusterfs fails to check return of mount command. 1174250 : Glusterfs outputs a lot of warnings and errors when quota is enabled 1177339 : entry self-heal in 3.5 and 3.6 are not compatible 1177928 : Directories not visible anymore after add-brick, new brick dirs not part of old bricks 1184528 : Some newly created folders have root ownership although created by unprivileged user 1186121 : tar on a gluster directory gives message \"file changed as we read it\" even though no updates to file in progress 1190633 : self-heal-algorithm with option \"full\" doesn't heal sparse files correctly 1191006 : Building argp-standalone breaks nightly builds on Fedora Rawhide 1192832 : log files get flooded when removexattr() can't find a specified key or value 1200764 : [AFR] Core dump and crash observed during disk replacement case 1202675 : Perf: readdirp in replicated volumes causes performance degrade 1211841 : glusterfs-api.pc versioning breaks QEMU 1222150 : readdirp return 64bits inodes even if enable-ino32 is set","title":"Bugs Fixed:"},{"location":"release-notes/3.5.4/#known-issues","text":"The following configuration changes are necessary for 'qemu' and 'samba vfs plugin' integration with libgfapi to work seamlessly: gluster volume set server.allow-insecure on restarting the volume is necessary ~~~ gluster volume stop gluster volume start ~~~ Edit /etc/glusterfs/glusterd.vol to contain this line: ~~~ option rpc-auth-allow-insecure on ~~~ restarting glusterd is necessary ~~~ service glusterd restart ~~~ More details are also documented in the Gluster Wiki on the Libgfapi with qemu libvirt page. For Block Device translator based volumes open-behind translator at the client side needs to be disabled. gluster volume set <volname> performance.open-behind disabled libgfapi clients calling glfs_fini before a successful glfs_init will cause the client to hang as reported here . The workaround is NOT to call glfs_fini for error cases encountered before a successful glfs_init . This is being tracked in Bug 1134050 for glusterfs-3.5 and Bug 1093594 for mainline. If the /var/run/gluster directory does not exist enabling quota will likely fail ( Bug 1117888 ).","title":"Known Issues:"},{"location":"release-notes/3.6.0/","text":"Major Changes and Features Documentation about major changes and features is also included in the doc/features/ directory of GlusterFS repository. Volume Snapshot Volume snapshot provides a point-in-time copy of a GlusterFS volume. The snapshot is an online operation and hence filesystem data continues to be available for the clients while the snapshot is being taken. For more information refer here . User Serviceable Snapshots User Serviceable Snapshots provides the ability for users to access snapshots of GlusterFS volumes without administrative intervention. For more information refer here . Erasure Coding The new disperse translator provides the ability to perform erasure coding across nodes. For more information refer here . Granular locking support for management operations Glusterd now holds a volume lock to support parallel management operations on different volumes. Journaling enhancements (changelog xlator) Introduction of history API to consume journal records which were persisted by the changelog translator. With this API, it's not longer required to perform an expensive filesystem crawl to identify changes. Geo-replication makes use of this (on [re]start) thereby optimizing remote replication for purges, hardlinks, etc. Better Support for bricks with heterogeneous sizes Prior to 3.6, bricks with heterogeneous sizes were treated as equal regardless of size, and would have been assigned an equal share of files. From 3.6, assignment of files to bricks will take into account the sizes of the bricks. Improved SSL support GlusterFS 3.6 provides better support to enable SSL on both management and data connections. This feature is currently being consumed by the GlusterFS native driver in OpenStack Manila. Better peer identification GlusterFS 3.6 improves peer identification. GlusterD will no longer complain when a mixture of FQDNs, shortnames and IP addresses are used. Changes done for this improvement have also laid down a base for improving multi network support in GlusterFS. Meta translator Meta translator provides a virtual interface for viewing internal state of translators. Improved synchronous replication support (AFRv2) The replication translator (AFR) in GlusterFS 3.6 has undergone a complete rewrite (http://review.gluster.org/#/c/6010/) and is referred to as AFRv2. From a user point of view, there is no change in the replication behaviour but there are some caveats to be noted from an admin point of view: Lookups do not trigger meta-data and data self-heals anymore. They only trigger entry-self-heals. Data and meta-data are healed by the self-heal daemon only. Bricks in a replica set do not mark any pending change log extended attributes for itself during pre or post op. They only mark it for other bricks in the replica set. For e.g.: In a replica 2 volume, trusted.afr.<volname>-client-0 for brick-0 and trusted.afr.<volname>-client-1 for brick-1 will always be 0x000000000000000000000000 . If the post-op changelog updation does not complete successfully on a brick, a trusted.afr.dirty extended attribute is set on that brick. Barrier translator The barrier translator allows file operations to be temporarily 'paused' on GlusterFS bricks, which is needed for performing consistent snapshots of a GlusterFS volume. For more information, see here . Remove brick moves data by default Prior to 3.6, volume remove-brick <volname> CLI would remove the brick from the volume without performing any data migration. Now the default behavior has been changed to perform data migration when this command is issued. Removing a brick without data migration can now be performed through volume remove-brick <volname> force interface. Experimental Features The following features are experimental with this release: support for rdma volumes. support for NUFA translator. disk-encryption On-Wire Compression + Decompression [CDC] Porting Status NetBSD and FreeBSD support is experimental, but regressions tests suggest that it is close to be fully supported. Please make sure you use latest NetBSD code from -current or netbsd-7 branches. OSX support is in an alpha state. More testing will help in maturing this support. Minor Improvements: Introduction of server.anonuid and server.anongid options for root squashing Root squashing doesn't happen for clients in trusted storage pool Memory accounting of glusterfs processes has been enabled by default The Gluster/NFS server now has support for setting access permissions on volumes with wildcard IP-addresses and IP-address/subnet (CIDR notation). More details and examples are in the commit message . More preparation for better integration with the nfs-ganesha user-space NFS-server. The changes are mostly related to the handle-based functions in libgfapi.so . A new logging framework that can suppress repetitive log messages and provide a dictionary of messages has been added. Few translators have now been integrated with the framework. More translators are expected to integrate with this framework in upcoming minor & major releases. Known Issues: The following configuration changes are necessary for qemu and samba integration with libgfapi to work seamlessly: gluster volume set <volname> server.allow-insecure on Edit /etc/glusterfs/glusterd.vol to contain this line: option rpc-auth-allow-insecure on Post 1, restarting the volume would be necessary: # gluster volume stop <volname> # gluster volume start <volname> Post 2, restarting glusterd would be necessary: # service glusterd restart For Block Device translator based volumes open-behind translator at the client side needs to be disabled. Renames happening on a file that is being migrated during rebalance will fail. Dispersed volumes do not work with self-heal daemon. Self-healing is only activated when a damaged file or directory is accessed. To force a full self-heal or to replace a brick requires to traverse the file system from a mount point. This is the recommended command to do so: find <mount> -d -exec getfattr -h -n test {} \\; Quota on dispersed volumes is not correctly computed, allowing to store more data than specified. A workaround to this problem is to define a smaller quota based on this formula: Q' = Q / (N - R) Where Q is the desired quota value, Q' is the new quota value to use, N is the number of bricks per disperse set, and R is the redundancy. Upgrading to 3.6.X Before upgrading to 3.6 version of gluster from 3.4.X or 3.5.x, please take a look at following link: Upgrade Gluster to 3.6","title":"3.6.0"},{"location":"release-notes/3.6.0/#major-changes-and-features","text":"Documentation about major changes and features is also included in the doc/features/ directory of GlusterFS repository.","title":"Major Changes and Features"},{"location":"release-notes/3.6.0/#volume-snapshot","text":"Volume snapshot provides a point-in-time copy of a GlusterFS volume. The snapshot is an online operation and hence filesystem data continues to be available for the clients while the snapshot is being taken. For more information refer here .","title":"Volume Snapshot"},{"location":"release-notes/3.6.0/#user-serviceable-snapshots","text":"User Serviceable Snapshots provides the ability for users to access snapshots of GlusterFS volumes without administrative intervention. For more information refer here .","title":"User Serviceable Snapshots"},{"location":"release-notes/3.6.0/#erasure-coding","text":"The new disperse translator provides the ability to perform erasure coding across nodes. For more information refer here .","title":"Erasure Coding"},{"location":"release-notes/3.6.0/#granular-locking-support-for-management-operations","text":"Glusterd now holds a volume lock to support parallel management operations on different volumes.","title":"Granular locking support for management operations"},{"location":"release-notes/3.6.0/#journaling-enhancements-changelog-xlator","text":"Introduction of history API to consume journal records which were persisted by the changelog translator. With this API, it's not longer required to perform an expensive filesystem crawl to identify changes. Geo-replication makes use of this (on [re]start) thereby optimizing remote replication for purges, hardlinks, etc.","title":"Journaling enhancements (changelog xlator)"},{"location":"release-notes/3.6.0/#better-support-for-bricks-with-heterogeneous-sizes","text":"Prior to 3.6, bricks with heterogeneous sizes were treated as equal regardless of size, and would have been assigned an equal share of files. From 3.6, assignment of files to bricks will take into account the sizes of the bricks.","title":"Better Support for bricks with heterogeneous sizes"},{"location":"release-notes/3.6.0/#improved-ssl-support","text":"GlusterFS 3.6 provides better support to enable SSL on both management and data connections. This feature is currently being consumed by the GlusterFS native driver in OpenStack Manila.","title":"Improved SSL support"},{"location":"release-notes/3.6.0/#better-peer-identification","text":"GlusterFS 3.6 improves peer identification. GlusterD will no longer complain when a mixture of FQDNs, shortnames and IP addresses are used. Changes done for this improvement have also laid down a base for improving multi network support in GlusterFS.","title":"Better peer identification"},{"location":"release-notes/3.6.0/#meta-translator","text":"Meta translator provides a virtual interface for viewing internal state of translators.","title":"Meta translator"},{"location":"release-notes/3.6.0/#improved-synchronous-replication-support-afrv2","text":"The replication translator (AFR) in GlusterFS 3.6 has undergone a complete rewrite (http://review.gluster.org/#/c/6010/) and is referred to as AFRv2. From a user point of view, there is no change in the replication behaviour but there are some caveats to be noted from an admin point of view: Lookups do not trigger meta-data and data self-heals anymore. They only trigger entry-self-heals. Data and meta-data are healed by the self-heal daemon only. Bricks in a replica set do not mark any pending change log extended attributes for itself during pre or post op. They only mark it for other bricks in the replica set. For e.g.: In a replica 2 volume, trusted.afr.<volname>-client-0 for brick-0 and trusted.afr.<volname>-client-1 for brick-1 will always be 0x000000000000000000000000 . If the post-op changelog updation does not complete successfully on a brick, a trusted.afr.dirty extended attribute is set on that brick.","title":"Improved synchronous replication support (AFRv2)"},{"location":"release-notes/3.6.0/#barrier-translator","text":"The barrier translator allows file operations to be temporarily 'paused' on GlusterFS bricks, which is needed for performing consistent snapshots of a GlusterFS volume. For more information, see here .","title":"Barrier translator"},{"location":"release-notes/3.6.0/#remove-brick-moves-data-by-default","text":"Prior to 3.6, volume remove-brick <volname> CLI would remove the brick from the volume without performing any data migration. Now the default behavior has been changed to perform data migration when this command is issued. Removing a brick without data migration can now be performed through volume remove-brick <volname> force interface.","title":"Remove brick moves data by default"},{"location":"release-notes/3.6.0/#experimental-features","text":"The following features are experimental with this release: support for rdma volumes. support for NUFA translator. disk-encryption On-Wire Compression + Decompression [CDC]","title":"Experimental Features"},{"location":"release-notes/3.6.0/#porting-status","text":"NetBSD and FreeBSD support is experimental, but regressions tests suggest that it is close to be fully supported. Please make sure you use latest NetBSD code from -current or netbsd-7 branches. OSX support is in an alpha state. More testing will help in maturing this support.","title":"Porting Status"},{"location":"release-notes/3.6.0/#minor-improvements","text":"Introduction of server.anonuid and server.anongid options for root squashing Root squashing doesn't happen for clients in trusted storage pool Memory accounting of glusterfs processes has been enabled by default The Gluster/NFS server now has support for setting access permissions on volumes with wildcard IP-addresses and IP-address/subnet (CIDR notation). More details and examples are in the commit message . More preparation for better integration with the nfs-ganesha user-space NFS-server. The changes are mostly related to the handle-based functions in libgfapi.so . A new logging framework that can suppress repetitive log messages and provide a dictionary of messages has been added. Few translators have now been integrated with the framework. More translators are expected to integrate with this framework in upcoming minor & major releases.","title":"Minor Improvements:"},{"location":"release-notes/3.6.0/#known-issues","text":"The following configuration changes are necessary for qemu and samba integration with libgfapi to work seamlessly: gluster volume set <volname> server.allow-insecure on Edit /etc/glusterfs/glusterd.vol to contain this line: option rpc-auth-allow-insecure on Post 1, restarting the volume would be necessary: # gluster volume stop <volname> # gluster volume start <volname> Post 2, restarting glusterd would be necessary: # service glusterd restart For Block Device translator based volumes open-behind translator at the client side needs to be disabled. Renames happening on a file that is being migrated during rebalance will fail. Dispersed volumes do not work with self-heal daemon. Self-healing is only activated when a damaged file or directory is accessed. To force a full self-heal or to replace a brick requires to traverse the file system from a mount point. This is the recommended command to do so: find <mount> -d -exec getfattr -h -n test {} \\; Quota on dispersed volumes is not correctly computed, allowing to store more data than specified. A workaround to this problem is to define a smaller quota based on this formula: Q' = Q / (N - R) Where Q is the desired quota value, Q' is the new quota value to use, N is the number of bricks per disperse set, and R is the redundancy.","title":"Known Issues:"},{"location":"release-notes/3.6.0/#upgrading-to-36x","text":"Before upgrading to 3.6 version of gluster from 3.4.X or 3.5.x, please take a look at following link: Upgrade Gluster to 3.6","title":"Upgrading to 3.6.X"},{"location":"release-notes/3.6.3/","text":"Release Notes for GlusterFS 3.6.3 This is a bugfix release. The Release Notes for 3.6.0 contain a listing of all the new features that were added and bugs fixed in the GlusterFS 3.6 stable release. Bugs Fixed: 1187526 : Disperse volume mounted through NFS doesn't list any files/directories 1188471 : When the volume is in stopped state/all the bricks are down mount of the volume hangs 1201484 : glusterfs-3.6.2 fails to build on Ubuntu Precise: 'RDMA_OPTION_ID_REUSEADDR' undeclared 1202212 : Performance enhancement for RDMA 1189023 : Directories not visible anymore after add-brick, new brick dirs not part of old bricks 1202673 : Perf: readdirp in replicated volumes causes performance degrade 1203081 : Entries in indices/xattrop directory not removed appropriately 1203648 : Quota: Build ancestry in the lookup 1199936 : readv on /var/run/6b8f1f2526c6af8a87f1bb611ae5a86f.socket failed when NFS is disabled 1200297 : cli crashes when listing quota limits with xml output 1201622 : Convert quota size from n-to-h order before using it 1194141 : AFR : failure in self-heald.t 1201624 : Spurious failure of tests/bugs/quota/bug-1038598.t 1194306 : Do not count files which did not need index heal in the first place as successfully healed 1200258 : Quota: features.quota-deem-statfs is \"on\" even after disabling quota. 1165938 : Fix regression test spurious failures 1197598 : NFS logs are filled with system.posix_acl_access messages 1199577 : mount.glusterfs uses /dev/stderr and fails if the device does not exist 1197598 : NFS logs are filled with system.posix_acl_access messages 1188066 : logging improvements in marker translator 1191537 : With afrv2 + ext4, lookups on directories with large offsets could result in duplicate/missing entries 1165129 : libgfapi: use versioned symbols in libgfapi.so for compatibility 1179136 : glusterd: Gluster rebalance status returns failure 1176756 : glusterd: remote locking failure when multiple synctask transactions are run 1188064 : log files get flooded when removexattr() can't find a specified key or value 1165938 : Fix regression test spurious failures 1192522 : index heal doesn't continue crawl on self-heal failure 1193970 : Fix spurious ssl-authz.t regression failure (backport) 1138897 : NetBSD port 1184527 : Some newly created folders have root ownership although created by unprivileged user 1181977 : gluster vol clear-locks vol-name path kind all inode return IO error in a disperse volume 1159471 : rename operation leads to core dump 1173528 : Change in volume heal info command output 1186119 : tar on a gluster directory gives message \"file changed as we read it\" even though no updates to file in progress 1183716 : Force replace-brick lead to the persistent write(use dd) return Input/output error 1138897 : NetBSD port 1178590 : Enable quota(default) leads to heal directory's xattr failed. 1182490 : Internal ec xattrs are allowed to be modified 1187547 : self-heal-algorithm with option \"full\" doesn't heal sparse files correctly 1174170 : Glusterfs outputs a lot of warnings and errors when quota is enabled 1212684 : - GlusterD segfaults when started with management SSL Known Issues: The following configuration changes are necessary for 'qemu' and 'samba vfs plugin' integration with libgfapi to work seamlessly: gluster volume set server.allow-insecure on restarting the volume is necessary ~~~ gluster volume stop gluster volume start ~~~ Edit /etc/glusterfs/glusterd.vol to contain this line: ~~~ option rpc-auth-allow-insecure on ~~~ restarting glusterd is necessary ~~~ service glusterd restart ~~~ More details are also documented in the Gluster Wiki on the Libgfapi with qemu libvirt page. For Block Device translator based volumes open-behind translator at the client side needs to be disabled. gluster volume set <volname> performance.open-behind disable","title":"3.6.3"},{"location":"release-notes/3.6.3/#release-notes-for-glusterfs-363","text":"This is a bugfix release. The Release Notes for 3.6.0 contain a listing of all the new features that were added and bugs fixed in the GlusterFS 3.6 stable release.","title":"Release Notes for GlusterFS 3.6.3"},{"location":"release-notes/3.6.3/#bugs-fixed","text":"1187526 : Disperse volume mounted through NFS doesn't list any files/directories 1188471 : When the volume is in stopped state/all the bricks are down mount of the volume hangs 1201484 : glusterfs-3.6.2 fails to build on Ubuntu Precise: 'RDMA_OPTION_ID_REUSEADDR' undeclared 1202212 : Performance enhancement for RDMA 1189023 : Directories not visible anymore after add-brick, new brick dirs not part of old bricks 1202673 : Perf: readdirp in replicated volumes causes performance degrade 1203081 : Entries in indices/xattrop directory not removed appropriately 1203648 : Quota: Build ancestry in the lookup 1199936 : readv on /var/run/6b8f1f2526c6af8a87f1bb611ae5a86f.socket failed when NFS is disabled 1200297 : cli crashes when listing quota limits with xml output 1201622 : Convert quota size from n-to-h order before using it 1194141 : AFR : failure in self-heald.t 1201624 : Spurious failure of tests/bugs/quota/bug-1038598.t 1194306 : Do not count files which did not need index heal in the first place as successfully healed 1200258 : Quota: features.quota-deem-statfs is \"on\" even after disabling quota. 1165938 : Fix regression test spurious failures 1197598 : NFS logs are filled with system.posix_acl_access messages 1199577 : mount.glusterfs uses /dev/stderr and fails if the device does not exist 1197598 : NFS logs are filled with system.posix_acl_access messages 1188066 : logging improvements in marker translator 1191537 : With afrv2 + ext4, lookups on directories with large offsets could result in duplicate/missing entries 1165129 : libgfapi: use versioned symbols in libgfapi.so for compatibility 1179136 : glusterd: Gluster rebalance status returns failure 1176756 : glusterd: remote locking failure when multiple synctask transactions are run 1188064 : log files get flooded when removexattr() can't find a specified key or value 1165938 : Fix regression test spurious failures 1192522 : index heal doesn't continue crawl on self-heal failure 1193970 : Fix spurious ssl-authz.t regression failure (backport) 1138897 : NetBSD port 1184527 : Some newly created folders have root ownership although created by unprivileged user 1181977 : gluster vol clear-locks vol-name path kind all inode return IO error in a disperse volume 1159471 : rename operation leads to core dump 1173528 : Change in volume heal info command output 1186119 : tar on a gluster directory gives message \"file changed as we read it\" even though no updates to file in progress 1183716 : Force replace-brick lead to the persistent write(use dd) return Input/output error 1138897 : NetBSD port 1178590 : Enable quota(default) leads to heal directory's xattr failed. 1182490 : Internal ec xattrs are allowed to be modified 1187547 : self-heal-algorithm with option \"full\" doesn't heal sparse files correctly 1174170 : Glusterfs outputs a lot of warnings and errors when quota is enabled 1212684 : - GlusterD segfaults when started with management SSL","title":"Bugs Fixed:"},{"location":"release-notes/3.6.3/#known-issues","text":"The following configuration changes are necessary for 'qemu' and 'samba vfs plugin' integration with libgfapi to work seamlessly: gluster volume set server.allow-insecure on restarting the volume is necessary ~~~ gluster volume stop gluster volume start ~~~ Edit /etc/glusterfs/glusterd.vol to contain this line: ~~~ option rpc-auth-allow-insecure on ~~~ restarting glusterd is necessary ~~~ service glusterd restart ~~~ More details are also documented in the Gluster Wiki on the Libgfapi with qemu libvirt page. For Block Device translator based volumes open-behind translator at the client side needs to be disabled. gluster volume set <volname> performance.open-behind disable","title":"Known Issues:"},{"location":"release-notes/3.7.0/","text":"Release Notes for GlusterFS 3.7.0 Major Changes and Features Documentation about major changes and features is included in the doc/features/ directory of GlusterFS repository. Geo Replication Many improvements have gone in the geo replication. A detailed documentation about all the improvements can be found here Bitrot Detection Bitrot detection is a technique used to identify an \u201cinsidious\u201d type of disk error where data is silently corrupted with no indication from the disk to the storage software layer that an error has occurred. When bitrot detection is enabled on a volume, gluster performs signing of all files/objects in the volume and scrubs data periodically for signature verification. All anomalies observed will be noted in log files. For more information, refer here . Multi threaded epoll for performance improvements Gluster 3.7 introduces multiple threads to dequeue and process more requests from epoll queues. This improves performance by processing more I/O requests. Workloads that involve read/write operations on a lot of small files can benefit from this enhancement. For more information refer here . Volume Tiering [Experimental] Policy based tiering for placement of files. This feature will serve as a foundational piece for building support for data classification. For more information refer here . Volume Tiering is marked as an experimental feature for this release. It is expected to be fully supported in a 3.7.x minor release. Trashcan This feature will enable administrators to temporarily store deleted files from Gluster volumes for a specified time period. For more information refer here . Efficient Object Count and Inode Quota Support This improvement enables an easy mechanism to retrieve the number of objects per directory or volume. Count of objects/files within a directory hierarchy is stored as an extended attribute of a directory. The extended attribute can be queried to retrieve the count. For more information refer here . This feature has been utilized to add support for inode quotas. For more details about inode quotas, refer here . Pro-active Self healing for Erasure Coding Gluster 3.7 adds pro-active self healing support for erasure coded volumes. Exports and Netgroups Authentication for NFS This feature adds Linux-style exports & netgroups authentication to the native NFS server. This enables administrators to restrict access to specific clients & netgroups for volume/sub-directory NFSv3 exports. For more information refer here . GlusterFind GlusterFind is a new tool that provides a mechanism to monitor data events within a volume. Detection of events like modified files is made easier without having to traverse the entire volume. For more information refer here . Rebalance Performance Improvements Rebalance and remove brick operations in Gluster get a performance boost by speeding up identification of files needing movement and a multi-threaded mechanism to move all such files. For more information refer here . NFSv4 and pNFS support Gluster 3.7 supports export of volumes through NFSv4, NFSv4.1 and pNFS. This support is enabled via NFS Ganesha. Infrastructure changes done in Gluster 3.7 to support this feature include: Addition of upcall infrastructure for cache invalidation. Support for lease locks and delegations. Support for enabling Ganesha through Gluster CLI. Corosync and pacemaker based implementation providing resource monitoring and failover to accomplish NFS HA. For more information refer the below links: NFS Ganesha Integration Upcall Infrastructure Gluster CLI for NFS Ganesha High Availability for NFS Ganesha pNFS support for Gluster pNFS support for Gluster volumes and NFSv4 delegations are in beta for this release. Infrastructure changes to support Lease locks and NFSv4 delegations are targeted for a 3.7.x minor release. Snapshot Scheduling With this enhancement, administrators can schedule volume snapshots. For more information, see here . Snapshot Cloning Volume snapshots can now be cloned to create a new writeable volume. For more information, see here . Sharding [Experimental] Sharding addresses the problem of fragmentation of space within a volume. This feature adds support for files that are larger than the size of an individual brick. Sharding works by chunking files to blobs of a configurabe size. For more information, see here . Sharding is an experimental feature for this release. It is expected to be fully supported in a 3.7.x minor release. RCU in glusterd Thread synchronization and critical section access has been improved by introducing userspace RCU in glusterd Arbiter Volumes Arbiter volumes are 3 way replicated volumes where the 3rd brick of the replica is automatically configured as an arbiter. The 3rd brick contains only metadata which provides network partition tolerance and prevents split-brains from happening. For more information, see here . Better split-brain resolution split-brain resolutions can now be also driven by users without administrative intervention. For more information, see the 'Resolution of split-brain from the mount point' section here . Minor Improvements Message ID based logging has been added for several translators. Quorum support for reads. Snapshot names contain timestamps by default.Subsequent access to the snapshots should be done by the name listed in gluster snapshot list Support for gluster volume get <volname> added. libgfapi has added handle based functions to get/set POSIX ACLs based on common libacl structures. Known Issues Enabling Bitrot on volumes with more than 2 bricks on a node is known to cause problems. Addition of bricks dynamically to cold or hot tiers in a tiered volume is not supported. The following configuration changes are necessary for qemu and samba integration with libgfapi to work seamlessly: ~~~ gluster volume set server.allow-insecure on ~~~ Edit /etc/glusterfs/glusterd.vol to contain this line: option rpc-auth-allow-insecure on Post 1, restarting the volume would be necessary: ~~~ gluster volume stop gluster volume start ~~~ Post 2, restarting glusterd would be necessary: ~~~ service glusterd restart ~~~ or ~~~ systemctl restart glusterd ~~~ Upgrading to 3.7.0 Instructions for upgrading from previous versions of GlusterFS are maintained on this page .","title":"3.7.0"},{"location":"release-notes/3.7.0/#major-changes-and-features","text":"Documentation about major changes and features is included in the doc/features/ directory of GlusterFS repository.","title":"Major Changes and Features"},{"location":"release-notes/3.7.0/#geo-replication","text":"Many improvements have gone in the geo replication. A detailed documentation about all the improvements can be found here","title":"Geo Replication"},{"location":"release-notes/3.7.0/#bitrot-detection","text":"Bitrot detection is a technique used to identify an \u201cinsidious\u201d type of disk error where data is silently corrupted with no indication from the disk to the storage software layer that an error has occurred. When bitrot detection is enabled on a volume, gluster performs signing of all files/objects in the volume and scrubs data periodically for signature verification. All anomalies observed will be noted in log files. For more information, refer here .","title":"Bitrot Detection"},{"location":"release-notes/3.7.0/#multi-threaded-epoll-for-performance-improvements","text":"Gluster 3.7 introduces multiple threads to dequeue and process more requests from epoll queues. This improves performance by processing more I/O requests. Workloads that involve read/write operations on a lot of small files can benefit from this enhancement. For more information refer here .","title":"Multi threaded epoll for performance improvements"},{"location":"release-notes/3.7.0/#volume-tiering-experimental","text":"Policy based tiering for placement of files. This feature will serve as a foundational piece for building support for data classification. For more information refer here . Volume Tiering is marked as an experimental feature for this release. It is expected to be fully supported in a 3.7.x minor release.","title":"Volume Tiering [Experimental]"},{"location":"release-notes/3.7.0/#trashcan","text":"This feature will enable administrators to temporarily store deleted files from Gluster volumes for a specified time period. For more information refer here .","title":"Trashcan"},{"location":"release-notes/3.7.0/#efficient-object-count-and-inode-quota-support","text":"This improvement enables an easy mechanism to retrieve the number of objects per directory or volume. Count of objects/files within a directory hierarchy is stored as an extended attribute of a directory. The extended attribute can be queried to retrieve the count. For more information refer here . This feature has been utilized to add support for inode quotas. For more details about inode quotas, refer here .","title":"Efficient Object Count and Inode Quota Support"},{"location":"release-notes/3.7.0/#pro-active-self-healing-for-erasure-coding","text":"Gluster 3.7 adds pro-active self healing support for erasure coded volumes.","title":"Pro-active Self healing for Erasure Coding"},{"location":"release-notes/3.7.0/#exports-and-netgroups-authentication-for-nfs","text":"This feature adds Linux-style exports & netgroups authentication to the native NFS server. This enables administrators to restrict access to specific clients & netgroups for volume/sub-directory NFSv3 exports. For more information refer here .","title":"Exports and Netgroups Authentication for NFS"},{"location":"release-notes/3.7.0/#glusterfind","text":"GlusterFind is a new tool that provides a mechanism to monitor data events within a volume. Detection of events like modified files is made easier without having to traverse the entire volume. For more information refer here .","title":"GlusterFind"},{"location":"release-notes/3.7.0/#rebalance-performance-improvements","text":"Rebalance and remove brick operations in Gluster get a performance boost by speeding up identification of files needing movement and a multi-threaded mechanism to move all such files. For more information refer here .","title":"Rebalance Performance Improvements"},{"location":"release-notes/3.7.0/#nfsv4-and-pnfs-support","text":"Gluster 3.7 supports export of volumes through NFSv4, NFSv4.1 and pNFS. This support is enabled via NFS Ganesha. Infrastructure changes done in Gluster 3.7 to support this feature include: Addition of upcall infrastructure for cache invalidation. Support for lease locks and delegations. Support for enabling Ganesha through Gluster CLI. Corosync and pacemaker based implementation providing resource monitoring and failover to accomplish NFS HA. For more information refer the below links: NFS Ganesha Integration Upcall Infrastructure Gluster CLI for NFS Ganesha High Availability for NFS Ganesha pNFS support for Gluster pNFS support for Gluster volumes and NFSv4 delegations are in beta for this release. Infrastructure changes to support Lease locks and NFSv4 delegations are targeted for a 3.7.x minor release.","title":"NFSv4 and pNFS support"},{"location":"release-notes/3.7.0/#snapshot-scheduling","text":"With this enhancement, administrators can schedule volume snapshots. For more information, see here .","title":"Snapshot Scheduling"},{"location":"release-notes/3.7.0/#snapshot-cloning","text":"Volume snapshots can now be cloned to create a new writeable volume. For more information, see here .","title":"Snapshot Cloning"},{"location":"release-notes/3.7.0/#sharding-experimental","text":"Sharding addresses the problem of fragmentation of space within a volume. This feature adds support for files that are larger than the size of an individual brick. Sharding works by chunking files to blobs of a configurabe size. For more information, see here . Sharding is an experimental feature for this release. It is expected to be fully supported in a 3.7.x minor release.","title":"Sharding [Experimental]"},{"location":"release-notes/3.7.0/#rcu-in-glusterd","text":"Thread synchronization and critical section access has been improved by introducing userspace RCU in glusterd","title":"RCU in glusterd"},{"location":"release-notes/3.7.0/#arbiter-volumes","text":"Arbiter volumes are 3 way replicated volumes where the 3rd brick of the replica is automatically configured as an arbiter. The 3rd brick contains only metadata which provides network partition tolerance and prevents split-brains from happening. For more information, see here .","title":"Arbiter Volumes"},{"location":"release-notes/3.7.0/#better-split-brain-resolution","text":"split-brain resolutions can now be also driven by users without administrative intervention. For more information, see the 'Resolution of split-brain from the mount point' section here .","title":"Better split-brain resolution"},{"location":"release-notes/3.7.0/#minor-improvements","text":"Message ID based logging has been added for several translators. Quorum support for reads. Snapshot names contain timestamps by default.Subsequent access to the snapshots should be done by the name listed in gluster snapshot list Support for gluster volume get <volname> added. libgfapi has added handle based functions to get/set POSIX ACLs based on common libacl structures.","title":"Minor Improvements"},{"location":"release-notes/3.7.0/#known-issues","text":"Enabling Bitrot on volumes with more than 2 bricks on a node is known to cause problems. Addition of bricks dynamically to cold or hot tiers in a tiered volume is not supported. The following configuration changes are necessary for qemu and samba integration with libgfapi to work seamlessly: ~~~","title":"Known Issues"},{"location":"release-notes/3.7.0/#gluster-volume-set-serverallow-insecure-on","text":"~~~ Edit /etc/glusterfs/glusterd.vol to contain this line: option rpc-auth-allow-insecure on Post 1, restarting the volume would be necessary: ~~~","title":"gluster volume set  server.allow-insecure on"},{"location":"release-notes/3.7.0/#gluster-volume-stop","text":"","title":"gluster volume stop "},{"location":"release-notes/3.7.0/#gluster-volume-start","text":"~~~ Post 2, restarting glusterd would be necessary: ~~~","title":"gluster volume start "},{"location":"release-notes/3.7.0/#service-glusterd-restart","text":"~~~ or ~~~","title":"service glusterd restart"},{"location":"release-notes/3.7.0/#systemctl-restart-glusterd","text":"~~~","title":"systemctl restart glusterd"},{"location":"release-notes/3.7.0/#upgrading-to-370","text":"Instructions for upgrading from previous versions of GlusterFS are maintained on this page .","title":"Upgrading to 3.7.0"},{"location":"release-notes/3.7.1/","text":"Release Notes for GlusterFS 3.7.1 This is a bugfix release. The Release Notes for 3.7.0 , contain a listing of all the new features that were added. Note: Enabling Bitrot on volumes with more than 2 bricks on a node works with this release. Bugs Fixed 1212676 : NetBSD port 1218863 : `ls' on a directory which has files with mismatching gfid's does not list anything 1219782 : Regression failures in tests/bugs/snapshot/bug-1112559.t 1221000 : detach-tier status emulates like detach-tier stop 1221470 : dHT rebalance: Dict_copy log messages when running rebalance on a dist-rep volume 1221476 : Data Tiering:rebalance fails on a tiered volume 1221477 : The tiering feature requires counters. 1221503 : DHT Rebalance : Misleading log messages for linkfiles 1221507 : NFS-Ganesha: ACL should not be enabled by default 1221534 : rebalance failed after attaching the tier to the volume. 1221967 : Do not allow detach-tier commands on a non tiered volume 1221969 : tiering: use sperate log/socket/pid file for tiering 1222198 : Fix nfs/mount3.c build warnings reported in Koji 1222750 : non-root geo-replication session goes to faulty state, when the session is started 1222869 : [SELinux] [BVT]: Selinux throws AVC errors while running DHT automation on Rhel6.6 1223215 : gluster volume status fails with locking failed error message 1223286 : [geo-rep]: worker died with \"ESTALE\" when performed rm -rf on a directory from mount of master volume 1223644 : [geo-rep]: With tarssh the file is created at slave but it doesnt get sync 1224100 : [geo-rep]: Even after successful sync, the DATA counter did not reset to 0 1224241 : gfapi: zero size issue in glfs_h_acl_set() 1224292 : peers connected in the middle of a transaction are participating in the transaction 1224647 : [RFE] Provide hourly scrubbing option 1224650 : SIGNING FAILURE Error messages are poping up in the bitd log 1224894 : Quota: spurious failures with quota testcases 1225077 : Fix regression test spurious failures 1225279 : Different client can not execute \"for((i=0;i<1000;i++));do ls -al;done\" in a same directory at the sametime 1225318 : glusterd could crash in remove-brick-status when local remove-brick process has just completed 1225320 : ls command failed with features.read-only on while mounting ec volume. 1225331 : [geo-rep] stop-all-gluster-processes.sh fails to stop all gluster processes 1225543 : [geo-rep]: snapshot creation timesout even if geo-replication is in pause/stop/delete state 1225552 : [Backup]: Unable to create a glusterfind session 1225709 : [RFE] Move signing trigger mechanism to [f]setxattr() 1225743 : [AFR-V2] - afr_final_errno() should treat op_ret > 0 also as success 1225796 : Spurious failure in tests/bugs/disperse/bug-1161621.t 1225919 : Log EEXIST errors in DEBUG level in fops MKNOD and MKDIR 1225922 : Sharding - Skip update of block count and size for directories in readdirp callback 1226024 : cli/tiering:typo errors in tiering 1226029 : I/O's hanging on tiered volumes (NFS) 1226032 : glusterd crashed on the node when tried to detach a tier after restoring data from the snapshot. 1226117 : [RFE] Return proper error codes in case of snapshot failure 1226120 : [Snapshot] Do not run scheduler if ovirt scheduler is running 1226139 : Implement MKNOD fop in bit-rot. 1226146 : BitRot :- bitd is not signing Objects if more than 3 bricks are present on same node 1226153 : Quota: Do not allow set/unset of quota limit in heterogeneous cluster 1226629 : bug-973073.t fails spuriously 1226853 : Volume start fails when glusterfs is source compiled with GCC v5.1.1 Known Issues 1227677 : Glusterd crashes and cannot start after rebalance 1227656 : Glusted dies when adding new brick to a distributed volume and converting to replicated volume 1210256 : gluster volume info --xml gives back incorrect typrStr in xml 1212842 : tar on a glusterfs mount displays \"file changed as we read it\" even though the file was not changed 1220347 : Read operation on a file which is in split-brain condition is successful 1213352 : nfs-ganesha: HA issue, the iozone process is not moving ahead, once the nfs-ganesha is killed 1220270 : nfs-ganesha: Rename fails while exectuing Cthon general category test 1214169 : glusterfsd crashed while rebalance and self-heal were in progress 1221941 : glusterfsd: bricks crash while executing ls on nfs-ganesha vers=3 1225809 : [DHT-REBALANCE]-DataLoss: The data appended to a file during its migration will be lost once the migration is done 1225940 : DHT: lookup-unhashed feature breaks runtime compatibility with older client versions Addition of bricks dynamically to cold or hot tiers in a tiered volume is not supported. The following configuration changes are necessary for qemu and samba integration with libgfapi to work seamlessly: # gluster volume set <volname> server.allow-insecure on Edit /etc/glusterfs/glusterd.vol to contain this line: option rpc-auth-allow-insecure on Post 1, restarting the volume would be necessary: # gluster volume stop <volname> # gluster volume start <volname> Post 2, restarting glusterd would be necessary: # service glusterd restart or # systemctl restart glusterd","title":"3.7.1"},{"location":"release-notes/3.7.1/#release-notes-for-glusterfs-371","text":"This is a bugfix release. The Release Notes for 3.7.0 , contain a listing of all the new features that were added. Note: Enabling Bitrot on volumes with more than 2 bricks on a node works with this release.","title":"Release Notes for GlusterFS 3.7.1"},{"location":"release-notes/3.7.1/#bugs-fixed","text":"1212676 : NetBSD port 1218863 : `ls' on a directory which has files with mismatching gfid's does not list anything 1219782 : Regression failures in tests/bugs/snapshot/bug-1112559.t 1221000 : detach-tier status emulates like detach-tier stop 1221470 : dHT rebalance: Dict_copy log messages when running rebalance on a dist-rep volume 1221476 : Data Tiering:rebalance fails on a tiered volume 1221477 : The tiering feature requires counters. 1221503 : DHT Rebalance : Misleading log messages for linkfiles 1221507 : NFS-Ganesha: ACL should not be enabled by default 1221534 : rebalance failed after attaching the tier to the volume. 1221967 : Do not allow detach-tier commands on a non tiered volume 1221969 : tiering: use sperate log/socket/pid file for tiering 1222198 : Fix nfs/mount3.c build warnings reported in Koji 1222750 : non-root geo-replication session goes to faulty state, when the session is started 1222869 : [SELinux] [BVT]: Selinux throws AVC errors while running DHT automation on Rhel6.6 1223215 : gluster volume status fails with locking failed error message 1223286 : [geo-rep]: worker died with \"ESTALE\" when performed rm -rf on a directory from mount of master volume 1223644 : [geo-rep]: With tarssh the file is created at slave but it doesnt get sync 1224100 : [geo-rep]: Even after successful sync, the DATA counter did not reset to 0 1224241 : gfapi: zero size issue in glfs_h_acl_set() 1224292 : peers connected in the middle of a transaction are participating in the transaction 1224647 : [RFE] Provide hourly scrubbing option 1224650 : SIGNING FAILURE Error messages are poping up in the bitd log 1224894 : Quota: spurious failures with quota testcases 1225077 : Fix regression test spurious failures 1225279 : Different client can not execute \"for((i=0;i<1000;i++));do ls -al;done\" in a same directory at the sametime 1225318 : glusterd could crash in remove-brick-status when local remove-brick process has just completed 1225320 : ls command failed with features.read-only on while mounting ec volume. 1225331 : [geo-rep] stop-all-gluster-processes.sh fails to stop all gluster processes 1225543 : [geo-rep]: snapshot creation timesout even if geo-replication is in pause/stop/delete state 1225552 : [Backup]: Unable to create a glusterfind session 1225709 : [RFE] Move signing trigger mechanism to [f]setxattr() 1225743 : [AFR-V2] - afr_final_errno() should treat op_ret > 0 also as success 1225796 : Spurious failure in tests/bugs/disperse/bug-1161621.t 1225919 : Log EEXIST errors in DEBUG level in fops MKNOD and MKDIR 1225922 : Sharding - Skip update of block count and size for directories in readdirp callback 1226024 : cli/tiering:typo errors in tiering 1226029 : I/O's hanging on tiered volumes (NFS) 1226032 : glusterd crashed on the node when tried to detach a tier after restoring data from the snapshot. 1226117 : [RFE] Return proper error codes in case of snapshot failure 1226120 : [Snapshot] Do not run scheduler if ovirt scheduler is running 1226139 : Implement MKNOD fop in bit-rot. 1226146 : BitRot :- bitd is not signing Objects if more than 3 bricks are present on same node 1226153 : Quota: Do not allow set/unset of quota limit in heterogeneous cluster 1226629 : bug-973073.t fails spuriously 1226853 : Volume start fails when glusterfs is source compiled with GCC v5.1.1","title":"Bugs Fixed"},{"location":"release-notes/3.7.1/#known-issues","text":"1227677 : Glusterd crashes and cannot start after rebalance 1227656 : Glusted dies when adding new brick to a distributed volume and converting to replicated volume 1210256 : gluster volume info --xml gives back incorrect typrStr in xml 1212842 : tar on a glusterfs mount displays \"file changed as we read it\" even though the file was not changed 1220347 : Read operation on a file which is in split-brain condition is successful 1213352 : nfs-ganesha: HA issue, the iozone process is not moving ahead, once the nfs-ganesha is killed 1220270 : nfs-ganesha: Rename fails while exectuing Cthon general category test 1214169 : glusterfsd crashed while rebalance and self-heal were in progress 1221941 : glusterfsd: bricks crash while executing ls on nfs-ganesha vers=3 1225809 : [DHT-REBALANCE]-DataLoss: The data appended to a file during its migration will be lost once the migration is done 1225940 : DHT: lookup-unhashed feature breaks runtime compatibility with older client versions Addition of bricks dynamically to cold or hot tiers in a tiered volume is not supported. The following configuration changes are necessary for qemu and samba integration with libgfapi to work seamlessly: # gluster volume set <volname> server.allow-insecure on Edit /etc/glusterfs/glusterd.vol to contain this line: option rpc-auth-allow-insecure on Post 1, restarting the volume would be necessary: # gluster volume stop <volname> # gluster volume start <volname> Post 2, restarting glusterd would be necessary: # service glusterd restart or # systemctl restart glusterd","title":"Known Issues"},{"location":"release-notes/3.9.0/","text":"Release notes for Gluster 3.9.0 This is a major release that includes a huge number of changes. Many improvements contribute to better support of Gluster with containers and running your storage on the same server as your hypervisors. Lots of work has been done to integrate with other projects that are part of the Open Source storage ecosystem. The most notable features and changes are documented on this page. A full list of bugs that has been addressed is included further below. Major changes and features Introducing reset-brick command Notes for users: The reset-brick command provides support to reformat/replace the disk(s) represented by a brick within a volume. This is helpful when a disk goes bad etc Start reset process - gluster volume reset-brick VOLNAME HOSTNAME:BRICKPATH start The above command kills the respective brick process. Now the brick can be reformatted. To restart the brick after modifying configuration - gluster volume reset-brick VOLNAME HOSTNAME:BRICKPATH HOSTNAME:BRICKPATH commit If the brick was killed to replace the brick with same brick path, restart with following command - gluster volume reset-brick VOLNAME HOSTNAME:BRICKPATH HOSTNAME:BRICKPATH commit force Limitations: 1. resetting a brick kills a brick process in concern. During this period the brick will not be available for IO's. 2. Replacing a brick with this command will work only if both the brick paths are same and belong to same volume. Get node level status of a cluster Notes for users: The get-state command provides node level status of a trusted storage pool from the point of view of glusterd in a parseable format. Using get-state command, external applications can invoke the command on all nodes of the cluster, and parse and collate the data obtained from all these nodes to get a complete picture of the state of the cluster. # gluster get-state <glusterd> [odir <path/to/output/dir] [file <filename>] This would dump data points that reflect the local state representation of the cluster as maintained in glusterd (no other daemons are supported as of now) to a file inside the specified output directory. The default output directory and filename is /var/run/gluster and glusterd_state_ respectively. Following are the sections in the output: 1. Global : UUID and op-version of glusterd 2. Global options : Displays cluster specific options that have been set explicitly through the volume set command. 3. Peers : Displays the peer node information including its hostname and connection status 4. Volumes : Displays the list of volumes created on this node along with detailed information on each volume. 5. Services : Displays the list of the services configured on this node along with their corresponding statuses. Limitations: 1. This only supports glusterd. 2. Does not provide complete cluster state. Data to be collated from all nodes by external application to get the complete cluster state. Multi threaded self-heal for Disperse volumes Notes for users: Users now have the ability to configure multi-threaded self-heal in disperse volumes using the following commands: Option below can be used to control number of parallel heals in SHD # gluster volume set <volname> disperse.shd-max-threads [1-64] # default is 1 Option below can be used to control number of heals that can wait in SHD # gluster volume set <volname> disperse.shd-wait-qlength [1-65536] # default is 1024 Hardware extention acceleration in Disperse volumes Notes for users: If the user has hardware that has special instructions which can be used in erasure code calculations on the client it will be automatically used. At the moment this support is added for cpu-extentions: x64 , sse , avx Lock revocation feature Notes for users: 1. Motivation: Prevents cluster instability by mis-behaving clients causing bricks to OOM due to inode/entry lock pile-ups. 2. Adds option to strip clients of entry/inode locks after N seconds 3. Adds option to clear ALL locks should the revocation threshold get hit 4. Adds option to clear all or granted locks should the max-blocked threshold get hit (can be used in combination w/ revocation-clear-all). 5. Adds logging to indicate revocation event & reason 6. Options are: # gluster volume set <volname> features.locks-revocation-secs <integer; 0 to disable> # gluster volume set <volname> features.locks-revocation-clear-all [on/off] # gluster volume set <volname> features.locks-revocation-max-blocked <integer> On demand scrubbing for Bitrot Detection: Notes for users: With 'ondemand' scrub option, you don't need to wait for the scrub-frequency to expire. As the option name itself says, the scrubber can be initiated on demand to detect the corruption. If the scrubber is already running, this option is a no op. # gluster volume bitrot <volume-name> scrub ondemand ### Improvements in Gluster NFS-Ganesha integration Notes for users: With this release the major change done is to store all the ganesha related configuration files in the shared storage volume mount point instead of having separate local copy in '/etc/ganesha' folder on each node. For new users, before enabling nfs-ganesha create a directory named nfs-ganesha in the shared storage mount point ( /var/run/gluster/shared_storage/ ) Create ganesha.conf & ganesha-ha.conf in that directory with the required details filled in. For existing users, before starting nfs-ganesha service do the following : Copy all the contents of /etc/ganesha directory (including .export_added file) to /var/run/gluster/shared_storage/nfs-ganesha from any of the ganesha nodes Create symlink using /var/run/gluster/shared_storage/nfs-ganesha/ganesha.conf on /etc/ganesha one each node in ganesha-cluster Change path for each export entry in ganesha.conf file Example: if a volume \"test\" was exported, then ganesha.conf shall have below export entry - %include \"/etc/ganesha/exports/export.test.conf\" export entry. Change that line to %include \"/var/run/gluster/shared_storage/nfs-ganesha/exports/export.test.conf\" In addition, following changes have been made - * The entity \"HA_VOL_SERVER= \" in ganesha-ha.conf is no longer required. * A new resource-agent called portblock (available in >= resource-agents-3.9.5 package) is added to the cluster configuration to speed up the nfs-client connections post IP failover or failback. This may be noticed while looking at the cluster configuration status using the command pcs status . Availability of python bindings to libgfapi The official python bindings for GlusterFS libgfapi C library interface is mostly API complete. The complete API reference and documentation can be found at libgfapi-python.rtfd.io The python bindings have been packaged and has been made available over PyPI . Small file improvements in Gluster with md-cache (Experimental) Notes for users: With this release, metadata cache on the client side is integrated with the cache-invalidation feature so that the clients can cache longer without compromising on consistency. By enabling, the metadata cache and cache invalidation feature and extending the cache timeout to 600s, we have seen performance improvements in metadata operation like creates, ls/stat, chmod, rename, delete. The perf improvements is significant in SMB access of gluster volume, but as a cascading effect the improvements is also seen on FUSE/Native access and NFS access. Use the below options in the order mentioned, to enable the features: # gluster volume set <volname> features.cache-invalidation on # gluster volume set <volname> features.cache-invalidation-timeout 600 # gluster volume set <volname> performance.stat-prefetch on # gluster volume set <volname> performance.cache-invalidation on # gluster volume set <volname> performance.cache-samba-metadata on # Only for SMB access # gluster volume set <volname> performance.md-cache-timeout 600 Real time Cluster notifications using Events APIs Let us imagine we have a Gluster monitoring system which displays list of volumes and its state, to show the realtime status, monitoring app need to query the Gluster in regular interval to check volume status, new volumes etc. Assume if the polling interval is 5 seconds then monitoring app has to run gluster volume info command ~17000 times a day! With Gluster 3.9 release, Gluster provides close to realtime notification and alerts for the Gluster cluster state changes and alerts. Webhooks can be registered to listen to Events emitted by Gluster. More details about this new feature is available here. http://docs.gluster.org/en/latest/Administrator%20Guide/Events%20APIs Geo-replication improvements Documentation improvements: Upstream documentation is rewritten to reflect the latest version of Geo-replication. Removed the stale/duplicate documentation. We are still working on to add Troubleshooting, Cluster expand/shrink notes to it. Latest version of documentation is available here http://docs.gluster.org/en/latest/Administrator%20Guide/Geo%20Replication Geo-replication Events are available for Events API consumers: Events APIs is the new Gluster feature available with 3.9 release, most of the events from Geo-replication are added to eventsapi. Read more about the Events APIs and Geo-replication events here http://docs.gluster.org/en/latest/Administrator%20Guide/Events%20APIs New simplified command to setup Non root Geo-replication Non root Geo-replication setup was not easy with multiple manual steps. Non root Geo-replication steps are simplified. Read more about the new steps in Admin guide. http://docs.gluster.org/en/latest/Administrator%20Guide/Geo%20Replication/#slave-user-setup New command to generate SSH keys(Alternative command to gsec_create ) gluster system:: execute gsec_create command generates ssh keys in every Master cluster nodes and copies to initiated node. This command silently ignores error if any node is down in cluster. It will not collect SSH keys from that node. When Geo-rep create push-pem command is issued it will copy public keys from those nodes which were up during gsec_create. This causes Geo-rep to go to Faulty when that master node tries to make the connection to slave nodes. With the new command, output shows if any Master node was down while generating ssh keys. Read more about `gluster-georep-sshkey http://docs.gluster.org/en/latest/Administrator%20Guide/Geo%20Replication/#setting-up-the-environment-for-geo-replication Logging improvements New logs are added, now from the log we can clearly understand what is going on. Note: This feature may change logging format of existing log messages, Please update your parsers if used to parse Geo-rep logs. Patch: http://review.gluster.org/15710 New Configuration options available: changelog-log-level All the changelog related log messages are logged in /var/log/glusterfs/geo-replication/<SESSION>/*.changes.log in Master nodes. Log level was hard coded as TRACE for Changelog logs. New configuration option provided to modify the changelog log level and defaulted to INFO Behavior changes #1221623 : Earlier the ports GlusterD used to allocate for the daemons like brick processes, quotad, shd et all were persistent through the volume's life cycle, so every restart of the process(es) or a node reboot will try to use the same ports which were allocated for the first time. With release-3.9 onwards, GlusterD will try to allocate a fresh port once a daemon is restarted or the node is rebooted. #1348944 : with 3.9 release the default log file for glusterd has been renamed to glusterd.log from etc-glusterfs-glusterd.vol.log Known Issues #1387878 :add-brick on a vm-store configuration which has sharding enabled is leading to vm corruption. To work around this issue, one can scale up by creating more volumes until this issue is fixed. Bugs addressed A total of 571 patches has been sent, addressing 422 bugs: #762184 : Support mandatory locking in glusterfs #789278 : Issues reported by Coverity static analysis tool #1005257 : [PATCH] Small typo fixes #1175711 : posix: Set correct d_type for readdirp() calls #1193929 : GlusterFS can be improved #1198849 : Minor improvements and cleanup for the build system #1200914 : pathinfo is wrong for striped replicated volumes #1202274 : Minor improvements and code cleanup for libgfapi #1207604 : [rfe] glusterfs snapshot cli commands should provide xml output. #1211863 : RFE: Support in md-cache to use upcall notifications to invalidate its cache #1221623 : glusterd: add brick command should re-use the port for listening which is freed by remove-brick. #1222915 : usage text is wrong for use-readdirp mount default #1223937 : Outdated autotools helper config.* files #1225718 : [FEAT] DHT - rebalance - rebalance status o/p should be different for 'fix-layout' option, it should not show 'Rebalanced-files' , 'Size', 'Scanned' etc as it is not migrating any files. #1227667 : Minor improvements and code cleanup for protocol server/client #1228142 : clang-analyzer: adding clang static analysis support #1231224 : Misleading error messages on brick logs while creating directory (mkdir) on fuse mount #1236009 : do an explicit lookup on the inodes linked in readdirp #1254067 : remove unused variables #1266876 : cluster/afr: AFR2 returns empty readdir results to clients if brick is added back into cluster after re-imaging/formatting #1278325 : DHT: Once remove brick start failed in between Remove brick commit should not be allowed #1285152 : store afr pending xattrs as a volume option #1292020 : quota: client gets IO error instead of disk quota exceed when the limit is exceeded #1294813 : [geo-rep]: Multiple geo-rep session to the same slave is allowed for different users #1296043 : Wrong usage of dict functions #1302277 : Wrong XML output for Volume Options #1302948 : tar complains: : file changed as we read it #1303668 : packaging: rpmlint warning and errors - Documentation URL 404 #1305031 : AFR winds a few reads of a file in metadata split-brain. #1306398 : Tiering and AFR may result in data loss #1311002 : NFS+attach tier:IOs hang while attach tier is issued #1311926 : [georep]: If a georep session is recreated the existing files which are deleted from slave doesn't get sync again from master #1315666 : Data Tiering:tier volume status shows as in-progress on all nodes of a cluster even if the node is not part of volume #1316178 : changelog/rpc: Memory leak- rpc_clnt_t object is never freed #1316389 : georep: tests for logrotate, create+rename and hard-link rename #1318204 : Input / Output when chmoding files on NFS mount point #1318289 : [RFE] Add arbiter brick hotplug #1318591 : Glusterd not operational due to snapshot conflicting with nfs-ganesha export file in \"/var/lib/glusterd/snaps\" #1319992 : RFE: Lease support for gluster #1320388 : [GSS]-gluster v heal volname info does not work with enabled ssl/tls #1321836 : gluster volume info --xml returns 0 for nonexistent volume #1322214 : [HC] Add disk in a Hyper-converged environment fails when glusterfs is running in directIO mode #1322805 : [scale] Brick process does not start after node reboot #1322825 : IO-stats, client profile is overwritten when it is on the same node as bricks #1324439 : SAMBA+TIER : Wrong message display.On detach tier success the message reflects Tier command failed. #1325831 : gluster snap status xml output shows incorrect details when the snapshots are in deactivated state #1326410 : /var/lib/glusterd/$few-directories not owned by any package, causing it to remain after glusterfs-server is uninstalled #1327171 : Disperse: Provide description of disperse.eager-lock option. #1328224 : RFE : Feature: Automagic unsplit-brain policies for AFR #1329211 : values for Number of Scrubbed files, Number of Unsigned files, Last completed scrub time and Duration of last scrub are shown as zeros in bit rot scrub status #1330032 : rm -rf to a dir gives directory not empty(ENOTEMPTY) error #1330097 : ganesha exported volumes doesn't get synced up on shutdown node when it comes up. #1330583 : glusterfs-libs postun ldconfig: relative path `1' used to build cache #1331254 : Disperse volume fails on high load and logs show some assertion failures #1331287 : No xml output on gluster volume heal info command with --xml #1331323 : [Granular entry sh] - Implement renaming of indices in index translator #1331423 : distaf: Add io_libs to namespace package list #1331720 : implement meta-lock/unlock for lock migration #1331721 : distaf: Add README and HOWTO to distaflibs as well #1331860 : Wrong constant used in length based comparison for XATTR_SECURITY_PREFIX #1331969 : Ganesha+Tiering: Continuous \"0-glfs_h_poll_cache_invalidation: invalid argument\" messages getting logged in ganesha-gfapi logs. #1332020 : multiple regression failures for tests/basic/quota-ancestry-building.t #1332021 : multiple failures for testcase: tests/basic/inode-quota-enforcing.t #1332054 : multiple failures of tests/bugs/disperse/bug-1236065.t #1332073 : EINVAL errors while aggregating the directory size by quotad #1332134 : bitrot: Build generates Compilation Warning. #1332136 : Detach tier fire before the background fixlayout is complete may result in failure #1332156 : SMB:while running I/O on cifs mount and doing graph switch causes cifs mount to hang. #1332219 : tier: avoid pthread_join if pthread_create fails #1332413 : Wrong op-version for mandatory-locks volume set option #1332419 : geo-rep: address potential leak of memory #1332460 : [features/worm] - when disabled, worm xl should simply pass requested fops to its child xl #1332465 : glusterd + bitrot : Creating clone of snapshot. error \"xlator.c:148:xlator_volopt_dynload] 0-xlator: /usr/lib64/glusterfs/3.7.9/xlator/features/bitrot.so: cannot open shared object file: #1332473 : tests: 'tests/bitrot/br-state-check.t' fails in netbsd #1332501 : Mandatory locks are not migrated during lock migration #1332566 : [granular entry sh] - Add more tests #1332798 : [AFR]: \"volume heal info\" command is failing during in-service upgrade to latest. #1332822 : distaf: Add library functions for gluster snapshot operations #1332885 : distaf: Add library functions for gluster bitrot operations and generic library utility functions generic to all components #1332952 : distaf: Add library functions for gluster quota operations #1332994 : Self Heal fails on a replica3 volume with 'disk quota exceeded' #1333023 : readdir-ahead does not fetch xattrs that md-cache needs in it's internal calls #1333043 : Fix excessive logging due to NULL dict in dht #1333263 : [features/worm] Unwind FOPs with op_errno and add gf_worm prefix to functions #1333317 : rpc_clnt will sometimes not reconnect when using encryption #1333319 : Unexporting a volume sometimes fails with \"Dynamic export addition/deletion failed\". #1333370 : [FEAT] jbr-server handle lock/unlock fops #1333738 : distaf: Add GlusterBaseClass (gluster_base_class.py) to distaflibs-gluster. #1333912 : client ID should logged when SSL connection fails #1333925 : libglusterfs: race conditions and illegal mem access in timer #1334044 : [RFE] Eventing for Gluster #1334164 : Worker dies with [Errno 5] Input/output error upon creation of entries at slave #1334208 : distaf: Add library functions for gluster rebalance operations #1334269 : GlusterFS 3.8 fails to build in the CentOS Community Build System #1334270 : glusterd: glusterd provides stale port information when a volume is recreated with same brick path #1334285 : Under high read load, sometimes the message \"XDR decoding failed\" appears in the logs and read fails #1334314 : changelog: changelog_rollover breaks when number of fds opened is more than 1024 #1334444 : SAMBA-VSS : Permission denied issue while restoring the directory from windows client 1 when files are deleted from windows client 2 #1334620 : stop all gluster processes should also include glusterfs mount process #1334621 : set errno in case of inode_link failures #1334721 : distaf: Add library functions for gluster tiering operations #1334839 : [Tiering]: Files remain in hot tier even after detach tier completes #1335019 : Add graph for decompounder xlator #1335091 : mount/fuse: Logging improvements #1335231 : features/locks: clang compile warning in posix.c #1335232 : features/index: clang compile warnings in index.c #1335429 : Self heal shows different information for the same volume from each node #1335494 : Modifying peer ops library #1335531 : Modified volume options are not syncing once glusterd comes up. #1335652 : Heal info shows split-brain for .shard directory though only one brick was down #1335717 : PREFIX is not honoured during build and install #1335776 : rpc: change client insecure port ceiling from 65535 to 49151 #1335818 : Revert \"features/shard: Make o-direct writes work with sharding: http://review.gluster.org/#/c/13846/\" #1335858 : Files present in the .shard folder even after deleting all the vms from the UI #1335973 : [Tiering]: The message 'Max cycle time reached..exiting migration' incorrectly displayed as an 'error' in the logs #1336197 : failover is not working with latest builds. #1336328 : [FEAT] jbr: Improve code modularity #1336354 : Provide a way to configure gluster source location in devel-vagrant #1336373 : Distaf: Add gluster specific config file #1336381 : ENOTCONN error during parallel rmdir #1336508 : rpc-transport: compiler warning format string #1336612 : one of vm goes to paused state when network goes down and comes up back #1336630 : ERROR and Warning message on writing a file from mount point \"null gfid for path (null)\" repeated 3 times between\" #1336642 : [RFE] git-branch-diff: wrapper script for git to visualize backports #1336698 : DHT : few Files are not accessible and not listed on mount + more than one Directory have same gfid + (sometimes) attributes has ?? in ls output after renaming Directories from multiple client at same time #1336793 : assorted typos and spelling mistakes from Debian lintian #1336818 : Add ability to set oom_score_adj for glusterfs process #1336853 : scripts: bash-isms in scripts #1336945 : [NFS-Ganesha] : stonith-enabled option not set with new versions of cman,pacemaker,corosync and pcs #1337160 : distaf: Added libraries to setup nfs-ganesha in gluster through distaf #1337227 : [tiering]: error message shown during the failure of detach tier commit isn't intuitive #1337405 : Some of VMs go to paused state when there is concurrent I/O on vms #1337473 : upgrade path when slave volume uuid used in geo-rep session #1337597 : Mounting a volume over NFS with a subdir followed by a / returns \"Invalid argument\" #1337650 : log flooded with Could not map name=xxxx to a UUID when config'd with long hostnames #1337777 : tests/bugs/write-behind/1279730.t fails spuriously #1337791 : tests/basic/afr/tarissue.t fails regression #1337899 : Misleading error message on rebalance start when one of the glusterd instance is down #1338544 : fuse: In fuse_first_lookup(), dict is not un-referenced in case create_frame returns an empty pointer. #1338634 : AFR : fuse,nfs mount hangs when directories with same names are created and deleted continuously #1338733 : __inode_ctx_put: fix mem leak on failure #1338967 : common-ha: ganesha.nfsd not put into NFS-GRACE after fail-back #1338991 : DHT2: Tracker bug #1339071 : dht/rebalance: mark hardlink migration failure as skipped for rebalance process #1339149 : Error and warning messages related to xlator/features/snapview-client.so adding up to the client log on performing IO operations #1339166 : distaf: Added timeout value to wait for rebalance to complete and removed older rebalance library file #1339181 : Full heal of a sub-directory does not clean up name-indices when granular-entry-heal is enabled. #1339214 : gfapi: set mem_acct for the variables created for upcall #1339471 : [geo-rep]: Worker died with [Errno 2] No such file or directory #1339472 : [geo-rep]: Monitor crashed with [Errno 3] No such process #1339541 : Added libraries to setup CTDB in gluster through distaf #1339553 : gfapi: in case of handle based APIs, close glfd after successful create #1339689 : RFE - capacity info (df -h on a mount) is incorrect for a tiered volume #1340488 : copy-export-ganesha.sh does not have a correct shebang #1340623 : Directory creation(mkdir) fails when the remove brick is initiated for replicated volumes accessing via nfs-ganesha #1340853 : [geo-rep]: If the session is renamed, geo-rep configuration are not retained #1340936 : Automount fails because /sbin/mount.glusterfs does not accept the -s option #1341007 : gfapi : throwing warning message for unused variable in glfs_h_find_handle() #1341009 : Log parameters such as the gfid, fd address, offset and length of the reads upon failure for easier debugging #1341294 : build: RHEL7 unpackaged files /var/lib/glusterd/hooks/.../S57glusterfind-delete-post.{pyc,pyo} #1341474 : [geo-rep]: Snapshot creation having geo-rep session is broken #1341650 : conservative merge happening on a x3 volume for a deleted file #1341768 : After setting up ganesha on RHEL 6, nodes remains in stopped state and grace related failures observed in pcs status #1341796 : [quota+snapshot]: Directories are inaccessible from activated snapshot, when the snapshot was created during directory creation #1342171 : O_DIRECT support for sharding #1342259 : [features/worm] - write FOP should pass for the normal files #1342298 : reading file with size less than 512 fails with odirect read #1342356 : [RFE] Python library for creating Cluster aware CLI tools for Gluster #1342420 : [georep]: Stopping volume fails if it has geo-rep session (Even in stopped state) #1342796 : self heal deamon killed due to oom kills on a dist-disperse volume using nfs ganesha #1342979 : [geo-rep]: Add-Brick use case: create push-pem force on existing geo-rep fails #1343038 : IO ERROR when multiple graph switches #1343286 : enabling glusternfs with nfs.rpc-auth-allow to many hosts failed #1343333 : [RFE] Simplify Non Root Geo-replication Setup #1343374 : Gluster fuse client crashed generating core dump #1343838 : Implement API to get page aligned iobufs in iobuf.c #1343906 : [Stress/Scale] : I/O errors out from gNFS mount points during high load on an erasure coded volume,Logs flooded with Error messages. #1343943 : Old documentation link in log during Geo-rep MISCONFIGURATION #1344277 : [disperse] mkdir after re balance give Input/Output Error #1344340 : Unsafe access to inode->fd_list #1344396 : fd leak in disperse #1344407 : fail delete volume operation if one of the glusterd instance is down in cluster #1344686 : tiering : Multiple brick processes crashed on tiered volume while taking snapshots #1344714 : removal of file from nfs mount crashs ganesha server #1344836 : [Disperse volume]: IO hang seen on mount with file ops #1344885 : inode leak in brick process #1345727 : Bricks are starting when server quorum not met. #1345744 : [geo-rep]: Worker crashed with \"KeyError: \" #1345748 : SAMBA-DHT : Crash seen while rename operations in cifs mount and windows access of share mount #1345846 : quota : rectify quota-deem-statfs default value in gluster v set help command #1345855 : Possible crash due to a timer cancellation race #1346138 : [RFE] Non root Geo-replication Error logs improvements #1346211 : cleanup glusterd-georep code #1346551 : wrong understanding of function's parameter #1346719 : [Disperse] dd + rm + ls lead to IO hang #1346821 : cli core dumped while providing/not wrong values during arbiter replica volume #1347249 : libgfapi : variables allocated by glfs_set_volfile_server is not freed #1347354 : glusterd: SuSE build system error for incorrect strcat, strncat usage #1347686 : IO error seen with Rolling or non-disruptive upgrade of an distribute-disperse(EC) volume from 3.7.5 to 3.7.9 #1348897 : Add relative path validation for gluster copy file utility #1348904 : [geo-rep]: If the data is copied from .snaps directory to the master, it doesn't get sync to slave [First Copy] #1348944 : Change the glusterd log file name to glusterd.log #1349270 : ganesha.enable remains on in volume info file even after we disable nfs-ganesha on the cluster. #1349273 : Geo-rep silently ignores config parser errors #1349276 : Buffer overflow when attempting to create filesystem using libgfapi as driver on OpenStack #1349284 : [tiering]: Files of size greater than that of high watermark level should not be promoted #1349398 : nfs-ganesha disable doesn't delete nfs-ganesha folder from /var/run/gluster/shared_storage #1349657 : process glusterd set TCP_USER_TIMEOUT failed #1349709 : Polling failure errors getting when volume is started&stopped with SSL enabled setup. #1349723 : Added libraries to get server_brick dictionaries #1350017 : Change distaf glusterbase class and mount according to the config file changes #1350168 : distaf: made changes to create_volume function #1350173 : distaf: Adding samba_ops library #1350188 : distaf: minor import changes in ganesha.py #1350191 : race condition when set ctx->timer in function gf_timer_registry_init #1350237 : Gluster/NFS does not accept dashes in hostnames in exports/netgroups files #1350245 : distaf: Add library functions for gluster volume operations #1350248 : distaf: Modified get_pathinfo function in lib_utils.py #1350256 : Distaf: Modifying the ctdb_libs to get server host from the server dict #1350258 : Distaf: add a sample test case to the framework #1350327 : Protocol client not mounting volumes running on older versions. #1350371 : ganesha/glusterd : remove 'HA_VOL_SERVER' from ganesha-ha.conf #1350383 : distaf: Modified distaf gluster config file #1350427 : distaf: Modified tier_attach() to get bricks path for attaching tier from the available bricks in server #1350744 : GlusterFS 3.9.0 tracker #1350793 : build: remove absolute paths from glusterfs spec file #1350867 : RFE: FEATURE: Lock revocation for features/locks xlator #1351021 : [DHT]: Rebalance info for remove brick operation is not showing after glusterd restart #1351071 : [geo-rep] Stopped geo-rep session gets started automatically once all the master nodes are upgraded #1351134 : [SSL] : gluster v set help does not show ssl options #1351537 : [Bitrot] Need a way to set scrub interval to a minute, for ease of testing #1351880 : gluster volume status client\" isn't showing any information when one of the nodes in a 3-way Distributed-Replicate volume is shut down #1352019 : RFE: Move throttling code to libglusterfs from bitrot #1352277 : a two node glusterfs seems not possible anymore?! #1352279 : [scale]: Bricks not started after node reboot. #1352423 : should find_library(\"c\") be used instead of find_library(\"libc\") in geo-replication/syncdaemon/libcxattr.py? #1352634 : qemu libgfapi clients hang when doing I/O #1352671 : RFE: As a part of xattr invalidation, send the stat info as well #1352854 : GlusterFS - Memory Leak - High Memory Utilization #1352871 : [Bitrot]: Scrub status- Certain fields continue to show previous run's details, even if the current run is in progress #1353156 : [RFE] CLI to get local state representation for a cluster #1354141 : several problems found in failure handle logic #1354221 : noisy compilation warnning with Wstrict-prototypes #1354372 : Fix timing issue in tests/bugs/glusterd/bug-963541.t #1354439 : nfs client I/O stuck post IP failover #1354489 : service file is executable #1355604 : afr coverity fixes #1355628 : Upgrade from 3.7.8 to 3.8.1 doesn't regenerate the volfiles #1355706 : [Bitrot]: Sticky bit files considered and skipped by the scrubber, instead of getting ignored. #1355956 : RFE : move ganesha related configuration into shared storage #1356032 : quota: correct spelling mistakes in quota src files #1356068 : observing \" Too many levels of symbolic links\" after adding bricks and then issuing a replace brick #1356504 : Move gf_log->gf_msg in index feature #1356508 : [RFE] Handle errors during SSH key generation(gsec_create) #1356528 : memory leak in glusterd-georeplication #1356851 : [Bitrot+Sharding] Scrub status shows incorrect values for 'files scrubbed' and 'files skipped' #1356868 : File not found errors during rpmbuild: /var/lib/glusterd/hooks/1/delete/post/S57glusterfind-delete-post.py{c,o} #1356888 : Correct code in socket.c to avoid fd leak #1356998 : syscalls: readdir_r() is deprecated in newer glibc #1357210 : add several fops support in io-threads #1357226 : add a basis function to reduce verbose code #1357397 : Trash translator fails to create 'internal_op' directory under already existing trash directory #1357463 : Error: quota context not set inode (gfid:nnn) [Invalid argument] #1357490 : libglusterfs : update correct memory segments in glfs-message-id #1357821 : Make install fails second time without uninstall #1358114 : tests: ./tests/bitrot/br-stub.t fails intermittently #1358195 : Fix spurious failure of tests/bugs/glusterd/bug-1111041.t #1358196 : Tiering related core observed with \"uuid_is_null () message\". #1358244 : [SNAPSHOT]: The PID for snapd is displayed even after snapd process is killed. #1358594 : Enable gfapi test cases in Gluster upstream regression #1358608 : Memory leak observed with upcall polling #1358671 : Add Events for Volume Set and Reset #1358922 : missunderstanding about GF_PROTOCOL_DICT_SERIALIZE #1358936 : coverity: iobuf_get_page_aligned calling iobuf_get2 should check the return pointer #1358944 : jbr resource leak, forget free \"path\" #1358976 : Fix spurious failures in split-brain-favorite-child-policy.t #1359001 : Fix spurious failures in ec.t #1359190 : Glusterd crashes upon receiving SIGUSR1 #1359370 : glfs: fix glfs_set_volfile_server doc #1359711 : [GSS] Rebalance crashed #1359717 : Fix failure of ./tests/bugs/snapshot/bug-1316437.t #1360169 : Fix bugs in compound fops framework #1360401 : RFE: support multiple bricks within one process #1360402 : Clients can starve under heavy load #1360647 : gfapi: deprecate the rdma support for management connections #1360670 : Add output option --xml to man page of gluster #1360679 : Bricks doesn't come online after reboot [ Brick Full ] #1360682 : tests: ./tests/bitrot/bug-1244613.t fails intermittently #1360693 : [RFE] Add a count of snapshots associated with a volume to the output of the vol info command #1360809 : [RFE] Capture events in GlusterD #1361094 : Auto generate header files during Make #1361249 : posix: leverage FALLOC_FL_ZERO_RANGE in zerofill fop #1361300 : Direct io to sharded files fails when on zfs backend #1361678 : thread CPU saturation limiting throughput on write workloads #1361983 : Move USE_EVENTS in gf_events API #1361999 : Remove ganesha xlator code from gluster code base #1362144 : Python library to send Events #1362151 : [libgfchangelog]: If changelogs are not available for the requested time range, no proper error message #1362397 : Mem leak in meta_default_readv in meta xlators #1362520 : Per xlator logging not working #1362602 : [Open SSL] : Unable to mount an SSL enabled volume via SMB v3/Ganesha v4 #1363591 : Geo-replication user driven Events #1363721 : [HC]: After bringing down and up of the bricks VM's are getting paused #1363948 : Spurious failure in tests/bugs/glusterd/bug-1089668.t #1364026 : glfs_fini() crashes with SIGSEGV #1364420 : [RFE] History Crawl performance improvement #1364449 : posix: honour fsync flags in posix_do_zerofill #1364529 : api: revert glfs_ipc_xd intended for 4.0 #1365455 : [AFR]: Files not available in the mount point after converting Distributed volume type to Replicated one. #1365489 : glfs_truncate missing #1365506 : gfapi: use const qualifier for glfs_*timens() #1366195 : [Bitrot - RFE]: On demand scrubbing option to scrub #1366222 : \"heal info --xml\" not showing the brick name of offline bricks. #1366226 : Move alloca0 definition to common-utils #1366284 : fix bug in protocol/client lookup callback #1367258 : Log EEXIST errors at DEBUG level #1367478 : Second gluster volume is offline after daemon restart or server reboot #1367527 : core: use for makedev(3), major(3), minor(3) #1367665 : rotated FUSE mount log is using to populate the information after log rotate. #1367771 : Introduce graceful mode in stop-all-gluster-processes.sh #1367774 : Support for Client side Events #1367815 : [Bitrot - RFE]: Bitrot Events #1368042 : make fails if Events APIs are disabled #1368349 : tests/bugs/cli/bug-1320388.t: Infrequent failures #1368451 : [RFE] Implement multi threaded self-heal for ec volumes #1368842 : Applications not calling glfs_h_poll_upcall() have upcall events cached for no use #1368882 : log level set in glfs_set_logging() does not work #1368931 : [ RFE] Quota Events #1368953 : spurious netbsd run failures in tests/basic/glusterd/volfile_server_switch.t #1369124 : fix unused variable warnings from out-of-tree builds generate XDR headers and source files i... #1369331 : Memory leak with a replica 3 arbiter 1 configuration #1369401 : NetBSD hangs at /tests/features/lock_revocation.t #1369430 : Track the client that performed readdirp #1369432 : IATT cache invalidation should be sent when permission changes on file #1369524 : segment fault while join thread reaper_thr in fini() #1369530 : protocol/server: readlink rsp xdr failed while readlink got an error #1369638 : DHT stale layout issue will be seen often with md-cache prolonged cache of lookups #1369721 : EventApis will not work if compiled using ./configure --disable-glupy #1370053 : fix EXPECT_WITHIN #1370074 : Fix mistakes in self-heald.t #1370406 : build: eventtypes.h is missing #1370445 : Geo-replication server side events #1370862 : dht: fix the broken build #1371541 : Spurious regressions in ./tests/bugs/gfapi/bug-1093594.t #1371543 : Add cache invalidation stat in profile info #1371775 : gluster system:: uuid get hangs #1372278 : [RFE] Provide snapshot events for the new eventing framework #1372586 : Fix the test case http://review.gluster.org/#/c/15385/ #1372686 : [RFE]Reducing number of network round trips #1373529 : Node remains in stopped state in pcs status with \"/usr/lib/ocf/resource.d/heartbeat/ganesha_mon: line 137: [: too many arguments ]\" messages in logs. #1373735 : Event pushed even if Answer is No in the Volume Stop and Delete prompt #1373740 : [RFE]: events from protocol server #1373743 : [RFE]: AFR events #1374153 : [RFE] History Crawl performance improvement #1374167 : disperse: Integrate important events with events framework #1374278 : rpc/xdr: generated files are filtered with a sed extended regex #1374298 : \"gluster vol status all clients --xml\" doesn't generate xml if there is a failure in between #1374324 : [RFE] Tier Events #1374567 : [Bitrot]: Recovery fails of a corrupted hardlink (and the corresponding parent file) in a disperse volume #1374581 : Geo-rep worker Faulty with OSError: [Errno 21] Is a directory #1374597 : [geo-rep]: AttributeError: 'Popen' object has no attribute 'elines' #1374608 : geo-replication *changes.log does not respect the log-level configured #1374626 : Worker crashes with EINVAL errors #1374630 : [geo-replication]: geo-rep Status is not showing bricks from one of the nodes #1374639 : glusterfs: create a directory with 0464 mode return EIO error #1374649 : Support for rc.d and init for Service management #1374841 : Implement SIMD support on EC #1375042 : bug-963541.t spurious failure #1375537 : gf_event python fails with ImportError #1375543 : [geo-rep]: defunct tar process while using tar+ssh sync #1375570 : Detach tier commit is allowed when detach tier start goes into failed state #1375914 : posix: Integrate important events with events framework #1376331 : Rpm installation fails with conflicts error for eventsconfig.json file #1376396 : /var/tmp/rpm-tmp.KPCugR: line 2: /bin/systemctl: No such file or directory #1376477 : [RFE] DHT Events #1376874 : RFE : move ganesha related configuration into shared storage #1377288 : The GlusterFS Callback RPC-calls always use RPC/XID 42 #1377386 : glusterd experiencing repeated connect/disconnect messages when shd is down #1377570 : EC: Set/unset dirty flag for all the update operations #1378814 : Files not being opened with o_direct flag during random read operation (Glusterfs 3.8.2) #1378948 : removal of file from nfs mount crashes ganesha server #1379028 : Modifications to AFR Events #1379287 : warning messages seen in glusterd logs for each 'gluster volume status' command #1379528 : Poor smallfile read performance on Arbiter volume compared to Replica 3 volume #1379707 : gfapi: Fix fd ref leaks #1379996 : Volume restart couldn't re-export the volume exported via ganesha. #1380252 : glusterd fails to start without installing glusterfs-events package #1383591 : glfs_realpath() should not return malloc()'d allocated memory #1383692 : GlusterFS fails to build on old Linux distros with linux/oom.h missing #1383913 : spurious heal info as pending heal entries never end on an EC volume while IOs are going on #1385224 : arbiter volume write performance is bad with sharding #1385236 : invalid argument warning messages seen in fuse client logs 2016-09-30 06:34:58.938667] W [dict.c:418ict_set] (-->/usr/lib64/glusterfs/3.8.4/xlator/cluster/replicate.so(+0x58722) 0-dict: !this || !value for key=link-count [Invalid argument] #1385451 : \"nfs.disable: on\" is not showing in Vol info by default for the 3.7.x volumes after updating to 3.9.0 #1386072 : Spurious permission denied problems observed #1386178 : eventsapi/georep: Events are not available for Checkpoint and Status Change #1386338 : pmap_signin event fails to update brickinfo->signed_in flag #1387099 : Boolean attributes are published as string #1387492 : Error and warning message getting while removing glusterfs-events package #1387502 : Incorrect volume type in the \"glusterd_state\" file generated using CLI \"gluster get-state\" #1387564 : [Eventing]: UUID is showing zeros in the event message for the peer probe operation. #1387894 : Regression caused by enabling client-io-threads by default #1387960 : Sequential volume start&stop is failing with SSL enabled setup. #1387964 : [Eventing]: 'gluster vol bitrot scrub ondemand' does not produce an event #1387975 : Continuous warning messages getting when one of the cluster node is down on SSL setup. #1387981 : [Eventing]: 'gluster volume tier start force' does not generate a TIER_START event #1387984 : Add a test script for compound fops changes in AFR #1387990 : [RFE] Geo-replication Logging Improvements #1388150 : geo-replica slave node goes faulty for non-root user session due to fail to locate gluster binary #1388323 : fuse mount point not accessible #1388350 : Memory Leaks in snapshot code path #1388470 : throw warning to show that older tier commands are depricated and will be removed. #1388563 : [Eventing]: 'VOLUME_REBALANCE' event messages have an incorrect volume name #1388579 : crypt: changes needed for openssl-1.1 (coming in Fedora 26) #1388731 : [GSS]glusterfind pre session hangs indefinitely in RHGS 3.1.3 #1388912 : glusterfs can't self heal character dev file for invalid dev_t parameters #1389675 : Experimental translators and 4.0 features need to be disabled for release-3.9 #1389742 : build: incorrect Requires: for portblock resource agent #1390837 : write-behind: flush stuck by former failed write #1391448 : md-cache: Invalidate cache entry in case of OPEN with O_TRUNC #1392286 : gfapi clients crash while using async calls due to double fd_unref #1392718 : Quota version not changing in the quota.conf after upgrading to 3.7.1 from 3.6.1 #1392844 : Hosted Engine VM paused post replace-brick operation #1392869 : The FUSE client log is filling up with posix_acl_default and posix_acl_access messages","title":"3.9.0"},{"location":"release-notes/3.9.0/#release-notes-for-gluster-390","text":"This is a major release that includes a huge number of changes. Many improvements contribute to better support of Gluster with containers and running your storage on the same server as your hypervisors. Lots of work has been done to integrate with other projects that are part of the Open Source storage ecosystem. The most notable features and changes are documented on this page. A full list of bugs that has been addressed is included further below.","title":"Release notes for Gluster 3.9.0"},{"location":"release-notes/3.9.0/#major-changes-and-features","text":"","title":"Major changes and features"},{"location":"release-notes/3.9.0/#introducing-reset-brick-command","text":"Notes for users: The reset-brick command provides support to reformat/replace the disk(s) represented by a brick within a volume. This is helpful when a disk goes bad etc Start reset process - gluster volume reset-brick VOLNAME HOSTNAME:BRICKPATH start The above command kills the respective brick process. Now the brick can be reformatted. To restart the brick after modifying configuration - gluster volume reset-brick VOLNAME HOSTNAME:BRICKPATH HOSTNAME:BRICKPATH commit If the brick was killed to replace the brick with same brick path, restart with following command - gluster volume reset-brick VOLNAME HOSTNAME:BRICKPATH HOSTNAME:BRICKPATH commit force Limitations: 1. resetting a brick kills a brick process in concern. During this period the brick will not be available for IO's. 2. Replacing a brick with this command will work only if both the brick paths are same and belong to same volume.","title":"Introducing reset-brick command"},{"location":"release-notes/3.9.0/#get-node-level-status-of-a-cluster","text":"Notes for users: The get-state command provides node level status of a trusted storage pool from the point of view of glusterd in a parseable format. Using get-state command, external applications can invoke the command on all nodes of the cluster, and parse and collate the data obtained from all these nodes to get a complete picture of the state of the cluster. # gluster get-state <glusterd> [odir <path/to/output/dir] [file <filename>] This would dump data points that reflect the local state representation of the cluster as maintained in glusterd (no other daemons are supported as of now) to a file inside the specified output directory. The default output directory and filename is /var/run/gluster and glusterd_state_ respectively. Following are the sections in the output: 1. Global : UUID and op-version of glusterd 2. Global options : Displays cluster specific options that have been set explicitly through the volume set command. 3. Peers : Displays the peer node information including its hostname and connection status 4. Volumes : Displays the list of volumes created on this node along with detailed information on each volume. 5. Services : Displays the list of the services configured on this node along with their corresponding statuses. Limitations: 1. This only supports glusterd. 2. Does not provide complete cluster state. Data to be collated from all nodes by external application to get the complete cluster state.","title":"Get node level status of a cluster"},{"location":"release-notes/3.9.0/#multi-threaded-self-heal-for-disperse-volumes","text":"Notes for users: Users now have the ability to configure multi-threaded self-heal in disperse volumes using the following commands: Option below can be used to control number of parallel heals in SHD # gluster volume set <volname> disperse.shd-max-threads [1-64] # default is 1 Option below can be used to control number of heals that can wait in SHD # gluster volume set <volname> disperse.shd-wait-qlength [1-65536] # default is 1024","title":"Multi threaded self-heal for Disperse volumes"},{"location":"release-notes/3.9.0/#hardware-extention-acceleration-in-disperse-volumes","text":"Notes for users: If the user has hardware that has special instructions which can be used in erasure code calculations on the client it will be automatically used. At the moment this support is added for cpu-extentions: x64 , sse , avx","title":"Hardware extention acceleration in Disperse volumes"},{"location":"release-notes/3.9.0/#lock-revocation-feature","text":"Notes for users: 1. Motivation: Prevents cluster instability by mis-behaving clients causing bricks to OOM due to inode/entry lock pile-ups. 2. Adds option to strip clients of entry/inode locks after N seconds 3. Adds option to clear ALL locks should the revocation threshold get hit 4. Adds option to clear all or granted locks should the max-blocked threshold get hit (can be used in combination w/ revocation-clear-all). 5. Adds logging to indicate revocation event & reason 6. Options are: # gluster volume set <volname> features.locks-revocation-secs <integer; 0 to disable> # gluster volume set <volname> features.locks-revocation-clear-all [on/off] # gluster volume set <volname> features.locks-revocation-max-blocked <integer>","title":"Lock revocation feature"},{"location":"release-notes/3.9.0/#on-demand-scrubbing-for-bitrot-detection","text":"Notes for users: With 'ondemand' scrub option, you don't need to wait for the scrub-frequency to expire. As the option name itself says, the scrubber can be initiated on demand to detect the corruption. If the scrubber is already running, this option is a no op. # gluster volume bitrot <volume-name> scrub ondemand ### Improvements in Gluster NFS-Ganesha integration Notes for users: With this release the major change done is to store all the ganesha related configuration files in the shared storage volume mount point instead of having separate local copy in '/etc/ganesha' folder on each node. For new users, before enabling nfs-ganesha create a directory named nfs-ganesha in the shared storage mount point ( /var/run/gluster/shared_storage/ ) Create ganesha.conf & ganesha-ha.conf in that directory with the required details filled in. For existing users, before starting nfs-ganesha service do the following : Copy all the contents of /etc/ganesha directory (including .export_added file) to /var/run/gluster/shared_storage/nfs-ganesha from any of the ganesha nodes Create symlink using /var/run/gluster/shared_storage/nfs-ganesha/ganesha.conf on /etc/ganesha one each node in ganesha-cluster Change path for each export entry in ganesha.conf file Example: if a volume \"test\" was exported, then ganesha.conf shall have below export entry - %include \"/etc/ganesha/exports/export.test.conf\" export entry. Change that line to %include \"/var/run/gluster/shared_storage/nfs-ganesha/exports/export.test.conf\" In addition, following changes have been made - * The entity \"HA_VOL_SERVER= \" in ganesha-ha.conf is no longer required. * A new resource-agent called portblock (available in >= resource-agents-3.9.5 package) is added to the cluster configuration to speed up the nfs-client connections post IP failover or failback. This may be noticed while looking at the cluster configuration status using the command pcs status .","title":"On demand scrubbing for Bitrot Detection:"},{"location":"release-notes/3.9.0/#availability-of-python-bindings-to-libgfapi","text":"The official python bindings for GlusterFS libgfapi C library interface is mostly API complete. The complete API reference and documentation can be found at libgfapi-python.rtfd.io The python bindings have been packaged and has been made available over PyPI .","title":"Availability of python bindings to libgfapi"},{"location":"release-notes/3.9.0/#small-file-improvements-in-gluster-with-md-cache-experimental","text":"Notes for users: With this release, metadata cache on the client side is integrated with the cache-invalidation feature so that the clients can cache longer without compromising on consistency. By enabling, the metadata cache and cache invalidation feature and extending the cache timeout to 600s, we have seen performance improvements in metadata operation like creates, ls/stat, chmod, rename, delete. The perf improvements is significant in SMB access of gluster volume, but as a cascading effect the improvements is also seen on FUSE/Native access and NFS access. Use the below options in the order mentioned, to enable the features: # gluster volume set <volname> features.cache-invalidation on # gluster volume set <volname> features.cache-invalidation-timeout 600 # gluster volume set <volname> performance.stat-prefetch on # gluster volume set <volname> performance.cache-invalidation on # gluster volume set <volname> performance.cache-samba-metadata on # Only for SMB access # gluster volume set <volname> performance.md-cache-timeout 600","title":"Small file improvements in Gluster with md-cache (Experimental)"},{"location":"release-notes/3.9.0/#real-time-cluster-notifications-using-events-apis","text":"Let us imagine we have a Gluster monitoring system which displays list of volumes and its state, to show the realtime status, monitoring app need to query the Gluster in regular interval to check volume status, new volumes etc. Assume if the polling interval is 5 seconds then monitoring app has to run gluster volume info command ~17000 times a day! With Gluster 3.9 release, Gluster provides close to realtime notification and alerts for the Gluster cluster state changes and alerts. Webhooks can be registered to listen to Events emitted by Gluster. More details about this new feature is available here. http://docs.gluster.org/en/latest/Administrator%20Guide/Events%20APIs","title":"Real time Cluster notifications using Events APIs"},{"location":"release-notes/3.9.0/#geo-replication-improvements","text":"","title":"Geo-replication improvements"},{"location":"release-notes/3.9.0/#documentation-improvements","text":"Upstream documentation is rewritten to reflect the latest version of Geo-replication. Removed the stale/duplicate documentation. We are still working on to add Troubleshooting, Cluster expand/shrink notes to it. Latest version of documentation is available here http://docs.gluster.org/en/latest/Administrator%20Guide/Geo%20Replication","title":"Documentation improvements:"},{"location":"release-notes/3.9.0/#geo-replication-events-are-available-for-events-api-consumers","text":"Events APIs is the new Gluster feature available with 3.9 release, most of the events from Geo-replication are added to eventsapi. Read more about the Events APIs and Geo-replication events here http://docs.gluster.org/en/latest/Administrator%20Guide/Events%20APIs","title":"Geo-replication Events are available for Events API consumers:"},{"location":"release-notes/3.9.0/#new-simplified-command-to-setup-non-root-geo-replication","text":"Non root Geo-replication setup was not easy with multiple manual steps. Non root Geo-replication steps are simplified. Read more about the new steps in Admin guide. http://docs.gluster.org/en/latest/Administrator%20Guide/Geo%20Replication/#slave-user-setup","title":"New simplified command to setup Non root Geo-replication"},{"location":"release-notes/3.9.0/#new-command-to-generate-ssh-keysalternative-command-to-gsec_create","text":"gluster system:: execute gsec_create command generates ssh keys in every Master cluster nodes and copies to initiated node. This command silently ignores error if any node is down in cluster. It will not collect SSH keys from that node. When Geo-rep create push-pem command is issued it will copy public keys from those nodes which were up during gsec_create. This causes Geo-rep to go to Faulty when that master node tries to make the connection to slave nodes. With the new command, output shows if any Master node was down while generating ssh keys. Read more about `gluster-georep-sshkey http://docs.gluster.org/en/latest/Administrator%20Guide/Geo%20Replication/#setting-up-the-environment-for-geo-replication","title":"New command to generate SSH keys(Alternative command to gsec_create)"},{"location":"release-notes/3.9.0/#logging-improvements","text":"New logs are added, now from the log we can clearly understand what is going on. Note: This feature may change logging format of existing log messages, Please update your parsers if used to parse Geo-rep logs. Patch: http://review.gluster.org/15710","title":"Logging improvements"},{"location":"release-notes/3.9.0/#new-configuration-options-available-changelog-log-level","text":"All the changelog related log messages are logged in /var/log/glusterfs/geo-replication/<SESSION>/*.changes.log in Master nodes. Log level was hard coded as TRACE for Changelog logs. New configuration option provided to modify the changelog log level and defaulted to INFO","title":"New Configuration options available: changelog-log-level"},{"location":"release-notes/3.9.0/#behavior-changes","text":"#1221623 : Earlier the ports GlusterD used to allocate for the daemons like brick processes, quotad, shd et all were persistent through the volume's life cycle, so every restart of the process(es) or a node reboot will try to use the same ports which were allocated for the first time. With release-3.9 onwards, GlusterD will try to allocate a fresh port once a daemon is restarted or the node is rebooted. #1348944 : with 3.9 release the default log file for glusterd has been renamed to glusterd.log from etc-glusterfs-glusterd.vol.log","title":"Behavior changes"},{"location":"release-notes/3.9.0/#known-issues","text":"#1387878 :add-brick on a vm-store configuration which has sharding enabled is leading to vm corruption. To work around this issue, one can scale up by creating more volumes until this issue is fixed.","title":"Known Issues"},{"location":"release-notes/3.9.0/#bugs-addressed","text":"A total of 571 patches has been sent, addressing 422 bugs: #762184 : Support mandatory locking in glusterfs #789278 : Issues reported by Coverity static analysis tool #1005257 : [PATCH] Small typo fixes #1175711 : posix: Set correct d_type for readdirp() calls #1193929 : GlusterFS can be improved #1198849 : Minor improvements and cleanup for the build system #1200914 : pathinfo is wrong for striped replicated volumes #1202274 : Minor improvements and code cleanup for libgfapi #1207604 : [rfe] glusterfs snapshot cli commands should provide xml output. #1211863 : RFE: Support in md-cache to use upcall notifications to invalidate its cache #1221623 : glusterd: add brick command should re-use the port for listening which is freed by remove-brick. #1222915 : usage text is wrong for use-readdirp mount default #1223937 : Outdated autotools helper config.* files #1225718 : [FEAT] DHT - rebalance - rebalance status o/p should be different for 'fix-layout' option, it should not show 'Rebalanced-files' , 'Size', 'Scanned' etc as it is not migrating any files. #1227667 : Minor improvements and code cleanup for protocol server/client #1228142 : clang-analyzer: adding clang static analysis support #1231224 : Misleading error messages on brick logs while creating directory (mkdir) on fuse mount #1236009 : do an explicit lookup on the inodes linked in readdirp #1254067 : remove unused variables #1266876 : cluster/afr: AFR2 returns empty readdir results to clients if brick is added back into cluster after re-imaging/formatting #1278325 : DHT: Once remove brick start failed in between Remove brick commit should not be allowed #1285152 : store afr pending xattrs as a volume option #1292020 : quota: client gets IO error instead of disk quota exceed when the limit is exceeded #1294813 : [geo-rep]: Multiple geo-rep session to the same slave is allowed for different users #1296043 : Wrong usage of dict functions #1302277 : Wrong XML output for Volume Options #1302948 : tar complains: : file changed as we read it #1303668 : packaging: rpmlint warning and errors - Documentation URL 404 #1305031 : AFR winds a few reads of a file in metadata split-brain. #1306398 : Tiering and AFR may result in data loss #1311002 : NFS+attach tier:IOs hang while attach tier is issued #1311926 : [georep]: If a georep session is recreated the existing files which are deleted from slave doesn't get sync again from master #1315666 : Data Tiering:tier volume status shows as in-progress on all nodes of a cluster even if the node is not part of volume #1316178 : changelog/rpc: Memory leak- rpc_clnt_t object is never freed #1316389 : georep: tests for logrotate, create+rename and hard-link rename #1318204 : Input / Output when chmoding files on NFS mount point #1318289 : [RFE] Add arbiter brick hotplug #1318591 : Glusterd not operational due to snapshot conflicting with nfs-ganesha export file in \"/var/lib/glusterd/snaps\" #1319992 : RFE: Lease support for gluster #1320388 : [GSS]-gluster v heal volname info does not work with enabled ssl/tls #1321836 : gluster volume info --xml returns 0 for nonexistent volume #1322214 : [HC] Add disk in a Hyper-converged environment fails when glusterfs is running in directIO mode #1322805 : [scale] Brick process does not start after node reboot #1322825 : IO-stats, client profile is overwritten when it is on the same node as bricks #1324439 : SAMBA+TIER : Wrong message display.On detach tier success the message reflects Tier command failed. #1325831 : gluster snap status xml output shows incorrect details when the snapshots are in deactivated state #1326410 : /var/lib/glusterd/$few-directories not owned by any package, causing it to remain after glusterfs-server is uninstalled #1327171 : Disperse: Provide description of disperse.eager-lock option. #1328224 : RFE : Feature: Automagic unsplit-brain policies for AFR #1329211 : values for Number of Scrubbed files, Number of Unsigned files, Last completed scrub time and Duration of last scrub are shown as zeros in bit rot scrub status #1330032 : rm -rf to a dir gives directory not empty(ENOTEMPTY) error #1330097 : ganesha exported volumes doesn't get synced up on shutdown node when it comes up. #1330583 : glusterfs-libs postun ldconfig: relative path `1' used to build cache #1331254 : Disperse volume fails on high load and logs show some assertion failures #1331287 : No xml output on gluster volume heal info command with --xml #1331323 : [Granular entry sh] - Implement renaming of indices in index translator #1331423 : distaf: Add io_libs to namespace package list #1331720 : implement meta-lock/unlock for lock migration #1331721 : distaf: Add README and HOWTO to distaflibs as well #1331860 : Wrong constant used in length based comparison for XATTR_SECURITY_PREFIX #1331969 : Ganesha+Tiering: Continuous \"0-glfs_h_poll_cache_invalidation: invalid argument\" messages getting logged in ganesha-gfapi logs. #1332020 : multiple regression failures for tests/basic/quota-ancestry-building.t #1332021 : multiple failures for testcase: tests/basic/inode-quota-enforcing.t #1332054 : multiple failures of tests/bugs/disperse/bug-1236065.t #1332073 : EINVAL errors while aggregating the directory size by quotad #1332134 : bitrot: Build generates Compilation Warning. #1332136 : Detach tier fire before the background fixlayout is complete may result in failure #1332156 : SMB:while running I/O on cifs mount and doing graph switch causes cifs mount to hang. #1332219 : tier: avoid pthread_join if pthread_create fails #1332413 : Wrong op-version for mandatory-locks volume set option #1332419 : geo-rep: address potential leak of memory #1332460 : [features/worm] - when disabled, worm xl should simply pass requested fops to its child xl #1332465 : glusterd + bitrot : Creating clone of snapshot. error \"xlator.c:148:xlator_volopt_dynload] 0-xlator: /usr/lib64/glusterfs/3.7.9/xlator/features/bitrot.so: cannot open shared object file: #1332473 : tests: 'tests/bitrot/br-state-check.t' fails in netbsd #1332501 : Mandatory locks are not migrated during lock migration #1332566 : [granular entry sh] - Add more tests #1332798 : [AFR]: \"volume heal info\" command is failing during in-service upgrade to latest. #1332822 : distaf: Add library functions for gluster snapshot operations #1332885 : distaf: Add library functions for gluster bitrot operations and generic library utility functions generic to all components #1332952 : distaf: Add library functions for gluster quota operations #1332994 : Self Heal fails on a replica3 volume with 'disk quota exceeded' #1333023 : readdir-ahead does not fetch xattrs that md-cache needs in it's internal calls #1333043 : Fix excessive logging due to NULL dict in dht #1333263 : [features/worm] Unwind FOPs with op_errno and add gf_worm prefix to functions #1333317 : rpc_clnt will sometimes not reconnect when using encryption #1333319 : Unexporting a volume sometimes fails with \"Dynamic export addition/deletion failed\". #1333370 : [FEAT] jbr-server handle lock/unlock fops #1333738 : distaf: Add GlusterBaseClass (gluster_base_class.py) to distaflibs-gluster. #1333912 : client ID should logged when SSL connection fails #1333925 : libglusterfs: race conditions and illegal mem access in timer #1334044 : [RFE] Eventing for Gluster #1334164 : Worker dies with [Errno 5] Input/output error upon creation of entries at slave #1334208 : distaf: Add library functions for gluster rebalance operations #1334269 : GlusterFS 3.8 fails to build in the CentOS Community Build System #1334270 : glusterd: glusterd provides stale port information when a volume is recreated with same brick path #1334285 : Under high read load, sometimes the message \"XDR decoding failed\" appears in the logs and read fails #1334314 : changelog: changelog_rollover breaks when number of fds opened is more than 1024 #1334444 : SAMBA-VSS : Permission denied issue while restoring the directory from windows client 1 when files are deleted from windows client 2 #1334620 : stop all gluster processes should also include glusterfs mount process #1334621 : set errno in case of inode_link failures #1334721 : distaf: Add library functions for gluster tiering operations #1334839 : [Tiering]: Files remain in hot tier even after detach tier completes #1335019 : Add graph for decompounder xlator #1335091 : mount/fuse: Logging improvements #1335231 : features/locks: clang compile warning in posix.c #1335232 : features/index: clang compile warnings in index.c #1335429 : Self heal shows different information for the same volume from each node #1335494 : Modifying peer ops library #1335531 : Modified volume options are not syncing once glusterd comes up. #1335652 : Heal info shows split-brain for .shard directory though only one brick was down #1335717 : PREFIX is not honoured during build and install #1335776 : rpc: change client insecure port ceiling from 65535 to 49151 #1335818 : Revert \"features/shard: Make o-direct writes work with sharding: http://review.gluster.org/#/c/13846/\" #1335858 : Files present in the .shard folder even after deleting all the vms from the UI #1335973 : [Tiering]: The message 'Max cycle time reached..exiting migration' incorrectly displayed as an 'error' in the logs #1336197 : failover is not working with latest builds. #1336328 : [FEAT] jbr: Improve code modularity #1336354 : Provide a way to configure gluster source location in devel-vagrant #1336373 : Distaf: Add gluster specific config file #1336381 : ENOTCONN error during parallel rmdir #1336508 : rpc-transport: compiler warning format string #1336612 : one of vm goes to paused state when network goes down and comes up back #1336630 : ERROR and Warning message on writing a file from mount point \"null gfid for path (null)\" repeated 3 times between\" #1336642 : [RFE] git-branch-diff: wrapper script for git to visualize backports #1336698 : DHT : few Files are not accessible and not listed on mount + more than one Directory have same gfid + (sometimes) attributes has ?? in ls output after renaming Directories from multiple client at same time #1336793 : assorted typos and spelling mistakes from Debian lintian #1336818 : Add ability to set oom_score_adj for glusterfs process #1336853 : scripts: bash-isms in scripts #1336945 : [NFS-Ganesha] : stonith-enabled option not set with new versions of cman,pacemaker,corosync and pcs #1337160 : distaf: Added libraries to setup nfs-ganesha in gluster through distaf #1337227 : [tiering]: error message shown during the failure of detach tier commit isn't intuitive #1337405 : Some of VMs go to paused state when there is concurrent I/O on vms #1337473 : upgrade path when slave volume uuid used in geo-rep session #1337597 : Mounting a volume over NFS with a subdir followed by a / returns \"Invalid argument\" #1337650 : log flooded with Could not map name=xxxx to a UUID when config'd with long hostnames #1337777 : tests/bugs/write-behind/1279730.t fails spuriously #1337791 : tests/basic/afr/tarissue.t fails regression #1337899 : Misleading error message on rebalance start when one of the glusterd instance is down #1338544 : fuse: In fuse_first_lookup(), dict is not un-referenced in case create_frame returns an empty pointer. #1338634 : AFR : fuse,nfs mount hangs when directories with same names are created and deleted continuously #1338733 : __inode_ctx_put: fix mem leak on failure #1338967 : common-ha: ganesha.nfsd not put into NFS-GRACE after fail-back #1338991 : DHT2: Tracker bug #1339071 : dht/rebalance: mark hardlink migration failure as skipped for rebalance process #1339149 : Error and warning messages related to xlator/features/snapview-client.so adding up to the client log on performing IO operations #1339166 : distaf: Added timeout value to wait for rebalance to complete and removed older rebalance library file #1339181 : Full heal of a sub-directory does not clean up name-indices when granular-entry-heal is enabled. #1339214 : gfapi: set mem_acct for the variables created for upcall #1339471 : [geo-rep]: Worker died with [Errno 2] No such file or directory #1339472 : [geo-rep]: Monitor crashed with [Errno 3] No such process #1339541 : Added libraries to setup CTDB in gluster through distaf #1339553 : gfapi: in case of handle based APIs, close glfd after successful create #1339689 : RFE - capacity info (df -h on a mount) is incorrect for a tiered volume #1340488 : copy-export-ganesha.sh does not have a correct shebang #1340623 : Directory creation(mkdir) fails when the remove brick is initiated for replicated volumes accessing via nfs-ganesha #1340853 : [geo-rep]: If the session is renamed, geo-rep configuration are not retained #1340936 : Automount fails because /sbin/mount.glusterfs does not accept the -s option #1341007 : gfapi : throwing warning message for unused variable in glfs_h_find_handle() #1341009 : Log parameters such as the gfid, fd address, offset and length of the reads upon failure for easier debugging #1341294 : build: RHEL7 unpackaged files /var/lib/glusterd/hooks/.../S57glusterfind-delete-post.{pyc,pyo} #1341474 : [geo-rep]: Snapshot creation having geo-rep session is broken #1341650 : conservative merge happening on a x3 volume for a deleted file #1341768 : After setting up ganesha on RHEL 6, nodes remains in stopped state and grace related failures observed in pcs status #1341796 : [quota+snapshot]: Directories are inaccessible from activated snapshot, when the snapshot was created during directory creation #1342171 : O_DIRECT support for sharding #1342259 : [features/worm] - write FOP should pass for the normal files #1342298 : reading file with size less than 512 fails with odirect read #1342356 : [RFE] Python library for creating Cluster aware CLI tools for Gluster #1342420 : [georep]: Stopping volume fails if it has geo-rep session (Even in stopped state) #1342796 : self heal deamon killed due to oom kills on a dist-disperse volume using nfs ganesha #1342979 : [geo-rep]: Add-Brick use case: create push-pem force on existing geo-rep fails #1343038 : IO ERROR when multiple graph switches #1343286 : enabling glusternfs with nfs.rpc-auth-allow to many hosts failed #1343333 : [RFE] Simplify Non Root Geo-replication Setup #1343374 : Gluster fuse client crashed generating core dump #1343838 : Implement API to get page aligned iobufs in iobuf.c #1343906 : [Stress/Scale] : I/O errors out from gNFS mount points during high load on an erasure coded volume,Logs flooded with Error messages. #1343943 : Old documentation link in log during Geo-rep MISCONFIGURATION #1344277 : [disperse] mkdir after re balance give Input/Output Error #1344340 : Unsafe access to inode->fd_list #1344396 : fd leak in disperse #1344407 : fail delete volume operation if one of the glusterd instance is down in cluster #1344686 : tiering : Multiple brick processes crashed on tiered volume while taking snapshots #1344714 : removal of file from nfs mount crashs ganesha server #1344836 : [Disperse volume]: IO hang seen on mount with file ops #1344885 : inode leak in brick process #1345727 : Bricks are starting when server quorum not met. #1345744 : [geo-rep]: Worker crashed with \"KeyError: \" #1345748 : SAMBA-DHT : Crash seen while rename operations in cifs mount and windows access of share mount #1345846 : quota : rectify quota-deem-statfs default value in gluster v set help command #1345855 : Possible crash due to a timer cancellation race #1346138 : [RFE] Non root Geo-replication Error logs improvements #1346211 : cleanup glusterd-georep code #1346551 : wrong understanding of function's parameter #1346719 : [Disperse] dd + rm + ls lead to IO hang #1346821 : cli core dumped while providing/not wrong values during arbiter replica volume #1347249 : libgfapi : variables allocated by glfs_set_volfile_server is not freed #1347354 : glusterd: SuSE build system error for incorrect strcat, strncat usage #1347686 : IO error seen with Rolling or non-disruptive upgrade of an distribute-disperse(EC) volume from 3.7.5 to 3.7.9 #1348897 : Add relative path validation for gluster copy file utility #1348904 : [geo-rep]: If the data is copied from .snaps directory to the master, it doesn't get sync to slave [First Copy] #1348944 : Change the glusterd log file name to glusterd.log #1349270 : ganesha.enable remains on in volume info file even after we disable nfs-ganesha on the cluster. #1349273 : Geo-rep silently ignores config parser errors #1349276 : Buffer overflow when attempting to create filesystem using libgfapi as driver on OpenStack #1349284 : [tiering]: Files of size greater than that of high watermark level should not be promoted #1349398 : nfs-ganesha disable doesn't delete nfs-ganesha folder from /var/run/gluster/shared_storage #1349657 : process glusterd set TCP_USER_TIMEOUT failed #1349709 : Polling failure errors getting when volume is started&stopped with SSL enabled setup. #1349723 : Added libraries to get server_brick dictionaries #1350017 : Change distaf glusterbase class and mount according to the config file changes #1350168 : distaf: made changes to create_volume function #1350173 : distaf: Adding samba_ops library #1350188 : distaf: minor import changes in ganesha.py #1350191 : race condition when set ctx->timer in function gf_timer_registry_init #1350237 : Gluster/NFS does not accept dashes in hostnames in exports/netgroups files #1350245 : distaf: Add library functions for gluster volume operations #1350248 : distaf: Modified get_pathinfo function in lib_utils.py #1350256 : Distaf: Modifying the ctdb_libs to get server host from the server dict #1350258 : Distaf: add a sample test case to the framework #1350327 : Protocol client not mounting volumes running on older versions. #1350371 : ganesha/glusterd : remove 'HA_VOL_SERVER' from ganesha-ha.conf #1350383 : distaf: Modified distaf gluster config file #1350427 : distaf: Modified tier_attach() to get bricks path for attaching tier from the available bricks in server #1350744 : GlusterFS 3.9.0 tracker #1350793 : build: remove absolute paths from glusterfs spec file #1350867 : RFE: FEATURE: Lock revocation for features/locks xlator #1351021 : [DHT]: Rebalance info for remove brick operation is not showing after glusterd restart #1351071 : [geo-rep] Stopped geo-rep session gets started automatically once all the master nodes are upgraded #1351134 : [SSL] : gluster v set help does not show ssl options #1351537 : [Bitrot] Need a way to set scrub interval to a minute, for ease of testing #1351880 : gluster volume status client\" isn't showing any information when one of the nodes in a 3-way Distributed-Replicate volume is shut down #1352019 : RFE: Move throttling code to libglusterfs from bitrot #1352277 : a two node glusterfs seems not possible anymore?! #1352279 : [scale]: Bricks not started after node reboot. #1352423 : should find_library(\"c\") be used instead of find_library(\"libc\") in geo-replication/syncdaemon/libcxattr.py? #1352634 : qemu libgfapi clients hang when doing I/O #1352671 : RFE: As a part of xattr invalidation, send the stat info as well #1352854 : GlusterFS - Memory Leak - High Memory Utilization #1352871 : [Bitrot]: Scrub status- Certain fields continue to show previous run's details, even if the current run is in progress #1353156 : [RFE] CLI to get local state representation for a cluster #1354141 : several problems found in failure handle logic #1354221 : noisy compilation warnning with Wstrict-prototypes #1354372 : Fix timing issue in tests/bugs/glusterd/bug-963541.t #1354439 : nfs client I/O stuck post IP failover #1354489 : service file is executable #1355604 : afr coverity fixes #1355628 : Upgrade from 3.7.8 to 3.8.1 doesn't regenerate the volfiles #1355706 : [Bitrot]: Sticky bit files considered and skipped by the scrubber, instead of getting ignored. #1355956 : RFE : move ganesha related configuration into shared storage #1356032 : quota: correct spelling mistakes in quota src files #1356068 : observing \" Too many levels of symbolic links\" after adding bricks and then issuing a replace brick #1356504 : Move gf_log->gf_msg in index feature #1356508 : [RFE] Handle errors during SSH key generation(gsec_create) #1356528 : memory leak in glusterd-georeplication #1356851 : [Bitrot+Sharding] Scrub status shows incorrect values for 'files scrubbed' and 'files skipped' #1356868 : File not found errors during rpmbuild: /var/lib/glusterd/hooks/1/delete/post/S57glusterfind-delete-post.py{c,o} #1356888 : Correct code in socket.c to avoid fd leak #1356998 : syscalls: readdir_r() is deprecated in newer glibc #1357210 : add several fops support in io-threads #1357226 : add a basis function to reduce verbose code #1357397 : Trash translator fails to create 'internal_op' directory under already existing trash directory #1357463 : Error: quota context not set inode (gfid:nnn) [Invalid argument] #1357490 : libglusterfs : update correct memory segments in glfs-message-id #1357821 : Make install fails second time without uninstall #1358114 : tests: ./tests/bitrot/br-stub.t fails intermittently #1358195 : Fix spurious failure of tests/bugs/glusterd/bug-1111041.t #1358196 : Tiering related core observed with \"uuid_is_null () message\". #1358244 : [SNAPSHOT]: The PID for snapd is displayed even after snapd process is killed. #1358594 : Enable gfapi test cases in Gluster upstream regression #1358608 : Memory leak observed with upcall polling #1358671 : Add Events for Volume Set and Reset #1358922 : missunderstanding about GF_PROTOCOL_DICT_SERIALIZE #1358936 : coverity: iobuf_get_page_aligned calling iobuf_get2 should check the return pointer #1358944 : jbr resource leak, forget free \"path\" #1358976 : Fix spurious failures in split-brain-favorite-child-policy.t #1359001 : Fix spurious failures in ec.t #1359190 : Glusterd crashes upon receiving SIGUSR1 #1359370 : glfs: fix glfs_set_volfile_server doc #1359711 : [GSS] Rebalance crashed #1359717 : Fix failure of ./tests/bugs/snapshot/bug-1316437.t #1360169 : Fix bugs in compound fops framework #1360401 : RFE: support multiple bricks within one process #1360402 : Clients can starve under heavy load #1360647 : gfapi: deprecate the rdma support for management connections #1360670 : Add output option --xml to man page of gluster #1360679 : Bricks doesn't come online after reboot [ Brick Full ] #1360682 : tests: ./tests/bitrot/bug-1244613.t fails intermittently #1360693 : [RFE] Add a count of snapshots associated with a volume to the output of the vol info command #1360809 : [RFE] Capture events in GlusterD #1361094 : Auto generate header files during Make #1361249 : posix: leverage FALLOC_FL_ZERO_RANGE in zerofill fop #1361300 : Direct io to sharded files fails when on zfs backend #1361678 : thread CPU saturation limiting throughput on write workloads #1361983 : Move USE_EVENTS in gf_events API #1361999 : Remove ganesha xlator code from gluster code base #1362144 : Python library to send Events #1362151 : [libgfchangelog]: If changelogs are not available for the requested time range, no proper error message #1362397 : Mem leak in meta_default_readv in meta xlators #1362520 : Per xlator logging not working #1362602 : [Open SSL] : Unable to mount an SSL enabled volume via SMB v3/Ganesha v4 #1363591 : Geo-replication user driven Events #1363721 : [HC]: After bringing down and up of the bricks VM's are getting paused #1363948 : Spurious failure in tests/bugs/glusterd/bug-1089668.t #1364026 : glfs_fini() crashes with SIGSEGV #1364420 : [RFE] History Crawl performance improvement #1364449 : posix: honour fsync flags in posix_do_zerofill #1364529 : api: revert glfs_ipc_xd intended for 4.0 #1365455 : [AFR]: Files not available in the mount point after converting Distributed volume type to Replicated one. #1365489 : glfs_truncate missing #1365506 : gfapi: use const qualifier for glfs_*timens() #1366195 : [Bitrot - RFE]: On demand scrubbing option to scrub #1366222 : \"heal info --xml\" not showing the brick name of offline bricks. #1366226 : Move alloca0 definition to common-utils #1366284 : fix bug in protocol/client lookup callback #1367258 : Log EEXIST errors at DEBUG level #1367478 : Second gluster volume is offline after daemon restart or server reboot #1367527 : core: use for makedev(3), major(3), minor(3) #1367665 : rotated FUSE mount log is using to populate the information after log rotate. #1367771 : Introduce graceful mode in stop-all-gluster-processes.sh #1367774 : Support for Client side Events #1367815 : [Bitrot - RFE]: Bitrot Events #1368042 : make fails if Events APIs are disabled #1368349 : tests/bugs/cli/bug-1320388.t: Infrequent failures #1368451 : [RFE] Implement multi threaded self-heal for ec volumes #1368842 : Applications not calling glfs_h_poll_upcall() have upcall events cached for no use #1368882 : log level set in glfs_set_logging() does not work #1368931 : [ RFE] Quota Events #1368953 : spurious netbsd run failures in tests/basic/glusterd/volfile_server_switch.t #1369124 : fix unused variable warnings from out-of-tree builds generate XDR headers and source files i... #1369331 : Memory leak with a replica 3 arbiter 1 configuration #1369401 : NetBSD hangs at /tests/features/lock_revocation.t #1369430 : Track the client that performed readdirp #1369432 : IATT cache invalidation should be sent when permission changes on file #1369524 : segment fault while join thread reaper_thr in fini() #1369530 : protocol/server: readlink rsp xdr failed while readlink got an error #1369638 : DHT stale layout issue will be seen often with md-cache prolonged cache of lookups #1369721 : EventApis will not work if compiled using ./configure --disable-glupy #1370053 : fix EXPECT_WITHIN #1370074 : Fix mistakes in self-heald.t #1370406 : build: eventtypes.h is missing #1370445 : Geo-replication server side events #1370862 : dht: fix the broken build #1371541 : Spurious regressions in ./tests/bugs/gfapi/bug-1093594.t #1371543 : Add cache invalidation stat in profile info #1371775 : gluster system:: uuid get hangs #1372278 : [RFE] Provide snapshot events for the new eventing framework #1372586 : Fix the test case http://review.gluster.org/#/c/15385/ #1372686 : [RFE]Reducing number of network round trips #1373529 : Node remains in stopped state in pcs status with \"/usr/lib/ocf/resource.d/heartbeat/ganesha_mon: line 137: [: too many arguments ]\" messages in logs. #1373735 : Event pushed even if Answer is No in the Volume Stop and Delete prompt #1373740 : [RFE]: events from protocol server #1373743 : [RFE]: AFR events #1374153 : [RFE] History Crawl performance improvement #1374167 : disperse: Integrate important events with events framework #1374278 : rpc/xdr: generated files are filtered with a sed extended regex #1374298 : \"gluster vol status all clients --xml\" doesn't generate xml if there is a failure in between #1374324 : [RFE] Tier Events #1374567 : [Bitrot]: Recovery fails of a corrupted hardlink (and the corresponding parent file) in a disperse volume #1374581 : Geo-rep worker Faulty with OSError: [Errno 21] Is a directory #1374597 : [geo-rep]: AttributeError: 'Popen' object has no attribute 'elines' #1374608 : geo-replication *changes.log does not respect the log-level configured #1374626 : Worker crashes with EINVAL errors #1374630 : [geo-replication]: geo-rep Status is not showing bricks from one of the nodes #1374639 : glusterfs: create a directory with 0464 mode return EIO error #1374649 : Support for rc.d and init for Service management #1374841 : Implement SIMD support on EC #1375042 : bug-963541.t spurious failure #1375537 : gf_event python fails with ImportError #1375543 : [geo-rep]: defunct tar process while using tar+ssh sync #1375570 : Detach tier commit is allowed when detach tier start goes into failed state #1375914 : posix: Integrate important events with events framework #1376331 : Rpm installation fails with conflicts error for eventsconfig.json file #1376396 : /var/tmp/rpm-tmp.KPCugR: line 2: /bin/systemctl: No such file or directory #1376477 : [RFE] DHT Events #1376874 : RFE : move ganesha related configuration into shared storage #1377288 : The GlusterFS Callback RPC-calls always use RPC/XID 42 #1377386 : glusterd experiencing repeated connect/disconnect messages when shd is down #1377570 : EC: Set/unset dirty flag for all the update operations #1378814 : Files not being opened with o_direct flag during random read operation (Glusterfs 3.8.2) #1378948 : removal of file from nfs mount crashes ganesha server #1379028 : Modifications to AFR Events #1379287 : warning messages seen in glusterd logs for each 'gluster volume status' command #1379528 : Poor smallfile read performance on Arbiter volume compared to Replica 3 volume #1379707 : gfapi: Fix fd ref leaks #1379996 : Volume restart couldn't re-export the volume exported via ganesha. #1380252 : glusterd fails to start without installing glusterfs-events package #1383591 : glfs_realpath() should not return malloc()'d allocated memory #1383692 : GlusterFS fails to build on old Linux distros with linux/oom.h missing #1383913 : spurious heal info as pending heal entries never end on an EC volume while IOs are going on #1385224 : arbiter volume write performance is bad with sharding #1385236 : invalid argument warning messages seen in fuse client logs 2016-09-30 06:34:58.938667] W [dict.c:418ict_set] (-->/usr/lib64/glusterfs/3.8.4/xlator/cluster/replicate.so(+0x58722) 0-dict: !this || !value for key=link-count [Invalid argument] #1385451 : \"nfs.disable: on\" is not showing in Vol info by default for the 3.7.x volumes after updating to 3.9.0 #1386072 : Spurious permission denied problems observed #1386178 : eventsapi/georep: Events are not available for Checkpoint and Status Change #1386338 : pmap_signin event fails to update brickinfo->signed_in flag #1387099 : Boolean attributes are published as string #1387492 : Error and warning message getting while removing glusterfs-events package #1387502 : Incorrect volume type in the \"glusterd_state\" file generated using CLI \"gluster get-state\" #1387564 : [Eventing]: UUID is showing zeros in the event message for the peer probe operation. #1387894 : Regression caused by enabling client-io-threads by default #1387960 : Sequential volume start&stop is failing with SSL enabled setup. #1387964 : [Eventing]: 'gluster vol bitrot scrub ondemand' does not produce an event #1387975 : Continuous warning messages getting when one of the cluster node is down on SSL setup. #1387981 : [Eventing]: 'gluster volume tier start force' does not generate a TIER_START event #1387984 : Add a test script for compound fops changes in AFR #1387990 : [RFE] Geo-replication Logging Improvements #1388150 : geo-replica slave node goes faulty for non-root user session due to fail to locate gluster binary #1388323 : fuse mount point not accessible #1388350 : Memory Leaks in snapshot code path #1388470 : throw warning to show that older tier commands are depricated and will be removed. #1388563 : [Eventing]: 'VOLUME_REBALANCE' event messages have an incorrect volume name #1388579 : crypt: changes needed for openssl-1.1 (coming in Fedora 26) #1388731 : [GSS]glusterfind pre session hangs indefinitely in RHGS 3.1.3 #1388912 : glusterfs can't self heal character dev file for invalid dev_t parameters #1389675 : Experimental translators and 4.0 features need to be disabled for release-3.9 #1389742 : build: incorrect Requires: for portblock resource agent #1390837 : write-behind: flush stuck by former failed write #1391448 : md-cache: Invalidate cache entry in case of OPEN with O_TRUNC #1392286 : gfapi clients crash while using async calls due to double fd_unref #1392718 : Quota version not changing in the quota.conf after upgrading to 3.7.1 from 3.6.1 #1392844 : Hosted Engine VM paused post replace-brick operation #1392869 : The FUSE client log is filling up with posix_acl_default and posix_acl_access messages","title":"Bugs addressed"},{"location":"release-notes/4.0.0/","text":"Release notes for Gluster 4.0.0 The Gluster community celebrates 13 years of development with this latest release, Gluster 4.0. This release enables improved integration with containers, an enhanced user experience, and a next-generation management framework. The 4.0 release helps cloud-native app developers choose Gluster as the default scale-out distributed file system. A selection of the important features and changes are documented on this page. A full list of bugs that have been addressed is included further below. Announcements Major changes and features Major issues Bugs addressed in the release Announcements As 3.13 was a short term maintenance release, features which have been included in that release are available with 4.0.0 as well.These features may be of interest to users upgrading to 4.0.0 from older than 3.13 releases. The 3.13 release notes captures the list of features that were introduced with 3.13. NOTE: As 3.13 was a short term maintenance release, it will reach end of life (EOL) with the release of 4.0.0. ( reference ) Releases that receive maintenance updates post 4.0 release are, 3.10, 3.12, 4.0 ( reference ) With this release, the CentOS storage SIG will not build server packages for CentOS6. Server packages will be available for CentOS7 only. For ease of migrations, client packages on CentOS6 will be published and maintained. NOTE : This change was announced here Major changes and features Features are categorized into the following sections, Management Monitoring Performance Geo-replication Standalone Developer related Management GlusterD2 (GD2) is new management daemon for Gluster-4.0. It is a complete rewrite, with all new internal core frameworks, that make it more scalable, easier to integrate with and has lower maintenance requirements. A quick start guide is available to get started with GD2. GD2 in Gluster-4.0 is a technical preview release. It is not recommended for production use. For the current release glusterd is the preferred management daemon. More information is available in the Limitations section. GD2 brings many new changes and improvements, that affect both users and developers. Features The most significant new features brought by GD2 are below. Native REST APIs GD2 exposes all of its management functionality via ReST APIs . The ReST APIs accept and return data encoded in JSON. This enables external projects such as Heketi to be better integrated with GD2. CLI GD2 provides a new CLI, glustercli , built on top of the ReST API. The CLI retains much of the syntax of the old gluster command. In addition we have, - Improved CLI help messages - Auto completion for sub commands - Improved CLI error messages on failure - Framework to run glustercli from outside the Cluster. In this release, the following CLI commands are available, - Peer management - Peer Probe/Attach - Peer Detach - Peer Status - Volume Management - Create/Start/Stop/Delete - Expand - Options Set/Get - Bitrot - Enable/Disable - Configure - Status - Geo-replication - Create/Start/Pause/Resume/Stop/Delete - Configure - Status Configuration store GD2 uses etcd to store the Gluster pool configuration, which solves the config synchronize issues reported against the Gluster management daemon. GD2 embeds etcd, and automatically creates and manages an etcd cluster when forming the trusted storage pool. If required, GD2 can also connect to an already existing etcd cluster. Transaction Framework GD2 brings a newer more flexible distributed framework, to help it perform actions across the storage pool. The transaction framework provides better control for choosing peers for a Gluster operation and it also provides a mechanism to roll back the changes when something goes bad. Volume Options GD2 intelligently fetches and builds the list of volume options by directly reading xlators *.so files. It does required validations during volume set without maintaining duplicate list of options. This avoids lot of issues which can happen due to mismatch in the information between Glusterd and xlator shared libraries. Volume options listing is also improved, to clearly distinguish configured options and default options. Work is still in progress to categorize these options and tune the list for better understanding and ease of use. Volfiles generation and management GD2 has a newer and better structured way for developers to define volfile structure. The new method reduces the effort required to extend graphs or add new graphs. Also, volfiles are generated in single peer and stored in etcd store. This is very important for scalability since Volfiles are not stored in every node. Security GD2 supports TLS for ReST and internal communication, and authentication for the ReST API.If enabled, ReST APIs are currently limited to CLI, or the users who have access to the Token file present in $GLUSTERD2_WORKDIR/auth file. Features integration - Self Heal Self Heal feature integrated for the new Volumes created using Glusterd2. Geo-replication With GD2 integration Geo-replication setup becomes very easy. If Master and Remote volume are available and running, Geo-replication can be setup with just a single command. glustercli geo-replication create <mastervol> <remotehost>::<remotevol> Geo-replication status is improved, Status clearly distinguishes the multiple session details in status output. Order of status rows was not predictable in earlier releases. It was very difficult to correlate the Geo-replication status with Bricks. With this release, Master worker status rows will always match with Bricks list in Volume info. Status can be checked using, glustercli geo-replication status glustercli geo-replication status <mastervol> <remotehost>::<remotevol> All the other commands are available as usual. Limitations: On Remote nodes, Geo-replication is not yet creates the log directories. As a workaround, create the required log directories in Remote Volume nodes. Events APIs Events API feature is integrated with GD2. Webhooks can be registered to listen for GlusterFS events. Work is in progress for exposing an REST API to view all the events happened in last 15 minutes. Limitations Backward compatibility GD2 is not backwards compatible with the older GlusterD. Heterogeneous clusters running both GD2 and GlusterD are not possible. GD2 retains compatibility with Gluster-3.x clients. Old clients will still be able to mount and use volumes exported using GD2. Upgrade and migration GD2 does not support upgrade from Gluster-3.x releases, in Gluster-4.0. Gluster-4.0 will be shipping with both GD2 and the existing GlusterD. Users will be able to upgrade to Gluster-4.0 while continuing to use GlusterD. In Gluster-4.1, users will be able to migrate from GlusterD to GD2. Further, upgrades from Gluster-4.1 running GD2 to higher Gluster versions would be supported from release 4.1 onwards. Post Gluster-4.1, GlusterD would be maintained for a couple of releases, post which the only option to manage the cluster would be GD2. Missing and partial commands Not all commands from GlusterD, have been implemented for GD2. Some have been only partially implemented. This means not all GlusterFS features are available in GD2. We aim to bring most of the commands back in Gluster-4.1. Recovery from full shutdown With GD2, the process of recovery from a situation of a full cluster shutdown requires reading the document available as well as some expertise. Known Issues 2-node clusters GD2 does not work well in 2-node clusters. Two main issues exist in this regard. - Restarting GD2 fails in 2-node clusters #352 - Detach fails in 2-node clusters #332 So it is recommended right now to run GD2 only in clusters of 3 or larger. Other issues Other known issues are tracked on github issues right now. Please file any other issue you find on github issues. Monitoring Till date, the absence of support for live monitoring on GlusterFS created constrained user experience for both users and developers. Statedump is useful for debugging, but is heavy for live monitoring. Further, the existence of debug/io-stats translator was not known to many and gluster volume profile was not recommended as it impacted performance. In this release, GlusterFS enables a lightweight method to access internal information and avoids the performance penalty and complexities of previous approaches. 1. Metrics collection across every FOP in every xlator Notes for users: Now, Gluster now has in-built latency measures in the xlator abstraction, thus enabling capture of metrics and usage patterns across workloads. These measures are currently enabled by default. Limitations: This feature is auto-enabled and cannot be disabled. 2. Monitoring support Notes for users: Currently, the only project which consumes metrics and provides basic monitoring is glustermetrics , which provides a good idea on how to utilize the metrics dumped from the processes. Users can send SIGUSR2 signal to the process to dump the metrics, in /var/run/gluster/metrics/ directory. Limitations: Currently core gluster stack and memory management systems provide metrics. A framework to generate more metrics is present for other translators and core components. However, additional metrics are not added in this release. Performance 1. EC: Make metadata [F]GETXATTR operations faster Notes for users: Disperse translator has made performance improvements to the [F]GETXATTR operation. Workloads involving heavy use of extended attributes on files and directories, will gain from the improvements made. 2. Allow md-cache to serve nameless lookup from cache Notes for users: The md-cache translator is enhanced to cache nameless lookups (typically seen with NFS workloads). This helps speed up overall operations on the volume reducing the number of lookups done over the network. Typical workloads that will benefit from this enhancement are, - NFS based access - Directory listing with FUSE, when ACLs are enabled 3. md-cache: Allow runtime addition of xattrs to the list of xattrs that md-cache caches Notes for users: md-cache was enhanced to cache extended attributes of a file or directory, for gluster specific attributes. This has now been enhanced to cache user provided attributes (xattrs) as well. To add specific xattrs to the cache list, use the following command: # gluster volume set <volname> xattr-cache-list \"<xattr-name>,<xattr-name>,...\" Existing options, such as \"cache-samba-metadata\" \"cache-swift-metadata\" continue to function. The new option \"xattr-cache-list\" appends to the list generated by the existing options. Limitations: Setting this option overwrites the previous value set for this option. The append to the existing list of xattr is not supported with this release. 4. Cache last stripe of an EC volume while write is going on Notes for users: Disperse translator now has the option to retain a write-through cache of the last write stripe. This helps in improved small append sequential IO patterns by reducing the need to read a partial stripe for appending operations. To enable this use, # gluster volume set <volname> disperse.stripe-cache <N> Where, is the number of stripes to cache. 5. tie-breaker logic for blocking inodelks/entrylk in SHD Notes for users: Self-heal deamon locking has been enhanced to identify situations where an selfheal deamon is actively working on an inode. This enables other selfheal daemons to proceed with other entries in the queue, than waiting on a particular entry, thus preventing starvation among selfheal threads. 6. Independent eager-lock options for file and directory accesses Notes for users: A new option named 'disperse.other-eager-lock' has been added to make it possible to have different settings for regular file accesses and accesses to other types of files (like directories). By default this option is enabled to ensure the same behavior as the previous versions. If you have multiple clients creating, renaming or removing files from the same directory, you can disable this option to improve the performance for these users while still keeping best performance for file accesses. 7. md-cache: Added an option to cache statfs data Notes for users: This can be controlled with option performance.md-cache-statfs gluster volume set <volname> performance.md-cache-statfs <on|off> 8. Improved disperse performance due to parallel xattrop updates Notes for users: Disperse translator has been optimized to perform xattrop update operation in parallel on the bricks during self-heal to improve performance. Geo-replication 1. Geo-replication: Improve gverify.sh logs Notes for users: gverify.sh is the script which runs during geo-rep session creation which validates pre-requisites. The logs have been improved and locations are changed as follows, 1. Slave mount log file is changed from <logdir>/geo-replication-slaves/slave.log to, <logdir>/geo-replication/gverify-slavemnt.log 2. Master mount log file is separated from the slave log file under, <logdir>/geo-replication/gverify-mastermnt.log 2. Geo-rep: Cleanup stale (unusable) XSYNC changelogs. Notes for users: Stale xsync logs were not cleaned up, causing accumulation of these on the system. This change cleans up the stale xsync logs, if geo-replication has to restart from a faulty state. Standalone 1. Ability to force permissions while creating files/directories on a volume Notes for users: Options have been added to the posix translator, to override default umask values with which files and directories are created. This is particularly useful when sharing content by applications based on GID. As the default mode bits prevent such useful sharing, and supersede ACLs in this regard, these options are provided to control this behavior. Command usage is as follows: # gluster volume set <volume name> storage.<option-name> <value> The valid <value> ranges from 0000 to 0777 <option-name> are: - create-mask - create-directory-mask - force-create-mode - force-create-directory Options \"create-mask\" and \"create-directory-mask\" are added to remove the mode bits set on a file or directory when its created. Default value of these options is 0777. Options \"force-create-mode\" and \"force-create-directory\" sets the default permission for a file or directory irrespective of the clients umask. Default value of these options is 0000. 2. Replace MD5 usage to enable FIPS support Notes for users: Previously, if Gluster was run on a FIPS enabled system, it used to crash because MD5 is not FIPS compliant and Gluster consumes MD5 checksum in various places like self-heal and geo-replication. By replacing MD5 with a FIPS complaint SHA256, Gluster no longer crashes on a FIPS enabled system. However, in order for AFR self-heal to work correctly during rolling upgrade to 4.0, we have tied this to a volume option called fips-mode-rchecksum . gluster volume set <VOLNAME> fips-mode-rchecksum on has to be performed post upgrade to change the defaults from MD5 to SHA256. Post this gluster processes will run clean on a FIPS enabled system. NOTE: Once glusterfs 3.x is EOL'ed, the usage of the option to control this change will be removed. Limitations Snapshot feature in Gluster still uses md5 checksums, hence running in FIPS compliant systems requires that the snapshot feature is not used. 3. Dentry fop serializer xlator on brick stack Notes for users: This feature strengthens consistency of the file system, trading it for some performance and is strongly suggested for workloads where consistency is required. In previous releases the meta-data about the files and directories shared across the clients were not always consistent when the use-cases/workloads involved a large number of renames, frequent creations and deletions. They do eventually become consistent, but a large proportion of applications are not built to handle eventual consistency. This feature can be enabled as follows, # gluster volume set <volname> features.sdfs enable Limitations: This feature is released as a technical preview, as performance implications are not known completely. 4. Add option to disable nftw() based deletes when purging the landfill directory Notes for users: The gluster brick processes use an optimized manner of deleting entire sub-trees using the nftw call. With this release, an option is being added to toggle this behavior in cases where this optimization is not desired. This is not an exposed option, and needs to be controlled using the volume graph. Adding the disable-landfill-purge option to the storage/posix translator helps toggle this feature. The default is always enabled, as in the older releases. 5. Add option in POSIX to limit hardlinks per inode Notes for users: Added an option to POSIX that limits the number of hard links that can be created against an inode (file). This helps when there needs to be a different hardlink limit than what the local FS provides for the bricks. The option to control this behavior is, # gluster volume set <volname> storage.max-hardlinks <N> Where, <N> is 0-0xFFFFFFFF. If the local file system that the brick is using has a lower limit than this setting, that would be honored. Default is set to 100, setting this to 0 turns it off and leaves it to the local file system defaults. Setting it to 1 turns off hard links. 6. Enhancements for directory listing in readdirp Notes for users: Prior to this release, rebalance performed a fix-layout on a directory before healing its subdirectories. If there were a lot of subdirs, it could take a while before all subdirs were created on the newly added bricks. This led to some missed directory listings. This is changed with this release to process children directories before the parents, thereby changing the way rebalance acts (files within sub directories are migrated first) and also resolving the directory listing issue. 7. Rebalance skips migration of file if it detects writes from application Notes for users: Rebalance process skips migration of file if it detects writes from application. To force migration even in the presence of writes from application to file, \"cluster.force-migration\" has to be turned on, which is off by default. The option to control this behavior is, # gluster volume set <volname> cluster.force-migration <on/off> Limitations: It is suggested to run remove-brick with cluster.force-migration turned off. This results in files which have writes from clients being skipped during rebalance. It is suggested to copy these files manually to a Gluster mount post remove brick commit is performed. Rebalancing files with active write IO to them has a chance of data corruption. Developer related 1. xlators should not provide init(), fini() and others directly, but have class_methods Notes for developers: This release brings in a new unified manner of defining xlator methods. Which avoids certain unwanted side-effects of the older method (like having to have certain symbols being defined always), and helps a cleaner single point registration mechanism for all xlator methods. The new method, needs just a single symbol in the translator code to be exposed, which is named xlator_api. The elements of this structure is defined here and an example usage of the same can be seen here . The older mechanism is still supported, but not preferred. 2. Framework for distributed testing Notes for developers: A new framework for running the regression tests for Gluster is added. The README has details on how to use the same. 3. New API for acquiring mandatory locks Notes for developers: The current API for byte-range locks glfs_posix_lock doesn't allow applications to specify whether it is advisory or mandatory type lock. This particular change is to introduce an extended byte-range lock API with an additional argument for including the byte-range lock mode to be one among advisory(default) or mandatory. Refer to the header for details on how to use this API. A sample test program can be found here that also helps in understanding the usage of this API. 4. New on-wire protocol (XDR) needed to support iattx and cleaner dictionary structure Notes for developers: With changes in the code to adapt to a newer iatt structure, and stricter data format enforcement within dictionaries passed across the wire, and also as a part of reducing technical debt around the RPC layer, this release introduces a new RPC Gluster protocol version (4.0.0). Typically this does not impact any development, other than to ensure that newer RPCs that are added would need to be on the 4.0.0 version of the protocol and dictionaries on the wire need to be better encoded. The newer iatt structure can be viewed here . An example of better encoding dictionary values for wire transfers can be seen here . Here is some additional information on Gluster RPC programs for the inquisitive. 5. The protocol xlators should prevent sending binary values in a dict over the networks Notes for developers: Dict data over the wire in Gluster was sent in binary. This has been changed with this release, as the on-wire protocol wire is also new, to send XDR encoded dict values across. In the future, any new dict type needs to also handle the required XDR encoding of the same. 6. Translator to handle 'global' options Notes for developers: GlusterFS process has around 50 command line arguments to itself. While many of the options are initial settings, many others can change its value in volume lifetime. Prior to this release there was no way to change a setting, other than restarting the process for many of these options. With the introduction of global option translator, it is now possible to handle these options without restarts. If contributing code that adds to the process options, strongly consider adding the same to the global option translator. An example is provided here . Major issues None Bugs addressed Bugs addressed since release-3.13.0 are listed below. #827334 : gfid is not there in the fsetattr and rchecksum requests being sent from protocol client #1336889 : Gluster's XDR does not conform to RFC spec #1369028 : rpc: Change the way client uuid is built #1370116 : Tests : Adding a test to check for inode leak #1428060 : write-behind: Allow trickling-writes to be configurable, fix usage of page_size and window_size #1430305 : Fix memory leak in rebalance #1431955 : [Disperse] Implement open fd heal for disperse volume #1440659 : Add events to notify disk getting fill #1443145 : Free runtime allocated resources upon graph switch or glfs_fini() #1446381 : detach start does not kill the tierd #1467250 : Accessing a file when source brick is down results in that FOP being hung #1467614 : Gluster read/write performance improvements on NVMe backend #1469487 : sys_xxx() functions should guard against bad return values from fs #1471031 : dht_(f)xattrop does not implement migration checks #1471753 : [disperse] Keep stripe in in-memory cache for the non aligned write #1474768 : The output of the \"gluster help\" command is difficult to read #1479528 : Rebalance estimate(ETA) shows wrong details(as intial message of 10min wait reappears) when still in progress #1480491 : tests: Enable geo-rep test cases #1482064 : Bringing down data bricks in cyclic order results in arbiter brick becoming the source for heal. #1488103 : Rebalance fails on NetBSD because fallocate is not implemented #1492625 : Directory listings on fuse mount are very slow due to small number of getdents() entries #1496335 : Extreme Load from self-heal #1498966 : Test case ./tests/bugs/bug-1371806_1.t is failing #1499566 : [Geo-rep]: Directory renames are not synced in hybrid crawl #1501054 : Structured logging support for Gluster logs #1501132 : posix health check should validate time taken between write timestamp and read timestamp cycle #1502610 : disperse eager-lock degrades performance for file create workloads #1503227 : [RFE] Changelog option in a gluster volume disables with no warning if geo-rep is configured #1505660 : [QUOTA] man page of gluster should be updated to list quota commands #1506104 : gluster volume splitbrain info needs to display output of each brick in a stream fashion instead of buffering and dumping at the end #1506140 : Add quorum checks in post-op #1506197 : [Parallel-Readdir]Warning messages in client log saying 'parallel-readdir' is not recognized. #1508898 : Add new configuration option to manage deletion of Worm files #1508947 : glusterfs: Include path in pkgconfig file is wrong #1509189 : timer: Possible race condition between gf_timer_* routines #1509254 : snapshot remove does not cleans lvm for deactivated snaps #1509340 : glusterd does not write pidfile correctly when forking #1509412 : Change default versions of certain features to 3.13 from 4.0 #1509644 : rpc: make actor search parallel #1509647 : rpc: optimize fop program lookup #1509845 : In distribute volume after glusterd restart, brick goes offline #1510324 : Master branch is broken because of the conflicts #1510397 : Compiler atomic built-ins are not correctly detected #1510401 : fstat returns ENOENT/ESTALE #1510415 : spurious failure of tests/bugs/glusterd/bug-1345727-bricks-stop-on-no-quorum-validation.t #1510874 : print-backtrace.sh failing with cpio version 2.11 or older #1510940 : The number of bytes of the quota specified in version 3.7 or later is incorrect #1511310 : Test bug-1483058-replace-brick-quorum-validation.t fails inconsistently #1511339 : In Replica volume 2*2 when quorum is set, after glusterd restart nfs server is coming up instead of self-heal daemon #1512437 : parallel-readdir = TRUE prevents directories listing #1512451 : Not able to create snapshot #1512455 : glustereventsd hardcodes working-directory #1512483 : Not all files synced using geo-replication #1513692 : io-stats appends now instead of overwriting which floods filesystem with logs #1513928 : call stack group list leaks #1514329 : bug-1247563.t is failing on master #1515161 : Memory leak in locks xlator #1515163 : centos regression fails for tests/bugs/replicate/bug-1292379.t #1515266 : Prevent ec from continue processing heal operations after PARENT_DOWN #1516206 : EC DISCARD doesn't punch hole properly #1517068 : Unable to change the Slave configurations #1517554 : help for volume profile is not in man page #1517633 : Geo-rep: access-mount config is not working #1517904 : tests/bugs/core/multiplex-limit-issue-151.t fails sometimes in upstream master #1517961 : Failure of some regression tests on Centos7 (passes on centos6) #1518508 : Change GD_OP_VERSION to 3_13_0 from 3_12_0 for RFE https://bugzilla.redhat.com/show_bug.cgi?id=1464350 #1518582 : Reduce lock contention on fdtable lookup #1519598 : Reduce lock contention on protocol client manipulating fd #1520245 : High mem/cpu usage, brick processes not starting and ssl encryption issues while testing scaling with multiplexing (500-800 vols) #1520758 : [Disperse] Add stripe in cache even if file/data does not exist #1520974 : Compiler warning in dht-common.c because of a switch statement on a boolean #1521013 : rfc.sh should allow custom remote names for ORIGIN #1521014 : quota_unlink_cbk crashes when loc.inode is null #1521116 : Absorb all test fixes from 3.8-fb branch into master #1521213 : crash when gifs_set_logging is called concurrently #1522651 : rdma transport may access an obsolete item in gf_rdma_device_t->all_mr, and causes glusterfsd/glusterfs process crash. #1522662 : Store allocated objects in the mem_acct #1522775 : glusterd consuming high memory #1522847 : gNFS Bug Fixes #1522950 : io-threads is unnecessarily calling accurate time calls on every FOP #1522968 : glusterd bug fixes #1523295 : md-cache should have an option to cache STATFS calls #1523353 : io-stats bugs and features #1524252 : quick-read: Discard cache for fallocate, zerofill and discard ops #1524365 : feature/bitrot: remove internal xattrs from lookup cbk #1524816 : heketi was not removing the LVs associated with Bricks removed when Gluster Volumes were deleted #1526402 : glusterd crashes when 'gluster volume set help' is executed #1526780 : ./run-tests-in-vagrant.sh fails because of disabled Gluster/NFS #1528558 : /usr/sbin/glusterfs crashing on Red Hat OpenShift Container Platform node #1528975 : Fedora 28 (Rawhide) renamed the pyxattr package to python2-pyxattr #1529440 : Files are not rebalanced if destination brick(available size) is of smaller size than source brick(available size) #1529463 : JWT support without external dependency #1529480 : Improve geo-replication logging #1529488 : entries not getting cleared post healing of softlinks (stale entries showing up in heal info) #1529515 : AFR: 3-way-replication: gluster volume set cluster.quorum-count should validate max no. of brick count to accept #1529883 : glusterfind is extremely slow if there are lots of changes #1530281 : glustershd fails to start on a volume force start after a brick is down #1530910 : Use after free in cli_cmd_volume_create_cbk #1531149 : memory leak: get-state leaking memory in small amounts #1531987 : increment of a boolean expression warning #1532238 : Failed to access volume via Samba with undefined symbol from socket.so #1532591 : Tests: Geo-rep tests are failing in few regression machines #1533594 : EC test fails when brick mux is enabled #1533736 : posix_statfs returns incorrect f_bfree values if brick is full. #1533804 : readdir-ahead: change of cache-size should be atomic #1533815 : Mark ./tests/basic/ec/heal-info.t as bad #1534602 : FUSE reverse notificatons are not written to fuse dump #1535438 : Take full lock on files in 3 way replication #1535772 : Random GlusterFSD process dies during rebalance #1536913 : tests/bugs/cli/bug-822830.t fails on Centos 7 and locally #1538723 : build: glibc has removed legacy rpc headers and rpcgen in Fedora28, use libtirpc #1539657 : Georeplication tests intermittently fail #1539701 : gsyncd is running gluster command to get config file path is not required #1539842 : GlusterFS 4.0.0 tracker #1540438 : Remove lock recovery logic from client and server protocol translators #1540554 : Optimize glusterd_import_friend_volume code path #1540882 : Do lock conflict check correctly for wait-list #1541117 : sdfs: crashes if the features is enabled #1541277 : dht_layout_t leak in dht_populate_inode_for_dentry #1541880 : Volume wrong size #1541928 : A down brick is incorrectly considered to be online and makes the volume to be started without any brick available #1542380 : Changes to self-heal logic w.r.t. detecting of split-brains #1542382 : Add quorum checks in post-op #1542829 : Too many log messages about dictionary and options #1543487 : dht_lookup_unlink_of_false_linkto_cbk fails with \"Permission denied\" #1543706 : glusterd fails to attach brick during restart of the node #1543711 : glustershd/glusterd is not using right port when connecting to glusterfsd process #1544366 : Rolling upgrade to 4.0 is broken #1544638 : 3.8 -> 3.10 rolling upgrade fails (same for 3.12 or 3.13) on Ubuntu 14 #1545724 : libgfrpc does not export IPv6 RPC methods even with --with-ipv6-default #1547635 : add option to bulld rpm without server #1547842 : Typo error in __dht_check_free_space function log message #1548264 : [Rebalance] \"Migrate file failed: : failed to get xattr [No data available]\" warnings in rebalance logs #1548271 : DHT calls dht_lookup_everywhere for 1xn volumes #1550808 : memory leak in pre-op in replicate volumes for every write #1551112 : Rolling upgrade to 4.0 is broken #1551640 : GD2 fails to dlopen server xlator #1554077 : 4.0 clients may fail to convert iatt in dict when recieving the same from older (< 4.0) servers","title":"4.0.0"},{"location":"release-notes/4.0.0/#release-notes-for-gluster-400","text":"The Gluster community celebrates 13 years of development with this latest release, Gluster 4.0. This release enables improved integration with containers, an enhanced user experience, and a next-generation management framework. The 4.0 release helps cloud-native app developers choose Gluster as the default scale-out distributed file system. A selection of the important features and changes are documented on this page. A full list of bugs that have been addressed is included further below. Announcements Major changes and features Major issues Bugs addressed in the release","title":"Release notes for Gluster 4.0.0"},{"location":"release-notes/4.0.0/#announcements","text":"As 3.13 was a short term maintenance release, features which have been included in that release are available with 4.0.0 as well.These features may be of interest to users upgrading to 4.0.0 from older than 3.13 releases. The 3.13 release notes captures the list of features that were introduced with 3.13. NOTE: As 3.13 was a short term maintenance release, it will reach end of life (EOL) with the release of 4.0.0. ( reference ) Releases that receive maintenance updates post 4.0 release are, 3.10, 3.12, 4.0 ( reference ) With this release, the CentOS storage SIG will not build server packages for CentOS6. Server packages will be available for CentOS7 only. For ease of migrations, client packages on CentOS6 will be published and maintained. NOTE : This change was announced here","title":"Announcements"},{"location":"release-notes/4.0.0/#major-changes-and-features","text":"Features are categorized into the following sections, Management Monitoring Performance Geo-replication Standalone Developer related","title":"Major changes and features"},{"location":"release-notes/4.0.0/#management","text":"GlusterD2 (GD2) is new management daemon for Gluster-4.0. It is a complete rewrite, with all new internal core frameworks, that make it more scalable, easier to integrate with and has lower maintenance requirements. A quick start guide is available to get started with GD2. GD2 in Gluster-4.0 is a technical preview release. It is not recommended for production use. For the current release glusterd is the preferred management daemon. More information is available in the Limitations section. GD2 brings many new changes and improvements, that affect both users and developers.","title":"Management"},{"location":"release-notes/4.0.0/#features","text":"The most significant new features brought by GD2 are below.","title":"Features"},{"location":"release-notes/4.0.0/#native-rest-apis","text":"GD2 exposes all of its management functionality via ReST APIs . The ReST APIs accept and return data encoded in JSON. This enables external projects such as Heketi to be better integrated with GD2.","title":"Native REST APIs"},{"location":"release-notes/4.0.0/#cli","text":"GD2 provides a new CLI, glustercli , built on top of the ReST API. The CLI retains much of the syntax of the old gluster command. In addition we have, - Improved CLI help messages - Auto completion for sub commands - Improved CLI error messages on failure - Framework to run glustercli from outside the Cluster. In this release, the following CLI commands are available, - Peer management - Peer Probe/Attach - Peer Detach - Peer Status - Volume Management - Create/Start/Stop/Delete - Expand - Options Set/Get - Bitrot - Enable/Disable - Configure - Status - Geo-replication - Create/Start/Pause/Resume/Stop/Delete - Configure - Status","title":"CLI"},{"location":"release-notes/4.0.0/#configuration-store","text":"GD2 uses etcd to store the Gluster pool configuration, which solves the config synchronize issues reported against the Gluster management daemon. GD2 embeds etcd, and automatically creates and manages an etcd cluster when forming the trusted storage pool. If required, GD2 can also connect to an already existing etcd cluster.","title":"Configuration store"},{"location":"release-notes/4.0.0/#transaction-framework","text":"GD2 brings a newer more flexible distributed framework, to help it perform actions across the storage pool. The transaction framework provides better control for choosing peers for a Gluster operation and it also provides a mechanism to roll back the changes when something goes bad.","title":"Transaction Framework"},{"location":"release-notes/4.0.0/#volume-options","text":"GD2 intelligently fetches and builds the list of volume options by directly reading xlators *.so files. It does required validations during volume set without maintaining duplicate list of options. This avoids lot of issues which can happen due to mismatch in the information between Glusterd and xlator shared libraries. Volume options listing is also improved, to clearly distinguish configured options and default options. Work is still in progress to categorize these options and tune the list for better understanding and ease of use.","title":"Volume Options"},{"location":"release-notes/4.0.0/#volfiles-generation-and-management","text":"GD2 has a newer and better structured way for developers to define volfile structure. The new method reduces the effort required to extend graphs or add new graphs. Also, volfiles are generated in single peer and stored in etcd store. This is very important for scalability since Volfiles are not stored in every node.","title":"Volfiles generation and management"},{"location":"release-notes/4.0.0/#security","text":"GD2 supports TLS for ReST and internal communication, and authentication for the ReST API.If enabled, ReST APIs are currently limited to CLI, or the users who have access to the Token file present in $GLUSTERD2_WORKDIR/auth file.","title":"Security"},{"location":"release-notes/4.0.0/#features-integration-self-heal","text":"Self Heal feature integrated for the new Volumes created using Glusterd2.","title":"Features integration - Self Heal"},{"location":"release-notes/4.0.0/#geo-replication","text":"With GD2 integration Geo-replication setup becomes very easy. If Master and Remote volume are available and running, Geo-replication can be setup with just a single command. glustercli geo-replication create <mastervol> <remotehost>::<remotevol> Geo-replication status is improved, Status clearly distinguishes the multiple session details in status output. Order of status rows was not predictable in earlier releases. It was very difficult to correlate the Geo-replication status with Bricks. With this release, Master worker status rows will always match with Bricks list in Volume info. Status can be checked using, glustercli geo-replication status glustercli geo-replication status <mastervol> <remotehost>::<remotevol> All the other commands are available as usual. Limitations: On Remote nodes, Geo-replication is not yet creates the log directories. As a workaround, create the required log directories in Remote Volume nodes.","title":"Geo-replication"},{"location":"release-notes/4.0.0/#events-apis","text":"Events API feature is integrated with GD2. Webhooks can be registered to listen for GlusterFS events. Work is in progress for exposing an REST API to view all the events happened in last 15 minutes.","title":"Events APIs"},{"location":"release-notes/4.0.0/#limitations","text":"","title":"Limitations"},{"location":"release-notes/4.0.0/#backward-compatibility","text":"GD2 is not backwards compatible with the older GlusterD. Heterogeneous clusters running both GD2 and GlusterD are not possible. GD2 retains compatibility with Gluster-3.x clients. Old clients will still be able to mount and use volumes exported using GD2.","title":"Backward compatibility"},{"location":"release-notes/4.0.0/#upgrade-and-migration","text":"GD2 does not support upgrade from Gluster-3.x releases, in Gluster-4.0. Gluster-4.0 will be shipping with both GD2 and the existing GlusterD. Users will be able to upgrade to Gluster-4.0 while continuing to use GlusterD. In Gluster-4.1, users will be able to migrate from GlusterD to GD2. Further, upgrades from Gluster-4.1 running GD2 to higher Gluster versions would be supported from release 4.1 onwards. Post Gluster-4.1, GlusterD would be maintained for a couple of releases, post which the only option to manage the cluster would be GD2.","title":"Upgrade and migration"},{"location":"release-notes/4.0.0/#missing-and-partial-commands","text":"Not all commands from GlusterD, have been implemented for GD2. Some have been only partially implemented. This means not all GlusterFS features are available in GD2. We aim to bring most of the commands back in Gluster-4.1.","title":"Missing and partial commands"},{"location":"release-notes/4.0.0/#recovery-from-full-shutdown","text":"With GD2, the process of recovery from a situation of a full cluster shutdown requires reading the document available as well as some expertise.","title":"Recovery from full shutdown"},{"location":"release-notes/4.0.0/#known-issues","text":"","title":"Known Issues"},{"location":"release-notes/4.0.0/#2-node-clusters","text":"GD2 does not work well in 2-node clusters. Two main issues exist in this regard. - Restarting GD2 fails in 2-node clusters #352 - Detach fails in 2-node clusters #332 So it is recommended right now to run GD2 only in clusters of 3 or larger.","title":"2-node clusters"},{"location":"release-notes/4.0.0/#other-issues","text":"Other known issues are tracked on github issues right now. Please file any other issue you find on github issues.","title":"Other issues"},{"location":"release-notes/4.0.0/#monitoring","text":"Till date, the absence of support for live monitoring on GlusterFS created constrained user experience for both users and developers. Statedump is useful for debugging, but is heavy for live monitoring. Further, the existence of debug/io-stats translator was not known to many and gluster volume profile was not recommended as it impacted performance. In this release, GlusterFS enables a lightweight method to access internal information and avoids the performance penalty and complexities of previous approaches.","title":"Monitoring"},{"location":"release-notes/4.0.0/#1-metrics-collection-across-every-fop-in-every-xlator","text":"Notes for users: Now, Gluster now has in-built latency measures in the xlator abstraction, thus enabling capture of metrics and usage patterns across workloads. These measures are currently enabled by default. Limitations: This feature is auto-enabled and cannot be disabled.","title":"1. Metrics collection across every FOP in every xlator"},{"location":"release-notes/4.0.0/#2-monitoring-support","text":"Notes for users: Currently, the only project which consumes metrics and provides basic monitoring is glustermetrics , which provides a good idea on how to utilize the metrics dumped from the processes. Users can send SIGUSR2 signal to the process to dump the metrics, in /var/run/gluster/metrics/ directory. Limitations: Currently core gluster stack and memory management systems provide metrics. A framework to generate more metrics is present for other translators and core components. However, additional metrics are not added in this release.","title":"2. Monitoring support"},{"location":"release-notes/4.0.0/#performance","text":"","title":"Performance"},{"location":"release-notes/4.0.0/#1-ec-make-metadata-fgetxattr-operations-faster","text":"Notes for users: Disperse translator has made performance improvements to the [F]GETXATTR operation. Workloads involving heavy use of extended attributes on files and directories, will gain from the improvements made.","title":"1. EC: Make metadata [F]GETXATTR operations faster"},{"location":"release-notes/4.0.0/#2-allow-md-cache-to-serve-nameless-lookup-from-cache","text":"Notes for users: The md-cache translator is enhanced to cache nameless lookups (typically seen with NFS workloads). This helps speed up overall operations on the volume reducing the number of lookups done over the network. Typical workloads that will benefit from this enhancement are, - NFS based access - Directory listing with FUSE, when ACLs are enabled","title":"2. Allow md-cache to serve nameless lookup from cache"},{"location":"release-notes/4.0.0/#3-md-cache-allow-runtime-addition-of-xattrs-to-the-list-of-xattrs-that-md-cache-caches","text":"Notes for users: md-cache was enhanced to cache extended attributes of a file or directory, for gluster specific attributes. This has now been enhanced to cache user provided attributes (xattrs) as well. To add specific xattrs to the cache list, use the following command: # gluster volume set <volname> xattr-cache-list \"<xattr-name>,<xattr-name>,...\" Existing options, such as \"cache-samba-metadata\" \"cache-swift-metadata\" continue to function. The new option \"xattr-cache-list\" appends to the list generated by the existing options. Limitations: Setting this option overwrites the previous value set for this option. The append to the existing list of xattr is not supported with this release.","title":"3. md-cache: Allow runtime addition of xattrs to the list of xattrs that md-cache caches"},{"location":"release-notes/4.0.0/#4-cache-last-stripe-of-an-ec-volume-while-write-is-going-on","text":"Notes for users: Disperse translator now has the option to retain a write-through cache of the last write stripe. This helps in improved small append sequential IO patterns by reducing the need to read a partial stripe for appending operations. To enable this use, # gluster volume set <volname> disperse.stripe-cache <N> Where, is the number of stripes to cache.","title":"4. Cache last stripe of an EC volume while write is going on"},{"location":"release-notes/4.0.0/#5-tie-breaker-logic-for-blocking-inodelksentrylk-in-shd","text":"Notes for users: Self-heal deamon locking has been enhanced to identify situations where an selfheal deamon is actively working on an inode. This enables other selfheal daemons to proceed with other entries in the queue, than waiting on a particular entry, thus preventing starvation among selfheal threads.","title":"5. tie-breaker logic for blocking inodelks/entrylk in SHD"},{"location":"release-notes/4.0.0/#6-independent-eager-lock-options-for-file-and-directory-accesses","text":"Notes for users: A new option named 'disperse.other-eager-lock' has been added to make it possible to have different settings for regular file accesses and accesses to other types of files (like directories). By default this option is enabled to ensure the same behavior as the previous versions. If you have multiple clients creating, renaming or removing files from the same directory, you can disable this option to improve the performance for these users while still keeping best performance for file accesses.","title":"6. Independent eager-lock options for file and directory accesses"},{"location":"release-notes/4.0.0/#7-md-cache-added-an-option-to-cache-statfs-data","text":"Notes for users: This can be controlled with option performance.md-cache-statfs gluster volume set <volname> performance.md-cache-statfs <on|off>","title":"7. md-cache: Added an option to cache statfs data"},{"location":"release-notes/4.0.0/#8-improved-disperse-performance-due-to-parallel-xattrop-updates","text":"Notes for users: Disperse translator has been optimized to perform xattrop update operation in parallel on the bricks during self-heal to improve performance.","title":"8. Improved disperse performance due to parallel xattrop updates"},{"location":"release-notes/4.0.0/#geo-replication_1","text":"","title":"Geo-replication"},{"location":"release-notes/4.0.0/#1-geo-replication-improve-gverifysh-logs","text":"Notes for users: gverify.sh is the script which runs during geo-rep session creation which validates pre-requisites. The logs have been improved and locations are changed as follows, 1. Slave mount log file is changed from <logdir>/geo-replication-slaves/slave.log to, <logdir>/geo-replication/gverify-slavemnt.log 2. Master mount log file is separated from the slave log file under, <logdir>/geo-replication/gverify-mastermnt.log","title":"1. Geo-replication: Improve gverify.sh logs"},{"location":"release-notes/4.0.0/#2-geo-rep-cleanup-stale-unusable-xsync-changelogs","text":"Notes for users: Stale xsync logs were not cleaned up, causing accumulation of these on the system. This change cleans up the stale xsync logs, if geo-replication has to restart from a faulty state.","title":"2. Geo-rep: Cleanup stale (unusable) XSYNC changelogs."},{"location":"release-notes/4.0.0/#standalone","text":"","title":"Standalone"},{"location":"release-notes/4.0.0/#1-ability-to-force-permissions-while-creating-filesdirectories-on-a-volume","text":"Notes for users: Options have been added to the posix translator, to override default umask values with which files and directories are created. This is particularly useful when sharing content by applications based on GID. As the default mode bits prevent such useful sharing, and supersede ACLs in this regard, these options are provided to control this behavior. Command usage is as follows: # gluster volume set <volume name> storage.<option-name> <value> The valid <value> ranges from 0000 to 0777 <option-name> are: - create-mask - create-directory-mask - force-create-mode - force-create-directory Options \"create-mask\" and \"create-directory-mask\" are added to remove the mode bits set on a file or directory when its created. Default value of these options is 0777. Options \"force-create-mode\" and \"force-create-directory\" sets the default permission for a file or directory irrespective of the clients umask. Default value of these options is 0000.","title":"1. Ability to force permissions while creating files/directories on a volume"},{"location":"release-notes/4.0.0/#2-replace-md5-usage-to-enable-fips-support","text":"Notes for users: Previously, if Gluster was run on a FIPS enabled system, it used to crash because MD5 is not FIPS compliant and Gluster consumes MD5 checksum in various places like self-heal and geo-replication. By replacing MD5 with a FIPS complaint SHA256, Gluster no longer crashes on a FIPS enabled system. However, in order for AFR self-heal to work correctly during rolling upgrade to 4.0, we have tied this to a volume option called fips-mode-rchecksum . gluster volume set <VOLNAME> fips-mode-rchecksum on has to be performed post upgrade to change the defaults from MD5 to SHA256. Post this gluster processes will run clean on a FIPS enabled system. NOTE: Once glusterfs 3.x is EOL'ed, the usage of the option to control this change will be removed. Limitations Snapshot feature in Gluster still uses md5 checksums, hence running in FIPS compliant systems requires that the snapshot feature is not used.","title":"2. Replace MD5 usage to enable FIPS support"},{"location":"release-notes/4.0.0/#3-dentry-fop-serializer-xlator-on-brick-stack","text":"Notes for users: This feature strengthens consistency of the file system, trading it for some performance and is strongly suggested for workloads where consistency is required. In previous releases the meta-data about the files and directories shared across the clients were not always consistent when the use-cases/workloads involved a large number of renames, frequent creations and deletions. They do eventually become consistent, but a large proportion of applications are not built to handle eventual consistency. This feature can be enabled as follows, # gluster volume set <volname> features.sdfs enable Limitations: This feature is released as a technical preview, as performance implications are not known completely.","title":"3. Dentry fop serializer xlator on brick stack"},{"location":"release-notes/4.0.0/#4-add-option-to-disable-nftw-based-deletes-when-purging-the-landfill-directory","text":"Notes for users: The gluster brick processes use an optimized manner of deleting entire sub-trees using the nftw call. With this release, an option is being added to toggle this behavior in cases where this optimization is not desired. This is not an exposed option, and needs to be controlled using the volume graph. Adding the disable-landfill-purge option to the storage/posix translator helps toggle this feature. The default is always enabled, as in the older releases.","title":"4. Add option to disable nftw() based deletes when purging the landfill directory"},{"location":"release-notes/4.0.0/#5-add-option-in-posix-to-limit-hardlinks-per-inode","text":"Notes for users: Added an option to POSIX that limits the number of hard links that can be created against an inode (file). This helps when there needs to be a different hardlink limit than what the local FS provides for the bricks. The option to control this behavior is, # gluster volume set <volname> storage.max-hardlinks <N> Where, <N> is 0-0xFFFFFFFF. If the local file system that the brick is using has a lower limit than this setting, that would be honored. Default is set to 100, setting this to 0 turns it off and leaves it to the local file system defaults. Setting it to 1 turns off hard links.","title":"5. Add option in POSIX to limit hardlinks per inode"},{"location":"release-notes/4.0.0/#6-enhancements-for-directory-listing-in-readdirp","text":"Notes for users: Prior to this release, rebalance performed a fix-layout on a directory before healing its subdirectories. If there were a lot of subdirs, it could take a while before all subdirs were created on the newly added bricks. This led to some missed directory listings. This is changed with this release to process children directories before the parents, thereby changing the way rebalance acts (files within sub directories are migrated first) and also resolving the directory listing issue.","title":"6. Enhancements for directory listing in readdirp"},{"location":"release-notes/4.0.0/#7-rebalance-skips-migration-of-file-if-it-detects-writes-from-application","text":"Notes for users: Rebalance process skips migration of file if it detects writes from application. To force migration even in the presence of writes from application to file, \"cluster.force-migration\" has to be turned on, which is off by default. The option to control this behavior is, # gluster volume set <volname> cluster.force-migration <on/off> Limitations: It is suggested to run remove-brick with cluster.force-migration turned off. This results in files which have writes from clients being skipped during rebalance. It is suggested to copy these files manually to a Gluster mount post remove brick commit is performed. Rebalancing files with active write IO to them has a chance of data corruption.","title":"7. Rebalance skips migration of file if it detects writes from application"},{"location":"release-notes/4.0.0/#developer-related","text":"","title":"Developer related"},{"location":"release-notes/4.0.0/#1-xlators-should-not-provide-init-fini-and-others-directly-but-have-class_methods","text":"Notes for developers: This release brings in a new unified manner of defining xlator methods. Which avoids certain unwanted side-effects of the older method (like having to have certain symbols being defined always), and helps a cleaner single point registration mechanism for all xlator methods. The new method, needs just a single symbol in the translator code to be exposed, which is named xlator_api. The elements of this structure is defined here and an example usage of the same can be seen here . The older mechanism is still supported, but not preferred.","title":"1. xlators should not provide init(), fini() and others directly, but have class_methods"},{"location":"release-notes/4.0.0/#2-framework-for-distributed-testing","text":"Notes for developers: A new framework for running the regression tests for Gluster is added. The README has details on how to use the same.","title":"2. Framework for distributed testing"},{"location":"release-notes/4.0.0/#3-new-api-for-acquiring-mandatory-locks","text":"Notes for developers: The current API for byte-range locks glfs_posix_lock doesn't allow applications to specify whether it is advisory or mandatory type lock. This particular change is to introduce an extended byte-range lock API with an additional argument for including the byte-range lock mode to be one among advisory(default) or mandatory. Refer to the header for details on how to use this API. A sample test program can be found here that also helps in understanding the usage of this API.","title":"3. New API for acquiring mandatory locks"},{"location":"release-notes/4.0.0/#4-new-on-wire-protocol-xdr-needed-to-support-iattx-and-cleaner-dictionary-structure","text":"Notes for developers: With changes in the code to adapt to a newer iatt structure, and stricter data format enforcement within dictionaries passed across the wire, and also as a part of reducing technical debt around the RPC layer, this release introduces a new RPC Gluster protocol version (4.0.0). Typically this does not impact any development, other than to ensure that newer RPCs that are added would need to be on the 4.0.0 version of the protocol and dictionaries on the wire need to be better encoded. The newer iatt structure can be viewed here . An example of better encoding dictionary values for wire transfers can be seen here . Here is some additional information on Gluster RPC programs for the inquisitive.","title":"4. New on-wire protocol (XDR) needed to support iattx and cleaner dictionary structure"},{"location":"release-notes/4.0.0/#5-the-protocol-xlators-should-prevent-sending-binary-values-in-a-dict-over-the-networks","text":"Notes for developers: Dict data over the wire in Gluster was sent in binary. This has been changed with this release, as the on-wire protocol wire is also new, to send XDR encoded dict values across. In the future, any new dict type needs to also handle the required XDR encoding of the same.","title":"5. The protocol xlators should prevent sending binary values in a dict over the networks"},{"location":"release-notes/4.0.0/#6-translator-to-handle-global-options","text":"Notes for developers: GlusterFS process has around 50 command line arguments to itself. While many of the options are initial settings, many others can change its value in volume lifetime. Prior to this release there was no way to change a setting, other than restarting the process for many of these options. With the introduction of global option translator, it is now possible to handle these options without restarts. If contributing code that adds to the process options, strongly consider adding the same to the global option translator. An example is provided here .","title":"6. Translator to handle 'global' options"},{"location":"release-notes/4.0.0/#major-issues","text":"None","title":"Major issues"},{"location":"release-notes/4.0.0/#bugs-addressed","text":"Bugs addressed since release-3.13.0 are listed below. #827334 : gfid is not there in the fsetattr and rchecksum requests being sent from protocol client #1336889 : Gluster's XDR does not conform to RFC spec #1369028 : rpc: Change the way client uuid is built #1370116 : Tests : Adding a test to check for inode leak #1428060 : write-behind: Allow trickling-writes to be configurable, fix usage of page_size and window_size #1430305 : Fix memory leak in rebalance #1431955 : [Disperse] Implement open fd heal for disperse volume #1440659 : Add events to notify disk getting fill #1443145 : Free runtime allocated resources upon graph switch or glfs_fini() #1446381 : detach start does not kill the tierd #1467250 : Accessing a file when source brick is down results in that FOP being hung #1467614 : Gluster read/write performance improvements on NVMe backend #1469487 : sys_xxx() functions should guard against bad return values from fs #1471031 : dht_(f)xattrop does not implement migration checks #1471753 : [disperse] Keep stripe in in-memory cache for the non aligned write #1474768 : The output of the \"gluster help\" command is difficult to read #1479528 : Rebalance estimate(ETA) shows wrong details(as intial message of 10min wait reappears) when still in progress #1480491 : tests: Enable geo-rep test cases #1482064 : Bringing down data bricks in cyclic order results in arbiter brick becoming the source for heal. #1488103 : Rebalance fails on NetBSD because fallocate is not implemented #1492625 : Directory listings on fuse mount are very slow due to small number of getdents() entries #1496335 : Extreme Load from self-heal #1498966 : Test case ./tests/bugs/bug-1371806_1.t is failing #1499566 : [Geo-rep]: Directory renames are not synced in hybrid crawl #1501054 : Structured logging support for Gluster logs #1501132 : posix health check should validate time taken between write timestamp and read timestamp cycle #1502610 : disperse eager-lock degrades performance for file create workloads #1503227 : [RFE] Changelog option in a gluster volume disables with no warning if geo-rep is configured #1505660 : [QUOTA] man page of gluster should be updated to list quota commands #1506104 : gluster volume splitbrain info needs to display output of each brick in a stream fashion instead of buffering and dumping at the end #1506140 : Add quorum checks in post-op #1506197 : [Parallel-Readdir]Warning messages in client log saying 'parallel-readdir' is not recognized. #1508898 : Add new configuration option to manage deletion of Worm files #1508947 : glusterfs: Include path in pkgconfig file is wrong #1509189 : timer: Possible race condition between gf_timer_* routines #1509254 : snapshot remove does not cleans lvm for deactivated snaps #1509340 : glusterd does not write pidfile correctly when forking #1509412 : Change default versions of certain features to 3.13 from 4.0 #1509644 : rpc: make actor search parallel #1509647 : rpc: optimize fop program lookup #1509845 : In distribute volume after glusterd restart, brick goes offline #1510324 : Master branch is broken because of the conflicts #1510397 : Compiler atomic built-ins are not correctly detected #1510401 : fstat returns ENOENT/ESTALE #1510415 : spurious failure of tests/bugs/glusterd/bug-1345727-bricks-stop-on-no-quorum-validation.t #1510874 : print-backtrace.sh failing with cpio version 2.11 or older #1510940 : The number of bytes of the quota specified in version 3.7 or later is incorrect #1511310 : Test bug-1483058-replace-brick-quorum-validation.t fails inconsistently #1511339 : In Replica volume 2*2 when quorum is set, after glusterd restart nfs server is coming up instead of self-heal daemon #1512437 : parallel-readdir = TRUE prevents directories listing #1512451 : Not able to create snapshot #1512455 : glustereventsd hardcodes working-directory #1512483 : Not all files synced using geo-replication #1513692 : io-stats appends now instead of overwriting which floods filesystem with logs #1513928 : call stack group list leaks #1514329 : bug-1247563.t is failing on master #1515161 : Memory leak in locks xlator #1515163 : centos regression fails for tests/bugs/replicate/bug-1292379.t #1515266 : Prevent ec from continue processing heal operations after PARENT_DOWN #1516206 : EC DISCARD doesn't punch hole properly #1517068 : Unable to change the Slave configurations #1517554 : help for volume profile is not in man page #1517633 : Geo-rep: access-mount config is not working #1517904 : tests/bugs/core/multiplex-limit-issue-151.t fails sometimes in upstream master #1517961 : Failure of some regression tests on Centos7 (passes on centos6) #1518508 : Change GD_OP_VERSION to 3_13_0 from 3_12_0 for RFE https://bugzilla.redhat.com/show_bug.cgi?id=1464350 #1518582 : Reduce lock contention on fdtable lookup #1519598 : Reduce lock contention on protocol client manipulating fd #1520245 : High mem/cpu usage, brick processes not starting and ssl encryption issues while testing scaling with multiplexing (500-800 vols) #1520758 : [Disperse] Add stripe in cache even if file/data does not exist #1520974 : Compiler warning in dht-common.c because of a switch statement on a boolean #1521013 : rfc.sh should allow custom remote names for ORIGIN #1521014 : quota_unlink_cbk crashes when loc.inode is null #1521116 : Absorb all test fixes from 3.8-fb branch into master #1521213 : crash when gifs_set_logging is called concurrently #1522651 : rdma transport may access an obsolete item in gf_rdma_device_t->all_mr, and causes glusterfsd/glusterfs process crash. #1522662 : Store allocated objects in the mem_acct #1522775 : glusterd consuming high memory #1522847 : gNFS Bug Fixes #1522950 : io-threads is unnecessarily calling accurate time calls on every FOP #1522968 : glusterd bug fixes #1523295 : md-cache should have an option to cache STATFS calls #1523353 : io-stats bugs and features #1524252 : quick-read: Discard cache for fallocate, zerofill and discard ops #1524365 : feature/bitrot: remove internal xattrs from lookup cbk #1524816 : heketi was not removing the LVs associated with Bricks removed when Gluster Volumes were deleted #1526402 : glusterd crashes when 'gluster volume set help' is executed #1526780 : ./run-tests-in-vagrant.sh fails because of disabled Gluster/NFS #1528558 : /usr/sbin/glusterfs crashing on Red Hat OpenShift Container Platform node #1528975 : Fedora 28 (Rawhide) renamed the pyxattr package to python2-pyxattr #1529440 : Files are not rebalanced if destination brick(available size) is of smaller size than source brick(available size) #1529463 : JWT support without external dependency #1529480 : Improve geo-replication logging #1529488 : entries not getting cleared post healing of softlinks (stale entries showing up in heal info) #1529515 : AFR: 3-way-replication: gluster volume set cluster.quorum-count should validate max no. of brick count to accept #1529883 : glusterfind is extremely slow if there are lots of changes #1530281 : glustershd fails to start on a volume force start after a brick is down #1530910 : Use after free in cli_cmd_volume_create_cbk #1531149 : memory leak: get-state leaking memory in small amounts #1531987 : increment of a boolean expression warning #1532238 : Failed to access volume via Samba with undefined symbol from socket.so #1532591 : Tests: Geo-rep tests are failing in few regression machines #1533594 : EC test fails when brick mux is enabled #1533736 : posix_statfs returns incorrect f_bfree values if brick is full. #1533804 : readdir-ahead: change of cache-size should be atomic #1533815 : Mark ./tests/basic/ec/heal-info.t as bad #1534602 : FUSE reverse notificatons are not written to fuse dump #1535438 : Take full lock on files in 3 way replication #1535772 : Random GlusterFSD process dies during rebalance #1536913 : tests/bugs/cli/bug-822830.t fails on Centos 7 and locally #1538723 : build: glibc has removed legacy rpc headers and rpcgen in Fedora28, use libtirpc #1539657 : Georeplication tests intermittently fail #1539701 : gsyncd is running gluster command to get config file path is not required #1539842 : GlusterFS 4.0.0 tracker #1540438 : Remove lock recovery logic from client and server protocol translators #1540554 : Optimize glusterd_import_friend_volume code path #1540882 : Do lock conflict check correctly for wait-list #1541117 : sdfs: crashes if the features is enabled #1541277 : dht_layout_t leak in dht_populate_inode_for_dentry #1541880 : Volume wrong size #1541928 : A down brick is incorrectly considered to be online and makes the volume to be started without any brick available #1542380 : Changes to self-heal logic w.r.t. detecting of split-brains #1542382 : Add quorum checks in post-op #1542829 : Too many log messages about dictionary and options #1543487 : dht_lookup_unlink_of_false_linkto_cbk fails with \"Permission denied\" #1543706 : glusterd fails to attach brick during restart of the node #1543711 : glustershd/glusterd is not using right port when connecting to glusterfsd process #1544366 : Rolling upgrade to 4.0 is broken #1544638 : 3.8 -> 3.10 rolling upgrade fails (same for 3.12 or 3.13) on Ubuntu 14 #1545724 : libgfrpc does not export IPv6 RPC methods even with --with-ipv6-default #1547635 : add option to bulld rpm without server #1547842 : Typo error in __dht_check_free_space function log message #1548264 : [Rebalance] \"Migrate file failed: : failed to get xattr [No data available]\" warnings in rebalance logs #1548271 : DHT calls dht_lookup_everywhere for 1xn volumes #1550808 : memory leak in pre-op in replicate volumes for every write #1551112 : Rolling upgrade to 4.0 is broken #1551640 : GD2 fails to dlopen server xlator #1554077 : 4.0 clients may fail to convert iatt in dict when recieving the same from older (< 4.0) servers","title":"Bugs addressed"},{"location":"release-notes/4.0.1/","text":"Release notes for Gluster 4.0.1 This is a bugfix release. The release notes for 4.0.0 , contain a listing of all the new features that were added and bugs fixed in the GlusterFS 4.0 release. Major changes, features and limitations addressed in this release No Major changes Major issues No Major issues Bugs addressed Bugs addressed since release-4.0.0 are listed below. #1550946 : [brick-mux] performance bottleneck introduced while solving ping timer expiry #1552404 : [CIOT] : Gluster CLI says \"io-threads : enabled\" on existing volumes post upgrade. #1554235 : Memory corruption is causing crashes, hangs and invalid answers #1555198 : After a replace brick command, self-heal takes some time to start healing files on disperse volumes #1555309 : core: libtirpc, backport XDR macro refactor #1557906 : [EC] Read performance of EC volume exported over gNFS is significantly lower than write performance","title":"4.0.1"},{"location":"release-notes/4.0.1/#release-notes-for-gluster-401","text":"This is a bugfix release. The release notes for 4.0.0 , contain a listing of all the new features that were added and bugs fixed in the GlusterFS 4.0 release.","title":"Release notes for Gluster 4.0.1"},{"location":"release-notes/4.0.1/#major-changes-features-and-limitations-addressed-in-this-release","text":"No Major changes","title":"Major changes, features and limitations addressed in this release"},{"location":"release-notes/4.0.1/#major-issues","text":"No Major issues","title":"Major issues"},{"location":"release-notes/4.0.1/#bugs-addressed","text":"Bugs addressed since release-4.0.0 are listed below. #1550946 : [brick-mux] performance bottleneck introduced while solving ping timer expiry #1552404 : [CIOT] : Gluster CLI says \"io-threads : enabled\" on existing volumes post upgrade. #1554235 : Memory corruption is causing crashes, hangs and invalid answers #1555198 : After a replace brick command, self-heal takes some time to start healing files on disperse volumes #1555309 : core: libtirpc, backport XDR macro refactor #1557906 : [EC] Read performance of EC volume exported over gNFS is significantly lower than write performance","title":"Bugs addressed"},{"location":"release-notes/4.0.2/","text":"Release notes for Gluster 4.0.2 This is a bugfix release. The release notes for 4.0.0 , and 4.0.1 contain a listing of all the new features that were added and bugs fixed in the GlusterFS 4.0 release. Major changes, features and limitations addressed in this release This release contains a fix for a security vulerability in Gluster as follows, - http://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2018-1088 - https://nvd.nist.gov/vuln/detail/CVE-2018-1088 Installing the updated packages and restarting gluster services, will update the Gluster shared storage volume volfiles, that are more secure than the defaults currently in place. Further, for increased security, the Gluster shared storage volume can be TLS enabled, and access to the same restricted using the auth.ssl-allow option. See, this guide for more details. Major issues No Major issues Bugs addressed Bugs addressed since release-4.0.1 are listed below. #1558959 : [brick-mux] incorrect event-thread scaling in server_reconfigure() #1559079 : test ./tests/bugs/ec/bug-1236065.t is generating crash on build #1559244 : enable ownthread feature for glusterfs4_0_fop_prog #1561721 : Rebalance failures on a dispersed volume with lookup-optimize enabled #1562728 : SHD is not healing entries in halo replication #1564461 : gfapi: fix a couple of minor issues #1565654 : /var/log/glusterfs/bricks/export_vdb.log flooded with this error message \"Not able to add to index [Too many links]\" #1566822 : [Remove-brick] Many files were not migrated from the decommissioned bricks; commit results in data loss #1569403 : EIO errors on some operations when volume has mixed brick versions on a disperse volume #1570432 : CVE-2018-1088 glusterfs: Privilege escalation via gluster_shared_storage when snapshot scheduling is enabled [fedora-all]","title":"4.0.2"},{"location":"release-notes/4.0.2/#release-notes-for-gluster-402","text":"This is a bugfix release. The release notes for 4.0.0 , and 4.0.1 contain a listing of all the new features that were added and bugs fixed in the GlusterFS 4.0 release.","title":"Release notes for Gluster 4.0.2"},{"location":"release-notes/4.0.2/#major-changes-features-and-limitations-addressed-in-this-release","text":"This release contains a fix for a security vulerability in Gluster as follows, - http://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2018-1088 - https://nvd.nist.gov/vuln/detail/CVE-2018-1088 Installing the updated packages and restarting gluster services, will update the Gluster shared storage volume volfiles, that are more secure than the defaults currently in place. Further, for increased security, the Gluster shared storage volume can be TLS enabled, and access to the same restricted using the auth.ssl-allow option. See, this guide for more details.","title":"Major changes, features and limitations addressed in this release"},{"location":"release-notes/4.0.2/#major-issues","text":"No Major issues","title":"Major issues"},{"location":"release-notes/4.0.2/#bugs-addressed","text":"Bugs addressed since release-4.0.1 are listed below. #1558959 : [brick-mux] incorrect event-thread scaling in server_reconfigure() #1559079 : test ./tests/bugs/ec/bug-1236065.t is generating crash on build #1559244 : enable ownthread feature for glusterfs4_0_fop_prog #1561721 : Rebalance failures on a dispersed volume with lookup-optimize enabled #1562728 : SHD is not healing entries in halo replication #1564461 : gfapi: fix a couple of minor issues #1565654 : /var/log/glusterfs/bricks/export_vdb.log flooded with this error message \"Not able to add to index [Too many links]\" #1566822 : [Remove-brick] Many files were not migrated from the decommissioned bricks; commit results in data loss #1569403 : EIO errors on some operations when volume has mixed brick versions on a disperse volume #1570432 : CVE-2018-1088 glusterfs: Privilege escalation via gluster_shared_storage when snapshot scheduling is enabled [fedora-all]","title":"Bugs addressed"},{"location":"release-notes/4.1.0/","text":"Release notes for Gluster 4.1.0 This is a major release that includes a range of features enhancing management, performance, monitoring, and providing newer functionality like thin arbiters, cloud archival, time consistency. It also contains several bug fixes. A selection of the important features and changes are documented on this page. A full list of bugs that have been addressed is included further below. Announcements Major changes and features Major issues Bugs addressed in the release Announcements As 4.0 was a short term maintenance release, features which have been included in that release are available with 4.1.0 as well. These features may be of interest to users upgrading to 4.1.0 from older than 4.0 releases. The 4.0 release notes captures the list of features that were introduced with 4.0. NOTE: As 4.0 was a short term maintenance release, it will reach end of life (EOL) with the release of 4.1.0. ( reference ) Releases that receive maintenance updates post 4.1 release are, 3.12, and 4.1 ( reference ) NOTE: 3.10 long term maintenance release, will reach end of life (EOL) with the release of 4.1.0. ( reference ) Continuing with this release, the CentOS storage SIG will not build server packages for CentOS6. Server packages will be available for CentOS7 only. For ease of migrations, client packages on CentOS6 will be published and maintained. NOTE : This change was announced here Major changes and features Features are categorized into the following sections, Management Monitoring Performance Standalone Developer related Management GlusterD2 IMP: GlusterD2 in Gluster-4.1.0 is still considered a preview and is experimental. It should not be considered for production use. Users should still expect breaking changes to be possible, though efforts will be taken to avoid such changes. As GD2 is still under heavy development, new features can be expected throughout the 4.1 release. GD2 brings initial support for rebalance, snapshots, intelligent volume provisioning and a lot of other bug fixes and internal changes. Rebalance #786 GD2 supports running rebalance on volumes. Supported rebalance operations include, - rebalance start - rebalance start with fix-layout - rebalance stop - rebalance status Support only exists in the ReST API right now. CLI support will be introduced in subsequent releases. Snapshot #533 Initial support for volume snapshot has been introduced. At the moment, snapshots are supported only on Thin-LVM bricks. Support snapshot operations include, - create - activate/deactivate - list - info Intelligent volume provisioning (IVP) #661 GD2 brings very early preview for intelligent volume creation, similar to Heketi . IMP: This is considered experimental, and the API and implementation is not final. It is very possible that both the API and the implementation will change. IVP enables users to create volumes by just providing the expected volume type and a size, without providing the bricks layout. IVP is supported in CLI in the normal volume create command. More information on IVP can be found in the pull-request. To support IVP, support for adding and managing block devices, and basic support for zones is available. #783 #785 Other changes Other notable changes include, - Support for volume option levels (experimental, advanced, deprecated) #591 - Support for resetting volume options #545 - Option hooks for volume set #708 - Support for setting quota options #583 - Changes to transaction locking #808 - Support for setting metadata on peers and volume #600 #689 #704 - Thin arbiter support #673 #702 In addition to the above, a lot of smaller bug-fixes and enhancements to internal frameworks and tests have also been done. Known issues GD2 is still under heavy development and has lots of known bugs. For filing new bugs or tracking known bugs, please use the GD2 github issue tracker . 2. Changes to gluster based smb.conf share management Previously Gluster used to delete the entire volume share section from smb.conf either after volume is stopped or while disabling user.cifs/user.smb volume set options. With this release those volume share sections, that were added by samba hook scripts inside smb.conf, will not get removed post a volume stop or on disabling user.cifs/user.smb volume set options. Instead we add the following share specific smb.conf parameter to the end of corresponding volume share section to make it unavailable for client access: available = no This will make sure that the additional smb.conf parameters configured externally are retained. For more details on the above parameter search under \"available (S)\" at smb.conf(5) manual page. Monitoring Various xlators are enhanced to provide additional metrics, that help in determining the effectiveness of the xlator in various workloads. These metrics can be dumped and visualized as detailed here . 1. Additional metrics added to negative lookup cache xlator Metrics added are: - negative_lookup_hit_count - negative_lookup_miss_count - get_real_filename_hit_count - get_real_filename_miss_count - nameless_lookup_count - inodes_with_positive_dentry_cache - inodes_with_negative_dentry_cache - dentry_invalidations_recieved - cache_limit - consumed_cache_size - inode_limit - consumed_inodes 2. Additional metrics added to md-cache xlator Metrics added are: - stat_cache_hit_count - stat_cache_miss_count - xattr_cache_hit_count - xattr_cache_miss_count - nameless_lookup_count - negative_lookup_count - stat_cache_invalidations_received - xattr_cache_invalidations_received 3. Additional metrics added to quick-read xlator Metrics added are: - total_files_cached - total_cache_used - cache-hit - cache-miss - cache-invalidations Performance 1. Support for fuse writeback cache Gluster FUSE mounts support FUSE extension to leverage the kernel \"writeback cache\". For usage help see man 8 glusterfs and man 8 mount.glusterfs , specifically the options -kernel-writeback-cache and -attr-times-granularity . 2. Extended eager-lock to metadata transactions in replicate xlator Eager lock feature in replicate xlator is extended to support metadata transactions in addition to data transactions. This helps in improving the performance when there are frequent metadata updates in the workload. This is typically seen with sharded volumes by default, and in other workloads that incur a higher rate of metadata modifications to the same set of files. As a part of this feature, compounded FOPs feature in AFR is deprecated, volumes that are configured to leverage compounding will start disregarding the option use-compound-fops . NOTE: This is an internal change in AFR xlator and is not user controlled or configurable. 3. Support for multi-threaded fuse readers FUSE based mounts can specify number of FUSE request processing threads during a mount. For workloads that have high concurrency on a single client, this helps in processing FUSE requests in parallel, than the existing single reader model. This is provided as a mount time option named reader-thread-count and can be used as follows, # mount -t glusterfs -o reader-thread-count=<n> <server>:<volname> <mntpoint> 4. Configurable aggregate size for write-behind xlator Write-behind xlator provides the option performance.aggregate-size to enable configurable aggregate write sizes. This option enables write-behind xlator to aggregate writes till the specified value before the writes are sent to the bricks. Existing behaviour set this size to a maximum of 128KB per file. The configurable option provides the ability to tune this up or down based on the workload to improve performance of writes. Usage: # gluster volume set <volname> performance.aggregate-size <size> 5. Adaptive read replica selection based on queue length AFR xlator is enhanced with a newer value for the option read-hash-mode . Providing this option with a value of 3 will distribute reads across AFR subvolumes based on the subvol having the least outstanding read requests. This helps in better distributing and hence improving workload performance on reads, in replicate based volumes. Standalone 1. Thin arbiter quorum for 2-way replication NOTE: This feature is available only with GlusterD2 Documentation for the feature is provided here . 2. Automatically configure backup volfile servers in clients NOTE: This feature is available only with GlusterD2 Clients connecting and mounting a Gluster volume, will automatically fetch and configure backup volfile servers, for future volfile updates and fetches, when the initial server used to fetch the volfile and mount is down. When using glusterd, this is achieved using the FUSE mount option backup-volfile-servers , and when using GlusterD2 this is done automatically. 3. (c/m)time equivalence across replicate and disperse subvolumes Enabling the utime feature, enables Gluster to maintain consistent change and modification time stamps on files and directories across bricks. This feature is useful when applications are sensitive to time deltas between operations (for example tar may report \"file changed as we read it\"), to maintain and report equal time stamps on the file across the subvolumes. To enable the feature use, # gluster volume set <volname> features.utime Limitations : - Mounting gluster volume with time attribute options (noatime, realatime...) is not supported with this feature - Certain entry operations (with differing creation flags) would reflect an eventual consistency w.r.t the time attributes - This feature does not guarantee consistent time for directories if hashed sub-volume for the directory is down - readdirp (or directory listing) is not supported with this feature Developer related 1. New API for acquiring leases and acting on lease recalls A new API to acquire a lease on an open file and also to receive callbacks when the lease is recalled, is provided with gfapi. Refer to the header for details on how to use this API. 2. Extended language bindings for gfapi to include perl See, libgfapi-perl - Libgfapi bindings for Perl using FFI Major issues None Bugs addressed Bugs addressed since release-4.0.0 are listed below. #1074947 : add option to build rpm without server #1234873 : glusterfs-resource-agents - volume - voldir is not properly set #1272030 : Remove lock recovery logic from client and server protocol translators #1304962 : Intermittent file creation fail,while doing concurrent writes on distributed volume has more than 40 bricks #1312830 : tests fail because bug-924726.t depends on netstat #1319992 : RFE: Lease support for gluster #1450546 : Paths to some tools are hardcoded to /sbin or /usr/sbin #1450593 : Gluster Python scripts do not check return value of find_library #1468483 : Sharding sends all application sent fsyncs to the main shard file #1495153 : xlator_t structure's 'client_latency' variable is not used #1500649 : Shellcheck errors in hook scripts #1505355 : quota: directories doesn't get heal on newly added bricks when quota is full on sub-directory #1506140 : Add quorum checks in post-op #1507230 : Man pages badly formatted #1512691 : PostgreSQL DB Restore: unexpected data beyond EOF #1517260 : Volume wrong size #1521030 : rpc: unregister programs before registering them again #1523122 : fix serval bugs found on testing protocol/client #1523219 : fuse xlator uses block size and fragment size 128KB leading to rounding off in df output #1530905 : Reducing regression time of glusterd test cases #1533342 : Syntactical errors in hook scripts for managing SELinux context on bricks #1536024 : Rebalance process is behaving differently for AFR and EC volume. #1536186 : build: glibc has removed legacy rpc headers and rpcgen in Fedora28, use libtirpc #1537362 : glustershd/glusterd is not using right port when connecting to glusterfsd process #1537364 : [RFE] - get-state option should mark profiling enabled flag at volume level #1537457 : DHT log messages: Found anomalies in (null) (gfid = 00000000-0000-0000-0000-000000000000). Holes=1 overlaps=0 #1537602 : Georeplication tests intermittently fail #1538258 : build: python-ctypes only in RHEL <= 7 #1538427 : Seeing timer errors in the rebalance logs #1539023 : Add ability to control verbosity settings while compiling #1539166 : [bitrot] scrub ondemand reports it's start as success without additional detail #1539358 : Changes to self-heal logic w.r.t. detecting of split-brains #1539510 : Optimize glusterd_import_friend_volume code path #1539545 : gsyncd is running gluster command to get config file path is not required #1539603 : Glusterfs crash when doing statedump with memory accounting is disabled #1540338 : Change op-version of master to 4.1.0 for future options that maybe added #1540607 : glusterd fails to attach brick during restart of the node #1540669 : Do lock conflict check correctly for wait-list #1541038 : A down brick is incorrectly considered to be online and makes the volume to be started without any brick available #1541264 : dht_layout_t leak in dht_populate_inode_for_dentry #1541916 : The used space in the volume increases when the volume is expanded #1542318 : dht_lookup_unlink_of_false_linkto_cbk fails with \"Permission denied\" #1542829 : Too many log messages about dictionary and options #1543279 : Moving multiple temporary files to the same destination concurrently causes ESTALE error #1544090 : possible memleak in glusterfsd process with brick multiplexing on #1544600 : 3.8 -> 3.10 rolling upgrade fails (same for 3.12 or 3.13) on Ubuntu 14 #1544699 : Rolling upgrade to 4.0 is broken #1544961 : libgfrpc does not export IPv6 RPC methods even with --with-ipv6-default #1545048 : [brick-mux] process termination race while killing glusterfsd on last brick detach #1545056 : [CIOT] : Gluster CLI says \"io-threads : enabled\" on existing volumes post upgrade. #1545891 : Provide a automated way to update bugzilla status with patch merge. #1546129 : Geo-rep: glibc fix breaks geo-replication #1546620 : DHT calls dht_lookup_everywhere for 1xn volumes #1546954 : [Rebalance] \"Migrate file failed: : failed to get xattr [No data available]\" warnings in rebalance logs #1547068 : Bricks getting assigned to different pids depending on whether brick path is IP or hostname based #1547128 : Typo error in __dht_check_free_space function log message #1547662 : After a replace brick command, self-heal takes some time to start healing files on disperse volumes #1547888 : [brick-mux] incorrect event-thread scaling in server_reconfigure() #1548361 : Make afr_fsync a transaction #1549000 : line-coverage tests not capturing details properly. #1549606 : Eager lock should be present for both metadata and data transactions #1549915 : [Fuse Sub-dir] After performing add-brick on volume,doing rm -rf * on subdir mount point fails with \"Transport endpoint is not connected\" #1550078 : memory leak in pre-op in replicate volumes for every write #1550339 : glusterd leaks memory when vol status is issued #1550895 : GD2 fails to dlopen server xlator #1550936 : Pause/Resume of geo-replication with wrong user specified returns success #1553129 : Memory corruption is causing crashes, hangs and invalid answers #1553598 : [Rebalance] ENOSPC errors on few files in rebalance logs #1553926 : configure --without-ipv6-default has odd behaviour #1553938 : configure summary TIRPC result is misleading #1554053 : 4.0 clients may fail to convert iatt in dict when recieving the same from older (< 4.0) servers #1554743 : [EC] Read performance of EC volume exported over gNFS is significantly lower than write performance #1555154 : glusterd: TLS verification fails when using intermediate CA instead of self-signed certificates #1555167 : namespace test failure #1557435 : Enable lookup-optimize by default #1557876 : Fuse mount crashed with only one VM running with its image on that volume #1557932 : Shard replicate volumes don't use eager-lock affectively #1558016 : test ./tests/bugs/ec/bug-1236065.t is generating crash on build #1558074 : [disperse] Add tests for in-memory stripe cache for the non aligned write #1558380 : Modify glfsheal binary to accept socket file path as an optional argument. #1559004 : /var/log/glusterfs/bricks/export_vdb.log flooded with this error message \"Not able to add to index [Too many links]\" #1559075 : enable ownthread feature for glusterfs4_0_fop_prog #1559126 : Incorrect error message in /features/changelog/lib/src/gf-history-changelog.c #1559130 : ssh stderr in glusterfind gets swallowed #1559235 : Increase the inode table size on server when upcall enabled #1560319 : NFS client gets \"Invalid argument\" when writing file through nfs-ganesha with quota #1560393 : Fix regresssion failure for ./tests/basic/md-cache/bug-1418249.t #1560411 : fallocate created data set is crossing storage reserve space limits resulting 100% brick full #1560441 : volume stop in mgmt v3 #1560589 : nl-cache.t fails #1560957 : After performing remove-brick followed by add-brick operation, brick went offline state #1561129 : When storage reserve limit is reached, appending data to an existing file throws EROFS error #1561406 : Rebalance failures on a dispersed volume with lookup-optimize enabled #1562052 : build: revert configure --without-ipv6-default behaviour #1562717 : SHD is not healing entries in halo replication #1562907 : set mgmt_v3_timer->timer to NULL after mgmt_v3_timer is deleted #1563273 : mark brick as online only when portmap registration is completed #1563334 : Honour cluster.localtime-logging option for all the daemons #1563511 : Redundant synchronization in rename codepath for a single subvolume DHT #1563945 : [EC] Turn ON the stripe-cache option by default for ec volume #1564198 : [Remove-brick] Many files were not migrated from the decommissioned bricks; commit results in data loss #1564235 : gfapi: fix a couple of minor issues #1564600 : Client can create denial of service (DOS) conditions on server #1566067 : Volume status inode is broken with brickmux #1566207 : Linux kernel untar failed with \"xz: (stdin): Read error: Invalid argument\" immediate after add-brick #1566303 : Removing directories from multiple clients throws ESTALE errors #1566386 : Disable choose-local in groups virt and gluster-block #1566732 : EIO errors on some operations when volume has mixed brick versions on a disperse volume #1567209 : Geo-rep: faulty session due to OSError: [Errno 95] Operation not supported #1567880 : Grant Deepshikha access to all CI-related infrastructure #1567881 : Halo replication I/O path is not working #1568348 : Rebalance on few nodes doesn't seem to complete - stuck at FUTEX_WAIT #1568521 : shard files present even after deleting vm from ovirt UI #1568820 : Add generated HMAC token in header for webhook calls #1568844 : [snapshot-scheduler]Prevent access of shared storage volume from the outside client #1569198 : bitrot scrub status does not show the brick where the object (file) is corrupted #1569489 : Need heal-timeout to be configured as low as 5 seconds #1570011 : test case is failing ./tests/bugs/glusterd/add-brick-and-validate-replicated-volume-options.t while brick mux is enabled #1570538 : linux untar errors out at completion during disperse volume inservice upgrade #1570962 : print the path of the corrupted object in scrub status #1571069 : [geo-rep]: Lot of changelogs retries and \"dict is null\" errors in geo-rep logs #1572076 : Dictionary response is not captured in syncop_(f)xattrop #1572581 : Remove-brick failed on Distributed volume while rm -rf is in-progress #1572586 : dht: do not allow migration if file is open #1573066 : growing glusterd memory usage with connected RHGSWA #1573119 : Amends in volume profile option 'gluster-block' #1573220 : Memory leak in volume tier status command #1574259 : Errors unintentionally reported for snapshot status #1574305 : rm command hangs in fuse_request_send #1574606 : the regression test \"tests/bugs/posix/bug-990028.t\" fails #1575294 : lease recall callback should be avoided on closed #1575386 : GlusterFS 4.1.0 tracker #1575707 : Gluster volume smb share options are getting overwritten after restating the gluster volume #1576814 : GlusterFS can be improved #1577162 : gfapi: broken symbol versions #1579674 : Remove EIO from the dht_inode_missing macro #1579736 : Additional log messages in dht_readdir(p)_cbk #1579757 : DHT Log flooding in mount log \"key=trusted.glusterfs.dht.mds [Invalid argument]\" #1580215 : [geo-rep]: Lot of changelogs retries and \"dict is null\" errors in geo-rep logs #1580540 : make getfattr return proper response for \"glusterfs.gfidtopath\" xattr for files created when gfid2path was off #1581548 : writes succeed when only good brick is down in 1x3 volume #1581745 : bug-1309462.t is failing reliably due to changes in security.capability changes in the kernel #1582056 : Input/Output errors on a disperse volume with concurrent reads and writes #1582063 : rpc: The gluster auth version is always AUTH_GLUSTERFS_v2 #1582068 : ctime: Rename and unlink does not update ctime #1582072 : posix/ctime: Access time is not updated for file with a hardlink #1582080 : posix/ctime: The first lookup on file is not healing the gfid #1582199 : posix unwinds readdirp calls with readdir signature #1582286 : Brick-mux regressions failing on 4.1 branch #1582531 : posix/ctime: Mtime is not updated on setting it to older date #1582549 : api: missing __THROW on pub function decls #1583016 : libgfapi: glfs init fails on afr volume with ctime feature enabled #1583734 : rpc_transport_unref() called for an unregistered socket fd #1583769 : Fix incorrect rebalance log message #1584633 : Brick process crashed after upgrade from RHGS-3.3.1 async(7.4) to RHGS-3.4(7.5) #1585894 : posix/ctime: EC self heal of directory is blocked with ctime feature enabled #1587908 : Fix deadlock in failure codepath of shard fsync #1590128 : xdata is leaking in server3_3_seek","title":"4.1.0"},{"location":"release-notes/4.1.0/#release-notes-for-gluster-410","text":"This is a major release that includes a range of features enhancing management, performance, monitoring, and providing newer functionality like thin arbiters, cloud archival, time consistency. It also contains several bug fixes. A selection of the important features and changes are documented on this page. A full list of bugs that have been addressed is included further below. Announcements Major changes and features Major issues Bugs addressed in the release","title":"Release notes for Gluster 4.1.0"},{"location":"release-notes/4.1.0/#announcements","text":"As 4.0 was a short term maintenance release, features which have been included in that release are available with 4.1.0 as well. These features may be of interest to users upgrading to 4.1.0 from older than 4.0 releases. The 4.0 release notes captures the list of features that were introduced with 4.0. NOTE: As 4.0 was a short term maintenance release, it will reach end of life (EOL) with the release of 4.1.0. ( reference ) Releases that receive maintenance updates post 4.1 release are, 3.12, and 4.1 ( reference ) NOTE: 3.10 long term maintenance release, will reach end of life (EOL) with the release of 4.1.0. ( reference ) Continuing with this release, the CentOS storage SIG will not build server packages for CentOS6. Server packages will be available for CentOS7 only. For ease of migrations, client packages on CentOS6 will be published and maintained. NOTE : This change was announced here","title":"Announcements"},{"location":"release-notes/4.1.0/#major-changes-and-features","text":"Features are categorized into the following sections, Management Monitoring Performance Standalone Developer related","title":"Major changes and features"},{"location":"release-notes/4.1.0/#management","text":"","title":"Management"},{"location":"release-notes/4.1.0/#glusterd2","text":"IMP: GlusterD2 in Gluster-4.1.0 is still considered a preview and is experimental. It should not be considered for production use. Users should still expect breaking changes to be possible, though efforts will be taken to avoid such changes. As GD2 is still under heavy development, new features can be expected throughout the 4.1 release. GD2 brings initial support for rebalance, snapshots, intelligent volume provisioning and a lot of other bug fixes and internal changes.","title":"GlusterD2"},{"location":"release-notes/4.1.0/#rebalance-786","text":"GD2 supports running rebalance on volumes. Supported rebalance operations include, - rebalance start - rebalance start with fix-layout - rebalance stop - rebalance status Support only exists in the ReST API right now. CLI support will be introduced in subsequent releases.","title":"Rebalance #786"},{"location":"release-notes/4.1.0/#snapshot-533","text":"Initial support for volume snapshot has been introduced. At the moment, snapshots are supported only on Thin-LVM bricks. Support snapshot operations include, - create - activate/deactivate - list - info","title":"Snapshot #533"},{"location":"release-notes/4.1.0/#intelligent-volume-provisioning-ivp-661","text":"GD2 brings very early preview for intelligent volume creation, similar to Heketi . IMP: This is considered experimental, and the API and implementation is not final. It is very possible that both the API and the implementation will change. IVP enables users to create volumes by just providing the expected volume type and a size, without providing the bricks layout. IVP is supported in CLI in the normal volume create command. More information on IVP can be found in the pull-request. To support IVP, support for adding and managing block devices, and basic support for zones is available. #783 #785","title":"Intelligent volume provisioning (IVP) #661"},{"location":"release-notes/4.1.0/#other-changes","text":"Other notable changes include, - Support for volume option levels (experimental, advanced, deprecated) #591 - Support for resetting volume options #545 - Option hooks for volume set #708 - Support for setting quota options #583 - Changes to transaction locking #808 - Support for setting metadata on peers and volume #600 #689 #704 - Thin arbiter support #673 #702 In addition to the above, a lot of smaller bug-fixes and enhancements to internal frameworks and tests have also been done.","title":"Other changes"},{"location":"release-notes/4.1.0/#known-issues","text":"GD2 is still under heavy development and has lots of known bugs. For filing new bugs or tracking known bugs, please use the GD2 github issue tracker .","title":"Known issues"},{"location":"release-notes/4.1.0/#2-changes-to-gluster-based-smbconf-share-management","text":"Previously Gluster used to delete the entire volume share section from smb.conf either after volume is stopped or while disabling user.cifs/user.smb volume set options. With this release those volume share sections, that were added by samba hook scripts inside smb.conf, will not get removed post a volume stop or on disabling user.cifs/user.smb volume set options. Instead we add the following share specific smb.conf parameter to the end of corresponding volume share section to make it unavailable for client access: available = no This will make sure that the additional smb.conf parameters configured externally are retained. For more details on the above parameter search under \"available (S)\" at smb.conf(5) manual page.","title":"2. Changes to gluster based smb.conf share management"},{"location":"release-notes/4.1.0/#monitoring","text":"Various xlators are enhanced to provide additional metrics, that help in determining the effectiveness of the xlator in various workloads. These metrics can be dumped and visualized as detailed here .","title":"Monitoring"},{"location":"release-notes/4.1.0/#1-additional-metrics-added-to-negative-lookup-cache-xlator","text":"Metrics added are: - negative_lookup_hit_count - negative_lookup_miss_count - get_real_filename_hit_count - get_real_filename_miss_count - nameless_lookup_count - inodes_with_positive_dentry_cache - inodes_with_negative_dentry_cache - dentry_invalidations_recieved - cache_limit - consumed_cache_size - inode_limit - consumed_inodes","title":"1. Additional metrics added to negative lookup cache xlator"},{"location":"release-notes/4.1.0/#2-additional-metrics-added-to-md-cache-xlator","text":"Metrics added are: - stat_cache_hit_count - stat_cache_miss_count - xattr_cache_hit_count - xattr_cache_miss_count - nameless_lookup_count - negative_lookup_count - stat_cache_invalidations_received - xattr_cache_invalidations_received","title":"2. Additional metrics added to md-cache xlator"},{"location":"release-notes/4.1.0/#3-additional-metrics-added-to-quick-read-xlator","text":"Metrics added are: - total_files_cached - total_cache_used - cache-hit - cache-miss - cache-invalidations","title":"3. Additional metrics added to quick-read xlator"},{"location":"release-notes/4.1.0/#performance","text":"","title":"Performance"},{"location":"release-notes/4.1.0/#1-support-for-fuse-writeback-cache","text":"Gluster FUSE mounts support FUSE extension to leverage the kernel \"writeback cache\". For usage help see man 8 glusterfs and man 8 mount.glusterfs , specifically the options -kernel-writeback-cache and -attr-times-granularity .","title":"1. Support for fuse writeback cache"},{"location":"release-notes/4.1.0/#2-extended-eager-lock-to-metadata-transactions-in-replicate-xlator","text":"Eager lock feature in replicate xlator is extended to support metadata transactions in addition to data transactions. This helps in improving the performance when there are frequent metadata updates in the workload. This is typically seen with sharded volumes by default, and in other workloads that incur a higher rate of metadata modifications to the same set of files. As a part of this feature, compounded FOPs feature in AFR is deprecated, volumes that are configured to leverage compounding will start disregarding the option use-compound-fops . NOTE: This is an internal change in AFR xlator and is not user controlled or configurable.","title":"2. Extended eager-lock to metadata transactions in replicate xlator"},{"location":"release-notes/4.1.0/#3-support-for-multi-threaded-fuse-readers","text":"FUSE based mounts can specify number of FUSE request processing threads during a mount. For workloads that have high concurrency on a single client, this helps in processing FUSE requests in parallel, than the existing single reader model. This is provided as a mount time option named reader-thread-count and can be used as follows, # mount -t glusterfs -o reader-thread-count=<n> <server>:<volname> <mntpoint>","title":"3. Support for multi-threaded fuse readers"},{"location":"release-notes/4.1.0/#4-configurable-aggregate-size-for-write-behind-xlator","text":"Write-behind xlator provides the option performance.aggregate-size to enable configurable aggregate write sizes. This option enables write-behind xlator to aggregate writes till the specified value before the writes are sent to the bricks. Existing behaviour set this size to a maximum of 128KB per file. The configurable option provides the ability to tune this up or down based on the workload to improve performance of writes. Usage: # gluster volume set <volname> performance.aggregate-size <size>","title":"4. Configurable aggregate size for write-behind xlator"},{"location":"release-notes/4.1.0/#5-adaptive-read-replica-selection-based-on-queue-length","text":"AFR xlator is enhanced with a newer value for the option read-hash-mode . Providing this option with a value of 3 will distribute reads across AFR subvolumes based on the subvol having the least outstanding read requests. This helps in better distributing and hence improving workload performance on reads, in replicate based volumes.","title":"5. Adaptive read replica selection based on queue length"},{"location":"release-notes/4.1.0/#standalone","text":"","title":"Standalone"},{"location":"release-notes/4.1.0/#1-thin-arbiter-quorum-for-2-way-replication","text":"NOTE: This feature is available only with GlusterD2 Documentation for the feature is provided here .","title":"1. Thin arbiter quorum for 2-way replication"},{"location":"release-notes/4.1.0/#2-automatically-configure-backup-volfile-servers-in-clients","text":"NOTE: This feature is available only with GlusterD2 Clients connecting and mounting a Gluster volume, will automatically fetch and configure backup volfile servers, for future volfile updates and fetches, when the initial server used to fetch the volfile and mount is down. When using glusterd, this is achieved using the FUSE mount option backup-volfile-servers , and when using GlusterD2 this is done automatically.","title":"2. Automatically configure backup volfile servers in clients"},{"location":"release-notes/4.1.0/#3-cmtime-equivalence-across-replicate-and-disperse-subvolumes","text":"Enabling the utime feature, enables Gluster to maintain consistent change and modification time stamps on files and directories across bricks. This feature is useful when applications are sensitive to time deltas between operations (for example tar may report \"file changed as we read it\"), to maintain and report equal time stamps on the file across the subvolumes. To enable the feature use, # gluster volume set <volname> features.utime Limitations : - Mounting gluster volume with time attribute options (noatime, realatime...) is not supported with this feature - Certain entry operations (with differing creation flags) would reflect an eventual consistency w.r.t the time attributes - This feature does not guarantee consistent time for directories if hashed sub-volume for the directory is down - readdirp (or directory listing) is not supported with this feature","title":"3. (c/m)time equivalence across replicate and disperse subvolumes"},{"location":"release-notes/4.1.0/#developer-related","text":"","title":"Developer related"},{"location":"release-notes/4.1.0/#1-new-api-for-acquiring-leases-and-acting-on-lease-recalls","text":"A new API to acquire a lease on an open file and also to receive callbacks when the lease is recalled, is provided with gfapi. Refer to the header for details on how to use this API.","title":"1. New API for acquiring leases and acting on lease recalls"},{"location":"release-notes/4.1.0/#2-extended-language-bindings-for-gfapi-to-include-perl","text":"See, libgfapi-perl - Libgfapi bindings for Perl using FFI","title":"2. Extended language bindings for gfapi to include perl"},{"location":"release-notes/4.1.0/#major-issues","text":"None","title":"Major issues"},{"location":"release-notes/4.1.0/#bugs-addressed","text":"Bugs addressed since release-4.0.0 are listed below. #1074947 : add option to build rpm without server #1234873 : glusterfs-resource-agents - volume - voldir is not properly set #1272030 : Remove lock recovery logic from client and server protocol translators #1304962 : Intermittent file creation fail,while doing concurrent writes on distributed volume has more than 40 bricks #1312830 : tests fail because bug-924726.t depends on netstat #1319992 : RFE: Lease support for gluster #1450546 : Paths to some tools are hardcoded to /sbin or /usr/sbin #1450593 : Gluster Python scripts do not check return value of find_library #1468483 : Sharding sends all application sent fsyncs to the main shard file #1495153 : xlator_t structure's 'client_latency' variable is not used #1500649 : Shellcheck errors in hook scripts #1505355 : quota: directories doesn't get heal on newly added bricks when quota is full on sub-directory #1506140 : Add quorum checks in post-op #1507230 : Man pages badly formatted #1512691 : PostgreSQL DB Restore: unexpected data beyond EOF #1517260 : Volume wrong size #1521030 : rpc: unregister programs before registering them again #1523122 : fix serval bugs found on testing protocol/client #1523219 : fuse xlator uses block size and fragment size 128KB leading to rounding off in df output #1530905 : Reducing regression time of glusterd test cases #1533342 : Syntactical errors in hook scripts for managing SELinux context on bricks #1536024 : Rebalance process is behaving differently for AFR and EC volume. #1536186 : build: glibc has removed legacy rpc headers and rpcgen in Fedora28, use libtirpc #1537362 : glustershd/glusterd is not using right port when connecting to glusterfsd process #1537364 : [RFE] - get-state option should mark profiling enabled flag at volume level #1537457 : DHT log messages: Found anomalies in (null) (gfid = 00000000-0000-0000-0000-000000000000). Holes=1 overlaps=0 #1537602 : Georeplication tests intermittently fail #1538258 : build: python-ctypes only in RHEL <= 7 #1538427 : Seeing timer errors in the rebalance logs #1539023 : Add ability to control verbosity settings while compiling #1539166 : [bitrot] scrub ondemand reports it's start as success without additional detail #1539358 : Changes to self-heal logic w.r.t. detecting of split-brains #1539510 : Optimize glusterd_import_friend_volume code path #1539545 : gsyncd is running gluster command to get config file path is not required #1539603 : Glusterfs crash when doing statedump with memory accounting is disabled #1540338 : Change op-version of master to 4.1.0 for future options that maybe added #1540607 : glusterd fails to attach brick during restart of the node #1540669 : Do lock conflict check correctly for wait-list #1541038 : A down brick is incorrectly considered to be online and makes the volume to be started without any brick available #1541264 : dht_layout_t leak in dht_populate_inode_for_dentry #1541916 : The used space in the volume increases when the volume is expanded #1542318 : dht_lookup_unlink_of_false_linkto_cbk fails with \"Permission denied\" #1542829 : Too many log messages about dictionary and options #1543279 : Moving multiple temporary files to the same destination concurrently causes ESTALE error #1544090 : possible memleak in glusterfsd process with brick multiplexing on #1544600 : 3.8 -> 3.10 rolling upgrade fails (same for 3.12 or 3.13) on Ubuntu 14 #1544699 : Rolling upgrade to 4.0 is broken #1544961 : libgfrpc does not export IPv6 RPC methods even with --with-ipv6-default #1545048 : [brick-mux] process termination race while killing glusterfsd on last brick detach #1545056 : [CIOT] : Gluster CLI says \"io-threads : enabled\" on existing volumes post upgrade. #1545891 : Provide a automated way to update bugzilla status with patch merge. #1546129 : Geo-rep: glibc fix breaks geo-replication #1546620 : DHT calls dht_lookup_everywhere for 1xn volumes #1546954 : [Rebalance] \"Migrate file failed: : failed to get xattr [No data available]\" warnings in rebalance logs #1547068 : Bricks getting assigned to different pids depending on whether brick path is IP or hostname based #1547128 : Typo error in __dht_check_free_space function log message #1547662 : After a replace brick command, self-heal takes some time to start healing files on disperse volumes #1547888 : [brick-mux] incorrect event-thread scaling in server_reconfigure() #1548361 : Make afr_fsync a transaction #1549000 : line-coverage tests not capturing details properly. #1549606 : Eager lock should be present for both metadata and data transactions #1549915 : [Fuse Sub-dir] After performing add-brick on volume,doing rm -rf * on subdir mount point fails with \"Transport endpoint is not connected\" #1550078 : memory leak in pre-op in replicate volumes for every write #1550339 : glusterd leaks memory when vol status is issued #1550895 : GD2 fails to dlopen server xlator #1550936 : Pause/Resume of geo-replication with wrong user specified returns success #1553129 : Memory corruption is causing crashes, hangs and invalid answers #1553598 : [Rebalance] ENOSPC errors on few files in rebalance logs #1553926 : configure --without-ipv6-default has odd behaviour #1553938 : configure summary TIRPC result is misleading #1554053 : 4.0 clients may fail to convert iatt in dict when recieving the same from older (< 4.0) servers #1554743 : [EC] Read performance of EC volume exported over gNFS is significantly lower than write performance #1555154 : glusterd: TLS verification fails when using intermediate CA instead of self-signed certificates #1555167 : namespace test failure #1557435 : Enable lookup-optimize by default #1557876 : Fuse mount crashed with only one VM running with its image on that volume #1557932 : Shard replicate volumes don't use eager-lock affectively #1558016 : test ./tests/bugs/ec/bug-1236065.t is generating crash on build #1558074 : [disperse] Add tests for in-memory stripe cache for the non aligned write #1558380 : Modify glfsheal binary to accept socket file path as an optional argument. #1559004 : /var/log/glusterfs/bricks/export_vdb.log flooded with this error message \"Not able to add to index [Too many links]\" #1559075 : enable ownthread feature for glusterfs4_0_fop_prog #1559126 : Incorrect error message in /features/changelog/lib/src/gf-history-changelog.c #1559130 : ssh stderr in glusterfind gets swallowed #1559235 : Increase the inode table size on server when upcall enabled #1560319 : NFS client gets \"Invalid argument\" when writing file through nfs-ganesha with quota #1560393 : Fix regresssion failure for ./tests/basic/md-cache/bug-1418249.t #1560411 : fallocate created data set is crossing storage reserve space limits resulting 100% brick full #1560441 : volume stop in mgmt v3 #1560589 : nl-cache.t fails #1560957 : After performing remove-brick followed by add-brick operation, brick went offline state #1561129 : When storage reserve limit is reached, appending data to an existing file throws EROFS error #1561406 : Rebalance failures on a dispersed volume with lookup-optimize enabled #1562052 : build: revert configure --without-ipv6-default behaviour #1562717 : SHD is not healing entries in halo replication #1562907 : set mgmt_v3_timer->timer to NULL after mgmt_v3_timer is deleted #1563273 : mark brick as online only when portmap registration is completed #1563334 : Honour cluster.localtime-logging option for all the daemons #1563511 : Redundant synchronization in rename codepath for a single subvolume DHT #1563945 : [EC] Turn ON the stripe-cache option by default for ec volume #1564198 : [Remove-brick] Many files were not migrated from the decommissioned bricks; commit results in data loss #1564235 : gfapi: fix a couple of minor issues #1564600 : Client can create denial of service (DOS) conditions on server #1566067 : Volume status inode is broken with brickmux #1566207 : Linux kernel untar failed with \"xz: (stdin): Read error: Invalid argument\" immediate after add-brick #1566303 : Removing directories from multiple clients throws ESTALE errors #1566386 : Disable choose-local in groups virt and gluster-block #1566732 : EIO errors on some operations when volume has mixed brick versions on a disperse volume #1567209 : Geo-rep: faulty session due to OSError: [Errno 95] Operation not supported #1567880 : Grant Deepshikha access to all CI-related infrastructure #1567881 : Halo replication I/O path is not working #1568348 : Rebalance on few nodes doesn't seem to complete - stuck at FUTEX_WAIT #1568521 : shard files present even after deleting vm from ovirt UI #1568820 : Add generated HMAC token in header for webhook calls #1568844 : [snapshot-scheduler]Prevent access of shared storage volume from the outside client #1569198 : bitrot scrub status does not show the brick where the object (file) is corrupted #1569489 : Need heal-timeout to be configured as low as 5 seconds #1570011 : test case is failing ./tests/bugs/glusterd/add-brick-and-validate-replicated-volume-options.t while brick mux is enabled #1570538 : linux untar errors out at completion during disperse volume inservice upgrade #1570962 : print the path of the corrupted object in scrub status #1571069 : [geo-rep]: Lot of changelogs retries and \"dict is null\" errors in geo-rep logs #1572076 : Dictionary response is not captured in syncop_(f)xattrop #1572581 : Remove-brick failed on Distributed volume while rm -rf is in-progress #1572586 : dht: do not allow migration if file is open #1573066 : growing glusterd memory usage with connected RHGSWA #1573119 : Amends in volume profile option 'gluster-block' #1573220 : Memory leak in volume tier status command #1574259 : Errors unintentionally reported for snapshot status #1574305 : rm command hangs in fuse_request_send #1574606 : the regression test \"tests/bugs/posix/bug-990028.t\" fails #1575294 : lease recall callback should be avoided on closed #1575386 : GlusterFS 4.1.0 tracker #1575707 : Gluster volume smb share options are getting overwritten after restating the gluster volume #1576814 : GlusterFS can be improved #1577162 : gfapi: broken symbol versions #1579674 : Remove EIO from the dht_inode_missing macro #1579736 : Additional log messages in dht_readdir(p)_cbk #1579757 : DHT Log flooding in mount log \"key=trusted.glusterfs.dht.mds [Invalid argument]\" #1580215 : [geo-rep]: Lot of changelogs retries and \"dict is null\" errors in geo-rep logs #1580540 : make getfattr return proper response for \"glusterfs.gfidtopath\" xattr for files created when gfid2path was off #1581548 : writes succeed when only good brick is down in 1x3 volume #1581745 : bug-1309462.t is failing reliably due to changes in security.capability changes in the kernel #1582056 : Input/Output errors on a disperse volume with concurrent reads and writes #1582063 : rpc: The gluster auth version is always AUTH_GLUSTERFS_v2 #1582068 : ctime: Rename and unlink does not update ctime #1582072 : posix/ctime: Access time is not updated for file with a hardlink #1582080 : posix/ctime: The first lookup on file is not healing the gfid #1582199 : posix unwinds readdirp calls with readdir signature #1582286 : Brick-mux regressions failing on 4.1 branch #1582531 : posix/ctime: Mtime is not updated on setting it to older date #1582549 : api: missing __THROW on pub function decls #1583016 : libgfapi: glfs init fails on afr volume with ctime feature enabled #1583734 : rpc_transport_unref() called for an unregistered socket fd #1583769 : Fix incorrect rebalance log message #1584633 : Brick process crashed after upgrade from RHGS-3.3.1 async(7.4) to RHGS-3.4(7.5) #1585894 : posix/ctime: EC self heal of directory is blocked with ctime feature enabled #1587908 : Fix deadlock in failure codepath of shard fsync #1590128 : xdata is leaking in server3_3_seek","title":"Bugs addressed"},{"location":"release-notes/4.1.1/","text":"Release notes for Gluster 4.1.1 This is a bugfix release. The release notes for 4.1.0 contains a listing of all the new features that were added and bugs fixed in the GlusterFS 4.1 stable release. Major changes, features and limitations addressed in this release This release contains a fix for a security vulerability in Gluster as follows, - http://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2018-10841 - https://nvd.nist.gov/vuln/detail/CVE-2018-10841 Installing the updated packages and restarting gluster services on gluster brick hosts, will help prevent the security issue. Major issues None Bugs addressed Bugs addressed since release-4.1.0 are listed below. #1590195 : /usr/sbin/gcron.py aborts with OSError #1591185 : Gluster Block PVC fails to mount on Jenkins pod #1593525 : CVE-2018-10841 glusterfs: access trusted peer group via remote-host command [glusterfs upstream]","title":"4.1.1"},{"location":"release-notes/4.1.1/#release-notes-for-gluster-411","text":"This is a bugfix release. The release notes for 4.1.0 contains a listing of all the new features that were added and bugs fixed in the GlusterFS 4.1 stable release.","title":"Release notes for Gluster 4.1.1"},{"location":"release-notes/4.1.1/#major-changes-features-and-limitations-addressed-in-this-release","text":"This release contains a fix for a security vulerability in Gluster as follows, - http://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2018-10841 - https://nvd.nist.gov/vuln/detail/CVE-2018-10841 Installing the updated packages and restarting gluster services on gluster brick hosts, will help prevent the security issue.","title":"Major changes, features and limitations addressed in this release"},{"location":"release-notes/4.1.1/#major-issues","text":"None","title":"Major issues"},{"location":"release-notes/4.1.1/#bugs-addressed","text":"Bugs addressed since release-4.1.0 are listed below. #1590195 : /usr/sbin/gcron.py aborts with OSError #1591185 : Gluster Block PVC fails to mount on Jenkins pod #1593525 : CVE-2018-10841 glusterfs: access trusted peer group via remote-host command [glusterfs upstream]","title":"Bugs addressed"},{"location":"release-notes/4.1.10/","text":"Release notes for Gluster 4.1.10 This is a bugfix release. The release notes for 4.1.0 , 4.1.1 , 4.1.2 , 4.1.3 , 4.1.4 , 4.1.5 , 4.1.6 , 4.1.7 , 4.1.8 and 4.1.9 contains a listing of all the new features that were added and bugs fixed in the GlusterFS 4.1 stable release. Major changes, features and limitations addressed in this release None Major issues None Bugs addressed Bugs addressed since release-4.1.9 are listed below. #1721109 : Failed to create volume which transport_type is \"tcp,rdma\" #1729221 : Upcall: Avoid sending upcalls for invalid Inode #1729223 : Ganesha-gfapi logs are flooded with error messages related to \"gf_uuid_is_null(gfid)) [Invalid argument]\" when lookups are running from multiple clients","title":"4.1.10"},{"location":"release-notes/4.1.10/#release-notes-for-gluster-4110","text":"This is a bugfix release. The release notes for 4.1.0 , 4.1.1 , 4.1.2 , 4.1.3 , 4.1.4 , 4.1.5 , 4.1.6 , 4.1.7 , 4.1.8 and 4.1.9 contains a listing of all the new features that were added and bugs fixed in the GlusterFS 4.1 stable release.","title":"Release notes for Gluster 4.1.10"},{"location":"release-notes/4.1.10/#major-changes-features-and-limitations-addressed-in-this-release","text":"None","title":"Major changes, features and limitations addressed in this release"},{"location":"release-notes/4.1.10/#major-issues","text":"None","title":"Major issues"},{"location":"release-notes/4.1.10/#bugs-addressed","text":"Bugs addressed since release-4.1.9 are listed below. #1721109 : Failed to create volume which transport_type is \"tcp,rdma\" #1729221 : Upcall: Avoid sending upcalls for invalid Inode #1729223 : Ganesha-gfapi logs are flooded with error messages related to \"gf_uuid_is_null(gfid)) [Invalid argument]\" when lookups are running from multiple clients","title":"Bugs addressed"},{"location":"release-notes/4.1.2/","text":"Release notes for Gluster 4.1.2 This is a bugfix release. The release notes for 4.1.0 and 4.1.1 contains a listing of all the new features that were added and bugs fixed in the GlusterFS 4.1 stable release. Major changes, features and limitations addressed in this release Release 4.1.0 notes incorrectly reported that all python code in Gluster packages are python3 compliant, this is not the case and the release note is amended accordingly. Major issues None Bugs addressed Bugs addressed since release-4.1.1 are listed below. #1593536 : ctime: Self heal of symlink is failing on EC subvolume #1593537 : posix/ctime: Mdata value of a directory is different across replica/EC subvolume #1595524 : rmdir is leaking softlinks to directories in .glusterfs #1597116 : afr: don't update readables if inode refresh failed on all children #1597117 : lookup not assigning gfid if file is not present in all bricks of replica #1597229 : glustershd crashes when index heal is launched before graph is initialized. #1598193 : Stale lock with lk-owner all-zeros is observed in some tests #1599629 : Don't execute statements after decrementing call count in afr #1599785 : _is_prefix should return false for 0-length strings #1600941 : [geo-rep]: geo-replication scheduler is failing due to unsuccessful umount #1603056 : When reserve limits are reached, append on an existing file after truncate operation results to hang #1603099 : directories are invisible on client side","title":"4.1.2"},{"location":"release-notes/4.1.2/#release-notes-for-gluster-412","text":"This is a bugfix release. The release notes for 4.1.0 and 4.1.1 contains a listing of all the new features that were added and bugs fixed in the GlusterFS 4.1 stable release.","title":"Release notes for Gluster 4.1.2"},{"location":"release-notes/4.1.2/#major-changes-features-and-limitations-addressed-in-this-release","text":"Release 4.1.0 notes incorrectly reported that all python code in Gluster packages are python3 compliant, this is not the case and the release note is amended accordingly.","title":"Major changes, features and limitations addressed in this release"},{"location":"release-notes/4.1.2/#major-issues","text":"None","title":"Major issues"},{"location":"release-notes/4.1.2/#bugs-addressed","text":"Bugs addressed since release-4.1.1 are listed below. #1593536 : ctime: Self heal of symlink is failing on EC subvolume #1593537 : posix/ctime: Mdata value of a directory is different across replica/EC subvolume #1595524 : rmdir is leaking softlinks to directories in .glusterfs #1597116 : afr: don't update readables if inode refresh failed on all children #1597117 : lookup not assigning gfid if file is not present in all bricks of replica #1597229 : glustershd crashes when index heal is launched before graph is initialized. #1598193 : Stale lock with lk-owner all-zeros is observed in some tests #1599629 : Don't execute statements after decrementing call count in afr #1599785 : _is_prefix should return false for 0-length strings #1600941 : [geo-rep]: geo-replication scheduler is failing due to unsuccessful umount #1603056 : When reserve limits are reached, append on an existing file after truncate operation results to hang #1603099 : directories are invisible on client side","title":"Bugs addressed"},{"location":"release-notes/4.1.3/","text":"Release notes for Gluster 4.1.3 This is a bugfix release. The release notes for 4.1.0 , 4.1.1 , and 4.1.2 contains a listing of all the new features that were added and bugs fixed in the GlusterFS 4.1 stable release. NOTE: Next minor release tentative date: Week of 24th September, 2018 Major changes, features and limitations addressed in this release None Major issues Bug #1601356 titled \"Problem with SSL/TLS encryption\", is not yet fixed with this release. Patch to fix the same is in progress and can be tracked here . Bugs addressed Bugs addressed since release-4.1.2 are listed below. #1425326 : gluster bash completion leaks TOP=0 into the environment #1596686 : key = trusted.glusterfs.protect.writes [Invalid argument]; key = glusterfs.avoid.overwrite [Invalid argument] #1609550 : glusterfs-resource-agents should not be built for el6 #1609551 : glusterfs-resource-agents should not be built for el6 #1611104 : [geo-rep]: Upgrade fails, session in FAULTY state #1611106 : Glusterd crashed on a few (master) nodes #1611108 : [geo-rep]: Geo-rep scheduler fails #1611110 : Glusterd memory leaking in gf_gld_mt_linebuf #1611111 : [geo-rep]: Geo-replication in FAULTY state - CENTOS 6 #1611113 : [geo-rep]: Geo-replication not syncing renamed symlink #1611114 : [geo-rep]: [Errno 2] No such file or directory #1611115 : avoid possible glusterd crash in glusterd_verify_slave #1611116 : 'custom extended attributes' set on a directory are not healed after bringing back the down sub-volumes #1618347 : [Ganesha] Ganesha crashed in mdcache_alloc_and_check_handle while running bonnie and untars with parallel lookups","title":"4.1.3"},{"location":"release-notes/4.1.3/#release-notes-for-gluster-413","text":"This is a bugfix release. The release notes for 4.1.0 , 4.1.1 , and 4.1.2 contains a listing of all the new features that were added and bugs fixed in the GlusterFS 4.1 stable release. NOTE: Next minor release tentative date: Week of 24th September, 2018","title":"Release notes for Gluster 4.1.3"},{"location":"release-notes/4.1.3/#major-changes-features-and-limitations-addressed-in-this-release","text":"None","title":"Major changes, features and limitations addressed in this release"},{"location":"release-notes/4.1.3/#major-issues","text":"Bug #1601356 titled \"Problem with SSL/TLS encryption\", is not yet fixed with this release. Patch to fix the same is in progress and can be tracked here .","title":"Major issues"},{"location":"release-notes/4.1.3/#bugs-addressed","text":"Bugs addressed since release-4.1.2 are listed below. #1425326 : gluster bash completion leaks TOP=0 into the environment #1596686 : key = trusted.glusterfs.protect.writes [Invalid argument]; key = glusterfs.avoid.overwrite [Invalid argument] #1609550 : glusterfs-resource-agents should not be built for el6 #1609551 : glusterfs-resource-agents should not be built for el6 #1611104 : [geo-rep]: Upgrade fails, session in FAULTY state #1611106 : Glusterd crashed on a few (master) nodes #1611108 : [geo-rep]: Geo-rep scheduler fails #1611110 : Glusterd memory leaking in gf_gld_mt_linebuf #1611111 : [geo-rep]: Geo-replication in FAULTY state - CENTOS 6 #1611113 : [geo-rep]: Geo-replication not syncing renamed symlink #1611114 : [geo-rep]: [Errno 2] No such file or directory #1611115 : avoid possible glusterd crash in glusterd_verify_slave #1611116 : 'custom extended attributes' set on a directory are not healed after bringing back the down sub-volumes #1618347 : [Ganesha] Ganesha crashed in mdcache_alloc_and_check_handle while running bonnie and untars with parallel lookups","title":"Bugs addressed"},{"location":"release-notes/4.1.4/","text":"Release notes for Gluster 4.1.4 This is a bugfix release. The release notes for 4.1.0 , 4.1.1 , 4.1.2 and 4.1.3 contains a listing of all the new features that were added and bugs fixed in the GlusterFS 4.1 stable release. Major changes, features and limitations addressed in this release This release contains fix for following security vulnerabilities, https://nvd.nist.gov/vuln/detail/CVE-2018-10904 https://nvd.nist.gov/vuln/detail/CVE-2018-10907 https://nvd.nist.gov/vuln/detail/CVE-2018-10911 https://nvd.nist.gov/vuln/detail/CVE-2018-10913 https://nvd.nist.gov/vuln/detail/CVE-2018-10914 https://nvd.nist.gov/vuln/detail/CVE-2018-10923 https://nvd.nist.gov/vuln/detail/CVE-2018-10926 https://nvd.nist.gov/vuln/detail/CVE-2018-10927 https://nvd.nist.gov/vuln/detail/CVE-2018-10928 https://nvd.nist.gov/vuln/detail/CVE-2018-10929 https://nvd.nist.gov/vuln/detail/CVE-2018-10930 To resolve the security vulnerabilities following limitations were made in GlusterFS open,read,write on special files like char and block are no longer permitted io-stat xlator can dump stat info only to /var/run/gluster directory Installing the updated packages and restarting gluster services on gluster brick hosts, will fix the security issues. Major issues Bug #1601356 titled \"Problem with SSL/TLS encryption\", is not yet fixed with this release. Patch to fix the same is in progress and can be tracked here . Bugs addressed Bugs addressed since release-4.1.3 are listed below. #1625089 : Improper deserialization in dict.c:dict_unserialize() can allow attackers to read arbitrary memory #1625095 : Files can be renamed outside volume #1625096 : I/O to arbitrary devices on storage server #1625097 : Stack-based buffer overflow in server-rpc-fops.c allows remote attackers to execute arbitrary code #1625102 : Information Exposure in posix_get_file_contents function in posix-helpers.c #1625106 : Unsanitized file names in debug/io-stats translator can allow remote attackers to execute arbitrary code","title":"4.1.4"},{"location":"release-notes/4.1.4/#release-notes-for-gluster-414","text":"This is a bugfix release. The release notes for 4.1.0 , 4.1.1 , 4.1.2 and 4.1.3 contains a listing of all the new features that were added and bugs fixed in the GlusterFS 4.1 stable release.","title":"Release notes for Gluster 4.1.4"},{"location":"release-notes/4.1.4/#major-changes-features-and-limitations-addressed-in-this-release","text":"This release contains fix for following security vulnerabilities, https://nvd.nist.gov/vuln/detail/CVE-2018-10904 https://nvd.nist.gov/vuln/detail/CVE-2018-10907 https://nvd.nist.gov/vuln/detail/CVE-2018-10911 https://nvd.nist.gov/vuln/detail/CVE-2018-10913 https://nvd.nist.gov/vuln/detail/CVE-2018-10914 https://nvd.nist.gov/vuln/detail/CVE-2018-10923 https://nvd.nist.gov/vuln/detail/CVE-2018-10926 https://nvd.nist.gov/vuln/detail/CVE-2018-10927 https://nvd.nist.gov/vuln/detail/CVE-2018-10928 https://nvd.nist.gov/vuln/detail/CVE-2018-10929 https://nvd.nist.gov/vuln/detail/CVE-2018-10930 To resolve the security vulnerabilities following limitations were made in GlusterFS open,read,write on special files like char and block are no longer permitted io-stat xlator can dump stat info only to /var/run/gluster directory Installing the updated packages and restarting gluster services on gluster brick hosts, will fix the security issues.","title":"Major changes, features and limitations addressed in this release"},{"location":"release-notes/4.1.4/#major-issues","text":"Bug #1601356 titled \"Problem with SSL/TLS encryption\", is not yet fixed with this release. Patch to fix the same is in progress and can be tracked here .","title":"Major issues"},{"location":"release-notes/4.1.4/#bugs-addressed","text":"Bugs addressed since release-4.1.3 are listed below. #1625089 : Improper deserialization in dict.c:dict_unserialize() can allow attackers to read arbitrary memory #1625095 : Files can be renamed outside volume #1625096 : I/O to arbitrary devices on storage server #1625097 : Stack-based buffer overflow in server-rpc-fops.c allows remote attackers to execute arbitrary code #1625102 : Information Exposure in posix_get_file_contents function in posix-helpers.c #1625106 : Unsanitized file names in debug/io-stats translator can allow remote attackers to execute arbitrary code","title":"Bugs addressed"},{"location":"release-notes/4.1.5/","text":"Release notes for Gluster 4.1.5 This is a bugfix release. The release notes for 4.1.0 , 4.1.1 , 4.1.2 , 4.1.3 and 4.1.4 contains a listing of all the new features that were added and bugs fixed in the GlusterFS 4.1 stable release. NOTE: Next minor release tentative date: Week of 19th November, 2018 Major changes, features and limitations addressed in this release None Major issues None Bugs addressed Bugs addressed since release-4.1.4 are listed below. #1601356 : Problem with SSL/TLS encryption on Gluster 4.0 & 4.1 #1625575 : Prevent hangs while increasing replica-count/replace-brick for directory hierarchy #1629548 : Excessive logging in posix_set_parent_ctime() #1630140 : geo-rep: geo-rep config set fails to set rsync-options #1630141 : libgfchangelog: History API fails #1630144 : Geo-rep: Geo-rep regression times out occasionally #1630145 : Geo-rep: Few workers fails to start with out any failure","title":"4.1.5"},{"location":"release-notes/4.1.5/#release-notes-for-gluster-415","text":"This is a bugfix release. The release notes for 4.1.0 , 4.1.1 , 4.1.2 , 4.1.3 and 4.1.4 contains a listing of all the new features that were added and bugs fixed in the GlusterFS 4.1 stable release. NOTE: Next minor release tentative date: Week of 19th November, 2018","title":"Release notes for Gluster 4.1.5"},{"location":"release-notes/4.1.5/#major-changes-features-and-limitations-addressed-in-this-release","text":"None","title":"Major changes, features and limitations addressed in this release"},{"location":"release-notes/4.1.5/#major-issues","text":"None","title":"Major issues"},{"location":"release-notes/4.1.5/#bugs-addressed","text":"Bugs addressed since release-4.1.4 are listed below. #1601356 : Problem with SSL/TLS encryption on Gluster 4.0 & 4.1 #1625575 : Prevent hangs while increasing replica-count/replace-brick for directory hierarchy #1629548 : Excessive logging in posix_set_parent_ctime() #1630140 : geo-rep: geo-rep config set fails to set rsync-options #1630141 : libgfchangelog: History API fails #1630144 : Geo-rep: Geo-rep regression times out occasionally #1630145 : Geo-rep: Few workers fails to start with out any failure","title":"Bugs addressed"},{"location":"release-notes/4.1.6/","text":"Release notes for Gluster 4.1.6 This is a bugfix release. The release notes for 4.1.0 , 4.1.1 , 4.1.2 , 4.1.3 , 4.1.4 and 4.1.5 contains a listing of all the new features that were added and bugs fixed in the GlusterFS 4.1 stable release. NOTE: Next minor release tentative date: Week of 20th January, 2019 Major changes, features and limitations addressed in this release This release contains fixes for several security vulnerabilities in Gluster as follows, - https://nvd.nist.gov/vuln/detail/CVE-2018-14651 - https://nvd.nist.gov/vuln/detail/CVE-2018-14652 - https://nvd.nist.gov/vuln/detail/CVE-2018-14653 - https://nvd.nist.gov/vuln/detail/CVE-2018-14654 - https://nvd.nist.gov/vuln/detail/CVE-2018-14659 - https://nvd.nist.gov/vuln/detail/CVE-2018-14660 - https://nvd.nist.gov/vuln/detail/CVE-2018-14661 Major issues None Bugs addressed Bugs addressed since release-4.1.5 are listed below. #1632013 : georep: hard-coded paths in gsyncd.conf.in #1633479 : 'df' shows half as much space on volume after upgrade to RHGS 3.4 #1633634 : split-brain observed on parent dir #1635979 : Writes taking very long time leading to system hogging #1635980 : Low Random write IOPS in VM workloads #1636218 : [SNAPSHOT]: with brick multiplexing, snapshot restore will make glusterd send wrong volfile #1637953 : data-self-heal in arbiter volume results in stale locks. #1641761 : Spurious failures in bug-1637802-arbiter-stale-data-heal-lock.t #1643052 : Seeing defunt translator and discrepancy in volume info when issued from node which doesn't host bricks in that volume #1643075 : tests/bugs/glusterd/optimized-basic-testcases-in-cluster.t failing #1643929 : geo-rep: gluster-mountbroker status crashes #1644163 : geo-rep: geo-replication gets stuck after file rename and gfid conflict #1644474 : afr/lease: Read child nodes from lease structure #1644516 : geo-rep: gluster-mountbroker status crashes #1644518 : [Geo-Replication] Geo-rep faulty sesion because of the directories are not synced to slave. #1644524 : Excessive logging in posix_update_utime_in_mdata #1645363 : CVE-2018-14652 glusterfs: Buffer overflow in \"features/locks\" translator allows for denial of service [fedora-all] #1646200 : CVE-2018-14654 glusterfs: \"features/index\" translator can create arbitrary, empty files [fedora-all] #1646806 : [Geo-rep]: Faulty geo-rep sessions due to link ownership on slave volume #1647667 : CVE-2018-14651 glusterfs: glusterfs server exploitable via symlinks to relative paths [fedora-all] #1647668 : CVE-2018-14661 glusterfs: features/locks translator passes an user-controlled string to snprintf without a proper format string resulting in a denial of service [fedora-all] #1647669 : CVE-2018-14659 glusterfs: Unlimited file creation via \"GF_XATTR_IOSTATS_DUMP_KEY\" xattr allows for denial of service [fedora-all] #1647670 : CVE-2018-14653 glusterfs: Heap-based buffer overflow via \"gf_getspec_req\" RPC message [fedora-all] #1647972 : CVE-2018-14660 glusterfs: Repeat use of \"GF_META_LOCK_KEY\" xattr allows for memory exhaustion [fedora-all] #1648367 : crash seen while running regression, intermittently. #1648938 : gfapi: fix bad dict setting of lease-id #1648982 : packaging: don't include bd.so in rpm when --without bd","title":"4.1.6"},{"location":"release-notes/4.1.6/#release-notes-for-gluster-416","text":"This is a bugfix release. The release notes for 4.1.0 , 4.1.1 , 4.1.2 , 4.1.3 , 4.1.4 and 4.1.5 contains a listing of all the new features that were added and bugs fixed in the GlusterFS 4.1 stable release. NOTE: Next minor release tentative date: Week of 20th January, 2019","title":"Release notes for Gluster 4.1.6"},{"location":"release-notes/4.1.6/#major-changes-features-and-limitations-addressed-in-this-release","text":"This release contains fixes for several security vulnerabilities in Gluster as follows, - https://nvd.nist.gov/vuln/detail/CVE-2018-14651 - https://nvd.nist.gov/vuln/detail/CVE-2018-14652 - https://nvd.nist.gov/vuln/detail/CVE-2018-14653 - https://nvd.nist.gov/vuln/detail/CVE-2018-14654 - https://nvd.nist.gov/vuln/detail/CVE-2018-14659 - https://nvd.nist.gov/vuln/detail/CVE-2018-14660 - https://nvd.nist.gov/vuln/detail/CVE-2018-14661","title":"Major changes, features and limitations addressed in this release"},{"location":"release-notes/4.1.6/#major-issues","text":"None","title":"Major issues"},{"location":"release-notes/4.1.6/#bugs-addressed","text":"Bugs addressed since release-4.1.5 are listed below. #1632013 : georep: hard-coded paths in gsyncd.conf.in #1633479 : 'df' shows half as much space on volume after upgrade to RHGS 3.4 #1633634 : split-brain observed on parent dir #1635979 : Writes taking very long time leading to system hogging #1635980 : Low Random write IOPS in VM workloads #1636218 : [SNAPSHOT]: with brick multiplexing, snapshot restore will make glusterd send wrong volfile #1637953 : data-self-heal in arbiter volume results in stale locks. #1641761 : Spurious failures in bug-1637802-arbiter-stale-data-heal-lock.t #1643052 : Seeing defunt translator and discrepancy in volume info when issued from node which doesn't host bricks in that volume #1643075 : tests/bugs/glusterd/optimized-basic-testcases-in-cluster.t failing #1643929 : geo-rep: gluster-mountbroker status crashes #1644163 : geo-rep: geo-replication gets stuck after file rename and gfid conflict #1644474 : afr/lease: Read child nodes from lease structure #1644516 : geo-rep: gluster-mountbroker status crashes #1644518 : [Geo-Replication] Geo-rep faulty sesion because of the directories are not synced to slave. #1644524 : Excessive logging in posix_update_utime_in_mdata #1645363 : CVE-2018-14652 glusterfs: Buffer overflow in \"features/locks\" translator allows for denial of service [fedora-all] #1646200 : CVE-2018-14654 glusterfs: \"features/index\" translator can create arbitrary, empty files [fedora-all] #1646806 : [Geo-rep]: Faulty geo-rep sessions due to link ownership on slave volume #1647667 : CVE-2018-14651 glusterfs: glusterfs server exploitable via symlinks to relative paths [fedora-all] #1647668 : CVE-2018-14661 glusterfs: features/locks translator passes an user-controlled string to snprintf without a proper format string resulting in a denial of service [fedora-all] #1647669 : CVE-2018-14659 glusterfs: Unlimited file creation via \"GF_XATTR_IOSTATS_DUMP_KEY\" xattr allows for denial of service [fedora-all] #1647670 : CVE-2018-14653 glusterfs: Heap-based buffer overflow via \"gf_getspec_req\" RPC message [fedora-all] #1647972 : CVE-2018-14660 glusterfs: Repeat use of \"GF_META_LOCK_KEY\" xattr allows for memory exhaustion [fedora-all] #1648367 : crash seen while running regression, intermittently. #1648938 : gfapi: fix bad dict setting of lease-id #1648982 : packaging: don't include bd.so in rpm when --without bd","title":"Bugs addressed"},{"location":"release-notes/4.1.7/","text":"Release notes for Gluster 4.1.7 This is a bugfix release. The release notes for 4.1.0 , 4.1.1 , 4.1.2 , 4.1.3 , 4.1.4 , 4.1.5 and 4.1.6 contains a listing of all the new features that were added and bugs fixed in the GlusterFS 4.1 stable release. NOTE: Next minor release tentative date: Week of 20th March, 2019 Major changes, features and limitations addressed in this release None Major issues None Bugs addressed Bugs addressed since release-4.1.6 are listed below. #1654118 : [geo-rep]: Failover / Failback shows fault status in a non-root setup #1654229 : Provide an option to silence glfsheal logs #1655527 : Incorrect usage of local->fd in afr_open_ftruncate_cbk #1655532 : Tracker bug for all leases related issues #1655561 : gfid heal does not happen when there is no source brick #1662635 : Fix tests/bugs/shard/zero-flag.t #1663132 : [Ganesha] Ganesha failed on one node while exporting volumes in loop","title":"4.1.7"},{"location":"release-notes/4.1.7/#release-notes-for-gluster-417","text":"This is a bugfix release. The release notes for 4.1.0 , 4.1.1 , 4.1.2 , 4.1.3 , 4.1.4 , 4.1.5 and 4.1.6 contains a listing of all the new features that were added and bugs fixed in the GlusterFS 4.1 stable release. NOTE: Next minor release tentative date: Week of 20th March, 2019","title":"Release notes for Gluster 4.1.7"},{"location":"release-notes/4.1.7/#major-changes-features-and-limitations-addressed-in-this-release","text":"None","title":"Major changes, features and limitations addressed in this release"},{"location":"release-notes/4.1.7/#major-issues","text":"None","title":"Major issues"},{"location":"release-notes/4.1.7/#bugs-addressed","text":"Bugs addressed since release-4.1.6 are listed below. #1654118 : [geo-rep]: Failover / Failback shows fault status in a non-root setup #1654229 : Provide an option to silence glfsheal logs #1655527 : Incorrect usage of local->fd in afr_open_ftruncate_cbk #1655532 : Tracker bug for all leases related issues #1655561 : gfid heal does not happen when there is no source brick #1662635 : Fix tests/bugs/shard/zero-flag.t #1663132 : [Ganesha] Ganesha failed on one node while exporting volumes in loop","title":"Bugs addressed"},{"location":"release-notes/4.1.8/","text":"Release notes for Gluster 4.1.8 This is a bugfix release. The release notes for 4.1.0 , 4.1.1 , 4.1.2 , 4.1.3 , 4.1.4 , 4.1.5 , 4.1.6 and 4.1.7 contains a listing of all the new features that were added and bugs fixed in the GlusterFS 4.1 stable release. NOTE: Next minor release tentative date: Week of 20th May, 2019 Major changes, features and limitations addressed in this release None Major issues None Bugs addressed Bugs addressed since release-4.1.7 are listed below. #1670303 : api: bad GFAPI_4.1.6 block #1672249 : quorum count value not updated in nfs-server vol file #1673265 : Fix timeouts so the tests pass on AWS #1687746 : [geo-rep]: Checksum mismatch when 2x2 vols are converted to arbiter #1691292 : glusterfs FUSE client crashing every few days with 'Failed to dispatch handler' #1693057 : dht_revalidate may not heal attrs on the brick root #1693201 : core: move \"dict is NULL\" logs to DEBUG log level","title":"4.1.8"},{"location":"release-notes/4.1.8/#release-notes-for-gluster-418","text":"This is a bugfix release. The release notes for 4.1.0 , 4.1.1 , 4.1.2 , 4.1.3 , 4.1.4 , 4.1.5 , 4.1.6 and 4.1.7 contains a listing of all the new features that were added and bugs fixed in the GlusterFS 4.1 stable release. NOTE: Next minor release tentative date: Week of 20th May, 2019","title":"Release notes for Gluster 4.1.8"},{"location":"release-notes/4.1.8/#major-changes-features-and-limitations-addressed-in-this-release","text":"None","title":"Major changes, features and limitations addressed in this release"},{"location":"release-notes/4.1.8/#major-issues","text":"None","title":"Major issues"},{"location":"release-notes/4.1.8/#bugs-addressed","text":"Bugs addressed since release-4.1.7 are listed below. #1670303 : api: bad GFAPI_4.1.6 block #1672249 : quorum count value not updated in nfs-server vol file #1673265 : Fix timeouts so the tests pass on AWS #1687746 : [geo-rep]: Checksum mismatch when 2x2 vols are converted to arbiter #1691292 : glusterfs FUSE client crashing every few days with 'Failed to dispatch handler' #1693057 : dht_revalidate may not heal attrs on the brick root #1693201 : core: move \"dict is NULL\" logs to DEBUG log level","title":"Bugs addressed"},{"location":"release-notes/4.1.9/","text":"Release notes for Gluster 4.1.9 This is a bugfix release. The release notes for 4.1.0 , 4.1.1 , 4.1.2 , 4.1.3 , 4.1.4 , 4.1.5 , 4.1.6 , 4.1.7 and 4.1.8 contains a listing of all the new features that were added and bugs fixed in the GlusterFS 4.1 stable release. Major changes, features and limitations addressed in this release None Major issues None Bugs addressed Bugs addressed since release-4.1.8 are listed below. #1660225 : geo-rep does not replicate mv or rename of file #1684404 : Multiple shd processes are running on brick_mux environmet #1694563 : gfapi: do not block epoll thread for upcall notifications #1696513 : Multiple shd processes are running on brick_mux environmet #1707200 : VM stuck in a shutdown because of a pending fuse request","title":"4.1.9"},{"location":"release-notes/4.1.9/#release-notes-for-gluster-419","text":"This is a bugfix release. The release notes for 4.1.0 , 4.1.1 , 4.1.2 , 4.1.3 , 4.1.4 , 4.1.5 , 4.1.6 , 4.1.7 and 4.1.8 contains a listing of all the new features that were added and bugs fixed in the GlusterFS 4.1 stable release.","title":"Release notes for Gluster 4.1.9"},{"location":"release-notes/4.1.9/#major-changes-features-and-limitations-addressed-in-this-release","text":"None","title":"Major changes, features and limitations addressed in this release"},{"location":"release-notes/4.1.9/#major-issues","text":"None","title":"Major issues"},{"location":"release-notes/4.1.9/#bugs-addressed","text":"Bugs addressed since release-4.1.8 are listed below. #1660225 : geo-rep does not replicate mv or rename of file #1684404 : Multiple shd processes are running on brick_mux environmet #1694563 : gfapi: do not block epoll thread for upcall notifications #1696513 : Multiple shd processes are running on brick_mux environmet #1707200 : VM stuck in a shutdown because of a pending fuse request","title":"Bugs addressed"},{"location":"release-notes/5.0/","text":"Release notes for Gluster 5.0 This is a major release that includes a range of code improvements and stability fixes among a few features as noted below. A selection of the key features and changes are documented on this page. A full list of bugs that have been addressed is included further below. Announcements Major changes and features Major issues Bugs addressed in the release Announcements Releases that receive maintenance updates post release 5 are, 4.1 ( reference ) NOTE: 3.12 long term maintenance release, will reach end of life (EOL) with the release of 5.0. ( reference ) Release 5 will receive maintenance updates around the 10th of every month for the first 3 months post release (i.e Nov'18, Dec'18, Jan'18). Post the initial 3 months, it will receive maintenance updates every 2 months till EOL. ( reference ) Major changes and features Features are categorized into the following sections, Management Standalone Management GlusterD2 IMP: GlusterD2 in Gluster-5 is still considered a preview and is experimental. It should not be considered ready for production use. Users should still expect some breaking changes even though all efforts would be taken to ensure that these can be avoided. As GD2 is still under heavy development, new features can be expected throughout the Gluster 5 release. The following major changes have been committed to GlusterD2 since v4.1.0. Volume snapshots : Most snapshot operations are available including create, delete, activate, deactivate, clone and restore. Volume heal: Support for full heal and index heal for replicate volumes has been implemented. Tracing with Opencensus: Support for tracing distributed operations has been implemented in GD2, using the Opencensus API. Tracing instrumentation has been done for volume create, list and delete operations. Other operations will follow subsequently. Portmap refactoring: Portmap in GlisterD2 no longer selects a port for the bricks to listen on, instead leaving the choice upto the bricks. Portmap only saves port information provided by brick during signin. Smartvol API merged with volume create API: The smart volume API which allows user to create a volume by just specifying a size has been merged with the normal volume create API. Configure GlusterD2 with environment variables: In addition to CLI flags, and the config file, GD2 configuration options can be set using environment variables. In addition to the above, many changes have been merged for minor bug-fixes and to help with testing. Refer to the user documentation section for details on how to get started with GlusterD2. Standalone 1. Entry creation and handling, consistency is improved The dentry serializer feature was introduced in Gluster 4.0, to strengthen the consistency handling of entry operations in the Gluster stack. Entry operations refer to creating, linking, renaming and unlinking of files and directory names into the filesystem space. When this feature was first introduced (in 4.0) it was optional, with this release this feature is enabled by default. 2. Python code in Gluster packages is Python 3 ready 3. Quota fsck script to correct quota accounting See usage documentation here 4. Added noatime option in utime xlator Enabling the utime and ctime feature, enables Gluster to maintain consistent change and modification time stamps on files and directories across bricks. The utime xlator is enhanced with a noatime option and is set by default to enabled, when the utime feature is enabled. This helps to ignore atime updates for operations that change may trigger an atime update on the file system objects. To enable the feature use, # gluster volume set <volname> features.utime on # gluster volume set <volname> features.ctime on 5. Added ctime-invalidation option in quick-read xlator Quick-read xlator by default uses mtime (files last modification time) to identify changes to file data. However, there are applications, like rsync, which explicitly set mtime making it unreliable for the purpose of identifying changes to the file content. Since ctime (files last status change time) also changes when content of a file changes and cannot be set explicitly by applications, it becomes a more reliable source to identify staleness of cached data. The ctime-invalidation option makes quick-read to prefer ctime over mtime to validate staleness of its cache. To enable this option use, # gluster volume set <volname> ctime-invalidation on NOTE: Using ctime can result in false positives as ctime is updated even on attribute changes, like mode bits, without changes to file data. As a result this option is recommended in situations where mtime is not reliable. 6. Added shard-deletion-rate option in shard xlator The shard-deletion-rate option is introduced, to configure the number of shards to delete in parallel when a file that is sharded is deleted. The default value is set at 100, but can be increased to delete more shards in parallel for faster space reclamation. To change the defaults for this option use, # gluster volume set <volname> shard-deletion-rate <n> NOTE: The upper limit is unbounded, use it with caution as a very large number will cause lock contention on the bricks. As an example, during testing, an upper limit of 125000 was enough to cause timeouts and hangs in the gluster processes due to lock contention. 7. Removed last usage of MD5 digest in code, towards better FIPS compliance In an effort to ensure that Gluster can be installed and deployed on machines that are compliant with the requirements for FIPS, remaining uses of MD5 digest is removed from the code base. Addressing this feature's requirements was initiated during the 4.0 release, at which point enabling user space snapshots, which still used MD5 for certain needs, broke the FIPS compliance requirements. This limitation is now addressed in this release. 8. Code improvements Over the course of this release, the contributors have been active in addressing various Coverity issues, GCC and clang warnings, clang formatting of the code base, micro improvements to GLibC API usage and memory handling around string handling and allocation routines. The above are ongoing efforts, but major strides were made during this release to actively address code quality in these areas. Major issues The following options are removed from the code base and require to be unset before an upgrade from releases older than release 4.1.0, features.lock-heal features.grace-timeout To check if these options are set use, # gluster volume info and ensure that the above options are not part of the Options Reconfigured: section in the output of all volumes in the cluster. If these are set, then unset them using the following commands, # gluster volume reset <volname> <option> NOTE: Failure to do the above may result in failure during online upgrades, and the reset of these options to their defaults needs to be done prior to upgrading the cluster. Bugs addressed Bugs addressed since release-4.1.0 are listed below. #853601 : working-directory should be protected from being a brick #1312832 : tests fail because bug-924726.t depends on netstat #1390050 : Elasticsearch get CorruptIndexException errors when running with GlusterFS persistent storage #1405147 : glusterfs (posix-acl xlator layer) checks for \"write permission\" instead for \"file owner\" during open() when writing to a file #1425325 : gluster bash completion leaks TOP=0 into the environment #1437780 : don't send lookup in fuse_getattr() #1455872 : [Perf]: 25% regression on sequential reads on EC over SMB3 #1492847 : core (named threads): flood of -Wformat-truncation warnings with gcc-7. #1512691 : PostgreSQL DB Restore: unexpected data beyond EOF #1524323 : No need to load ctr xlator if user has not configured tiering #1526780 : ./run-tests-in-vagrant.sh fails because of disabled Gluster/NFS #1533000 : Quota crawl regressed #1537602 : Georeplication tests intermittently fail #1543279 : Moving multiple temporary files to the same destination concurrently causes ESTALE error #1545048 : [brick-mux] process termination race while killing glusterfsd on last brick detach #1546103 : run-tests-in-vagrant.sh should return test status #1558574 : Coverity: Warning for singlton array.. #1558921 : Gluster volume smb share options are getting overwritten after restating the gluster volume #1561332 : merge ssl infra with epoll infra #1564071 : directories are invisible on client side #1564149 : Agree upon a coding standard, and automate check for this in smoke #1564419 : Client side memory leak in encryption xlator (crypt.c). #1568521 : shard files present even after deleting vm from ovirt UI #1569345 : Need COMMITMENT from community for GPL Cure. #1569399 : glusterfsd should be able to start without any other arguments than a single volfile. #1570538 : linux untar errors out at completion during disperse volume inservice upgrade #1570962 : print the path of the corrupted object in scrub status #1574421 : Provide a way to get the hashed-subvol for a file #1575381 : gluster volume heal info prints extra newlines #1575490 : [geo-rep]: Upgrade fails, session in FAULTY state #1575587 : Leverage MDS subvol for dht_removexattr also #1575716 : gfapi: broken symbol versions #1575742 : Change op-version of master to 4.2.0 for future options that maybe added #1575858 : quota crawler fails w/ TLS enabled #1575864 : glusterfsd crashing because of RHGS WA? #1575887 : Additional log messages in dht_readdir(p)_cbk #1575910 : DHT Log flooding in mount log \"key=trusted.glusterfs.dht.mds [Invalid argument]\" #1576179 : [geo-rep]: Geo-rep scheduler fails #1576392 : Glusterd crashed on a few (master) nodes #1576418 : Warning messages generated for the removal of extended attribute security.ima flodding client logs #1576767 : [geo-rep]: Lot of changelogs retries and \"dict is null\" errors in geo-rep logs #1576842 : cloudsync: make plugins configurable #1577574 : brick crash seen while creating and deleting two volumes in loop #1577627 : [Geo-rep]: Status in ACTIVE/Created state #1577672 : Brick-mux regressions failing for over 8+ weeks on master #1577731 : [Ganesha] \"Gluster nfs-ganesha enable\" commands sometimes gives output as \"failed\" with \"Unlocking failed\" error messages ,even though cluster is up and healthy in backend #1577744 : The tool to generate new xlator template code is not upto date #1578325 : Input/Output errors on a disperse volume with concurrent reads and writes #1578650 : If parallel-readdir is enabled, the readdir-optimize option even when it is set to on it behaves as off #1578721 : Statedump prints memory usage statistics twice #1578823 : Remove EIO from the dht_inode_missing macro #1579276 : rpc: The gluster auth version is always AUTH_GLUSTERFS_v2 #1579769 : inode status command is broken with distributed replicated volumes #1579786 : Thin-arbiter: Provide script to start and run thin arbiter process #1579788 : Thin-arbiter: Have the state of volume in memory #1580020 : ctime: Rename and unlink does not update ctime #1580238 : Fix incorrect rebalance log message #1580269 : [Remove-brick+Rename] Failure count shows zero though there are file migration failures #1580352 : Glusterd memory leaking in gf_gld_mt_linebuf #1580529 : posix/ctime: Access time is not updated for file with a hardlink #1580532 : posix/ctime: The first lookup on file is not healing the gfid #1581035 : posix/ctime: Mtime is not updated on setting it to older date #1581345 : posix unwinds readdirp calls with readdir signature #1581735 : bug-1309462.t is failing reliably due to changes in security.capability changes in the kernel #1582051 : Fix failure of readdir-ahead/bug-1439640.t in certain cases #1582516 : libgfapi: glfs init fails on afr volume with ctime feature enabled #1582704 : rpc_transport_unref() called for an unregistered socket fd #1583018 : changelog: Changelog is not capturing rename of files #1583565 : [distribute]: Excessive 'dict is null' errors in geo-rep logs #1583583 : \"connecting\" state in protocol client is useless #1583937 : Brick process crashed after upgrade from RHGS-3.3.1 async(7.4) to RHGS-3.4(7.5) #1584098 : 'custom extended attributes' set on a directory are not healed after bringing back the down sub-volumes #1584483 : afr: don't update readables if inode refresh failed on all children #1584517 : Inconsistent access permissions on directories after bringing back the down sub-volumes #1584864 : sometime messages #1584981 : posix/ctime: EC self heal of directory is blocked with ctime feature enabled #1585391 : glusteshd wrong status caused by gluterd big lock #1585585 : Cleanup \"connected\" state management of rpc-clnt #1586018 : (f)Setxattr and (f)removexattr invalidates the stat cache in md-cache #1586020 : [GSS] Pending heals are not getting completed in CNS environment #1586342 : Refactor the distributed test code to make it work for ipv4 #1586363 : Refactor rebalance code #1589253 : After creating and starting 601 volumes, self heal daemon went down and seeing continuous warning messages in glusterd log #1589691 : xdata is leaking in server3_3_seek #1589782 : [geo-rep]: Geo-replication in FAULTY state - CENTOS 6 #1589842 : [USS] snapview server does not go through the list of all the snapshots for validating a snap #1590193 : /usr/sbin/gcron.py aborts with OSError #1590385 : Refactor dht lookup code #1590655 : Excessive logging in posix_check_internal_writes() due to NULL dict #1590710 : Gluster Block PVC fails to mount on Jenkins pod #1591193 : lookup not assigning gfid if file is not present in all bricks of replica #1591580 : Remove code duplication in protocol/client #1591621 : Arequal checksum mismatch on older mount #1592141 : Null pointer deref in error paths #1592275 : posix/ctime: Mdata value of a directory is different across replica/EC subvolume #1592509 : ctime: Self heal of symlink is failing on EC subvolume #1593232 : CVE-2018-10841 glusterfs: access trusted peer group via remote-host command [glusterfs upstream] #1593351 : mount.glusterfs incorrectly reports \"getfattr not found\" #1593548 : Stack overflow in readdirp with parallel-readdir enabled #1593562 : Add new peers to Glusto #1593651 : gnfs nfs.register-with-portmap issue with ipv6_default #1595174 : Found an issue on using lock before init in md-cache #1595190 : rmdir is leaking softlinks to directories in .glusterfs #1595320 : gluster wrongly reports bricks online, even when brick path is not available #1595492 : tests: remove tarissue.t from BAD_TEST #1595726 : tests/geo-rep: Add test case for symlink rename #1596020 : Introduce database group profile #1596513 : glustershd crashes when index heal is launched before graph is initialized. #1596524 : 'replica 3 aribiter 1' is not a industry standard way of telling 2-way replicate with arbiter. #1596789 : Update mount-shared-storage.sh to automatically include all enabled glusterfs mounts in fstab #1597156 : Need a simpler way to find if a replica/ec subvolume is up #1597247 : restart all the daemons after all the bricks #1597473 : introduce cluster.daemon-log-level option #1597512 : Remove contrib/ipaddr-py #1597540 : tests/geo-rep: Add test cases for rsnapshot use case #1597563 : [geo-rep+tiering]: Hot and Cold tier brick changelogs report rsync failure #1597568 : Mark brick online after port registration even for brick-mux cases #1597627 : tests/bugs/core/bug-1432542-mpx-restart-crash.t is generated crash #1597662 : Stale entries of snapshots need to be removed from /var/run/gluster/snaps #1597776 : br-state-check.t crashed while brick multiplex is enabled #1597805 : Stale lock with lk-owner all-zeros is observed in some tests #1598325 : Replace the BROKEN_TESTS environment variable value #1598345 : gluster get-state command is crashing glusterd process when geo-replication is configured #1598390 : Remove extras/prot_filter.py #1598548 : Disabling iostats diagnostics.stats-dump-interval (set to 0) does not terminate the dump thread #1598663 : Don't execute statements after decrementing call count in afr #1598884 : [geo-rep]: [Errno 2] No such file or directory #1598926 : Misleading error messages on bricks caused by lseek #1598977 : [geo-rep]: geo-replication scheduler is failing due to unsuccessful umount #1599219 : configure fails complaining absence of libxml2-devel #1599250 : bug-1432542-mpx-restart-crash.t takes a lot of time to complete cleanup #1599628 : To find a compatible brick ignore diagnostics.brick-log-level option while brick mux is enabled #1599783 : _is_prefix should return false for 0-length strings #1600405 : [geo-rep]: Geo-replication not syncing renamed symlink #1600451 : crash on glusterfs_handle_brick_status of the glusterfsd #1600687 : fuse process segfault when use resolve-gids option #1600812 : A new volume set option to for GD2 quota integration #1600878 : crash seen while running regression, intermittently. #1600963 : get the failed test details into gerrit output itself #1601166 : performance.read-ahead causes huge increase in unnecessary network traffic #1601390 : Distributed testing: Fix build environment #1601423 : memory leak in get-state when geo-replication session is configured #1601683 : dht: remove useless argument from dht_iatt_merge #1602070 : [SNAPSHOT] snapshot daemon crashes if a fd from a deleted snapshot is accessed #1602121 : avoid possible glusterd crash in glusterd_verify_slave #1602236 : When reserve limits are reached, append on an existing file after truncate operation results to hang #1602866 : dht: Crash seen in thread dht_dir_attr_heal #1603063 : ./tests/bugs/glusterd/validating-server-quorum.t is generated core #1605056 : [RHHi] Mount hung and not accessible #1605077 : If a node disconnects during volume delete, it assumes deleted volume as a freshly created volume when it is back online #1607049 : Excessive logging in posix_set_parent_ctime() #1607319 : Remove uuid from contrib/ #1607689 : Memory leaks on glfs_fini #1607783 : Segmentation fault while using gfapi while getting volume utilization #1608175 : Skip hash checks in dht_readdirp_cbk if dht has a single child subvol. #1608564 : line coverage tests failing consistently over a week #1608566 : line coverage tests: glusterd crash in ./tests/basic/sdfs-sanity.t #1608568 : line coverage tests: bug-1432542-mpx-restart-crash.t times out consistently #1608684 : Change glusto ownership to reflect current reality #1608991 : Remove code duplication in socket #1609126 : Fix mem leak and smoke failure for gcc8 in cloudsync #1609207 : thin arbiter: set notify-contention option to yes #1609337 : Remove argp-standalone from contrib/ #1609551 : glusterfs-resource-agents should not be built for el6 #1610236 : [Ganesha] Ganesha crashed in mdcache_alloc_and_check_handle while running bonnie and untars with parallel lookups #1610256 : [Ganesha] While performing lookups from two of the clients, \"ls\" command got failed with \"Invalid argument\" #1610405 : Geo-rep: Geo-rep regression times out occasionally #1610726 : Fuse mount of volume fails when gluster_shared_storage is enabled #1611103 : online_brick_count check in volume.rc should ignore bitrot and scrubber daemons #1611566 : tests/bitrot: tests/bitrot/bug-1373520.t fails intermittently #1611692 : Mount process crashes on a sharded volume during rename when dst doesn't exist #1611834 : glusterfsd crashes when SEEK_DATA/HOLE is not supported #1612017 : MAINTAINERS: Add Xavier Hernandez as peer for shard xlator #1612037 : Entry will be present even if the gfid link creation inside .glusterfs fails #1612054 : Test case bug-1586020-mark-dirty-for-entry-txn-on-quorum-failure.t failure #1612418 : Brick not coming up on a volume after rebooting the node #1612750 : gfapi: Use inode_forget in case of unlink/rename objects #1613098 : posix-acl: skip acl_permits check when the owner setting GF_POSIX_ACL_xxxx #1613807 : Fix spurious failures in tests/basic/afr/granular-esh/replace-brick.t #1614062 : Provide/preserve tarball of retried tests #1614088 : kill_brick function needs to wait for brick to be killed #1614124 : glusterfsd process crashed in a multiplexed configuration during cleanup of a single brick-graph triggered by volume-stop. #1614142 : Fix the grammar error in the rpc log #1614168 : [uss]snapshot: posix acl authentication is not working as expected #1614654 : Potential fixes for tests/basic/afr/add-brick-self-heal.t failure #1614662 : ./tests/bugs/replicate/bug-1448804-check-quorum-type-values.t #1614718 : Fix spurious failures in tests/bugs/index/bug-1559004-EMLINK-handling.t #1614730 : Test case bug-1433571-undo-pending-only-on-up-bricks.t failure #1614799 : Geo-rep: Few workers fails to start with out any failure #1615037 : Multiplex tests use a cleanup pattern that results in empty tarballs on failure #1615078 : tests/bugs/replicate/bug-1408712.t fails. #1615092 : tests/bugs/shard/configure-lru-limit.t spurious failure #1615096 : ./tests/bugs/quick-read/bug-846240.t fails spuriously #1615239 : Fix ./tests/basic/afr/replace-brick-self-heal.t failure #1615331 : gfid-mismatch-resolution-with-fav-child-policy.t is failing #1615474 : Rebalance status shows wrong count of \"Rebalanced-files\" if the file has hardlinks #1615582 : test: ./tests/basic/stats-dump.t fails spuriously not finding queue_size in stats output for some brick #1615703 : [Disperse] Improve log messages for EC volume #1615789 : Come up with framework to test thin-arbiter #1618004 : [GSS] glusterd not starting after upgrade due to snapshots error in RHEV + RHGS #1619027 : geo-rep: Active/Passive status change logging is redundant #1619423 : cli: Command gluster volume statedump <volname> dumps core #1619475 : NetBSD memory detection issue #1619720 : posix_mknod does not update trusted.pgfid.xx xattr correctly #1619843 : Snapshot status fails with commit failure #1620544 : Brick process NOT ONLINE for heketidb and block-hosting volume #1621981 : dht: File rename removes the .glusterfs handle for linkto file #1622076 : [geo-rep]: geo-rep reverse sync in FO/FB can accidentally delete the content at original master incase of gfid conflict in 3.4.0 without explicit user rmdir #1622422 : glusterd cli is showing brick status N/A even brick is consumed by a brick process #1622549 : libgfchangelog: History API fails #1622665 : clang-scan report: glusterfs issues #1622821 : Prevent hangs while increasing replica-count/replace-brick for directory hierarchy #1623408 : rpc: log fuse request ID with gluster transaction ID #1623759 : [Disperse] Don't send final version update if non data fop succeeded #1624244 : DHT: Rework the virtual xattr to get the hash subvol #1624440 : Fail volume stop operation in case brick detach request fails #1625089 : CVE-2018-10911 glusterfs: Improper deserialization in dict.c:dict_unserialize() can allow attackers to read arbitrary memory #1625095 : CVE-2018-10930 glusterfs: Files can be renamed outside volume #1625096 : CVE-2018-10923 glusterfs: I/O to arbitrary devices on storage server #1625097 : CVE-2018-10907 glusterfs: Stack-based buffer overflow in server-rpc-fops.c allows remote attackers to execute arbitrary code #1625102 : CVE-2018-10913 glusterfs: Information Exposure in posix_get_file_contents function in posix-helpers.c #1625106 : CVE-2018-10904 glusterfs: Unsanitized file names in debug/io-stats translator can allow remote attackers to execute arbitrary code #1625643 : Use CALLOC in dht_layouts_init #1626319 : DH ciphers disabled errors are encountered on basic mount & unmount with ssl enabled setup #1626346 : dht: Use snprintf in dht_filter_loc_subvol_key #1626394 : dht_create: Create linkto files if required when using dht_filter_loc_subvol_key #1626787 : sas workload job getting stuck after sometime #1627044 : Converting to replica 2 volume is not throwing warning #1627620 : SAS job aborts complaining about file doesn't exist #1628668 : Update op-version from 4.2 to 5.0 #1629877 : GlusterFS can be improved (clone for Gluster-5) #1630673 : geo-rep: geo-rep config set fails to set rsync-options #1630804 : libgfapi-python: test_listdir_with_stat and test_scandir failure on release 5 branch #1633015 : ctime: Access time is different with in same replica/EC volume #1633242 : 'df' shows half as much space on volume after upgrade to RHGS 3.4 #1633552 : glusterd crash in regression build #1635373 : ASan (address sanitizer) fixes - Blanket bug #1635972 : Low Random write IOPS in VM workloads #1635975 : Writes taking very long time leading to system hogging #1636162 : [SNAPSHOT]: with brick multiplexing, snapshot restore will make glusterd send wrong volfile #1636842 : df shows Volume size as zero if Volume created and mounted using Glusterd2 #1638159 : data-self-heal in arbiter volume results in stale locks. #1638163 : split-brain observed on parent dir #1639688 : core: backport uuid fixes #1640392 : io-stats: garbage characters in the filenames generated","title":5.0},{"location":"release-notes/5.0/#release-notes-for-gluster-50","text":"This is a major release that includes a range of code improvements and stability fixes among a few features as noted below. A selection of the key features and changes are documented on this page. A full list of bugs that have been addressed is included further below. Announcements Major changes and features Major issues Bugs addressed in the release","title":"Release notes for Gluster 5.0"},{"location":"release-notes/5.0/#announcements","text":"Releases that receive maintenance updates post release 5 are, 4.1 ( reference ) NOTE: 3.12 long term maintenance release, will reach end of life (EOL) with the release of 5.0. ( reference ) Release 5 will receive maintenance updates around the 10th of every month for the first 3 months post release (i.e Nov'18, Dec'18, Jan'18). Post the initial 3 months, it will receive maintenance updates every 2 months till EOL. ( reference )","title":"Announcements"},{"location":"release-notes/5.0/#major-changes-and-features","text":"Features are categorized into the following sections, Management Standalone","title":"Major changes and features"},{"location":"release-notes/5.0/#management","text":"","title":"Management"},{"location":"release-notes/5.0/#glusterd2","text":"IMP: GlusterD2 in Gluster-5 is still considered a preview and is experimental. It should not be considered ready for production use. Users should still expect some breaking changes even though all efforts would be taken to ensure that these can be avoided. As GD2 is still under heavy development, new features can be expected throughout the Gluster 5 release. The following major changes have been committed to GlusterD2 since v4.1.0. Volume snapshots : Most snapshot operations are available including create, delete, activate, deactivate, clone and restore. Volume heal: Support for full heal and index heal for replicate volumes has been implemented. Tracing with Opencensus: Support for tracing distributed operations has been implemented in GD2, using the Opencensus API. Tracing instrumentation has been done for volume create, list and delete operations. Other operations will follow subsequently. Portmap refactoring: Portmap in GlisterD2 no longer selects a port for the bricks to listen on, instead leaving the choice upto the bricks. Portmap only saves port information provided by brick during signin. Smartvol API merged with volume create API: The smart volume API which allows user to create a volume by just specifying a size has been merged with the normal volume create API. Configure GlusterD2 with environment variables: In addition to CLI flags, and the config file, GD2 configuration options can be set using environment variables. In addition to the above, many changes have been merged for minor bug-fixes and to help with testing. Refer to the user documentation section for details on how to get started with GlusterD2.","title":"GlusterD2"},{"location":"release-notes/5.0/#standalone","text":"","title":"Standalone"},{"location":"release-notes/5.0/#1-entry-creation-and-handling-consistency-is-improved","text":"The dentry serializer feature was introduced in Gluster 4.0, to strengthen the consistency handling of entry operations in the Gluster stack. Entry operations refer to creating, linking, renaming and unlinking of files and directory names into the filesystem space. When this feature was first introduced (in 4.0) it was optional, with this release this feature is enabled by default.","title":"1. Entry creation and handling, consistency is improved"},{"location":"release-notes/5.0/#2-python-code-in-gluster-packages-is-python-3-ready","text":"","title":"2. Python code in Gluster packages is Python 3 ready"},{"location":"release-notes/5.0/#3-quota-fsck-script-to-correct-quota-accounting","text":"See usage documentation here","title":"3. Quota fsck script to correct quota accounting"},{"location":"release-notes/5.0/#4-added-noatime-option-in-utime-xlator","text":"Enabling the utime and ctime feature, enables Gluster to maintain consistent change and modification time stamps on files and directories across bricks. The utime xlator is enhanced with a noatime option and is set by default to enabled, when the utime feature is enabled. This helps to ignore atime updates for operations that change may trigger an atime update on the file system objects. To enable the feature use, # gluster volume set <volname> features.utime on # gluster volume set <volname> features.ctime on","title":"4. Added noatime option in utime xlator"},{"location":"release-notes/5.0/#5-added-ctime-invalidation-option-in-quick-read-xlator","text":"Quick-read xlator by default uses mtime (files last modification time) to identify changes to file data. However, there are applications, like rsync, which explicitly set mtime making it unreliable for the purpose of identifying changes to the file content. Since ctime (files last status change time) also changes when content of a file changes and cannot be set explicitly by applications, it becomes a more reliable source to identify staleness of cached data. The ctime-invalidation option makes quick-read to prefer ctime over mtime to validate staleness of its cache. To enable this option use, # gluster volume set <volname> ctime-invalidation on NOTE: Using ctime can result in false positives as ctime is updated even on attribute changes, like mode bits, without changes to file data. As a result this option is recommended in situations where mtime is not reliable.","title":"5. Added ctime-invalidation option in quick-read xlator"},{"location":"release-notes/5.0/#6-added-shard-deletion-rate-option-in-shard-xlator","text":"The shard-deletion-rate option is introduced, to configure the number of shards to delete in parallel when a file that is sharded is deleted. The default value is set at 100, but can be increased to delete more shards in parallel for faster space reclamation. To change the defaults for this option use, # gluster volume set <volname> shard-deletion-rate <n> NOTE: The upper limit is unbounded, use it with caution as a very large number will cause lock contention on the bricks. As an example, during testing, an upper limit of 125000 was enough to cause timeouts and hangs in the gluster processes due to lock contention.","title":"6. Added shard-deletion-rate option in shard xlator"},{"location":"release-notes/5.0/#7-removed-last-usage-of-md5-digest-in-code-towards-better-fips-compliance","text":"In an effort to ensure that Gluster can be installed and deployed on machines that are compliant with the requirements for FIPS, remaining uses of MD5 digest is removed from the code base. Addressing this feature's requirements was initiated during the 4.0 release, at which point enabling user space snapshots, which still used MD5 for certain needs, broke the FIPS compliance requirements. This limitation is now addressed in this release.","title":"7. Removed last usage of MD5 digest in code, towards better FIPS compliance"},{"location":"release-notes/5.0/#8-code-improvements","text":"Over the course of this release, the contributors have been active in addressing various Coverity issues, GCC and clang warnings, clang formatting of the code base, micro improvements to GLibC API usage and memory handling around string handling and allocation routines. The above are ongoing efforts, but major strides were made during this release to actively address code quality in these areas.","title":"8. Code improvements"},{"location":"release-notes/5.0/#major-issues","text":"The following options are removed from the code base and require to be unset before an upgrade from releases older than release 4.1.0, features.lock-heal features.grace-timeout To check if these options are set use, # gluster volume info and ensure that the above options are not part of the Options Reconfigured: section in the output of all volumes in the cluster. If these are set, then unset them using the following commands, # gluster volume reset <volname> <option> NOTE: Failure to do the above may result in failure during online upgrades, and the reset of these options to their defaults needs to be done prior to upgrading the cluster.","title":"Major issues"},{"location":"release-notes/5.0/#bugs-addressed","text":"Bugs addressed since release-4.1.0 are listed below. #853601 : working-directory should be protected from being a brick #1312832 : tests fail because bug-924726.t depends on netstat #1390050 : Elasticsearch get CorruptIndexException errors when running with GlusterFS persistent storage #1405147 : glusterfs (posix-acl xlator layer) checks for \"write permission\" instead for \"file owner\" during open() when writing to a file #1425325 : gluster bash completion leaks TOP=0 into the environment #1437780 : don't send lookup in fuse_getattr() #1455872 : [Perf]: 25% regression on sequential reads on EC over SMB3 #1492847 : core (named threads): flood of -Wformat-truncation warnings with gcc-7. #1512691 : PostgreSQL DB Restore: unexpected data beyond EOF #1524323 : No need to load ctr xlator if user has not configured tiering #1526780 : ./run-tests-in-vagrant.sh fails because of disabled Gluster/NFS #1533000 : Quota crawl regressed #1537602 : Georeplication tests intermittently fail #1543279 : Moving multiple temporary files to the same destination concurrently causes ESTALE error #1545048 : [brick-mux] process termination race while killing glusterfsd on last brick detach #1546103 : run-tests-in-vagrant.sh should return test status #1558574 : Coverity: Warning for singlton array.. #1558921 : Gluster volume smb share options are getting overwritten after restating the gluster volume #1561332 : merge ssl infra with epoll infra #1564071 : directories are invisible on client side #1564149 : Agree upon a coding standard, and automate check for this in smoke #1564419 : Client side memory leak in encryption xlator (crypt.c). #1568521 : shard files present even after deleting vm from ovirt UI #1569345 : Need COMMITMENT from community for GPL Cure. #1569399 : glusterfsd should be able to start without any other arguments than a single volfile. #1570538 : linux untar errors out at completion during disperse volume inservice upgrade #1570962 : print the path of the corrupted object in scrub status #1574421 : Provide a way to get the hashed-subvol for a file #1575381 : gluster volume heal info prints extra newlines #1575490 : [geo-rep]: Upgrade fails, session in FAULTY state #1575587 : Leverage MDS subvol for dht_removexattr also #1575716 : gfapi: broken symbol versions #1575742 : Change op-version of master to 4.2.0 for future options that maybe added #1575858 : quota crawler fails w/ TLS enabled #1575864 : glusterfsd crashing because of RHGS WA? #1575887 : Additional log messages in dht_readdir(p)_cbk #1575910 : DHT Log flooding in mount log \"key=trusted.glusterfs.dht.mds [Invalid argument]\" #1576179 : [geo-rep]: Geo-rep scheduler fails #1576392 : Glusterd crashed on a few (master) nodes #1576418 : Warning messages generated for the removal of extended attribute security.ima flodding client logs #1576767 : [geo-rep]: Lot of changelogs retries and \"dict is null\" errors in geo-rep logs #1576842 : cloudsync: make plugins configurable #1577574 : brick crash seen while creating and deleting two volumes in loop #1577627 : [Geo-rep]: Status in ACTIVE/Created state #1577672 : Brick-mux regressions failing for over 8+ weeks on master #1577731 : [Ganesha] \"Gluster nfs-ganesha enable\" commands sometimes gives output as \"failed\" with \"Unlocking failed\" error messages ,even though cluster is up and healthy in backend #1577744 : The tool to generate new xlator template code is not upto date #1578325 : Input/Output errors on a disperse volume with concurrent reads and writes #1578650 : If parallel-readdir is enabled, the readdir-optimize option even when it is set to on it behaves as off #1578721 : Statedump prints memory usage statistics twice #1578823 : Remove EIO from the dht_inode_missing macro #1579276 : rpc: The gluster auth version is always AUTH_GLUSTERFS_v2 #1579769 : inode status command is broken with distributed replicated volumes #1579786 : Thin-arbiter: Provide script to start and run thin arbiter process #1579788 : Thin-arbiter: Have the state of volume in memory #1580020 : ctime: Rename and unlink does not update ctime #1580238 : Fix incorrect rebalance log message #1580269 : [Remove-brick+Rename] Failure count shows zero though there are file migration failures #1580352 : Glusterd memory leaking in gf_gld_mt_linebuf #1580529 : posix/ctime: Access time is not updated for file with a hardlink #1580532 : posix/ctime: The first lookup on file is not healing the gfid #1581035 : posix/ctime: Mtime is not updated on setting it to older date #1581345 : posix unwinds readdirp calls with readdir signature #1581735 : bug-1309462.t is failing reliably due to changes in security.capability changes in the kernel #1582051 : Fix failure of readdir-ahead/bug-1439640.t in certain cases #1582516 : libgfapi: glfs init fails on afr volume with ctime feature enabled #1582704 : rpc_transport_unref() called for an unregistered socket fd #1583018 : changelog: Changelog is not capturing rename of files #1583565 : [distribute]: Excessive 'dict is null' errors in geo-rep logs #1583583 : \"connecting\" state in protocol client is useless #1583937 : Brick process crashed after upgrade from RHGS-3.3.1 async(7.4) to RHGS-3.4(7.5) #1584098 : 'custom extended attributes' set on a directory are not healed after bringing back the down sub-volumes #1584483 : afr: don't update readables if inode refresh failed on all children #1584517 : Inconsistent access permissions on directories after bringing back the down sub-volumes #1584864 : sometime messages #1584981 : posix/ctime: EC self heal of directory is blocked with ctime feature enabled #1585391 : glusteshd wrong status caused by gluterd big lock #1585585 : Cleanup \"connected\" state management of rpc-clnt #1586018 : (f)Setxattr and (f)removexattr invalidates the stat cache in md-cache #1586020 : [GSS] Pending heals are not getting completed in CNS environment #1586342 : Refactor the distributed test code to make it work for ipv4 #1586363 : Refactor rebalance code #1589253 : After creating and starting 601 volumes, self heal daemon went down and seeing continuous warning messages in glusterd log #1589691 : xdata is leaking in server3_3_seek #1589782 : [geo-rep]: Geo-replication in FAULTY state - CENTOS 6 #1589842 : [USS] snapview server does not go through the list of all the snapshots for validating a snap #1590193 : /usr/sbin/gcron.py aborts with OSError #1590385 : Refactor dht lookup code #1590655 : Excessive logging in posix_check_internal_writes() due to NULL dict #1590710 : Gluster Block PVC fails to mount on Jenkins pod #1591193 : lookup not assigning gfid if file is not present in all bricks of replica #1591580 : Remove code duplication in protocol/client #1591621 : Arequal checksum mismatch on older mount #1592141 : Null pointer deref in error paths #1592275 : posix/ctime: Mdata value of a directory is different across replica/EC subvolume #1592509 : ctime: Self heal of symlink is failing on EC subvolume #1593232 : CVE-2018-10841 glusterfs: access trusted peer group via remote-host command [glusterfs upstream] #1593351 : mount.glusterfs incorrectly reports \"getfattr not found\" #1593548 : Stack overflow in readdirp with parallel-readdir enabled #1593562 : Add new peers to Glusto #1593651 : gnfs nfs.register-with-portmap issue with ipv6_default #1595174 : Found an issue on using lock before init in md-cache #1595190 : rmdir is leaking softlinks to directories in .glusterfs #1595320 : gluster wrongly reports bricks online, even when brick path is not available #1595492 : tests: remove tarissue.t from BAD_TEST #1595726 : tests/geo-rep: Add test case for symlink rename #1596020 : Introduce database group profile #1596513 : glustershd crashes when index heal is launched before graph is initialized. #1596524 : 'replica 3 aribiter 1' is not a industry standard way of telling 2-way replicate with arbiter. #1596789 : Update mount-shared-storage.sh to automatically include all enabled glusterfs mounts in fstab #1597156 : Need a simpler way to find if a replica/ec subvolume is up #1597247 : restart all the daemons after all the bricks #1597473 : introduce cluster.daemon-log-level option #1597512 : Remove contrib/ipaddr-py #1597540 : tests/geo-rep: Add test cases for rsnapshot use case #1597563 : [geo-rep+tiering]: Hot and Cold tier brick changelogs report rsync failure #1597568 : Mark brick online after port registration even for brick-mux cases #1597627 : tests/bugs/core/bug-1432542-mpx-restart-crash.t is generated crash #1597662 : Stale entries of snapshots need to be removed from /var/run/gluster/snaps #1597776 : br-state-check.t crashed while brick multiplex is enabled #1597805 : Stale lock with lk-owner all-zeros is observed in some tests #1598325 : Replace the BROKEN_TESTS environment variable value #1598345 : gluster get-state command is crashing glusterd process when geo-replication is configured #1598390 : Remove extras/prot_filter.py #1598548 : Disabling iostats diagnostics.stats-dump-interval (set to 0) does not terminate the dump thread #1598663 : Don't execute statements after decrementing call count in afr #1598884 : [geo-rep]: [Errno 2] No such file or directory #1598926 : Misleading error messages on bricks caused by lseek #1598977 : [geo-rep]: geo-replication scheduler is failing due to unsuccessful umount #1599219 : configure fails complaining absence of libxml2-devel #1599250 : bug-1432542-mpx-restart-crash.t takes a lot of time to complete cleanup #1599628 : To find a compatible brick ignore diagnostics.brick-log-level option while brick mux is enabled #1599783 : _is_prefix should return false for 0-length strings #1600405 : [geo-rep]: Geo-replication not syncing renamed symlink #1600451 : crash on glusterfs_handle_brick_status of the glusterfsd #1600687 : fuse process segfault when use resolve-gids option #1600812 : A new volume set option to for GD2 quota integration #1600878 : crash seen while running regression, intermittently. #1600963 : get the failed test details into gerrit output itself #1601166 : performance.read-ahead causes huge increase in unnecessary network traffic #1601390 : Distributed testing: Fix build environment #1601423 : memory leak in get-state when geo-replication session is configured #1601683 : dht: remove useless argument from dht_iatt_merge #1602070 : [SNAPSHOT] snapshot daemon crashes if a fd from a deleted snapshot is accessed #1602121 : avoid possible glusterd crash in glusterd_verify_slave #1602236 : When reserve limits are reached, append on an existing file after truncate operation results to hang #1602866 : dht: Crash seen in thread dht_dir_attr_heal #1603063 : ./tests/bugs/glusterd/validating-server-quorum.t is generated core #1605056 : [RHHi] Mount hung and not accessible #1605077 : If a node disconnects during volume delete, it assumes deleted volume as a freshly created volume when it is back online #1607049 : Excessive logging in posix_set_parent_ctime() #1607319 : Remove uuid from contrib/ #1607689 : Memory leaks on glfs_fini #1607783 : Segmentation fault while using gfapi while getting volume utilization #1608175 : Skip hash checks in dht_readdirp_cbk if dht has a single child subvol. #1608564 : line coverage tests failing consistently over a week #1608566 : line coverage tests: glusterd crash in ./tests/basic/sdfs-sanity.t #1608568 : line coverage tests: bug-1432542-mpx-restart-crash.t times out consistently #1608684 : Change glusto ownership to reflect current reality #1608991 : Remove code duplication in socket #1609126 : Fix mem leak and smoke failure for gcc8 in cloudsync #1609207 : thin arbiter: set notify-contention option to yes #1609337 : Remove argp-standalone from contrib/ #1609551 : glusterfs-resource-agents should not be built for el6 #1610236 : [Ganesha] Ganesha crashed in mdcache_alloc_and_check_handle while running bonnie and untars with parallel lookups #1610256 : [Ganesha] While performing lookups from two of the clients, \"ls\" command got failed with \"Invalid argument\" #1610405 : Geo-rep: Geo-rep regression times out occasionally #1610726 : Fuse mount of volume fails when gluster_shared_storage is enabled #1611103 : online_brick_count check in volume.rc should ignore bitrot and scrubber daemons #1611566 : tests/bitrot: tests/bitrot/bug-1373520.t fails intermittently #1611692 : Mount process crashes on a sharded volume during rename when dst doesn't exist #1611834 : glusterfsd crashes when SEEK_DATA/HOLE is not supported #1612017 : MAINTAINERS: Add Xavier Hernandez as peer for shard xlator #1612037 : Entry will be present even if the gfid link creation inside .glusterfs fails #1612054 : Test case bug-1586020-mark-dirty-for-entry-txn-on-quorum-failure.t failure #1612418 : Brick not coming up on a volume after rebooting the node #1612750 : gfapi: Use inode_forget in case of unlink/rename objects #1613098 : posix-acl: skip acl_permits check when the owner setting GF_POSIX_ACL_xxxx #1613807 : Fix spurious failures in tests/basic/afr/granular-esh/replace-brick.t #1614062 : Provide/preserve tarball of retried tests #1614088 : kill_brick function needs to wait for brick to be killed #1614124 : glusterfsd process crashed in a multiplexed configuration during cleanup of a single brick-graph triggered by volume-stop. #1614142 : Fix the grammar error in the rpc log #1614168 : [uss]snapshot: posix acl authentication is not working as expected #1614654 : Potential fixes for tests/basic/afr/add-brick-self-heal.t failure #1614662 : ./tests/bugs/replicate/bug-1448804-check-quorum-type-values.t #1614718 : Fix spurious failures in tests/bugs/index/bug-1559004-EMLINK-handling.t #1614730 : Test case bug-1433571-undo-pending-only-on-up-bricks.t failure #1614799 : Geo-rep: Few workers fails to start with out any failure #1615037 : Multiplex tests use a cleanup pattern that results in empty tarballs on failure #1615078 : tests/bugs/replicate/bug-1408712.t fails. #1615092 : tests/bugs/shard/configure-lru-limit.t spurious failure #1615096 : ./tests/bugs/quick-read/bug-846240.t fails spuriously #1615239 : Fix ./tests/basic/afr/replace-brick-self-heal.t failure #1615331 : gfid-mismatch-resolution-with-fav-child-policy.t is failing #1615474 : Rebalance status shows wrong count of \"Rebalanced-files\" if the file has hardlinks #1615582 : test: ./tests/basic/stats-dump.t fails spuriously not finding queue_size in stats output for some brick #1615703 : [Disperse] Improve log messages for EC volume #1615789 : Come up with framework to test thin-arbiter #1618004 : [GSS] glusterd not starting after upgrade due to snapshots error in RHEV + RHGS #1619027 : geo-rep: Active/Passive status change logging is redundant #1619423 : cli: Command gluster volume statedump <volname> dumps core #1619475 : NetBSD memory detection issue #1619720 : posix_mknod does not update trusted.pgfid.xx xattr correctly #1619843 : Snapshot status fails with commit failure #1620544 : Brick process NOT ONLINE for heketidb and block-hosting volume #1621981 : dht: File rename removes the .glusterfs handle for linkto file #1622076 : [geo-rep]: geo-rep reverse sync in FO/FB can accidentally delete the content at original master incase of gfid conflict in 3.4.0 without explicit user rmdir #1622422 : glusterd cli is showing brick status N/A even brick is consumed by a brick process #1622549 : libgfchangelog: History API fails #1622665 : clang-scan report: glusterfs issues #1622821 : Prevent hangs while increasing replica-count/replace-brick for directory hierarchy #1623408 : rpc: log fuse request ID with gluster transaction ID #1623759 : [Disperse] Don't send final version update if non data fop succeeded #1624244 : DHT: Rework the virtual xattr to get the hash subvol #1624440 : Fail volume stop operation in case brick detach request fails #1625089 : CVE-2018-10911 glusterfs: Improper deserialization in dict.c:dict_unserialize() can allow attackers to read arbitrary memory #1625095 : CVE-2018-10930 glusterfs: Files can be renamed outside volume #1625096 : CVE-2018-10923 glusterfs: I/O to arbitrary devices on storage server #1625097 : CVE-2018-10907 glusterfs: Stack-based buffer overflow in server-rpc-fops.c allows remote attackers to execute arbitrary code #1625102 : CVE-2018-10913 glusterfs: Information Exposure in posix_get_file_contents function in posix-helpers.c #1625106 : CVE-2018-10904 glusterfs: Unsanitized file names in debug/io-stats translator can allow remote attackers to execute arbitrary code #1625643 : Use CALLOC in dht_layouts_init #1626319 : DH ciphers disabled errors are encountered on basic mount & unmount with ssl enabled setup #1626346 : dht: Use snprintf in dht_filter_loc_subvol_key #1626394 : dht_create: Create linkto files if required when using dht_filter_loc_subvol_key #1626787 : sas workload job getting stuck after sometime #1627044 : Converting to replica 2 volume is not throwing warning #1627620 : SAS job aborts complaining about file doesn't exist #1628668 : Update op-version from 4.2 to 5.0 #1629877 : GlusterFS can be improved (clone for Gluster-5) #1630673 : geo-rep: geo-rep config set fails to set rsync-options #1630804 : libgfapi-python: test_listdir_with_stat and test_scandir failure on release 5 branch #1633015 : ctime: Access time is different with in same replica/EC volume #1633242 : 'df' shows half as much space on volume after upgrade to RHGS 3.4 #1633552 : glusterd crash in regression build #1635373 : ASan (address sanitizer) fixes - Blanket bug #1635972 : Low Random write IOPS in VM workloads #1635975 : Writes taking very long time leading to system hogging #1636162 : [SNAPSHOT]: with brick multiplexing, snapshot restore will make glusterd send wrong volfile #1636842 : df shows Volume size as zero if Volume created and mounted using Glusterd2 #1638159 : data-self-heal in arbiter volume results in stale locks. #1638163 : split-brain observed on parent dir #1639688 : core: backport uuid fixes #1640392 : io-stats: garbage characters in the filenames generated","title":"Bugs addressed"},{"location":"release-notes/5.1/","text":"Release notes for Gluster 5.1 This is a bugfix release. The release notes for 5.0 contains a listing of all the new features that were added and bugs fixed in the GlusterFS 5 stable release. NOTE: Next minor release tentative date: Week of 10th December, 2018 Major changes, features and limitations addressed in this release This release contains fixes for several security vulnerabilities in Gluster as follows, - https://nvd.nist.gov/vuln/detail/CVE-2018-14651 - https://nvd.nist.gov/vuln/detail/CVE-2018-14652 - https://nvd.nist.gov/vuln/detail/CVE-2018-14653 - https://nvd.nist.gov/vuln/detail/CVE-2018-14654 - https://nvd.nist.gov/vuln/detail/CVE-2018-14659 - https://nvd.nist.gov/vuln/detail/CVE-2018-14660 - https://nvd.nist.gov/vuln/detail/CVE-2018-14661 Major issues None Bugs addressed Bugs addressed since release-5.0 are listed below. #1641429 : Gfid mismatch seen on shards when lookup and mknod are in progress at the same time #1641440 : [ovirt-gluster] Mount hung and not accessible #1641872 : Spurious failures in bug-1637802-arbiter-stale-data-heal-lock.t #1643078 : tests/bugs/glusterd/optimized-basic-testcases-in-cluster.t failing #1643402 : [Geo-Replication] Geo-rep faulty sesion because of the directories are not synced to slave. #1644158 : geo-rep: geo-replication gets stuck after file rename and gfid conflict #1644161 : cliutils: geo-rep cliutils' usage of Popen is not python3 compatible #1644314 : build/packaging: el-X (x > 7) isms #1644514 : geo-rep: On gluster command failure on slave, worker crashes with python3 #1644515 : geo-rep: gluster-mountbroker status crashes #1644526 : Excessive logging in posix_update_utime_in_mdata #1644622 : [Stress] : Mismatching iatt in glustershd logs during MTSH and continous IO from Ganesha mounts #1644645 : [AFR] : Start crawling indices and healing only if both data bricks are UP in replica 2 (thin-arbiter) #1646204 : CVE-2018-14654 glusterfs: \"features/index\" translator can create arbitrary, empty files [fedora-all] #1646896 : [Geo-Replication] Geo-rep faulty sesion because of the directories are not synced to slave. #1647663 : CVE-2018-14651 glusterfs: glusterfs server exploitable via symlinks to relative paths [fedora-all] #1647664 : CVE-2018-14653 glusterfs: Heap-based buffer overflow via \"gf_getspec_req\" RPC message [fedora-all] #1647665 : CVE-2018-14659 glusterfs: Unlimited file creation via \"GF_XATTR_IOSTATS_DUMP_KEY\" xattr allows for denial of service [fedora-all] #1647666 : CVE-2018-14661 glusterfs: features/locks translator passes an user-controlled string to snprintf without a proper format string resulting in a denial of service [fedora-all] #1647801 : can't enable shared-storage #1647962 : CVE-2018-14660 glusterfs: Repeat use of \"GF_META_LOCK_KEY\" xattr allows for memory exhaustion [fedora-all] #1647968 : Seeing defunt translator and discrepancy in volume info when issued from node which doesn't host bricks in that volume #1648923 : gfapi: fix bad dict setting of lease-id","title":5.1},{"location":"release-notes/5.1/#release-notes-for-gluster-51","text":"This is a bugfix release. The release notes for 5.0 contains a listing of all the new features that were added and bugs fixed in the GlusterFS 5 stable release. NOTE: Next minor release tentative date: Week of 10th December, 2018","title":"Release notes for Gluster 5.1"},{"location":"release-notes/5.1/#major-changes-features-and-limitations-addressed-in-this-release","text":"This release contains fixes for several security vulnerabilities in Gluster as follows, - https://nvd.nist.gov/vuln/detail/CVE-2018-14651 - https://nvd.nist.gov/vuln/detail/CVE-2018-14652 - https://nvd.nist.gov/vuln/detail/CVE-2018-14653 - https://nvd.nist.gov/vuln/detail/CVE-2018-14654 - https://nvd.nist.gov/vuln/detail/CVE-2018-14659 - https://nvd.nist.gov/vuln/detail/CVE-2018-14660 - https://nvd.nist.gov/vuln/detail/CVE-2018-14661","title":"Major changes, features and limitations addressed in this release"},{"location":"release-notes/5.1/#major-issues","text":"None","title":"Major issues"},{"location":"release-notes/5.1/#bugs-addressed","text":"Bugs addressed since release-5.0 are listed below. #1641429 : Gfid mismatch seen on shards when lookup and mknod are in progress at the same time #1641440 : [ovirt-gluster] Mount hung and not accessible #1641872 : Spurious failures in bug-1637802-arbiter-stale-data-heal-lock.t #1643078 : tests/bugs/glusterd/optimized-basic-testcases-in-cluster.t failing #1643402 : [Geo-Replication] Geo-rep faulty sesion because of the directories are not synced to slave. #1644158 : geo-rep: geo-replication gets stuck after file rename and gfid conflict #1644161 : cliutils: geo-rep cliutils' usage of Popen is not python3 compatible #1644314 : build/packaging: el-X (x > 7) isms #1644514 : geo-rep: On gluster command failure on slave, worker crashes with python3 #1644515 : geo-rep: gluster-mountbroker status crashes #1644526 : Excessive logging in posix_update_utime_in_mdata #1644622 : [Stress] : Mismatching iatt in glustershd logs during MTSH and continous IO from Ganesha mounts #1644645 : [AFR] : Start crawling indices and healing only if both data bricks are UP in replica 2 (thin-arbiter) #1646204 : CVE-2018-14654 glusterfs: \"features/index\" translator can create arbitrary, empty files [fedora-all] #1646896 : [Geo-Replication] Geo-rep faulty sesion because of the directories are not synced to slave. #1647663 : CVE-2018-14651 glusterfs: glusterfs server exploitable via symlinks to relative paths [fedora-all] #1647664 : CVE-2018-14653 glusterfs: Heap-based buffer overflow via \"gf_getspec_req\" RPC message [fedora-all] #1647665 : CVE-2018-14659 glusterfs: Unlimited file creation via \"GF_XATTR_IOSTATS_DUMP_KEY\" xattr allows for denial of service [fedora-all] #1647666 : CVE-2018-14661 glusterfs: features/locks translator passes an user-controlled string to snprintf without a proper format string resulting in a denial of service [fedora-all] #1647801 : can't enable shared-storage #1647962 : CVE-2018-14660 glusterfs: Repeat use of \"GF_META_LOCK_KEY\" xattr allows for memory exhaustion [fedora-all] #1647968 : Seeing defunt translator and discrepancy in volume info when issued from node which doesn't host bricks in that volume #1648923 : gfapi: fix bad dict setting of lease-id","title":"Bugs addressed"},{"location":"release-notes/5.10/","text":"Release notes for Gluster 5.10 This is a bugfix release. The release notes for 5.0 , 5.1 , 5.2 , 5.3 , 5.5 , 5.6 , 5.8 and 5.9 contains a listing of all the new features that were added and bugs fixed in the GlusterFS 5 stable release. Next minor release tentative date: Week of 10th December, 2019 Major changes, features and limitations addressed in this release None Major issues None Bugs addressed Bugs addressed since release-5.9 are listed below. #1749352 : Failures in remove-brick due to [Input/output error] errors #1750230 : [geo-rep]: Non-root - Unable to set up mountbroker root directory and group #1739336 : Multiple disconnect events being propagated for the same child","title":"5.10."},{"location":"release-notes/5.10/#release-notes-for-gluster-510","text":"This is a bugfix release. The release notes for 5.0 , 5.1 , 5.2 , 5.3 , 5.5 , 5.6 , 5.8 and 5.9 contains a listing of all the new features that were added and bugs fixed in the GlusterFS 5 stable release. Next minor release tentative date: Week of 10th December, 2019","title":"Release notes for Gluster 5.10"},{"location":"release-notes/5.10/#major-changes-features-and-limitations-addressed-in-this-release","text":"None","title":"Major changes, features and limitations addressed in this release"},{"location":"release-notes/5.10/#major-issues","text":"None","title":"Major issues"},{"location":"release-notes/5.10/#bugs-addressed","text":"Bugs addressed since release-5.9 are listed below. #1749352 : Failures in remove-brick due to [Input/output error] errors #1750230 : [geo-rep]: Non-root - Unable to set up mountbroker root directory and group #1739336 : Multiple disconnect events being propagated for the same child","title":"Bugs addressed"},{"location":"release-notes/5.11/","text":"Release notes for Gluster 5.11 This is a bugfix release. The release notes for 5.0 , 5.1 , 5.2 , 5.3 , 5.5 , 5.6 , 5.8 , 5.9 , and 5.10 contains a listing of all the new features that were added and bugs fixed in the GlusterFS 5 stable release. Next minor release tentative date: Week of 10th February, 2020 Major changes, features and limitations addressed in this release None Major issues None Bugs addressed Bugs addressed since release-5.10 are listed below. #1718734 : Memory leak in glusterfsd process #1760710 : glustershd can not decide heald_sinks, and skip repair, so some entries lingering in volume heal info #1767305 : READDIRP incorrectly updates posix-acl inode ctx #1779284 : Backport GNFS memory leak fix to version 5","title":5.11},{"location":"release-notes/5.11/#release-notes-for-gluster-511","text":"This is a bugfix release. The release notes for 5.0 , 5.1 , 5.2 , 5.3 , 5.5 , 5.6 , 5.8 , 5.9 , and 5.10 contains a listing of all the new features that were added and bugs fixed in the GlusterFS 5 stable release. Next minor release tentative date: Week of 10th February, 2020","title":"Release notes for Gluster 5.11"},{"location":"release-notes/5.11/#major-changes-features-and-limitations-addressed-in-this-release","text":"None","title":"Major changes, features and limitations addressed in this release"},{"location":"release-notes/5.11/#major-issues","text":"None","title":"Major issues"},{"location":"release-notes/5.11/#bugs-addressed","text":"Bugs addressed since release-5.10 are listed below. #1718734 : Memory leak in glusterfsd process #1760710 : glustershd can not decide heald_sinks, and skip repair, so some entries lingering in volume heal info #1767305 : READDIRP incorrectly updates posix-acl inode ctx #1779284 : Backport GNFS memory leak fix to version 5","title":"Bugs addressed"},{"location":"release-notes/5.12/","text":"Release notes for Gluster 5.12 This is a bugfix release. The release notes for 5.0 , 5.1 , 5.2 , 5.3 , 5.5 , 5.6 , 5.8 , 5.9 , 5.10 , and 5.11 contains a listing of all the new features that were added and bugs fixed in the GlusterFS 5 stable release. Next minor release tentative date: Week of 10th April, 2020 Major changes, features and limitations addressed in this release None Major issues None Bugs addressed Bugs addressed since release-5.11 are listed below. #1803810 : Functionality to enable log rotation for user serviceable snapshot's logs. #1804512 : Mounts fails after reboot of 1/3 gluster nodes #1804522 : Rebalance is causing glusterfs crash on client node #1805047 : I/O error on writes to a disperse volume when replace-brick is executed #1805049 : Glusterfsd crashing in ec-inode-write.c, in GF_ASSERT #1805050 : [Disperse] : Client side heal is not removing dirty flag for some of the files. #1805051 : Disperse volume : data corruption with ftruncate data in 4+2 config #1805052 : Disperse volume : Ganesha crash with IO in 4+2 config when one glusterfsd restart every 600s #1805053 : An Input/Output error happens on a disperse volume when doing unaligned writes to a sparse file #1805054 : Disperse volume : data corruption with ftruncate data in 4+2 config #1805055 : Open fd heal should filter O_APPEND/O_EXCL #1805056 : Disperse volume : data corruption with ftruncate data in 4+2 config #1805057 : [EC] shd crashed while heal failed due to out of memory error.","title":5.12},{"location":"release-notes/5.12/#release-notes-for-gluster-512","text":"This is a bugfix release. The release notes for 5.0 , 5.1 , 5.2 , 5.3 , 5.5 , 5.6 , 5.8 , 5.9 , 5.10 , and 5.11 contains a listing of all the new features that were added and bugs fixed in the GlusterFS 5 stable release. Next minor release tentative date: Week of 10th April, 2020","title":"Release notes for Gluster 5.12"},{"location":"release-notes/5.12/#major-changes-features-and-limitations-addressed-in-this-release","text":"None","title":"Major changes, features and limitations addressed in this release"},{"location":"release-notes/5.12/#major-issues","text":"None","title":"Major issues"},{"location":"release-notes/5.12/#bugs-addressed","text":"Bugs addressed since release-5.11 are listed below. #1803810 : Functionality to enable log rotation for user serviceable snapshot's logs. #1804512 : Mounts fails after reboot of 1/3 gluster nodes #1804522 : Rebalance is causing glusterfs crash on client node #1805047 : I/O error on writes to a disperse volume when replace-brick is executed #1805049 : Glusterfsd crashing in ec-inode-write.c, in GF_ASSERT #1805050 : [Disperse] : Client side heal is not removing dirty flag for some of the files. #1805051 : Disperse volume : data corruption with ftruncate data in 4+2 config #1805052 : Disperse volume : Ganesha crash with IO in 4+2 config when one glusterfsd restart every 600s #1805053 : An Input/Output error happens on a disperse volume when doing unaligned writes to a sparse file #1805054 : Disperse volume : data corruption with ftruncate data in 4+2 config #1805055 : Open fd heal should filter O_APPEND/O_EXCL #1805056 : Disperse volume : data corruption with ftruncate data in 4+2 config #1805057 : [EC] shd crashed while heal failed due to out of memory error.","title":"Bugs addressed"},{"location":"release-notes/5.13/","text":"Release notes for Gluster 5.13 This is a bugfix release. The release notes for 5.0 , 5.1 , 5.2 , 5.3 , 5.5 , 5.6 , 5.8 , 5.9 , 5.10 , 5.11 , and 5.12 contains a listing of all the new features that were added and bugs fixed in the GlusterFS 5 stable release. NOTE: This is supposed to be last minor release of 5. Major changes, features and limitations addressed in this release None Major issues None Bugs addressed Bugs addressed since release-5.12 are listed below. #1803810 : Functionality to enable log rotation for user serviceable snapshot's logs. #1127 : Mount crash during background shard cleanup #1103 :afr: prevent spurious entry heals leading to gfid split-brain #1067 :Metadata heal picks different brick each time as source if there are no pending xattrs #1028 :Segfault in FUSE process, potential use after free #1390914 : Glusterfs create a flock lock by anonymous fd, but can't release it forever. #1806931 : Changes to self-heal logic w.r.t. detecting metadata split-brains #1807007 : The result (hostname) of getnameinfo for all bricks (ipv6 addresses) are the same, while they are not. #1807431 : Setting cluster.heal-timeout requires volume restart #1807748 : bug-1402841.t-mt-dir-scan-race.t fails spuriously #1808256 : Glusterfs create a flock lock by anonymous fd, but can't release it forever. #1809440 : [brickmux]: glustershd crashed when rebooting 1/3 nodes at regular intervals","title":5.13},{"location":"release-notes/5.13/#release-notes-for-gluster-513","text":"This is a bugfix release. The release notes for 5.0 , 5.1 , 5.2 , 5.3 , 5.5 , 5.6 , 5.8 , 5.9 , 5.10 , 5.11 , and 5.12 contains a listing of all the new features that were added and bugs fixed in the GlusterFS 5 stable release. NOTE: This is supposed to be last minor release of 5.","title":"Release notes for Gluster 5.13"},{"location":"release-notes/5.13/#major-changes-features-and-limitations-addressed-in-this-release","text":"None","title":"Major changes, features and limitations addressed in this release"},{"location":"release-notes/5.13/#major-issues","text":"None","title":"Major issues"},{"location":"release-notes/5.13/#bugs-addressed","text":"Bugs addressed since release-5.12 are listed below. #1803810 : Functionality to enable log rotation for user serviceable snapshot's logs. #1127 : Mount crash during background shard cleanup #1103 :afr: prevent spurious entry heals leading to gfid split-brain #1067 :Metadata heal picks different brick each time as source if there are no pending xattrs #1028 :Segfault in FUSE process, potential use after free #1390914 : Glusterfs create a flock lock by anonymous fd, but can't release it forever. #1806931 : Changes to self-heal logic w.r.t. detecting metadata split-brains #1807007 : The result (hostname) of getnameinfo for all bricks (ipv6 addresses) are the same, while they are not. #1807431 : Setting cluster.heal-timeout requires volume restart #1807748 : bug-1402841.t-mt-dir-scan-race.t fails spuriously #1808256 : Glusterfs create a flock lock by anonymous fd, but can't release it forever. #1809440 : [brickmux]: glustershd crashed when rebooting 1/3 nodes at regular intervals","title":"Bugs addressed"},{"location":"release-notes/5.2/","text":"Release notes for Gluster 5.2 This is a bugfix release. The release notes for 5.0 and 5.1 contains a listing of all the new features that were added and bugs fixed in the GlusterFS 5 stable release. NOTE: Next minor release tentative date: Week of 10th January, 2019 Major changes, features and limitations addressed in this release None Major issues None Bugs addressed Bugs addressed since release-5.1 are listed below. #1651525 : Issuing a \"heal ... full\" on a disperse volume causes permanent high CPU utilization. #1654115 : [Geo-rep]: Faulty geo-rep sessions due to link ownership on slave volume #1654117 : [geo-rep]: Failover / Failback shows fault status in a non-root setup #1654236 : Provide an option to silence glfsheal logs #1654370 : Bitrot: Scrub status say file is corrupted even it was just created AND 'path' in the output is broken #1655545 : gfid heal does not happen when there is no source brick","title":5.2},{"location":"release-notes/5.2/#release-notes-for-gluster-52","text":"This is a bugfix release. The release notes for 5.0 and 5.1 contains a listing of all the new features that were added and bugs fixed in the GlusterFS 5 stable release. NOTE: Next minor release tentative date: Week of 10th January, 2019","title":"Release notes for Gluster 5.2"},{"location":"release-notes/5.2/#major-changes-features-and-limitations-addressed-in-this-release","text":"None","title":"Major changes, features and limitations addressed in this release"},{"location":"release-notes/5.2/#major-issues","text":"None","title":"Major issues"},{"location":"release-notes/5.2/#bugs-addressed","text":"Bugs addressed since release-5.1 are listed below. #1651525 : Issuing a \"heal ... full\" on a disperse volume causes permanent high CPU utilization. #1654115 : [Geo-rep]: Faulty geo-rep sessions due to link ownership on slave volume #1654117 : [geo-rep]: Failover / Failback shows fault status in a non-root setup #1654236 : Provide an option to silence glfsheal logs #1654370 : Bitrot: Scrub status say file is corrupted even it was just created AND 'path' in the output is broken #1655545 : gfid heal does not happen when there is no source brick","title":"Bugs addressed"},{"location":"release-notes/5.3/","text":"Release notes for Gluster 5.3 This is a bugfix release. The release notes for 5.0 , 5.1 and 5.2 contains a listing of all the new features that were added and bugs fixed in the GlusterFS 5 stable release. NOTE: Next minor release tentative date: Week of 10th March, 2019 Major changes, features and limitations addressed in this release None Major issues None Bugs addressed Bugs addressed since release-5.2 are listed below. #1623107 : FUSE client's memory leak #1648642 : fails to sync non-ascii (utf8) file and directory names, causes permanently faulty geo-replication state #1651323 : Tracker bug for all leases related issues #1659563 : gluster-blockd segfaults because of a null-dereference in shard.so #1659676 : Memory leak: dict_t leak in rda_opendir #1660736 : dht_revalidate may not heal attrs on the brick root #1660932 : Fix tests/bugs/shard/zero-flag.t #1662200 : NL cache: fix typos #1663131 : [Ganesha] Ganesha failed on one node while exporting volumes in loop #1665803 : [ovirt-gluster] Fuse mount crashed while deleting a 1 TB image file from ovirt","title":5.3},{"location":"release-notes/5.3/#release-notes-for-gluster-53","text":"This is a bugfix release. The release notes for 5.0 , 5.1 and 5.2 contains a listing of all the new features that were added and bugs fixed in the GlusterFS 5 stable release. NOTE: Next minor release tentative date: Week of 10th March, 2019","title":"Release notes for Gluster 5.3"},{"location":"release-notes/5.3/#major-changes-features-and-limitations-addressed-in-this-release","text":"None","title":"Major changes, features and limitations addressed in this release"},{"location":"release-notes/5.3/#major-issues","text":"None","title":"Major issues"},{"location":"release-notes/5.3/#bugs-addressed","text":"Bugs addressed since release-5.2 are listed below. #1623107 : FUSE client's memory leak #1648642 : fails to sync non-ascii (utf8) file and directory names, causes permanently faulty geo-replication state #1651323 : Tracker bug for all leases related issues #1659563 : gluster-blockd segfaults because of a null-dereference in shard.so #1659676 : Memory leak: dict_t leak in rda_opendir #1660736 : dht_revalidate may not heal attrs on the brick root #1660932 : Fix tests/bugs/shard/zero-flag.t #1662200 : NL cache: fix typos #1663131 : [Ganesha] Ganesha failed on one node while exporting volumes in loop #1665803 : [ovirt-gluster] Fuse mount crashed while deleting a 1 TB image file from ovirt","title":"Bugs addressed"},{"location":"release-notes/5.5/","text":"Release notes for Gluster 5.5 This is a bugfix release. The release notes for 5.0 , 5.1 , 5.2 and 5.3 contains a listing of all the new features that were added and bugs fixed in the GlusterFS 5 stable release. NOTE: Next minor release tentative date: Week of 10th May, 2019 NOTE: Release 5.4 was never announced as there was a fix which prevented rolling upgrades from working correctly. Hence this release notes contains a skip from 5.3 till 5.5 in terms of issues addressed and also addresses the issue were rolling upgrades were broken. Major changes, features and limitations addressed in this release None Major issues None Bugs addressed Bugs addressed since release-5.3 are listed below. #1684385 : [ovirt-gluster] Rolling gluster upgrade from 3.12.5 to 5.3 led to shard on-disk xattrs disappearing #1684569 : Upgrade from 4.1 and 5 is broken #1687249 : Error handling in /usr/sbin/gluster-eventsapi produces IndexError: tuple index out of range #1687687 : [geo-rep]: Checksum mismatch when 2x2 vols are converted to arbiter #1649054 : glustereventsd does not start on Ubuntu 16.04 LTS #1651246 : Failed to dispatch handler #1665145 : Writes on Gluster 5 volumes fail with EIO when \"cluster.consistent-metadata\" is set #1669382 : [ovirt-gluster] Fuse mount crashed while creating the preallocated image #1670307 : api: bad GFAPI_4.1.6 block #1671217 : core: move \"dict is NULL\" logs to DEBUG log level #1671556 : glusterfs FUSE client crashing every few days with 'Failed to dispatch handler' #1671611 : Unable to delete directories that contain linkto files that point to itself. #1672248 : quorum count not updated in nfs-server vol file #1672314 : thin-arbiter: Check with thin-arbiter file before marking new entry change log #1673268 : Fix timeouts so the tests pass on AWS #1678726 : Integer Overflow possible in md-cache.c due to data type inconsistency #1679968 : Upgrade from glusterfs 3.12 to gluster 4/5 broken","title":5.5},{"location":"release-notes/5.5/#release-notes-for-gluster-55","text":"This is a bugfix release. The release notes for 5.0 , 5.1 , 5.2 and 5.3 contains a listing of all the new features that were added and bugs fixed in the GlusterFS 5 stable release. NOTE: Next minor release tentative date: Week of 10th May, 2019 NOTE: Release 5.4 was never announced as there was a fix which prevented rolling upgrades from working correctly. Hence this release notes contains a skip from 5.3 till 5.5 in terms of issues addressed and also addresses the issue were rolling upgrades were broken.","title":"Release notes for Gluster 5.5"},{"location":"release-notes/5.5/#major-changes-features-and-limitations-addressed-in-this-release","text":"None","title":"Major changes, features and limitations addressed in this release"},{"location":"release-notes/5.5/#major-issues","text":"None","title":"Major issues"},{"location":"release-notes/5.5/#bugs-addressed","text":"Bugs addressed since release-5.3 are listed below. #1684385 : [ovirt-gluster] Rolling gluster upgrade from 3.12.5 to 5.3 led to shard on-disk xattrs disappearing #1684569 : Upgrade from 4.1 and 5 is broken #1687249 : Error handling in /usr/sbin/gluster-eventsapi produces IndexError: tuple index out of range #1687687 : [geo-rep]: Checksum mismatch when 2x2 vols are converted to arbiter #1649054 : glustereventsd does not start on Ubuntu 16.04 LTS #1651246 : Failed to dispatch handler #1665145 : Writes on Gluster 5 volumes fail with EIO when \"cluster.consistent-metadata\" is set #1669382 : [ovirt-gluster] Fuse mount crashed while creating the preallocated image #1670307 : api: bad GFAPI_4.1.6 block #1671217 : core: move \"dict is NULL\" logs to DEBUG log level #1671556 : glusterfs FUSE client crashing every few days with 'Failed to dispatch handler' #1671611 : Unable to delete directories that contain linkto files that point to itself. #1672248 : quorum count not updated in nfs-server vol file #1672314 : thin-arbiter: Check with thin-arbiter file before marking new entry change log #1673268 : Fix timeouts so the tests pass on AWS #1678726 : Integer Overflow possible in md-cache.c due to data type inconsistency #1679968 : Upgrade from glusterfs 3.12 to gluster 4/5 broken","title":"Bugs addressed"},{"location":"release-notes/5.6/","text":"Release notes for Gluster 5.6 This is a bugfix release. The release notes for 5.0 , 5.1 , 5.2 , 5.3 and 5.5 contains a listing of all the new features that were added and bugs fixed in the GlusterFS 5 stable release. NOTE: Next minor release tentative date: Week of 10th June, 2019 NOTE: Release 5.4 was never announced as there was a fix which prevented rolling upgrades from working correctly. Hence this release notes contains a skip from 5.3 till 5.5 in terms of issues addressed and also addresses the issue were rolling upgrades were broken. Major changes, features and limitations addressed in this release None Major issues Several users had issues around increased network usage after upgrading to 5.x release, this issue was tracked against bug#1673058 and is now addressed as a part of this minor release. Bugs addressed Bugs addressed since release-5.5 are listed below. #1673058 : Network throughput usage increased x5 #1690952 : lots of \"Matching lock not found for unlock xxx\" when using disperse (ec) xlator #1694562 : gfapi: do not block epoll thread for upcall notifications #1694612 : glusterd leaking memory when issued gluster vol status all tasks continuosly #1695391 : GF_LOG_OCCASSIONALLY API doesn't log at first instance #1695403 : rm -rf fails with \"Directory not empty\" #1696147 : Multiple shd processes are running on brick_mux environmet","title":5.6},{"location":"release-notes/5.6/#release-notes-for-gluster-56","text":"This is a bugfix release. The release notes for 5.0 , 5.1 , 5.2 , 5.3 and 5.5 contains a listing of all the new features that were added and bugs fixed in the GlusterFS 5 stable release. NOTE: Next minor release tentative date: Week of 10th June, 2019 NOTE: Release 5.4 was never announced as there was a fix which prevented rolling upgrades from working correctly. Hence this release notes contains a skip from 5.3 till 5.5 in terms of issues addressed and also addresses the issue were rolling upgrades were broken.","title":"Release notes for Gluster 5.6"},{"location":"release-notes/5.6/#major-changes-features-and-limitations-addressed-in-this-release","text":"None","title":"Major changes, features and limitations addressed in this release"},{"location":"release-notes/5.6/#major-issues","text":"Several users had issues around increased network usage after upgrading to 5.x release, this issue was tracked against bug#1673058 and is now addressed as a part of this minor release.","title":"Major issues"},{"location":"release-notes/5.6/#bugs-addressed","text":"Bugs addressed since release-5.5 are listed below. #1673058 : Network throughput usage increased x5 #1690952 : lots of \"Matching lock not found for unlock xxx\" when using disperse (ec) xlator #1694562 : gfapi: do not block epoll thread for upcall notifications #1694612 : glusterd leaking memory when issued gluster vol status all tasks continuosly #1695391 : GF_LOG_OCCASSIONALLY API doesn't log at first instance #1695403 : rm -rf fails with \"Directory not empty\" #1696147 : Multiple shd processes are running on brick_mux environmet","title":"Bugs addressed"},{"location":"release-notes/5.8/","text":"Release notes for Gluster 5.8 This is a bugfix release. The release notes for 5.0 , 5.1 , 5.2 , 5.3 , 5.5 , and 5.6 contains a listing of all the new features that were added and bugs fixed in the GlusterFS 5 stable release. NOTE: The 5.7 is dead by release due to #1728988 The packages weren't released. Please use 5.8. Next minor release tentative date: Week of 10th August, 2019 Major changes, features and limitations addressed in this release None Major issues A issue that was blocking the build was addressed #1728988 Bugs addressed Bugs addressed since release-5.6 are listed below. #1717282 : ec ignores lock contention notifications for partially acquired locks #1629877 : GlusterFS can be improved (clone for Gluster-5) #1695399 : With parallel-readdir enabled, deleting a directory containing stale linkto files fails with \"Directory not empty\" #1699500 : fix truncate lock to cover the write in tuncate clean #1699736 : Fops hang when inodelk fails on the first fop #1707198 : VM stuck in a shutdown because of a pending fuse request #1720634 : Upcall: Avoid sending upcalls for invalid Inode #1720636 : Ganesha-gfapi logs are flooded with error messages related to \"gf_uuid_is_null(gfid)) [Invalid argument]\" when lookups are running from multiple clients #1721106 : Failed to create volume which transport_type is \"tcp,rdma\" #1728988 : release-5.7 glupy is not getting built during packaging.","title":5.8},{"location":"release-notes/5.8/#release-notes-for-gluster-58","text":"This is a bugfix release. The release notes for 5.0 , 5.1 , 5.2 , 5.3 , 5.5 , and 5.6 contains a listing of all the new features that were added and bugs fixed in the GlusterFS 5 stable release. NOTE: The 5.7 is dead by release due to #1728988 The packages weren't released. Please use 5.8. Next minor release tentative date: Week of 10th August, 2019","title":"Release notes for Gluster 5.8"},{"location":"release-notes/5.8/#major-changes-features-and-limitations-addressed-in-this-release","text":"None","title":"Major changes, features and limitations addressed in this release"},{"location":"release-notes/5.8/#major-issues","text":"A issue that was blocking the build was addressed #1728988","title":"Major issues"},{"location":"release-notes/5.8/#bugs-addressed","text":"Bugs addressed since release-5.6 are listed below. #1717282 : ec ignores lock contention notifications for partially acquired locks #1629877 : GlusterFS can be improved (clone for Gluster-5) #1695399 : With parallel-readdir enabled, deleting a directory containing stale linkto files fails with \"Directory not empty\" #1699500 : fix truncate lock to cover the write in tuncate clean #1699736 : Fops hang when inodelk fails on the first fop #1707198 : VM stuck in a shutdown because of a pending fuse request #1720634 : Upcall: Avoid sending upcalls for invalid Inode #1720636 : Ganesha-gfapi logs are flooded with error messages related to \"gf_uuid_is_null(gfid)) [Invalid argument]\" when lookups are running from multiple clients #1721106 : Failed to create volume which transport_type is \"tcp,rdma\" #1728988 : release-5.7 glupy is not getting built during packaging.","title":"Bugs addressed"},{"location":"release-notes/5.9/","text":"Release notes for Gluster 5.9 This is a bugfix release. The release notes for 5.0 , 5.1 , 5.2 , 5.3 , 5.5 , 5.6 and 5.8 contains a listing of all the new features that were added and bugs fixed in the GlusterFS 5 stable release. Next minor release tentative date: Week of 10th October, 2019 Major changes, features and limitations addressed in this release None Major issues None Bugs addressed Bugs addressed since release-5.8 are listed below. #1733881 : [geo-rep]: gluster command not found while setting up a non-root session #1736342 : potential deadlock while processing callbacks in gfapi #1737716 : Unable to create geo-rep session on a non-root setup.","title":5.9},{"location":"release-notes/5.9/#release-notes-for-gluster-59","text":"This is a bugfix release. The release notes for 5.0 , 5.1 , 5.2 , 5.3 , 5.5 , 5.6 and 5.8 contains a listing of all the new features that were added and bugs fixed in the GlusterFS 5 stable release. Next minor release tentative date: Week of 10th October, 2019","title":"Release notes for Gluster 5.9"},{"location":"release-notes/5.9/#major-changes-features-and-limitations-addressed-in-this-release","text":"None","title":"Major changes, features and limitations addressed in this release"},{"location":"release-notes/5.9/#major-issues","text":"None","title":"Major issues"},{"location":"release-notes/5.9/#bugs-addressed","text":"Bugs addressed since release-5.8 are listed below. #1733881 : [geo-rep]: gluster command not found while setting up a non-root session #1736342 : potential deadlock while processing callbacks in gfapi #1737716 : Unable to create geo-rep session on a non-root setup.","title":"Bugs addressed"},{"location":"release-notes/6.0/","text":"Release notes for Gluster 6.0 This is a major release that includes a range of code improvements and stability fixes along with a few features as noted below. A selection of the key features and changes are documented in this page. A full list of bugs that have been addressed is included further below. Announcements Major changes and features Major issues Bugs addressed in the release Announcements Releases that receive maintenance updates post release 6 are, 4.1 and 5 ( reference ) Release 6 will receive maintenance updates around the 10th of every month for the first 3 months post release (i.e Apr'19, May'19, Jun'19). Post the initial 3 months, it will receive maintenance updates every 2 months till EOL. ( reference ) A series of features/xlators have been deprecated in release 6 as follows, for upgrade procedures from volumes that use these features to release 6 refer to the release 6 upgrade guide . This deprecation was announced at the gluster-users list here . Features deprecated: Block device (bd) xlator Decompounder feature Crypt xlator Symlink-cache xlator Stripe feature Tiering support (tier xlator and changetimerecorder) Major changes and features Highlights Several stability fixes addressing, coverity, clang-scan, address sanitizer and valgrind reported issues removal of unused and hence, deprecated code and features Client side inode garbage collection This release addresses one of the major concerns regarding FUSE mount process memory footprint, by introducing client side inode garbage collection See standalone section for more details Performance Improvements --auto-invalidation on FUSE mounts to leverage kernel page cache more effectively Features are categorized into the following sections, Management Standalone Developer Management NOTE: There have been several stability improvements around the brick multiplexing feature GlusterD2 GlusterD2 (or GD2, in short) was planned as the next generation management service for Gluster project. Currently, GD2s main focus is not replacing glusterd , but to serve as a thin management layer when using gluster with container orchestration systems. There is no specific update around GD2 provided as a part of this release. Standalone 1. client-side inode garbage collection via LRU list A FUSE mount's inode cache can now be limited to a maximum number, thus reducing the memory footprint of FUSE mount processes. See the lru-limit option in man 8 mount.glusterfs for details. NOTE: Setting this to a low value (say less than 4000), will evict inodes from FUSE and Gluster caches at a much faster rate, and can cause performance degrades. The setting has to be determined based on the available client memory and required performance. 2. Glusterfind tool enhanced with a filter option glusterfind tool has an added option \"--type\", to be used with the \"--full\" option. The option supports finding and listing files or directories only, and defaults to both if not specified. Example usage with the pre and query commands are given below, Pre command ( reference ): Lists both files and directories in OUTFILE: glusterfind pre SESSION_NAME VOLUME_NAME OUTFILE Lists only files in OUTFILE: glusterfind pre SESSION_NAME VOLUME_NAME OUTFILE --type f Lists only directories in OUTFILE: glusterfind pre SESSION_NAME VOLUME_NAME OUTFILE --type d Query command: Lists both files and directories in OUTFILE: glusterfind query VOLUME_NAME --full OUTFILE Lists only files in OUTFILE: glusterfind query VOLUME_NAME --full --type f OUTFILE Lists only directories in OUTFILE: glusterfind query VOLUME_NAME --full --type d OUTFILE 3. FUSE mounts are enhanced to handle interrupts to blocked lock requests FUSE mounts are enhanced to handle interrupts to blocked locks. For example, scripts using the flock ( man 1 flock ) utility without the -n(nonblock) option against files on a FUSE based gluster mount, can now be interrupted when the lock is not granted in time or using the -w option with the same utility. 4. Optimized/pass-through distribute functionality for 1-way distributed volumes NOTE: There are no user controllable changes with this feature The distribute xlator now skips unnecessary checks and operations when the distribute count is one for a volume, resulting in improved performance. 5. Options introduced to disable invalidations of kernel page cache For workloads, where multiple FUSE client mounts do not concurrently operate on any files in the volume, it is now possible to maintain a longer duration kernel page cache using the following options in conjunction, Setting --auto-invalidation option to \"no\" on the glusterfs FUSE mount process Disabling the volume option performance.global-cache-invalidation This enables better performance as the data is served from the kernel page cache where possible. 6. Changes to gluster based SMB share management Previously all GlusterFS volumes were being exported by default via smb.conf in a Samba-CTDB setup. This includes creating a share section for CTDB lock volume too which is not recommended. Along with few syntactical errors these scripts failed to execute in a non-Samba setup in the absence of necessary configuration and binary files. Hereafter newly created GlusterFS volumes are not exported as SMB share via Samba unless either of 'user.cifs' or 'user.smb' volume set options are enabled on the volume. The existing GlusterFS volume share sections in smb.conf will remain unchanged. 7. ctime feature is enabled by default The ctime feature which maintains (c/m) time consistency across replica and disperse subvolumes is enabled by default. Also, with this release, a single option is provided to enable/disable ctime feature, #gluster vol set <volname> ctime <on/off> NOTE: The time information used is from clients, hence it's required that clients are synced with respect to their times, using NTP or other such means. Limitations: - Mounting gluster volume with time attribute options (noatime, realatime...) is not supported with this feature - This feature does not guarantee consistent time for directories if the hashed sub-volume for the directory is down - Directory listing is not supported with this feature, and may report inconsistent time information - Older files created before upgrade, would witness update of ctime upon accessing after upgrade BUG:1593542 Developer 1. Gluster code can be compiled and executed using TSAN While configuring the sources for a build use the extra option --enable-tsan to enable thread sanitizer based builds. 2. gfapi: A class of APIs have been enhanced to return pre/post gluster_stat information A set of apis have been enhanced to return pre/post gluster_stat information. Applications using gfapi would need to adapt to the newer interfaces to compile against release-6 apis. Pre-compiled applications, or applications using the older API SDK will continue to work as before. Major issues None Bugs addressed Bugs addressed since release-5 are listed below. #1138841 : allow the use of the CIDR format with auth.allow #1236272 : socket: Use newer system calls that provide better interface/performance on Linux/*BSD when available #1243991 : \"gluster volume set group \" is not in the help text #1285126 : RFE: GlusterFS NFS does not implement an all_squash volume setting #1343926 : port-map: let brick choose its own port #1364707 : Remove deprecated stripe xlator #1427397 : script to strace processes consuming high CPU #1467614 : Gluster read/write performance improvements on NVMe backend #1486532 : need a script to resolve backtraces #1511339 : In Replica volume 2*2 when quorum is set, after glusterd restart nfs server is coming up instead of self-heal daemon #1535495 : Add option -h and --help to gluster cli #1535528 : Gluster cli show no help message in prompt #1560561 : systemd service file enhancements #1560969 : Garbage collect inactive inodes in fuse-bridge #1564149 : Agree upon a coding standard, and automate check for this in smoke #1564890 : mount.glusterfs: can't shift that many #1575836 : logic in S30samba-start.sh hook script needs tweaking #1579788 : Thin-arbiter: Have the state of volume in memory #1582516 : libgfapi: glfs init fails on afr volume with ctime feature enabled #1590385 : Refactor dht lookup code #1593538 : ctime: Access time is different with in same replica/EC volume #1596787 : glusterfs rpc-clnt.c: error returned while attempting to connect to host: (null), port 0 #1598345 : gluster get-state command is crashing glusterd process when geo-replication is configured #1600145 : [geo-rep]: Worker still ACTIVE after killing bricks #1605056 : [RHHi] Mount hung and not accessible #1605077 : If a node disconnects during volume delete, it assumes deleted volume as a freshly created volume when it is back online #1608512 : cluster.server-quorum-type help text is missing possible settings #1624006 : /var/run/gluster/metrics/ wasn't created automatically #1624332 : [Thin-arbiter]: Add tests for thin arbiter feature #1624724 : ctime: Enable ctime feature by default and also improve usability by providing single option to enable #1624796 : mkdir -p fails with \"No data available\" when root-squash is enabled #1625850 : tests: fixes to bug-1015990-rep.t #1625961 : Writes taking very long time leading to system hogging #1626313 : fix glfs_fini related problems #1626610 : [USS]: Change gf_log to gf_msg #1626994 : split-brain observed on parent dir #1627610 : glusterd crash in regression build #1627620 : SAS job aborts complaining about file doesn't exist #1628194 : tests/dht: Additional tests for dht operations #1628605 : One client hangs when another client loses communication with bricks during intensive write I/O #1628664 : Update op-version from 4.2 to 5.0 #1629561 : geo-rep: geo-rep config set fails to set rsync-options #1630368 : Low Random write IOPS in VM workloads #1630798 : Add performance options to virt profile #1630804 : libgfapi-python: test_listdir_with_stat and test_scandir failure on release 5 branch #1630922 : glusterd crashed and core generated at gd_mgmt_v3_unlock_timer_cbk after huge number of volumes were created #1631128 : rpc marks brick disconnected from glusterd & volume stop transaction gets timed out #1631357 : glusterfsd keeping fd open in index xlator after stop the volume #1631886 : Update database profile settings for gluster #1632161 : [Disperse] : Set others.eager-lock on for ec-1468261.t test to pass #1632236 : Provide indication at the console or in the logs about the progress being made with changelog processing. #1632503 : FUSE client segfaults when performance.md-cache-statfs is enabled for a volume #1632717 : EC crashes when running on non 64-bit architectures #1632889 : 'df' shows half as much space on volume after upgrade to RHGS 3.4 #1633926 : Script to collect system-stats #1634102 : MAINTAINERS: Add sunny kumar as a peer for snapshot component #1634220 : md-cache: some problems of cache virtual glusterfs ACLs for ganesha #1635050 : [SNAPSHOT]: with brick multiplexing, snapshot restore will make glusterd send wrong volfile #1635145 : I/O errors observed on the application side after the creation of a 'linkto' file #1635480 : Correction for glusterd memory leak because use \"gluster volume status volume_name --detail\" continuesly (cli) #1635593 : glusterd crashed in cleanup_and_exit when glusterd comes up with upgrade mode. #1635688 : Keep only the valid (maintained/supported) components in the build #1635820 : Seeing defunt translator and discrepancy in volume info when issued from node which doesn't host bricks in that volume #1635863 : Gluster peer probe doesn't work for IPv6 #1636570 : Cores due to SIGILL during multiplex regression tests #1636631 : Issuing a \"heal ... full\" on a disperse volume causes permanent high CPU utilization. #1637196 : Disperse volume 'df' usage is extremely incorrect after replace-brick. #1637249 : gfid heal does not happen when there is no source brick #1637802 : data-self-heal in arbiter volume results in stale locks. #1637934 : glusterfsd is keeping fd open in index xlator #1638453 : Gfid mismatch seen on shards when lookup and mknod are in progress at the same time #1639599 : Improve support-ability of glusterfs #1640026 : improper checking to avoid identical mounts #1640066 : [Stress] : Mismatching iatt in glustershd logs during MTSH and continous IO from Ganesha mounts #1640165 : io-stats: garbage characters in the filenames generated #1640489 : Invalid memory read after freed in dht_rmdir_readdirp_cbk #1640495 : [GSS] Fix log level issue with brick mux #1640581 : [AFR] : Start crawling indices and healing only if both data bricks are UP in replica 2 (thin-arbiter) #1641344 : Spurious failures in bug-1637802-arbiter-stale-data-heal-lock.t #1642448 : EC volume getting created without any redundant brick #1642597 : tests/bugs/glusterd/optimized-basic-testcases-in-cluster.t failing #1642800 : socket: log voluntary socket close/shutdown and EOF on socket at INFO log-level #1642807 : remove 'tier' translator from build and code #1642810 : remove glupy from code and build #1642850 : glusterd: raise default transport.listen-backlog to 1024 #1642865 : geo-rep: geo-replication gets stuck after file rename and gfid conflict #1643349 : [OpenSSL] : auth.ssl-allow has no option description. #1643402 : [Geo-Replication] Geo-rep faulty sesion because of the directories are not synced to slave. #1643519 : Provide an option to silence glfsheal logs #1643929 : geo-rep: gluster-mountbroker status crashes #1643932 : geo-rep: On gluster command failure on slave, worker crashes with python3 #1643935 : cliutils: geo-rep cliutils' usage of Popen is not python3 compatible #1644129 : Excessive logging in posix_update_utime_in_mdata #1644164 : Use GF_ATOMIC ops to update inode->nlookup #1644629 : [rpcsvc] Single request Queue for all event threads is a performance bottleneck #1644755 : CVE-2018-14651 glusterfs: glusterfs server exploitable via symlinks to relative paths [fedora-all] #1644756 : CVE-2018-14653 glusterfs: Heap-based buffer overflow via \"gf_getspec_req\" RPC message [fedora-all] #1644757 : CVE-2018-14659 glusterfs: Unlimited file creation via \"GF_XATTR_IOSTATS_DUMP_KEY\" xattr allows for denial of service [fedora-all] #1644758 : CVE-2018-14660 glusterfs: Repeat use of \"GF_META_LOCK_KEY\" xattr allows for memory exhaustion [fedora-all] #1644760 : CVE-2018-14654 glusterfs: \"features/index\" translator can create arbitrary, empty files [fedora-all] #1644763 : CVE-2018-14661 glusterfs: features/locks translator passes an user-controlled string to snprintf without a proper format string resulting in a denial of service [fedora-all] #1645986 : tests/bugs/glusterd/optimized-basic-testcases-in-cluster.t failing in distributed regression #1646104 : [Geo-rep]: Faulty geo-rep sessions due to link ownership on slave volume #1646728 : [snapview-server]:forget glfs handles during inode forget #1646869 : gNFS crashed when processing \"gluster v status [vol] nfs clients\" #1646892 : Portmap entries showing stale brick entries when bricks are down #1647029 : can't enable shared-storage #1647074 : when peer detach is issued, throw a warning to remount volumes using other cluster IPs before proceeding #1647651 : gfapi: fix bad dict setting of lease-id #1648237 : Bumping up of op-version times out on a scaled system with ~1200 volumes #1648298 : dht_revalidate may not heal attrs on the brick root #1648687 : Incorrect usage of local->fd in afr_open_ftruncate_cbk #1648768 : Tracker bug for all leases related issues #1649709 : profile info doesn't work when decompounder xlator is not in graph #1650115 : glusterd requests are timing out in a brick multiplex setup #1650389 : rpc: log flooding with ENODATA errors #1650403 : Memory leaks observed in brick-multiplex scenario on volume start/stop loop #1650893 : fails to sync non-ascii (utf8) file and directory names, causes permanently faulty geo-replication state #1651059 : [OpenSSL] : Retrieving the value of \"client.ssl\" option,before SSL is set up, fails . #1651165 : Race in per-thread mem-pool when a thread is terminated #1651431 : Resolve memory leak at the time of graph init #1651439 : gluster-NFS crash while expanding volume #1651463 : glusterd can't regenerate volfiles in container storage upgrade workflow #1651498 : [geo-rep]: Failover / Failback shows fault status in a non-root setup #1651584 : [geo-rep]: validate the config checkpoint date and fail if it is not is exact format hh:mm:ss #1652118 : default cluster.max-bricks-per-process to 250 #1652430 : glusterd fails to start, when glusterd is restarted in a loop for every 45 seconds while volume creation is in-progress #1652852 : \"gluster volume get\" doesn't show real default value for server.tcp-user-timeout #1652887 : Geo-rep help looks to have a typo. #1652911 : Add no-verify and ssh-port n options for create command in man page #1653277 : bump up default value of server.event-threads #1653359 : Self-heal:Improve heal performance #1653565 : tests/geo-rep: Add arbiter volume test case #1654138 : Optimize for virt store fails with distribute volume type #1654181 : glusterd segmentation fault: glusterd_op_ac_brick_op_failed (event=0x7f44e0e63f40, ctx=0x0) at glusterd-op-sm.c:5606 #1654187 : [geo-rep]: RFE - Make slave volume read-only while setting up geo-rep (by default) #1654270 : glusterd crashed with seg fault possibly during node reboot while volume creates and deletes were happening #1654521 : io-stats outputs json numbers as strings #1654805 : Bitrot: Scrub status say file is corrupted even it was just created AND 'path' in the output is broken #1654917 : cleanup resources in server_init in case of failure #1655050 : automatic split resolution with size as policy should not work on a directory which is in metadata splitbrain #1655052 : Automatic Splitbrain with size as policy must not resolve splitbrains when both the copies are of same size #1655827 : [Glusterd]: Glusterd crash while expanding volumes using heketi #1655854 : Converting distribute to replica-3/arbiter volume fails #1656100 : configure.ac does not enforce automake --foreign #1656264 : Fix tests/bugs/shard/zero-flag.t #1656348 : Commit c9bde3021202f1d5c5a2d19ac05a510fc1f788ac causes ls slowdown #1656517 : [GSS] Gluster client logs filling with 0-glusterfs-socket: invalid port messages #1656682 : brick memory consumed by volume is not getting released even after delete #1656771 : [Samba-Enhancement] Need for a single group command for setting up volume options for samba #1656951 : cluster.max-bricks-per-process 250 not working as expected #1657607 : Convert nr_files to gf_atomic in posix_private structure #1657744 : quorum count not updated in nfs-server vol file #1657783 : Rename of a file leading to stale reads #1658045 : Resolve memory leak in mgmt_pmap_signout_cbk #1658116 : python2 to python3 compatibilty issues #1659327 : 43% regression in small-file sequential read performance #1659432 : Memory leak: dict_t leak in rda_opendir #1659708 : Optimize by not stopping (restart) selfheal deamon (shd) when a volume is stopped unless it is the last volume #1659857 : change max-port value in glusterd vol file to 60999 #1659868 : glusterd : features.selinux was missing in glusterd-volume-set file #1659869 : improvements to io-cache #1659971 : Setting slave volume read-only option by default results in failure #1660577 : [Ganesha] Ganesha failed on one node while exporting volumes in loop #1660701 : Use adaptive mutex in rpcsvc_program_register to improve performance #1661214 : Brick is getting OOM for tests/bugs/core/bug-1432542-mpx-restart-crash.t #1662089 : NL cache: fix typos #1662264 : thin-arbiter: Check with thin-arbiter file before marking new entry change log #1662368 : [ovirt-gluster] Fuse mount crashed while deleting a 1 TB image file from ovirt #1662679 : Log connection_id in statedump for posix-locks as well for better debugging experience #1662906 : Longevity: glusterfsd(brick process) crashed when we do volume creates and deletes #1663077 : memory leak in mgmt handshake #1663102 : Change default value for client side heal to off for replicate volumes #1663223 : profile info command is not displaying information of bricks which are hosted on peers #1663243 : rebalance status does not display localhost statistics when op-version is not bumped up #1664122 : do not send bit-rot virtual xattrs in lookup response #1664124 : Improve information dumped from io-threads in statedump #1664551 : Wrong description of localtime-logging in manpages #1664647 : dht: Add NULL check for stbuf in dht_rmdir_lookup_cbk #1664934 : glusterfs-fuse client not benefiting from page cache on read after write #1665038 : glusterd crashed while running \"gluster get-state glusterd odir /get-state\" #1665332 : Wrong offset is used in offset for zerofill fop #1665358 : allow regression to not run tests with nfs, if nfs is disabled. #1665363 : Fix incorrect definition in index-mem-types.h #1665656 : testcaes glusterd/add-brick-and-validate-replicated-volume-options.t is crash while brick_mux is enable #1665826 : [geo-rep]: Directory renames not synced to slave in Hybrid Crawl #1666143 : Several fixes on socket pollin and pollout return value #1666833 : move few recurring logs to DEBUG level. #1667779 : glusterd leaks about 1GB memory per day on single machine of storage pool #1667804 : Unable to delete directories that contain linkto files that point to itself. #1667905 : dict_leak in __glusterd_handle_cli_uuid_get function #1668190 : Block hosting volume deletion via heketi-cli failed with error \"target is busy\" but deleted from gluster backend #1668268 : Unable to mount gluster volume #1669077 : [ovirt-gluster] Fuse mount crashed while creating the preallocated image #1669937 : Rebalance : While rebalance is in progress , SGID and sticky bit which is set on the files while file migration is in progress is seen on the mount point #1670031 : performance regression seen with smallfile workload tests #1670253 : Writes on Gluster 5 volumes fail with EIO when \"cluster.consistent-metadata\" is set #1670259 : New GFID file recreated in a replica set after a GFID mismatch resolution #1671213 : core: move \"dict is NULL\" logs to DEBUG log level #1671637 : geo-rep: Issue with configparser import #1672205 : 'gluster get-state' command fails if volume brick doesn't exist. #1672818 : GlusterFS 6.0 tracker #1673267 : Fix timeouts so the tests pass on AWS #1673972 : insufficient logging in glusterd_resolve_all_bricks #1674364 : glusterfs-fuse client not benefiting from page cache on read after write #1676429 : distribute: Perf regression in mkdir path #1677260 : rm -rf fails with \"Directory not empty\" #1678570 : glusterfs FUSE client crashing every few days with 'Failed to dispatch handler' #1679004 : With parallel-readdir enabled, deleting a directory containing stale linkto files fails with \"Directory not empty\" #1679275 : dht: fix double extra unref of inode at heal path #1679965 : Upgrade from glusterfs 3.12 to gluster 4/5 broken #1679998 : GlusterFS can be improved #1680020 : Integer Overflow possible in md-cache.c due to data type inconsistency #1680585 : remove glupy from code and build #1680586 : Building RPM packages with _for_fedora_koji_builds enabled fails on el6 #1683008 : glustereventsd does not start on Ubuntu 16.04 LTS #1683506 : remove experimental xlators informations from glusterd-volume-set.c #1683716 : glusterfind: revert shebangs to #!/usr/bin/python3 #1683880 : Multiple shd processes are running on brick_mux environmet #1683900 : Failed to dispatch handler #1684029 : upgrade from 3.12, 4.1 and 5 to 6 broken #1684777 : gNFS crashed when processing \"gluster v profile [vol] info nfs\" #1685771 : glusterd memory usage grows at 98 MB/h while being monitored by RHGSWA #1686364 : [ovirt-gluster] Rolling gluster upgrade from 3.12.5 to 5.3 led to shard on-disk xattrs disappearing #1686399 : listing a file while writing to it causes deadlock #1686875 : packaging: rdma on s390x, unnecessary ldconfig scriptlets #1687248 : Error handling in /usr/sbin/gluster-eventsapi produces IndexError: tuple index out of range #1687672 : [geo-rep]: Checksum mismatch when 2x2 vols are converted to arbiter #1688218 : Brick process has coredumped, when starting glusterd","title":6.0},{"location":"release-notes/6.0/#release-notes-for-gluster-60","text":"This is a major release that includes a range of code improvements and stability fixes along with a few features as noted below. A selection of the key features and changes are documented in this page. A full list of bugs that have been addressed is included further below. Announcements Major changes and features Major issues Bugs addressed in the release","title":"Release notes for Gluster 6.0"},{"location":"release-notes/6.0/#announcements","text":"Releases that receive maintenance updates post release 6 are, 4.1 and 5 ( reference ) Release 6 will receive maintenance updates around the 10th of every month for the first 3 months post release (i.e Apr'19, May'19, Jun'19). Post the initial 3 months, it will receive maintenance updates every 2 months till EOL. ( reference ) A series of features/xlators have been deprecated in release 6 as follows, for upgrade procedures from volumes that use these features to release 6 refer to the release 6 upgrade guide . This deprecation was announced at the gluster-users list here . Features deprecated: Block device (bd) xlator Decompounder feature Crypt xlator Symlink-cache xlator Stripe feature Tiering support (tier xlator and changetimerecorder)","title":"Announcements"},{"location":"release-notes/6.0/#major-changes-and-features","text":"","title":"Major changes and features"},{"location":"release-notes/6.0/#highlights","text":"Several stability fixes addressing, coverity, clang-scan, address sanitizer and valgrind reported issues removal of unused and hence, deprecated code and features Client side inode garbage collection This release addresses one of the major concerns regarding FUSE mount process memory footprint, by introducing client side inode garbage collection See standalone section for more details Performance Improvements --auto-invalidation on FUSE mounts to leverage kernel page cache more effectively Features are categorized into the following sections, Management Standalone Developer","title":"Highlights"},{"location":"release-notes/6.0/#management","text":"NOTE: There have been several stability improvements around the brick multiplexing feature","title":"Management"},{"location":"release-notes/6.0/#glusterd2","text":"GlusterD2 (or GD2, in short) was planned as the next generation management service for Gluster project. Currently, GD2s main focus is not replacing glusterd , but to serve as a thin management layer when using gluster with container orchestration systems. There is no specific update around GD2 provided as a part of this release.","title":"GlusterD2"},{"location":"release-notes/6.0/#standalone","text":"","title":"Standalone"},{"location":"release-notes/6.0/#1-client-side-inode-garbage-collection-via-lru-list","text":"A FUSE mount's inode cache can now be limited to a maximum number, thus reducing the memory footprint of FUSE mount processes. See the lru-limit option in man 8 mount.glusterfs for details. NOTE: Setting this to a low value (say less than 4000), will evict inodes from FUSE and Gluster caches at a much faster rate, and can cause performance degrades. The setting has to be determined based on the available client memory and required performance.","title":"1. client-side inode garbage collection via LRU list"},{"location":"release-notes/6.0/#2-glusterfind-tool-enhanced-with-a-filter-option","text":"glusterfind tool has an added option \"--type\", to be used with the \"--full\" option. The option supports finding and listing files or directories only, and defaults to both if not specified. Example usage with the pre and query commands are given below, Pre command ( reference ): Lists both files and directories in OUTFILE: glusterfind pre SESSION_NAME VOLUME_NAME OUTFILE Lists only files in OUTFILE: glusterfind pre SESSION_NAME VOLUME_NAME OUTFILE --type f Lists only directories in OUTFILE: glusterfind pre SESSION_NAME VOLUME_NAME OUTFILE --type d Query command: Lists both files and directories in OUTFILE: glusterfind query VOLUME_NAME --full OUTFILE Lists only files in OUTFILE: glusterfind query VOLUME_NAME --full --type f OUTFILE Lists only directories in OUTFILE: glusterfind query VOLUME_NAME --full --type d OUTFILE","title":"2. Glusterfind tool enhanced with a filter option"},{"location":"release-notes/6.0/#3-fuse-mounts-are-enhanced-to-handle-interrupts-to-blocked-lock-requests","text":"FUSE mounts are enhanced to handle interrupts to blocked locks. For example, scripts using the flock ( man 1 flock ) utility without the -n(nonblock) option against files on a FUSE based gluster mount, can now be interrupted when the lock is not granted in time or using the -w option with the same utility.","title":"3. FUSE mounts are enhanced to handle interrupts to blocked lock requests"},{"location":"release-notes/6.0/#4-optimizedpass-through-distribute-functionality-for-1-way-distributed-volumes","text":"NOTE: There are no user controllable changes with this feature The distribute xlator now skips unnecessary checks and operations when the distribute count is one for a volume, resulting in improved performance.","title":"4. Optimized/pass-through distribute functionality for 1-way distributed volumes"},{"location":"release-notes/6.0/#5-options-introduced-to-disable-invalidations-of-kernel-page-cache","text":"For workloads, where multiple FUSE client mounts do not concurrently operate on any files in the volume, it is now possible to maintain a longer duration kernel page cache using the following options in conjunction, Setting --auto-invalidation option to \"no\" on the glusterfs FUSE mount process Disabling the volume option performance.global-cache-invalidation This enables better performance as the data is served from the kernel page cache where possible.","title":"5. Options introduced to disable invalidations of kernel page cache"},{"location":"release-notes/6.0/#6-changes-to-gluster-based-smb-share-management","text":"Previously all GlusterFS volumes were being exported by default via smb.conf in a Samba-CTDB setup. This includes creating a share section for CTDB lock volume too which is not recommended. Along with few syntactical errors these scripts failed to execute in a non-Samba setup in the absence of necessary configuration and binary files. Hereafter newly created GlusterFS volumes are not exported as SMB share via Samba unless either of 'user.cifs' or 'user.smb' volume set options are enabled on the volume. The existing GlusterFS volume share sections in smb.conf will remain unchanged.","title":"6. Changes to gluster based SMB share management"},{"location":"release-notes/6.0/#7-ctime-feature-is-enabled-by-default","text":"The ctime feature which maintains (c/m) time consistency across replica and disperse subvolumes is enabled by default. Also, with this release, a single option is provided to enable/disable ctime feature, #gluster vol set <volname> ctime <on/off> NOTE: The time information used is from clients, hence it's required that clients are synced with respect to their times, using NTP or other such means. Limitations: - Mounting gluster volume with time attribute options (noatime, realatime...) is not supported with this feature - This feature does not guarantee consistent time for directories if the hashed sub-volume for the directory is down - Directory listing is not supported with this feature, and may report inconsistent time information - Older files created before upgrade, would witness update of ctime upon accessing after upgrade BUG:1593542","title":"7. ctime feature is enabled by default"},{"location":"release-notes/6.0/#developer","text":"","title":"Developer"},{"location":"release-notes/6.0/#1-gluster-code-can-be-compiled-and-executed-using-tsan","text":"While configuring the sources for a build use the extra option --enable-tsan to enable thread sanitizer based builds.","title":"1. Gluster code can be compiled and executed using TSAN"},{"location":"release-notes/6.0/#2-gfapi-a-class-of-apis-have-been-enhanced-to-return-prepost-gluster_stat-information","text":"A set of apis have been enhanced to return pre/post gluster_stat information. Applications using gfapi would need to adapt to the newer interfaces to compile against release-6 apis. Pre-compiled applications, or applications using the older API SDK will continue to work as before.","title":"2. gfapi: A class of APIs have been enhanced to return pre/post gluster_stat information"},{"location":"release-notes/6.0/#major-issues","text":"None","title":"Major issues"},{"location":"release-notes/6.0/#bugs-addressed","text":"Bugs addressed since release-5 are listed below. #1138841 : allow the use of the CIDR format with auth.allow #1236272 : socket: Use newer system calls that provide better interface/performance on Linux/*BSD when available #1243991 : \"gluster volume set group \" is not in the help text #1285126 : RFE: GlusterFS NFS does not implement an all_squash volume setting #1343926 : port-map: let brick choose its own port #1364707 : Remove deprecated stripe xlator #1427397 : script to strace processes consuming high CPU #1467614 : Gluster read/write performance improvements on NVMe backend #1486532 : need a script to resolve backtraces #1511339 : In Replica volume 2*2 when quorum is set, after glusterd restart nfs server is coming up instead of self-heal daemon #1535495 : Add option -h and --help to gluster cli #1535528 : Gluster cli show no help message in prompt #1560561 : systemd service file enhancements #1560969 : Garbage collect inactive inodes in fuse-bridge #1564149 : Agree upon a coding standard, and automate check for this in smoke #1564890 : mount.glusterfs: can't shift that many #1575836 : logic in S30samba-start.sh hook script needs tweaking #1579788 : Thin-arbiter: Have the state of volume in memory #1582516 : libgfapi: glfs init fails on afr volume with ctime feature enabled #1590385 : Refactor dht lookup code #1593538 : ctime: Access time is different with in same replica/EC volume #1596787 : glusterfs rpc-clnt.c: error returned while attempting to connect to host: (null), port 0 #1598345 : gluster get-state command is crashing glusterd process when geo-replication is configured #1600145 : [geo-rep]: Worker still ACTIVE after killing bricks #1605056 : [RHHi] Mount hung and not accessible #1605077 : If a node disconnects during volume delete, it assumes deleted volume as a freshly created volume when it is back online #1608512 : cluster.server-quorum-type help text is missing possible settings #1624006 : /var/run/gluster/metrics/ wasn't created automatically #1624332 : [Thin-arbiter]: Add tests for thin arbiter feature #1624724 : ctime: Enable ctime feature by default and also improve usability by providing single option to enable #1624796 : mkdir -p fails with \"No data available\" when root-squash is enabled #1625850 : tests: fixes to bug-1015990-rep.t #1625961 : Writes taking very long time leading to system hogging #1626313 : fix glfs_fini related problems #1626610 : [USS]: Change gf_log to gf_msg #1626994 : split-brain observed on parent dir #1627610 : glusterd crash in regression build #1627620 : SAS job aborts complaining about file doesn't exist #1628194 : tests/dht: Additional tests for dht operations #1628605 : One client hangs when another client loses communication with bricks during intensive write I/O #1628664 : Update op-version from 4.2 to 5.0 #1629561 : geo-rep: geo-rep config set fails to set rsync-options #1630368 : Low Random write IOPS in VM workloads #1630798 : Add performance options to virt profile #1630804 : libgfapi-python: test_listdir_with_stat and test_scandir failure on release 5 branch #1630922 : glusterd crashed and core generated at gd_mgmt_v3_unlock_timer_cbk after huge number of volumes were created #1631128 : rpc marks brick disconnected from glusterd & volume stop transaction gets timed out #1631357 : glusterfsd keeping fd open in index xlator after stop the volume #1631886 : Update database profile settings for gluster #1632161 : [Disperse] : Set others.eager-lock on for ec-1468261.t test to pass #1632236 : Provide indication at the console or in the logs about the progress being made with changelog processing. #1632503 : FUSE client segfaults when performance.md-cache-statfs is enabled for a volume #1632717 : EC crashes when running on non 64-bit architectures #1632889 : 'df' shows half as much space on volume after upgrade to RHGS 3.4 #1633926 : Script to collect system-stats #1634102 : MAINTAINERS: Add sunny kumar as a peer for snapshot component #1634220 : md-cache: some problems of cache virtual glusterfs ACLs for ganesha #1635050 : [SNAPSHOT]: with brick multiplexing, snapshot restore will make glusterd send wrong volfile #1635145 : I/O errors observed on the application side after the creation of a 'linkto' file #1635480 : Correction for glusterd memory leak because use \"gluster volume status volume_name --detail\" continuesly (cli) #1635593 : glusterd crashed in cleanup_and_exit when glusterd comes up with upgrade mode. #1635688 : Keep only the valid (maintained/supported) components in the build #1635820 : Seeing defunt translator and discrepancy in volume info when issued from node which doesn't host bricks in that volume #1635863 : Gluster peer probe doesn't work for IPv6 #1636570 : Cores due to SIGILL during multiplex regression tests #1636631 : Issuing a \"heal ... full\" on a disperse volume causes permanent high CPU utilization. #1637196 : Disperse volume 'df' usage is extremely incorrect after replace-brick. #1637249 : gfid heal does not happen when there is no source brick #1637802 : data-self-heal in arbiter volume results in stale locks. #1637934 : glusterfsd is keeping fd open in index xlator #1638453 : Gfid mismatch seen on shards when lookup and mknod are in progress at the same time #1639599 : Improve support-ability of glusterfs #1640026 : improper checking to avoid identical mounts #1640066 : [Stress] : Mismatching iatt in glustershd logs during MTSH and continous IO from Ganesha mounts #1640165 : io-stats: garbage characters in the filenames generated #1640489 : Invalid memory read after freed in dht_rmdir_readdirp_cbk #1640495 : [GSS] Fix log level issue with brick mux #1640581 : [AFR] : Start crawling indices and healing only if both data bricks are UP in replica 2 (thin-arbiter) #1641344 : Spurious failures in bug-1637802-arbiter-stale-data-heal-lock.t #1642448 : EC volume getting created without any redundant brick #1642597 : tests/bugs/glusterd/optimized-basic-testcases-in-cluster.t failing #1642800 : socket: log voluntary socket close/shutdown and EOF on socket at INFO log-level #1642807 : remove 'tier' translator from build and code #1642810 : remove glupy from code and build #1642850 : glusterd: raise default transport.listen-backlog to 1024 #1642865 : geo-rep: geo-replication gets stuck after file rename and gfid conflict #1643349 : [OpenSSL] : auth.ssl-allow has no option description. #1643402 : [Geo-Replication] Geo-rep faulty sesion because of the directories are not synced to slave. #1643519 : Provide an option to silence glfsheal logs #1643929 : geo-rep: gluster-mountbroker status crashes #1643932 : geo-rep: On gluster command failure on slave, worker crashes with python3 #1643935 : cliutils: geo-rep cliutils' usage of Popen is not python3 compatible #1644129 : Excessive logging in posix_update_utime_in_mdata #1644164 : Use GF_ATOMIC ops to update inode->nlookup #1644629 : [rpcsvc] Single request Queue for all event threads is a performance bottleneck #1644755 : CVE-2018-14651 glusterfs: glusterfs server exploitable via symlinks to relative paths [fedora-all] #1644756 : CVE-2018-14653 glusterfs: Heap-based buffer overflow via \"gf_getspec_req\" RPC message [fedora-all] #1644757 : CVE-2018-14659 glusterfs: Unlimited file creation via \"GF_XATTR_IOSTATS_DUMP_KEY\" xattr allows for denial of service [fedora-all] #1644758 : CVE-2018-14660 glusterfs: Repeat use of \"GF_META_LOCK_KEY\" xattr allows for memory exhaustion [fedora-all] #1644760 : CVE-2018-14654 glusterfs: \"features/index\" translator can create arbitrary, empty files [fedora-all] #1644763 : CVE-2018-14661 glusterfs: features/locks translator passes an user-controlled string to snprintf without a proper format string resulting in a denial of service [fedora-all] #1645986 : tests/bugs/glusterd/optimized-basic-testcases-in-cluster.t failing in distributed regression #1646104 : [Geo-rep]: Faulty geo-rep sessions due to link ownership on slave volume #1646728 : [snapview-server]:forget glfs handles during inode forget #1646869 : gNFS crashed when processing \"gluster v status [vol] nfs clients\" #1646892 : Portmap entries showing stale brick entries when bricks are down #1647029 : can't enable shared-storage #1647074 : when peer detach is issued, throw a warning to remount volumes using other cluster IPs before proceeding #1647651 : gfapi: fix bad dict setting of lease-id #1648237 : Bumping up of op-version times out on a scaled system with ~1200 volumes #1648298 : dht_revalidate may not heal attrs on the brick root #1648687 : Incorrect usage of local->fd in afr_open_ftruncate_cbk #1648768 : Tracker bug for all leases related issues #1649709 : profile info doesn't work when decompounder xlator is not in graph #1650115 : glusterd requests are timing out in a brick multiplex setup #1650389 : rpc: log flooding with ENODATA errors #1650403 : Memory leaks observed in brick-multiplex scenario on volume start/stop loop #1650893 : fails to sync non-ascii (utf8) file and directory names, causes permanently faulty geo-replication state #1651059 : [OpenSSL] : Retrieving the value of \"client.ssl\" option,before SSL is set up, fails . #1651165 : Race in per-thread mem-pool when a thread is terminated #1651431 : Resolve memory leak at the time of graph init #1651439 : gluster-NFS crash while expanding volume #1651463 : glusterd can't regenerate volfiles in container storage upgrade workflow #1651498 : [geo-rep]: Failover / Failback shows fault status in a non-root setup #1651584 : [geo-rep]: validate the config checkpoint date and fail if it is not is exact format hh:mm:ss #1652118 : default cluster.max-bricks-per-process to 250 #1652430 : glusterd fails to start, when glusterd is restarted in a loop for every 45 seconds while volume creation is in-progress #1652852 : \"gluster volume get\" doesn't show real default value for server.tcp-user-timeout #1652887 : Geo-rep help looks to have a typo. #1652911 : Add no-verify and ssh-port n options for create command in man page #1653277 : bump up default value of server.event-threads #1653359 : Self-heal:Improve heal performance #1653565 : tests/geo-rep: Add arbiter volume test case #1654138 : Optimize for virt store fails with distribute volume type #1654181 : glusterd segmentation fault: glusterd_op_ac_brick_op_failed (event=0x7f44e0e63f40, ctx=0x0) at glusterd-op-sm.c:5606 #1654187 : [geo-rep]: RFE - Make slave volume read-only while setting up geo-rep (by default) #1654270 : glusterd crashed with seg fault possibly during node reboot while volume creates and deletes were happening #1654521 : io-stats outputs json numbers as strings #1654805 : Bitrot: Scrub status say file is corrupted even it was just created AND 'path' in the output is broken #1654917 : cleanup resources in server_init in case of failure #1655050 : automatic split resolution with size as policy should not work on a directory which is in metadata splitbrain #1655052 : Automatic Splitbrain with size as policy must not resolve splitbrains when both the copies are of same size #1655827 : [Glusterd]: Glusterd crash while expanding volumes using heketi #1655854 : Converting distribute to replica-3/arbiter volume fails #1656100 : configure.ac does not enforce automake --foreign #1656264 : Fix tests/bugs/shard/zero-flag.t #1656348 : Commit c9bde3021202f1d5c5a2d19ac05a510fc1f788ac causes ls slowdown #1656517 : [GSS] Gluster client logs filling with 0-glusterfs-socket: invalid port messages #1656682 : brick memory consumed by volume is not getting released even after delete #1656771 : [Samba-Enhancement] Need for a single group command for setting up volume options for samba #1656951 : cluster.max-bricks-per-process 250 not working as expected #1657607 : Convert nr_files to gf_atomic in posix_private structure #1657744 : quorum count not updated in nfs-server vol file #1657783 : Rename of a file leading to stale reads #1658045 : Resolve memory leak in mgmt_pmap_signout_cbk #1658116 : python2 to python3 compatibilty issues #1659327 : 43% regression in small-file sequential read performance #1659432 : Memory leak: dict_t leak in rda_opendir #1659708 : Optimize by not stopping (restart) selfheal deamon (shd) when a volume is stopped unless it is the last volume #1659857 : change max-port value in glusterd vol file to 60999 #1659868 : glusterd : features.selinux was missing in glusterd-volume-set file #1659869 : improvements to io-cache #1659971 : Setting slave volume read-only option by default results in failure #1660577 : [Ganesha] Ganesha failed on one node while exporting volumes in loop #1660701 : Use adaptive mutex in rpcsvc_program_register to improve performance #1661214 : Brick is getting OOM for tests/bugs/core/bug-1432542-mpx-restart-crash.t #1662089 : NL cache: fix typos #1662264 : thin-arbiter: Check with thin-arbiter file before marking new entry change log #1662368 : [ovirt-gluster] Fuse mount crashed while deleting a 1 TB image file from ovirt #1662679 : Log connection_id in statedump for posix-locks as well for better debugging experience #1662906 : Longevity: glusterfsd(brick process) crashed when we do volume creates and deletes #1663077 : memory leak in mgmt handshake #1663102 : Change default value for client side heal to off for replicate volumes #1663223 : profile info command is not displaying information of bricks which are hosted on peers #1663243 : rebalance status does not display localhost statistics when op-version is not bumped up #1664122 : do not send bit-rot virtual xattrs in lookup response #1664124 : Improve information dumped from io-threads in statedump #1664551 : Wrong description of localtime-logging in manpages #1664647 : dht: Add NULL check for stbuf in dht_rmdir_lookup_cbk #1664934 : glusterfs-fuse client not benefiting from page cache on read after write #1665038 : glusterd crashed while running \"gluster get-state glusterd odir /get-state\" #1665332 : Wrong offset is used in offset for zerofill fop #1665358 : allow regression to not run tests with nfs, if nfs is disabled. #1665363 : Fix incorrect definition in index-mem-types.h #1665656 : testcaes glusterd/add-brick-and-validate-replicated-volume-options.t is crash while brick_mux is enable #1665826 : [geo-rep]: Directory renames not synced to slave in Hybrid Crawl #1666143 : Several fixes on socket pollin and pollout return value #1666833 : move few recurring logs to DEBUG level. #1667779 : glusterd leaks about 1GB memory per day on single machine of storage pool #1667804 : Unable to delete directories that contain linkto files that point to itself. #1667905 : dict_leak in __glusterd_handle_cli_uuid_get function #1668190 : Block hosting volume deletion via heketi-cli failed with error \"target is busy\" but deleted from gluster backend #1668268 : Unable to mount gluster volume #1669077 : [ovirt-gluster] Fuse mount crashed while creating the preallocated image #1669937 : Rebalance : While rebalance is in progress , SGID and sticky bit which is set on the files while file migration is in progress is seen on the mount point #1670031 : performance regression seen with smallfile workload tests #1670253 : Writes on Gluster 5 volumes fail with EIO when \"cluster.consistent-metadata\" is set #1670259 : New GFID file recreated in a replica set after a GFID mismatch resolution #1671213 : core: move \"dict is NULL\" logs to DEBUG log level #1671637 : geo-rep: Issue with configparser import #1672205 : 'gluster get-state' command fails if volume brick doesn't exist. #1672818 : GlusterFS 6.0 tracker #1673267 : Fix timeouts so the tests pass on AWS #1673972 : insufficient logging in glusterd_resolve_all_bricks #1674364 : glusterfs-fuse client not benefiting from page cache on read after write #1676429 : distribute: Perf regression in mkdir path #1677260 : rm -rf fails with \"Directory not empty\" #1678570 : glusterfs FUSE client crashing every few days with 'Failed to dispatch handler' #1679004 : With parallel-readdir enabled, deleting a directory containing stale linkto files fails with \"Directory not empty\" #1679275 : dht: fix double extra unref of inode at heal path #1679965 : Upgrade from glusterfs 3.12 to gluster 4/5 broken #1679998 : GlusterFS can be improved #1680020 : Integer Overflow possible in md-cache.c due to data type inconsistency #1680585 : remove glupy from code and build #1680586 : Building RPM packages with _for_fedora_koji_builds enabled fails on el6 #1683008 : glustereventsd does not start on Ubuntu 16.04 LTS #1683506 : remove experimental xlators informations from glusterd-volume-set.c #1683716 : glusterfind: revert shebangs to #!/usr/bin/python3 #1683880 : Multiple shd processes are running on brick_mux environmet #1683900 : Failed to dispatch handler #1684029 : upgrade from 3.12, 4.1 and 5 to 6 broken #1684777 : gNFS crashed when processing \"gluster v profile [vol] info nfs\" #1685771 : glusterd memory usage grows at 98 MB/h while being monitored by RHGSWA #1686364 : [ovirt-gluster] Rolling gluster upgrade from 3.12.5 to 5.3 led to shard on-disk xattrs disappearing #1686399 : listing a file while writing to it causes deadlock #1686875 : packaging: rdma on s390x, unnecessary ldconfig scriptlets #1687248 : Error handling in /usr/sbin/gluster-eventsapi produces IndexError: tuple index out of range #1687672 : [geo-rep]: Checksum mismatch when 2x2 vols are converted to arbiter #1688218 : Brick process has coredumped, when starting glusterd","title":"Bugs addressed"},{"location":"release-notes/6.1/","text":"Release notes for Gluster 6.1 This is a bugfix release. The release notes for 6.0 contains a listing of all the new features that were added and bugs fixed in the GlusterFS 6 stable release. NOTE: Next minor release tentative date: Week of 10th May, 2019 Major changes, features and limitations addressed in this release Major issues None Bugs addressed Bugs addressed since release-6.0 are listed below. #1679904 : client log flooding with intentional socket shutdown message when a brick is down #1690950 : lots of \"Matching lock not found for unlock xxx\" when using disperse (ec) xlator #1691187 : fix Coverity CID 1399758 #1692101 : Network throughput usage increased x5 #1692957 : rpclib: slow floating point math and libm #1693155 : Excessive AFR messages from gluster showing in RHGSWA. #1693223 : [Disperse] : Client side heal is not removing dirty flag for some of the files. #1693992 : Thin-arbiter minor fixes #1694002 : Geo-re: Geo replication failing in \"cannot allocate memory\" #1694561 : gfapi: do not block epoll thread for upcall notifications #1694610 : glusterd leaking memory when issued gluster vol status all tasks continuosly #1695436 : geo-rep session creation fails with IPV6 #1695445 : ssh-port config set is failing #1697764 : [cluster/ec] : Fix handling of heal info cases without locks #1698471 : ctime feature breaks old client to connect to new server #1699198 : Glusterfs create a flock lock by anonymous fd, but can't release it forever. #1699319 : Thin-Arbiter SHD minor fixes #1699499 : fix truncate lock to cover the write in tuncate clean #1699703 : ctime: Creation of tar file on gluster mount throws warning \"file changed as we read it\" #1699713 : glusterfs build is failing on rhel-6 #1699714 : Brick is not able to detach successfully in brick_mux environment #1699715 : Log level changes do not take effect until the process is restarted #1699731 : Fops hang when inodelk fails on the first fop","title":6.1},{"location":"release-notes/6.1/#release-notes-for-gluster-61","text":"This is a bugfix release. The release notes for 6.0 contains a listing of all the new features that were added and bugs fixed in the GlusterFS 6 stable release. NOTE: Next minor release tentative date: Week of 10th May, 2019","title":"Release notes for Gluster 6.1"},{"location":"release-notes/6.1/#major-changes-features-and-limitations-addressed-in-this-release","text":"","title":"Major changes, features and limitations addressed in this release"},{"location":"release-notes/6.1/#major-issues","text":"None","title":"Major issues"},{"location":"release-notes/6.1/#bugs-addressed","text":"Bugs addressed since release-6.0 are listed below. #1679904 : client log flooding with intentional socket shutdown message when a brick is down #1690950 : lots of \"Matching lock not found for unlock xxx\" when using disperse (ec) xlator #1691187 : fix Coverity CID 1399758 #1692101 : Network throughput usage increased x5 #1692957 : rpclib: slow floating point math and libm #1693155 : Excessive AFR messages from gluster showing in RHGSWA. #1693223 : [Disperse] : Client side heal is not removing dirty flag for some of the files. #1693992 : Thin-arbiter minor fixes #1694002 : Geo-re: Geo replication failing in \"cannot allocate memory\" #1694561 : gfapi: do not block epoll thread for upcall notifications #1694610 : glusterd leaking memory when issued gluster vol status all tasks continuosly #1695436 : geo-rep session creation fails with IPV6 #1695445 : ssh-port config set is failing #1697764 : [cluster/ec] : Fix handling of heal info cases without locks #1698471 : ctime feature breaks old client to connect to new server #1699198 : Glusterfs create a flock lock by anonymous fd, but can't release it forever. #1699319 : Thin-Arbiter SHD minor fixes #1699499 : fix truncate lock to cover the write in tuncate clean #1699703 : ctime: Creation of tar file on gluster mount throws warning \"file changed as we read it\" #1699713 : glusterfs build is failing on rhel-6 #1699714 : Brick is not able to detach successfully in brick_mux environment #1699715 : Log level changes do not take effect until the process is restarted #1699731 : Fops hang when inodelk fails on the first fop","title":"Bugs addressed"},{"location":"release-notes/6.10/","text":"Release notes for Gluster 6.10 This is a bugfix release. The release notes for 6.0 , 6.1 , 6.2 , 6.3 , 6.4 , 6.5 , 6.6 , 6.7 , 6.8 and 6.9 contain a listing of all the new features that were added and bugs fixed in the GlusterFS 6 stable release. NOTE: This is last minor release of 6. Users are highly encouraged to upgrade to newer releases of GlusterFS. Bugs addressed Bugs addressed since release-6.9 are listed below. #1740494 : Fencing: Added the tcmu-runner ALUA feature support but after one of node is rebooted the glfs_file_lock() get stucked #1000 [bug:1193929] GlusterFS can be improved #1016 [bug:1795609] glusterfsd memory leak observed after enable tls #1060 [bug:789278] Issues reported by Coverity static analysis tool #1127 Mount crash during background shard cleanup #1179 gnfs split brain when 1 server in 3x1 down (high load) #1220 cluster/ec: return correct error code and log the message in ... #1223 Failure of tests/basic/gfapi/gfapi-copy-file-range.t #1254 Prioritize ENOSPC over other lesser priority errors #1303 Failures in rebalance due to [Input/output error] #1307 Spurious failure of tests/bug-844688.t: test bug-844688.t on ... #1349 Issue for backporting https://review.gluster.org//c/glusterf... #1362 [bug: 1687326]: Revoke access from nodes using Certificate Re...","title":"6.10."},{"location":"release-notes/6.10/#release-notes-for-gluster-610","text":"This is a bugfix release. The release notes for 6.0 , 6.1 , 6.2 , 6.3 , 6.4 , 6.5 , 6.6 , 6.7 , 6.8 and 6.9 contain a listing of all the new features that were added and bugs fixed in the GlusterFS 6 stable release. NOTE: This is last minor release of 6. Users are highly encouraged to upgrade to newer releases of GlusterFS.","title":"Release notes for Gluster 6.10"},{"location":"release-notes/6.10/#bugs-addressed","text":"Bugs addressed since release-6.9 are listed below. #1740494 : Fencing: Added the tcmu-runner ALUA feature support but after one of node is rebooted the glfs_file_lock() get stucked #1000 [bug:1193929] GlusterFS can be improved #1016 [bug:1795609] glusterfsd memory leak observed after enable tls #1060 [bug:789278] Issues reported by Coverity static analysis tool #1127 Mount crash during background shard cleanup #1179 gnfs split brain when 1 server in 3x1 down (high load) #1220 cluster/ec: return correct error code and log the message in ... #1223 Failure of tests/basic/gfapi/gfapi-copy-file-range.t #1254 Prioritize ENOSPC over other lesser priority errors #1303 Failures in rebalance due to [Input/output error] #1307 Spurious failure of tests/bug-844688.t: test bug-844688.t on ... #1349 Issue for backporting https://review.gluster.org//c/glusterf... #1362 [bug: 1687326]: Revoke access from nodes using Certificate Re...","title":"Bugs addressed"},{"location":"release-notes/6.2/","text":"Release notes for Gluster 6.2 This is a bugfix release. The release notes for 6.0 and 6.1 contains a listing of all the new features that were added and bugs fixed in the GlusterFS 6 stable release. NOTE: Next minor release tentative date: Week of 10th June, 2019 Major changes, features and limitations addressed in this release None Major issues None Bugs addressed Bugs addressed since release-6.1 are listed below. #1699917 : I/O error on writes to a disperse volume when replace-brick is executed #1701818 : Syntactical errors in hook scripts for managing SELinux context on bricks #2 (S10selinux-label-brick.sh + S10selinux-del-fcontext.sh) #1702271 : Memory accounting information is not always accurate #1702734 : ctime: Logs are flooded with \"posix set mdata failed, No ctime\" error during open #1703759 : statedump is not capturing info related to glusterd #1707393 : Refactor dht lookup code #1709130 : thin-arbiter lock release fixes #1709143 : [Thin-arbiter] : send correct error code in case of failure #1709660 : Glusterfsd crashing in ec-inode-write.c, in GF_ASSERT #1709685 : Geo-rep: Value of pending entry operations in detail status output is going up after each synchronization. #1709734 : Geo-rep: Data inconsistency while syncing heavy renames with constant destination name #1709737 : geo-rep: Always uses rsync even with use_tarssh set to true #1709738 : geo-rep: Sync hangs with tarssh as sync-engine #1712220 : tests/geo-rep: arequal checksum comparison always succeeds #1712223 : geo-rep: With heavy rename workload geo-rep log if flooded","title":6.2},{"location":"release-notes/6.2/#release-notes-for-gluster-62","text":"This is a bugfix release. The release notes for 6.0 and 6.1 contains a listing of all the new features that were added and bugs fixed in the GlusterFS 6 stable release. NOTE: Next minor release tentative date: Week of 10th June, 2019","title":"Release notes for Gluster 6.2"},{"location":"release-notes/6.2/#major-changes-features-and-limitations-addressed-in-this-release","text":"None","title":"Major changes, features and limitations addressed in this release"},{"location":"release-notes/6.2/#major-issues","text":"None","title":"Major issues"},{"location":"release-notes/6.2/#bugs-addressed","text":"Bugs addressed since release-6.1 are listed below. #1699917 : I/O error on writes to a disperse volume when replace-brick is executed #1701818 : Syntactical errors in hook scripts for managing SELinux context on bricks #2 (S10selinux-label-brick.sh + S10selinux-del-fcontext.sh) #1702271 : Memory accounting information is not always accurate #1702734 : ctime: Logs are flooded with \"posix set mdata failed, No ctime\" error during open #1703759 : statedump is not capturing info related to glusterd #1707393 : Refactor dht lookup code #1709130 : thin-arbiter lock release fixes #1709143 : [Thin-arbiter] : send correct error code in case of failure #1709660 : Glusterfsd crashing in ec-inode-write.c, in GF_ASSERT #1709685 : Geo-rep: Value of pending entry operations in detail status output is going up after each synchronization. #1709734 : Geo-rep: Data inconsistency while syncing heavy renames with constant destination name #1709737 : geo-rep: Always uses rsync even with use_tarssh set to true #1709738 : geo-rep: Sync hangs with tarssh as sync-engine #1712220 : tests/geo-rep: arequal checksum comparison always succeeds #1712223 : geo-rep: With heavy rename workload geo-rep log if flooded","title":"Bugs addressed"},{"location":"release-notes/6.3/","text":"Release notes for Gluster 6.3 This is a bugfix release. The release notes for 6.0 , 6.1 and 6.2 contains a listing of all the new features that were added and bugs fixed in the GlusterFS 6 stable release. NOTE: Next minor release tentative date: Week of 10th July, 2019 Major changes, features and limitations addressed in this release None Major issues None Bugs addressed Bugs addressed since release-6.2 are listed below. #1714172 : ec ignores lock contention notifications for partially acquired locks #1715012 : Failure when glusterd is configured to bind specific IPv6 address. If bind-address is IPv6, *addr_len will be non-zero and it goes to ret = -1 branch, which will cause listen failure eventually","title":6.3},{"location":"release-notes/6.3/#release-notes-for-gluster-63","text":"This is a bugfix release. The release notes for 6.0 , 6.1 and 6.2 contains a listing of all the new features that were added and bugs fixed in the GlusterFS 6 stable release. NOTE: Next minor release tentative date: Week of 10th July, 2019","title":"Release notes for Gluster 6.3"},{"location":"release-notes/6.3/#major-changes-features-and-limitations-addressed-in-this-release","text":"None","title":"Major changes, features and limitations addressed in this release"},{"location":"release-notes/6.3/#major-issues","text":"None","title":"Major issues"},{"location":"release-notes/6.3/#bugs-addressed","text":"Bugs addressed since release-6.2 are listed below. #1714172 : ec ignores lock contention notifications for partially acquired locks #1715012 : Failure when glusterd is configured to bind specific IPv6 address. If bind-address is IPv6, *addr_len will be non-zero and it goes to ret = -1 branch, which will cause listen failure eventually","title":"Bugs addressed"},{"location":"release-notes/6.4/","text":"Release notes for Gluster 6.4 This is a bugfix release. The release notes for 6.0 , 6.1 , 6.2 and 6.3 contains a listing of all the new features that were added and bugs fixed in the GlusterFS 6 stable release. NOTE: Next minor release tentative date: Week of 10th August, 2019 Major changes, features and limitations addressed in this release None Major issues None Bugs addressed Bugs addressed since release-6.3 are listed below. #1679998 : GlusterFS can be improved #1683815 : Memory leak when peer detach fails #1716812 : Failed to create volume which transport_type is \"tcp,rdma\" #1716871 : Image size as reported from the fuse mount is incorrect #1718227 : SELinux context labels are missing for newly added bricks using add-brick command #1720633 : Upcall: Avoid sending upcalls for invalid Inode #1720635 : Ganesha-gfapi logs are flooded with error messages related to \"gf_uuid_is_null(gfid)) [Invalid argument]\" when lookups are running from multiple clients #1720993 : tests/features/subdir-mount.t is failing for brick_mux regrssion #1721105 : Failed to create volume which transport_type is \"tcp,rdma\" #1721783 : ctime changes: tar still complains file changed as we read it if uss is enabled #1722805 : Healing not proceeding during in-service upgrade on a disperse volume #1723658 : [In-service] Post upgrade glusterd is crashing with a backtrace on the upgraded node while issuing gluster volume status from non-upgraded nodes #1723659 : ESTALE change in fuse breaks get_real_filename implementation #1724210 : Incorrect power of two calculation in mem_pool_get_fn #1724558 : [Ganesha]: truncate operation not updating the ctime #1726294 : DHT: severe memory leak in dht rename #1726327 : tests/features/subdir-mount.t is failing for brick_mux regrssion #1727984 : User serviceable snapshots (USS) are not accessible after changing transport.socket.bind-address of glusterd #1728126 : [In-service] Post upgrade glusterd is crashing with a backtrace on the upgraded node while issuing gluster volume status from non-upgraded nodes #1729952 : Deadlock when generating statedumps","title":6.4},{"location":"release-notes/6.4/#release-notes-for-gluster-64","text":"This is a bugfix release. The release notes for 6.0 , 6.1 , 6.2 and 6.3 contains a listing of all the new features that were added and bugs fixed in the GlusterFS 6 stable release. NOTE: Next minor release tentative date: Week of 10th August, 2019","title":"Release notes for Gluster 6.4"},{"location":"release-notes/6.4/#major-changes-features-and-limitations-addressed-in-this-release","text":"None","title":"Major changes, features and limitations addressed in this release"},{"location":"release-notes/6.4/#major-issues","text":"None","title":"Major issues"},{"location":"release-notes/6.4/#bugs-addressed","text":"Bugs addressed since release-6.3 are listed below. #1679998 : GlusterFS can be improved #1683815 : Memory leak when peer detach fails #1716812 : Failed to create volume which transport_type is \"tcp,rdma\" #1716871 : Image size as reported from the fuse mount is incorrect #1718227 : SELinux context labels are missing for newly added bricks using add-brick command #1720633 : Upcall: Avoid sending upcalls for invalid Inode #1720635 : Ganesha-gfapi logs are flooded with error messages related to \"gf_uuid_is_null(gfid)) [Invalid argument]\" when lookups are running from multiple clients #1720993 : tests/features/subdir-mount.t is failing for brick_mux regrssion #1721105 : Failed to create volume which transport_type is \"tcp,rdma\" #1721783 : ctime changes: tar still complains file changed as we read it if uss is enabled #1722805 : Healing not proceeding during in-service upgrade on a disperse volume #1723658 : [In-service] Post upgrade glusterd is crashing with a backtrace on the upgraded node while issuing gluster volume status from non-upgraded nodes #1723659 : ESTALE change in fuse breaks get_real_filename implementation #1724210 : Incorrect power of two calculation in mem_pool_get_fn #1724558 : [Ganesha]: truncate operation not updating the ctime #1726294 : DHT: severe memory leak in dht rename #1726327 : tests/features/subdir-mount.t is failing for brick_mux regrssion #1727984 : User serviceable snapshots (USS) are not accessible after changing transport.socket.bind-address of glusterd #1728126 : [In-service] Post upgrade glusterd is crashing with a backtrace on the upgraded node while issuing gluster volume status from non-upgraded nodes #1729952 : Deadlock when generating statedumps","title":"Bugs addressed"},{"location":"release-notes/6.5/","text":"Release notes for Gluster 6.5 This is a bugfix release. The release notes for 6.0 , 6.1 , 6.2 , 6.3 and 6.4 contains a listing of all the new features that were added and bugs fixed in the GlusterFS 6 stable release. NOTE: Next minor release tentative date: Week of 30th October, 2019 Major changes, features and limitations addressed in this release None Major issues None Bugs addressed Bugs addressed since release-6.4 are listed below. #1716848 : DHT: directory permissions are wiped out #1730545 : gluster v geo-rep status command timing out #1731509 : snapd crashes sometimes #1736341 : potential deadlock while processing callbacks in gfapi- #1733880 : [geo-rep]: gluster command not found while setting up a non-root session #1733885 : ctime: Upgrade/Enabling ctime feature wrongly updates older files with latest {a|m|c}time #1737712 : Unable to create geo-rep session on a non-root setup. #1737745 : ctime: When healing ctime xattr for legacy files, if multiple clients access and modify the same file, the ctime might be updated incorrectly. #1737746 : ctime: nfs client gets bad ctime for copied file which is on glusterfs disperse volume with ctime on","title":6.5},{"location":"release-notes/6.5/#release-notes-for-gluster-65","text":"This is a bugfix release. The release notes for 6.0 , 6.1 , 6.2 , 6.3 and 6.4 contains a listing of all the new features that were added and bugs fixed in the GlusterFS 6 stable release. NOTE: Next minor release tentative date: Week of 30th October, 2019","title":"Release notes for Gluster 6.5"},{"location":"release-notes/6.5/#major-changes-features-and-limitations-addressed-in-this-release","text":"None","title":"Major changes, features and limitations addressed in this release"},{"location":"release-notes/6.5/#major-issues","text":"None","title":"Major issues"},{"location":"release-notes/6.5/#bugs-addressed","text":"Bugs addressed since release-6.4 are listed below. #1716848 : DHT: directory permissions are wiped out #1730545 : gluster v geo-rep status command timing out #1731509 : snapd crashes sometimes #1736341 : potential deadlock while processing callbacks in gfapi- #1733880 : [geo-rep]: gluster command not found while setting up a non-root session #1733885 : ctime: Upgrade/Enabling ctime feature wrongly updates older files with latest {a|m|c}time #1737712 : Unable to create geo-rep session on a non-root setup. #1737745 : ctime: When healing ctime xattr for legacy files, if multiple clients access and modify the same file, the ctime might be updated incorrectly. #1737746 : ctime: nfs client gets bad ctime for copied file which is on glusterfs disperse volume with ctime on","title":"Bugs addressed"},{"location":"release-notes/6.6/","text":"Release notes for Gluster 6.6 This is a bugfix release. The release notes for 6.0 , 6.1 , 6.2 , 6.3 , 6.4 and 6.5 contains a listing of all the new features that were added and bugs fixed in the GlusterFS 6 stable release. NOTE: Next minor release tentative date: Week of 30th December, 2019 Major changes, features and limitations addressed in this release None Major issues None Bugs addressed Bugs addressed since release-6.5 are listed below. #1726175 : CentOs 6 GlusterFS client creates files with time 01/01/1970 #1737141 : read() returns more than file size when using direct I/O #1739320 : The result (hostname) of getnameinfo for all bricks (ipv6 addresses) are the same, while they are not. #1739335 : Multiple disconnect events being propagated for the same child #1739451 : An Input/Output error happens on a disperse volume when doing unaligned writes to a sparse file #1740525 : event: rename event_XXX with gf_ prefixed to avoid crash when apps linked libevent at the same time #1741044 : atime/mtime is not restored after healing for entry self heals #1741402 : READDIRP incorrectly updates posix-acl inode ctx #1743219 : glusterd start is failed and throwing an error Address already in use #1743782 : Windows client fails to copy large file to GlusterFS volume share with fruit and streams_xattr VFS modules via Samba #1743988 : Setting cluster.heal-timeout requires volume restart #1745421 : ./tests/bugs/glusterd/bug-1595320.t is failing #1746118 : capture stat failure error while setting the gfid #1746138 : ctime: If atime is updated via utimensat syscall ctime is not getting updated #1749157 : bug-1402841.t-mt-dir-scan-race.t fails spuriously #1749307 : Failures in remove-brick due to [Input/output error] errors #1750228 : [geo-rep]: Non-root - Unable to set up mountbroker root directory and group #1751557 : syncop: Bail out if frame creation fails #1752413 : ctime: Cannot see the \"trusted.glusterfs.mdata\" xattr for directory on a new brick after rebalance #1753561 : Custom xattrs are not healed on newly added brick #1753571 : interrupts leak memory #1755679 : Segmentation fault occurs while truncate file #1755785 : git clone fails on gluster volumes exported via nfs-ganesha #1760361 : packaging: remove leftover bd cruft in rpm .spec.in #1760706 : glustershd can not decide heald_sinks, and skip repair, so some entries lingering in volume heal info #1760792 : afr: support split-brain CLI for replica 3 #1761907 : Rebalance causing IO Error - File descriptor in bad state #1763028 : [geo-rep] sync_method showing rsync instead of tarssh post in-service upgrade #1764171 : [Upgrade] Config files are not upgraded to new version #1764172 : geo-replication sessions going faulty #1764174 : geo-rep syncing significantly behind and also only one of the directories are synced with tracebacks seen #1764176 : geo-rep: Changelog archive file format is incorrect #1764178 : tests/geo-rep: Add test case to validate non-root geo-replication setup #1764183 : [GSS] geo-rep entering into faulty state with OSError: [Errno 13] Permission denied #1765433 : test: fix non-root geo-rep test case","title":6.6},{"location":"release-notes/6.6/#release-notes-for-gluster-66","text":"This is a bugfix release. The release notes for 6.0 , 6.1 , 6.2 , 6.3 , 6.4 and 6.5 contains a listing of all the new features that were added and bugs fixed in the GlusterFS 6 stable release. NOTE: Next minor release tentative date: Week of 30th December, 2019","title":"Release notes for Gluster 6.6"},{"location":"release-notes/6.6/#major-changes-features-and-limitations-addressed-in-this-release","text":"None","title":"Major changes, features and limitations addressed in this release"},{"location":"release-notes/6.6/#major-issues","text":"None","title":"Major issues"},{"location":"release-notes/6.6/#bugs-addressed","text":"Bugs addressed since release-6.5 are listed below. #1726175 : CentOs 6 GlusterFS client creates files with time 01/01/1970 #1737141 : read() returns more than file size when using direct I/O #1739320 : The result (hostname) of getnameinfo for all bricks (ipv6 addresses) are the same, while they are not. #1739335 : Multiple disconnect events being propagated for the same child #1739451 : An Input/Output error happens on a disperse volume when doing unaligned writes to a sparse file #1740525 : event: rename event_XXX with gf_ prefixed to avoid crash when apps linked libevent at the same time #1741044 : atime/mtime is not restored after healing for entry self heals #1741402 : READDIRP incorrectly updates posix-acl inode ctx #1743219 : glusterd start is failed and throwing an error Address already in use #1743782 : Windows client fails to copy large file to GlusterFS volume share with fruit and streams_xattr VFS modules via Samba #1743988 : Setting cluster.heal-timeout requires volume restart #1745421 : ./tests/bugs/glusterd/bug-1595320.t is failing #1746118 : capture stat failure error while setting the gfid #1746138 : ctime: If atime is updated via utimensat syscall ctime is not getting updated #1749157 : bug-1402841.t-mt-dir-scan-race.t fails spuriously #1749307 : Failures in remove-brick due to [Input/output error] errors #1750228 : [geo-rep]: Non-root - Unable to set up mountbroker root directory and group #1751557 : syncop: Bail out if frame creation fails #1752413 : ctime: Cannot see the \"trusted.glusterfs.mdata\" xattr for directory on a new brick after rebalance #1753561 : Custom xattrs are not healed on newly added brick #1753571 : interrupts leak memory #1755679 : Segmentation fault occurs while truncate file #1755785 : git clone fails on gluster volumes exported via nfs-ganesha #1760361 : packaging: remove leftover bd cruft in rpm .spec.in #1760706 : glustershd can not decide heald_sinks, and skip repair, so some entries lingering in volume heal info #1760792 : afr: support split-brain CLI for replica 3 #1761907 : Rebalance causing IO Error - File descriptor in bad state #1763028 : [geo-rep] sync_method showing rsync instead of tarssh post in-service upgrade #1764171 : [Upgrade] Config files are not upgraded to new version #1764172 : geo-replication sessions going faulty #1764174 : geo-rep syncing significantly behind and also only one of the directories are synced with tracebacks seen #1764176 : geo-rep: Changelog archive file format is incorrect #1764178 : tests/geo-rep: Add test case to validate non-root geo-replication setup #1764183 : [GSS] geo-rep entering into faulty state with OSError: [Errno 13] Permission denied #1765433 : test: fix non-root geo-rep test case","title":"Bugs addressed"},{"location":"release-notes/6.7/","text":"Release notes for Gluster 6.7 This is a bugfix release. The release notes for 6.0 , 6.1 , 6.2 , 6.3 , 6.4 , 6.5 and 6.6 contain a listing of all the new features that were added and bugs fixed in the GlusterFS 6 stable release. NOTE: Tentative date for next minor release: Week of 29th February, 2020 Major changes, features and limitations addressed in this release None Major issues We have come across a issue where the client undergoing IO crashes when a rebalance is running. https://bugzilla.redhat.com/show_bug.cgi?id=1786983 Workaround: We can avoid this issue by stopping the IOs while running rebalance. Fix: The fix is ready and will be a part of the next release 6.8 which is supposed to be out around 29th of February. https://review.gluster.org/#/c/glusterfs/+/23938/ Bugs addressed Bugs addressed since release-6.6 are listed below. #1739446 : [Disperse] : Client side heal is not removing dirty flag for some of the files. #1739449 : Disperse volume : data corruption with ftruncate data in 4+2 config #1739450 : Open fd heal should filter O_APPEND/O_EXCL #1749625 : [GlusterFS 6.1] GlusterFS brick process crash #1766425 : cgroup control-cpu-load.sh script not working #1768726 : Memory leak in glusterfsd process #1770100 : [geo-rep]: Geo-rep goes FAULTY with OSError #1771842 : [CENTOS 6] Geo-replication session not starting after creation #1778182 : glusterfsd crashed with \"'MemoryError' Cannot access memory at address\" #1782495 : GlusterFS brick process crash #1784796 : tests/00-geo-rep/00-georep-verify-non-root-setup.t fail on freshly installed builder","title":6.7},{"location":"release-notes/6.7/#release-notes-for-gluster-67","text":"This is a bugfix release. The release notes for 6.0 , 6.1 , 6.2 , 6.3 , 6.4 , 6.5 and 6.6 contain a listing of all the new features that were added and bugs fixed in the GlusterFS 6 stable release. NOTE: Tentative date for next minor release: Week of 29th February, 2020","title":"Release notes for Gluster 6.7"},{"location":"release-notes/6.7/#major-changes-features-and-limitations-addressed-in-this-release","text":"None","title":"Major changes, features and limitations addressed in this release"},{"location":"release-notes/6.7/#major-issues","text":"We have come across a issue where the client undergoing IO crashes when a rebalance is running. https://bugzilla.redhat.com/show_bug.cgi?id=1786983 Workaround: We can avoid this issue by stopping the IOs while running rebalance. Fix: The fix is ready and will be a part of the next release 6.8 which is supposed to be out around 29th of February. https://review.gluster.org/#/c/glusterfs/+/23938/","title":"Major issues"},{"location":"release-notes/6.7/#bugs-addressed","text":"Bugs addressed since release-6.6 are listed below. #1739446 : [Disperse] : Client side heal is not removing dirty flag for some of the files. #1739449 : Disperse volume : data corruption with ftruncate data in 4+2 config #1739450 : Open fd heal should filter O_APPEND/O_EXCL #1749625 : [GlusterFS 6.1] GlusterFS brick process crash #1766425 : cgroup control-cpu-load.sh script not working #1768726 : Memory leak in glusterfsd process #1770100 : [geo-rep]: Geo-rep goes FAULTY with OSError #1771842 : [CENTOS 6] Geo-replication session not starting after creation #1778182 : glusterfsd crashed with \"'MemoryError' Cannot access memory at address\" #1782495 : GlusterFS brick process crash #1784796 : tests/00-geo-rep/00-georep-verify-non-root-setup.t fail on freshly installed builder","title":"Bugs addressed"},{"location":"release-notes/6.8/","text":"Release notes for Gluster 6.8 This is a bugfix release. The release notes for 6.0 , 6.1 , 6.2 , 6.3 , 6.4 , 6.5 , 6.7 , and 6.8 contain a listing of all the new features that were added and bugs fixed in the GlusterFS 6 stable release. NOTE: Tentative date for next minor release: Week of 30th April, 2020 Major changes, features and limitations addressed in this release None Major issues None Bugs addressed Bugs addressed since release-6.7 are listed below. #1786754 : Functionality to enable log rotation for user serviceable snapshot's logs. #1786983 : Rebalance is causing glusterfs crash on client node #1789337 : glusterfs process memory leak in ior test #1790445 : glusterfind pre output file is empty #1790449 : S57glusterfind-delete-post.py not python3 ready (does not decode bytestring) #1790850 : Remove extra argument #1792857 : Memory corruption when sending events to an IPv6 host #1793096 : gf_event doesn't work for glfsheal process #1794020 : Mounts fails after reboot of 1/3 gluster nodes #1797985 : Brick logs inundated with [2019-04-27 22:14:53.378047] I [dict.c:541:dict_get] (-->/usr/lib64/glusterfs/6.0/xlator/features/worm.so(+0x7241) [0x7fe857bb3241] -->/usr/lib64/glusterfs/6.0/xlator/features/locks.so(+0x1c219) [0x7fe857dda219] [Invalid argumen #1804546 : [Thin-arbiter] : Wait for connection with TA node before sending lookup/create of ta-replica id file #1804594 : Heal pending on volume, even after all the bricks are up #1805097 : Changes to self-heal logic w.r.t. detecting metadata split-brains #1805671 : Memory corruption when glfs_init() is called after glfs_fini() #1806836 : [EC] shd crashed while heal failed due to out of memory error. #1806838 : Disperse volume : Ganesha crash with IO in 4+2 config when one glusterfsd restart every 600s #1807786 : seeing error message in glustershd.log on volume start(or may be as part of shd graph regeneration) inet_pton failed with return code 0 [Invalid argument] #1807793 : glusterfs-libs: usage of inet_addr() may impact IPv6","title":6.8},{"location":"release-notes/6.8/#release-notes-for-gluster-68","text":"This is a bugfix release. The release notes for 6.0 , 6.1 , 6.2 , 6.3 , 6.4 , 6.5 , 6.7 , and 6.8 contain a listing of all the new features that were added and bugs fixed in the GlusterFS 6 stable release. NOTE: Tentative date for next minor release: Week of 30th April, 2020","title":"Release notes for Gluster 6.8"},{"location":"release-notes/6.8/#major-changes-features-and-limitations-addressed-in-this-release","text":"None","title":"Major changes, features and limitations addressed in this release"},{"location":"release-notes/6.8/#major-issues","text":"None","title":"Major issues"},{"location":"release-notes/6.8/#bugs-addressed","text":"Bugs addressed since release-6.7 are listed below. #1786754 : Functionality to enable log rotation for user serviceable snapshot's logs. #1786983 : Rebalance is causing glusterfs crash on client node #1789337 : glusterfs process memory leak in ior test #1790445 : glusterfind pre output file is empty #1790449 : S57glusterfind-delete-post.py not python3 ready (does not decode bytestring) #1790850 : Remove extra argument #1792857 : Memory corruption when sending events to an IPv6 host #1793096 : gf_event doesn't work for glfsheal process #1794020 : Mounts fails after reboot of 1/3 gluster nodes #1797985 : Brick logs inundated with [2019-04-27 22:14:53.378047] I [dict.c:541:dict_get] (-->/usr/lib64/glusterfs/6.0/xlator/features/worm.so(+0x7241) [0x7fe857bb3241] -->/usr/lib64/glusterfs/6.0/xlator/features/locks.so(+0x1c219) [0x7fe857dda219] [Invalid argumen #1804546 : [Thin-arbiter] : Wait for connection with TA node before sending lookup/create of ta-replica id file #1804594 : Heal pending on volume, even after all the bricks are up #1805097 : Changes to self-heal logic w.r.t. detecting metadata split-brains #1805671 : Memory corruption when glfs_init() is called after glfs_fini() #1806836 : [EC] shd crashed while heal failed due to out of memory error. #1806838 : Disperse volume : Ganesha crash with IO in 4+2 config when one glusterfsd restart every 600s #1807786 : seeing error message in glustershd.log on volume start(or may be as part of shd graph regeneration) inet_pton failed with return code 0 [Invalid argument] #1807793 : glusterfs-libs: usage of inet_addr() may impact IPv6","title":"Bugs addressed"},{"location":"release-notes/6.9/","text":"Release notes for Gluster 6.9 This is a bugfix release. The release notes for 6.0 , 6.1 , 6.2 , 6.3 , 6.4 , 6.5 , 6.7 , 6.8 and 6.9 contain a listing of all the new features that were added and bugs fixed in the GlusterFS 6 stable release. NOTE: Tentative date for next minor release: Week of 30th June, 2020 Major changes, features and limitations addressed in this release None Major issues None Bugs addressed Bugs addressed since release-6.8 are listed below. #832 : Permission Denied in logs. #1152 : Spurious failure of tests/bugs/protocol/bug-1433815-auth-allow.t #1140 : getfattr returns ENOATTR for system.posix_acl_access on disperse type volumes #884 : [bug:1808688] Data corruption with asynchronous writes (please try to reproduce!) #1134 : snap_scheduler.py init failing with \"TypeError: Can't mix strings and bytes in path components\" #1067 : [bug:1661889] Metadata heal picks different brick each time as source if there are no pending xattrs. #1028 : [bug:1810934] Segfault in FUSE process, potential use after free #1146 : gfapi/Upcall: Potential deadlock in synctask threads processing upcall notifications #1808966 : Set volume option when one of the node is powered off, After powering the node brick processes are offline #1809439 : [brickmux]: glustershd crashed when rebooting 1/3 nodes at regular intervals","title":6.9},{"location":"release-notes/6.9/#release-notes-for-gluster-69","text":"This is a bugfix release. The release notes for 6.0 , 6.1 , 6.2 , 6.3 , 6.4 , 6.5 , 6.7 , 6.8 and 6.9 contain a listing of all the new features that were added and bugs fixed in the GlusterFS 6 stable release. NOTE: Tentative date for next minor release: Week of 30th June, 2020","title":"Release notes for Gluster 6.9"},{"location":"release-notes/6.9/#major-changes-features-and-limitations-addressed-in-this-release","text":"None","title":"Major changes, features and limitations addressed in this release"},{"location":"release-notes/6.9/#major-issues","text":"None","title":"Major issues"},{"location":"release-notes/6.9/#bugs-addressed","text":"Bugs addressed since release-6.8 are listed below. #832 : Permission Denied in logs. #1152 : Spurious failure of tests/bugs/protocol/bug-1433815-auth-allow.t #1140 : getfattr returns ENOATTR for system.posix_acl_access on disperse type volumes #884 : [bug:1808688] Data corruption with asynchronous writes (please try to reproduce!) #1134 : snap_scheduler.py init failing with \"TypeError: Can't mix strings and bytes in path components\" #1067 : [bug:1661889] Metadata heal picks different brick each time as source if there are no pending xattrs. #1028 : [bug:1810934] Segfault in FUSE process, potential use after free #1146 : gfapi/Upcall: Potential deadlock in synctask threads processing upcall notifications #1808966 : Set volume option when one of the node is powered off, After powering the node brick processes are offline #1809439 : [brickmux]: glustershd crashed when rebooting 1/3 nodes at regular intervals","title":"Bugs addressed"},{"location":"release-notes/7.0/","text":"Release notes for Gluster 7.0 This is a major release that includes a range of code improvements and stability fixes along with a few features as noted below. A selection of the key features and changes are documented in this page. A full list of bugs that have been addressed is included further below. Announcements Major changes and features Major issues Bugs addressed in the release Announcements Releases that receive maintenance updates post release 7 are, 5, 6 and 7 ( reference ) Release 7 will receive maintenance updates around the 10th of every month for the first 3 months post release (i.e Dec'19, Jan'20, Feb'20). Post the initial 3 months, it will receive maintenance updates every 2 months till EOL. Major changes and features Highlights Several stability fixes addressing, coverity, clang-scan, address sanitizer and valgrind reported issues removal of unused and hence, deprecated code and features Performance Improvements Features 1. Rpcbind not required in glusterd.service when gnfs isn't built. 2. Latency based read child to improve read workload latency in a cluster, especially in a cloud setup. Also provides a load balancing with the outstanding pending request. 3. Glusterfind: integrate with gfid2path, to improve performance. 4. Issue #532: Work towards implementing global thread pooling has started 5. This release includes extra coverage for glfs public APIs in our regression tests, so we don't break anything. 6. Thin-arbiter integration with GD1 Major issues #1771308 :Unable to build the gluster packages for centos-6 Note Any new volumes created with the release will have the fips-mode-rchecksum volume option set to on by default. If a client older than glusterfs-4.x (i.e. 3.x clients) accesses a volume which has the fips-mode-rchecksum volume option enabled, it can cause erroneous checksum computation/ unwanted behaviour during afr self-heal. This option is to be enabled only when all clients are also >=4.x. So if you are using these older clients, please explicitly turn this option off . Bugs addressed Bugs addressed since release-6 are listed below. #789278 : Issues reported by Coverity static analysis tool #1098991 : Dist-geo-rep: Invalid slave url (::: three or more colons) error out with unclear error message. #1193929 : GlusterFS can be improved #1241494 : [Backup]: Glusterfind CLI commands need to verify the accepted names for session/volume, before failing with error(s) #1512093 : Value of pending entry operations in detail status output is going up after each synchronization. #1535511 : Gluster CLI shouldn't stop if log file couldn't be opened #1542072 : Syntactical errors in hook scripts for managing SELinux context on bricks #2 (S10selinux-label-brick.sh + S10selinux-del-fcontext.sh) #1573226 : eventsapi: ABRT report for package glusterfs has reached 10 occurrences #1580315 : gluster volume status inode getting timed out after 30 minutes with no output/error #1590385 : Refactor dht lookup code #1593224 : [Disperse] : Client side heal is not removing dirty flag for some of the files. #1596787 : glusterfs rpc-clnt.c: error returned while attempting to connect to host: (null), port 0 #1622665 : clang-scan report: glusterfs issues #1624701 : error-out {inode,entry}lk fops with all-zero lk-owner #1628194 : tests/dht: Additional tests for dht operations #1633930 : ASan (address sanitizer) fixes - Blanket bug #1634664 : Inconsistent quorum checks during open and fd based operations #1635688 : Keep only the valid (maintained/supported) components in the build #1642168 : changes to cloudsync xlator #1642810 : remove glupy from code and build #1648169 : Fuse mount would crash if features.encryption is on in the version from 3.13.0 to 4.1.5 #1648768 : Tracker bug for all leases related issues #1650095 : Regression tests for geo-replication on EC volume is not available. It should be added. #1651246 : Failed to dispatch handler #1651439 : gluster-NFS crash while expanding volume #1651445 : [RFE] storage.reserve option should take size of disk as input instead of percentage #1652887 : Geo-rep help looks to have a typo. #1654021 : Gluster volume heal causes continuous info logging of \"invalid argument\" #1654270 : glusterd crashed with seg fault possibly during node reboot while volume creates and deletes were happening #1659334 : FUSE mount seems to be hung and not accessible #1659708 : Optimize by not stopping (restart) selfheal deamon (shd) when a volume is stopped unless it is the last volume #1664934 : glusterfs-fuse client not benefiting from page cache on read after write #1670031 : performance regression seen with smallfile workload tests #1672480 : Bugs Test Module tests failing on s390x #1672711 : Upgrade from glusterfs 3.12 to gluster 4/5 broken #1672727 : Fix timeouts so the tests pass on AWS #1672851 : With parallel-readdir enabled, deleting a directory containing stale linkto files fails with \"Directory not empty\" #1674389 : [thin arbiter] : rpm - add thin-arbiter package #1674406 : glusterfs FUSE client crashing every few days with 'Failed to dispatch handler' #1674412 : listing a file while writing to it causes deadlock #1675076 : [posix]: log the actual path wherever possible #1676400 : rm -rf fails with \"Directory not empty\" #1676430 : distribute: Perf regression in mkdir path #1676736 : tests: ./tests/bugs/distribute/bug-1161311.t times out #1676797 : server xlator doesn't handle dict unserialization failures correctly #1677559 : gNFS crashed when processing \"gluster v profile [vol] info nfs\" #1678726 : Integer Overflow possible in md-cache.c due to data type inconsistency #1679401 : Geo-rep setup creates an incorrectly formatted authorized_keys file #1679406 : glustereventsd does not start on Ubuntu 16.04 LTS #1680587 : Building RPM packages with _for_fedora_koji_builds enabled fails on el6 #1683352 : remove experimental xlators informations from glusterd-volume-set.c #1683594 : nfs ltp ftest* fstat gets mismatch size as except after turn on md-cache #1683816 : Memory leak when peer detach fails #1684385 : [ovirt-gluster] Rolling gluster upgrade from 3.12.5 to 5.3 led to shard on-disk xattrs disappearing #1684404 : Multiple shd processes are running on brick_mux environmet #1685027 : Error handling in /usr/sbin/gluster-eventsapi produces IndexError: tuple index out of range #1685120 : upgrade from 3.12, 4.1 and 5 to 6 broken #1685414 : glusterd memory usage grows at 98 MB/h while running \"gluster v profile\" in a loop #1685944 : WORM-XLator: Maybe integer overflow when computing new atime #1686371 : Cleanup nigel access and document it #1686398 : Thin-arbiter minor fixes #1686568 : [geo-rep]: Checksum mismatch when 2x2 vols are converted to arbiter #1686711 : [Thin-arbiter] : send correct error code in case of failure #1687326 : [RFE] Revoke access from nodes using Certificate Revoke List in SSL #1687705 : Brick process has coredumped, when starting glusterd #1687811 : core dump generated while running the test ./tests/00-geo-rep/georep-basic-dr-rsync-arbiter.t #1688068 : Proper error message needed for FUSE mount failure when /var is filled. #1688106 : Remove implementation of number of files opened in posix xlator #1688116 : Spurious failure in test ./tests/bugs/glusterfs/bug-844688.t #1688287 : ganesha crash on glusterfs with shard volume #1689097 : gfapi: provide an option for changing statedump path in glfs-api. #1689799 : [cluster/ec] : Fix handling of heal info cases without locks #1689920 : lots of \"Matching lock not found for unlock xxx\" when using disperse (ec) xlator #1690753 : Volume stop when quorum not met is successful #1691164 : glusterd leaking memory when issued gluster vol status all tasks continuosly #1691616 : client log flooding with intentional socket shutdown message when a brick is down #1692093 : Network throughput usage increased x5 #1692612 : Locking issue when restarting bricks #1692666 : ssh-port config set is failing #1693575 : gfapi: do not block epoll thread for upcall notifications #1693648 : Geo-re: Geo replication failing in \"cannot allocate memory\" #1693692 : Increase code coverage from regression tests #1694820 : Geo-rep: Data inconsistency while syncing heavy renames with constant destination name #1694925 : GF_LOG_OCCASSIONALLY API doesn't log at first instance #1695327 : regression test fails with brick mux enabled. #1696046 : Log level changes do not take effect until the process is restarted #1696077 : Add pause and resume test case for geo-rep #1696136 : gluster fuse mount crashed, when deleting 2T image file from oVirt Manager UI #1696512 : glusterfs build is failing on rhel-6 #1696599 : Fops hang when inodelk fails on the first fop #1697316 : Getting SEEK-2 and SEEK7 errors with [Invalid argument] in the bricks' logs #1697486 : bug-1650403.t && bug-858215.t are throwing error \"No such file\" at the time of access glustershd pidfile #1697866 : Provide a way to detach a failed node #1697907 : ctime feature breaks old client to connect to new server #1697930 : Thin-Arbiter SHD minor fixes #1698078 : ctime: Creation of tar file on gluster mount throws warning \"file changed as we read it\" #1698449 : thin-arbiter lock release fixes #1699025 : Brick is not able to detach successfully in brick_mux environment #1699176 : rebalance start command doesn't throw up error message if the command fails #1699189 : fix truncate lock to cover the write in tuncate clean #1699339 : With 1800+ vol and simultaneous 2 gluster pod restarts, running gluster commands gives issues once all pods are up #1699394 : [geo-rep]: Geo-rep goes FAULTY with OSError #1699866 : I/O error on writes to a disperse volume when replace-brick is executed #1700078 : disablle + reenable of bitrot leads to files marked as bad #1700865 : FUSE mount seems to be hung and not accessible #1701337 : issues with 'building' glusterfs packages if we do 'git clone --depth 1' #1701457 : ctime: Logs are flooded with \"posix set mdata failed, No ctime\" error during open #1702131 : The source file is left in EC volume after rename when glusterfsd out of service #1702185 : coredump reported by test ./tests/bugs/glusterd/bug-1699339.t #1702299 : Custom xattrs are not healed on newly added brick #1702303 : Enable enable fips-mode-rchecksum for new volumes by default #1702952 : remove tier related information from manual pages #1703020 : The cluster.heal-timeout option is unavailable for ec volume #1703629 : statedump is not capturing info related to glusterd #1703948 : Self-heal daemon resources are not cleaned properly after a ec fini #1704252 : Creation of bulkvoldict thread logic is not correct while brick_mux is enabled for single volume #1704888 : delete the snapshots and volume at the end of uss.t #1705865 : VM stuck in a shutdown because of a pending fuse request #1705884 : Image size as reported from the fuse mount is incorrect #1706603 : Glusterfsd crashing in ec-inode-write.c, in GF_ASSERT #1707081 : Self heal daemon not coming up after upgrade to glusterfs-6.0-2 (intermittently) on a brick mux setup #1707700 : maintain consistent values across for options when fetched at cluster level or volume level #1707728 : geo-rep: Sync hangs with tarssh as sync-engine #1707742 : tests/geo-rep: arequal checksum comparison always succeeds #1707746 : AFR-v2 does not log before attempting data self-heal #1708051 : Capture memory consumption for gluster process at the time of throwing no memory available message #1708156 : ec ignores lock contention notifications for partially acquired locks #1708163 : tests: fix bug-1319374.c compile warnings. #1708926 : Invalid memory access while executing cleanup_and_exit #1708929 : Add more test coverage for shd mux #1709248 : [geo-rep]: Non-root - Unable to set up mountbroker root directory and group #1709653 : geo-rep: With heavy rename workload geo-rep log if flooded #1710054 : Optimize the glustershd manager to send reconfigure #1710159 : glusterd: While upgrading (3-node cluster) 'gluster v status' times out on node to be upgraded #1711240 : [GNFS] gf_nfs_mt_inode_ctx serious memory leak #1711250 : bulkvoldict thread is not handling all volumes while brick multiplex is enabled #1711297 : Optimize glusterd code to copy dictionary in handshake code path #1711764 : Files inaccessible if one rebalance process is killed in a multinode volume #1711820 : Typo in cli return string. #1711827 : test case bug-1399598-uss-with-ssl.t is generating crash #1712322 : Brick logs inundated with [2019-04-27 22:14:53.378047] I [dict.c:541:dict_get] (-->/usr/lib64/glusterfs/6.0/xlator/features/worm.so(+0x7241) [0x7fe857bb3241] -->/usr/lib64/glusterfs/6.0/xlator/features/locks.so(+0x1c219) [0x7fe857dda219] [Invalid argumen #1712668 : Remove-brick shows warning cluster.force-migration enabled where as cluster.force-migration is disabled on the volume #1712741 : glusterd_svcs_stop should call individual wrapper function to stop rather than calling the glusterd_svc_stop #1713730 : Failure when glusterd is configured to bind specific IPv6 address. If bind-address is IPv6, *addr_len will be non-zero and it goes to ret = -1 branch, which will cause listen failure eventually #1714098 : Make debugging hung frames easier #1714415 : Script to make it easier to find hung frames #1714973 : upgrade after tier code removal results in peer rejection. #1715921 : uss.t tests times out with brick-mux regression #1716695 : Fix memory leaks that are present even after an xlator fini [client side xlator] #1716766 : [Thin-arbiter] TA process is not picking 24007 as port while starting up #1716812 : Failed to create volume which transport_type is \"tcp,rdma\" #1716830 : DHT: directory permissions are wiped out #1717757 : WORM: Segmentation Fault if bitrot stub do signature #1717782 : gluster v get all still showing storage.fips-mode-rchecksum off #1717819 : Changes to self-heal logic w.r.t. detecting metadata split-brains #1717953 : SELinux context labels are missing for newly added bricks using add-brick command #1718191 : Regression: Intermittent test failure for quick-read-with-upcall.t #1718273 : markdown formatting errors in files present under /doc directory of the project #1718316 : Ganesha-gfapi logs are flooded with error messages related to \"gf_uuid_is_null(gfid)) [Invalid argument]\" when lookups are running from multiple clients #1718338 : Upcall: Avoid sending upcalls for invalid Inode #1718848 : False positive logging of mount failure #1718998 : Fix test case \"tests/basic/afr/split-brain-favorite-child-policy.t\" failure #1720201 : Healing not proceeding during in-service upgrade on a disperse volume #1720290 : ctime changes: tar still complains file changed as we read it if uss is enabled #1720615 : [RHEL-8.1] yum update fails for rhel-8 glusterfs client packages 6.0-5.el8 #1720993 : tests/features/subdir-mount.t is failing for brick_mux regrssion #1721385 : glusterfs-libs: usage of inet_addr() may impact IPv6 #1721435 : DHT: Internal xattrs visible on the mount #1721441 : geo-rep: Fix permissions for GEOREP_DIR in non-root setup #1721601 : [SHD] : logs of one volume are going to log file of other volume #1722541 : stale shd process files leading to heal timing out and heal deamon not coming up for all volumes #1703322 : Need to document about fips-mode-rchecksum in gluster-7 release notes. #1722802 : Incorrect power of two calculation in mem_pool_get_fn #1723890 : Crash in glusterd when running test script bug-1699339.t #1728770 : Failures in remove-brick due to [Input/output error] errors #1736481 : capture stat failure error while setting the gfid #1739424 : Disperse volume : data corruption with ftruncate data in 4+2 config #1739426 : Open fd heal should filter O_APPEND/O_EXCL #1739427 : An Input/Output error happens on a disperse volume when doing unaligned writes to a sparse file #1741041 : atime/mtime is not restored after healing for entry self heals #1743200 : ./tests/bugs/glusterd/bug-1595320.t is failing #1744874 : interrupts leak memory #1745422 : ./tests/bugs/glusterd/bug-1595320.t is failing #1745914 : ESTALE change in fuse breaks get_real_filename implementation #1746142 : ctime: If atime is updated via utimensat syscall ctime is not getting updated #1746145 : CentOs 6 GlusterFS client creates files with time 01/01/1970 #1747301 : Setting cluster.heal-timeout requires volume restart #1747746 : The result (hostname) of getnameinfo for all bricks (ipv6 addresses) are the same, while they are not. #1748448 : syncop: Bail out if frame creation fails #1748774 : Incorrect power of two calculation in mem_pool_get_fn #1749155 : bug-1402841.t-mt-dir-scan-race.t fails spuriously #1749305 : Failures in remove-brick due to [Input/output error] errors #1749664 : The result (hostname) of getnameinfo for all bricks (ipv6 addresses) are the same, while they are not. #1751556 : syncop: Bail out if frame creation fails #1752245 : Crash in glusterd when running test script bug-1699339.t #1752429 : Ctime: Cannot see the \"trusted.glusterfs.mdata\" xattr for directory on a new brick after rebalance #1755212 : geo-rep: performance improvement while syncing heavy renames with existing destination #1755213 : geo-rep: non-root session going fault due improper sub-command #1755678 : Segmentation fault occurs while truncate file #1756002 : git clone fails on gluster volumes exported via nfs-ganesha","title":7.0},{"location":"release-notes/7.0/#release-notes-for-gluster-70","text":"This is a major release that includes a range of code improvements and stability fixes along with a few features as noted below. A selection of the key features and changes are documented in this page. A full list of bugs that have been addressed is included further below. Announcements Major changes and features Major issues Bugs addressed in the release","title":"Release notes for Gluster 7.0"},{"location":"release-notes/7.0/#announcements","text":"Releases that receive maintenance updates post release 7 are, 5, 6 and 7 ( reference ) Release 7 will receive maintenance updates around the 10th of every month for the first 3 months post release (i.e Dec'19, Jan'20, Feb'20). Post the initial 3 months, it will receive maintenance updates every 2 months till EOL.","title":"Announcements"},{"location":"release-notes/7.0/#major-changes-and-features","text":"","title":"Major changes and features"},{"location":"release-notes/7.0/#highlights","text":"Several stability fixes addressing, coverity, clang-scan, address sanitizer and valgrind reported issues removal of unused and hence, deprecated code and features Performance Improvements Features","title":"Highlights"},{"location":"release-notes/7.0/#1-rpcbind-not-required-in-glusterdservice-when-gnfs-isnt-built","text":"","title":"1. Rpcbind not required in glusterd.service when gnfs isn't built."},{"location":"release-notes/7.0/#2-latency-based-read-child-to-improve-read-workload-latency-in-a-cluster-especially-in-a-cloud-setup-also-provides-a-load-balancing-with-the-outstanding-pending-request","text":"","title":"2. Latency based read child to improve read workload latency in a cluster, especially in a cloud setup. Also provides a load balancing with the outstanding pending request."},{"location":"release-notes/7.0/#3-glusterfind-integrate-with-gfid2path-to-improve-performance","text":"","title":"3. Glusterfind: integrate with gfid2path, to improve performance."},{"location":"release-notes/7.0/#4-issue-532-work-towards-implementing-global-thread-pooling-has-started","text":"","title":"4. Issue #532: Work towards implementing global thread pooling has started"},{"location":"release-notes/7.0/#5-this-release-includes-extra-coverage-for-glfs-public-apis-in-our-regression-tests-so-we-dont-break-anything","text":"","title":"5. This release includes extra coverage for glfs public APIs in our regression tests, so we don't break anything."},{"location":"release-notes/7.0/#6-thin-arbiter-integration-with-gd1","text":"","title":"6. Thin-arbiter integration with GD1"},{"location":"release-notes/7.0/#major-issues","text":"#1771308 :Unable to build the gluster packages for centos-6","title":"Major issues"},{"location":"release-notes/7.0/#note","text":"Any new volumes created with the release will have the fips-mode-rchecksum volume option set to on by default. If a client older than glusterfs-4.x (i.e. 3.x clients) accesses a volume which has the fips-mode-rchecksum volume option enabled, it can cause erroneous checksum computation/ unwanted behaviour during afr self-heal. This option is to be enabled only when all clients are also >=4.x. So if you are using these older clients, please explicitly turn this option off .","title":"Note"},{"location":"release-notes/7.0/#bugs-addressed","text":"Bugs addressed since release-6 are listed below. #789278 : Issues reported by Coverity static analysis tool #1098991 : Dist-geo-rep: Invalid slave url (::: three or more colons) error out with unclear error message. #1193929 : GlusterFS can be improved #1241494 : [Backup]: Glusterfind CLI commands need to verify the accepted names for session/volume, before failing with error(s) #1512093 : Value of pending entry operations in detail status output is going up after each synchronization. #1535511 : Gluster CLI shouldn't stop if log file couldn't be opened #1542072 : Syntactical errors in hook scripts for managing SELinux context on bricks #2 (S10selinux-label-brick.sh + S10selinux-del-fcontext.sh) #1573226 : eventsapi: ABRT report for package glusterfs has reached 10 occurrences #1580315 : gluster volume status inode getting timed out after 30 minutes with no output/error #1590385 : Refactor dht lookup code #1593224 : [Disperse] : Client side heal is not removing dirty flag for some of the files. #1596787 : glusterfs rpc-clnt.c: error returned while attempting to connect to host: (null), port 0 #1622665 : clang-scan report: glusterfs issues #1624701 : error-out {inode,entry}lk fops with all-zero lk-owner #1628194 : tests/dht: Additional tests for dht operations #1633930 : ASan (address sanitizer) fixes - Blanket bug #1634664 : Inconsistent quorum checks during open and fd based operations #1635688 : Keep only the valid (maintained/supported) components in the build #1642168 : changes to cloudsync xlator #1642810 : remove glupy from code and build #1648169 : Fuse mount would crash if features.encryption is on in the version from 3.13.0 to 4.1.5 #1648768 : Tracker bug for all leases related issues #1650095 : Regression tests for geo-replication on EC volume is not available. It should be added. #1651246 : Failed to dispatch handler #1651439 : gluster-NFS crash while expanding volume #1651445 : [RFE] storage.reserve option should take size of disk as input instead of percentage #1652887 : Geo-rep help looks to have a typo. #1654021 : Gluster volume heal causes continuous info logging of \"invalid argument\" #1654270 : glusterd crashed with seg fault possibly during node reboot while volume creates and deletes were happening #1659334 : FUSE mount seems to be hung and not accessible #1659708 : Optimize by not stopping (restart) selfheal deamon (shd) when a volume is stopped unless it is the last volume #1664934 : glusterfs-fuse client not benefiting from page cache on read after write #1670031 : performance regression seen with smallfile workload tests #1672480 : Bugs Test Module tests failing on s390x #1672711 : Upgrade from glusterfs 3.12 to gluster 4/5 broken #1672727 : Fix timeouts so the tests pass on AWS #1672851 : With parallel-readdir enabled, deleting a directory containing stale linkto files fails with \"Directory not empty\" #1674389 : [thin arbiter] : rpm - add thin-arbiter package #1674406 : glusterfs FUSE client crashing every few days with 'Failed to dispatch handler' #1674412 : listing a file while writing to it causes deadlock #1675076 : [posix]: log the actual path wherever possible #1676400 : rm -rf fails with \"Directory not empty\" #1676430 : distribute: Perf regression in mkdir path #1676736 : tests: ./tests/bugs/distribute/bug-1161311.t times out #1676797 : server xlator doesn't handle dict unserialization failures correctly #1677559 : gNFS crashed when processing \"gluster v profile [vol] info nfs\" #1678726 : Integer Overflow possible in md-cache.c due to data type inconsistency #1679401 : Geo-rep setup creates an incorrectly formatted authorized_keys file #1679406 : glustereventsd does not start on Ubuntu 16.04 LTS #1680587 : Building RPM packages with _for_fedora_koji_builds enabled fails on el6 #1683352 : remove experimental xlators informations from glusterd-volume-set.c #1683594 : nfs ltp ftest* fstat gets mismatch size as except after turn on md-cache #1683816 : Memory leak when peer detach fails #1684385 : [ovirt-gluster] Rolling gluster upgrade from 3.12.5 to 5.3 led to shard on-disk xattrs disappearing #1684404 : Multiple shd processes are running on brick_mux environmet #1685027 : Error handling in /usr/sbin/gluster-eventsapi produces IndexError: tuple index out of range #1685120 : upgrade from 3.12, 4.1 and 5 to 6 broken #1685414 : glusterd memory usage grows at 98 MB/h while running \"gluster v profile\" in a loop #1685944 : WORM-XLator: Maybe integer overflow when computing new atime #1686371 : Cleanup nigel access and document it #1686398 : Thin-arbiter minor fixes #1686568 : [geo-rep]: Checksum mismatch when 2x2 vols are converted to arbiter #1686711 : [Thin-arbiter] : send correct error code in case of failure #1687326 : [RFE] Revoke access from nodes using Certificate Revoke List in SSL #1687705 : Brick process has coredumped, when starting glusterd #1687811 : core dump generated while running the test ./tests/00-geo-rep/georep-basic-dr-rsync-arbiter.t #1688068 : Proper error message needed for FUSE mount failure when /var is filled. #1688106 : Remove implementation of number of files opened in posix xlator #1688116 : Spurious failure in test ./tests/bugs/glusterfs/bug-844688.t #1688287 : ganesha crash on glusterfs with shard volume #1689097 : gfapi: provide an option for changing statedump path in glfs-api. #1689799 : [cluster/ec] : Fix handling of heal info cases without locks #1689920 : lots of \"Matching lock not found for unlock xxx\" when using disperse (ec) xlator #1690753 : Volume stop when quorum not met is successful #1691164 : glusterd leaking memory when issued gluster vol status all tasks continuosly #1691616 : client log flooding with intentional socket shutdown message when a brick is down #1692093 : Network throughput usage increased x5 #1692612 : Locking issue when restarting bricks #1692666 : ssh-port config set is failing #1693575 : gfapi: do not block epoll thread for upcall notifications #1693648 : Geo-re: Geo replication failing in \"cannot allocate memory\" #1693692 : Increase code coverage from regression tests #1694820 : Geo-rep: Data inconsistency while syncing heavy renames with constant destination name #1694925 : GF_LOG_OCCASSIONALLY API doesn't log at first instance #1695327 : regression test fails with brick mux enabled. #1696046 : Log level changes do not take effect until the process is restarted #1696077 : Add pause and resume test case for geo-rep #1696136 : gluster fuse mount crashed, when deleting 2T image file from oVirt Manager UI #1696512 : glusterfs build is failing on rhel-6 #1696599 : Fops hang when inodelk fails on the first fop #1697316 : Getting SEEK-2 and SEEK7 errors with [Invalid argument] in the bricks' logs #1697486 : bug-1650403.t && bug-858215.t are throwing error \"No such file\" at the time of access glustershd pidfile #1697866 : Provide a way to detach a failed node #1697907 : ctime feature breaks old client to connect to new server #1697930 : Thin-Arbiter SHD minor fixes #1698078 : ctime: Creation of tar file on gluster mount throws warning \"file changed as we read it\" #1698449 : thin-arbiter lock release fixes #1699025 : Brick is not able to detach successfully in brick_mux environment #1699176 : rebalance start command doesn't throw up error message if the command fails #1699189 : fix truncate lock to cover the write in tuncate clean #1699339 : With 1800+ vol and simultaneous 2 gluster pod restarts, running gluster commands gives issues once all pods are up #1699394 : [geo-rep]: Geo-rep goes FAULTY with OSError #1699866 : I/O error on writes to a disperse volume when replace-brick is executed #1700078 : disablle + reenable of bitrot leads to files marked as bad #1700865 : FUSE mount seems to be hung and not accessible #1701337 : issues with 'building' glusterfs packages if we do 'git clone --depth 1' #1701457 : ctime: Logs are flooded with \"posix set mdata failed, No ctime\" error during open #1702131 : The source file is left in EC volume after rename when glusterfsd out of service #1702185 : coredump reported by test ./tests/bugs/glusterd/bug-1699339.t #1702299 : Custom xattrs are not healed on newly added brick #1702303 : Enable enable fips-mode-rchecksum for new volumes by default #1702952 : remove tier related information from manual pages #1703020 : The cluster.heal-timeout option is unavailable for ec volume #1703629 : statedump is not capturing info related to glusterd #1703948 : Self-heal daemon resources are not cleaned properly after a ec fini #1704252 : Creation of bulkvoldict thread logic is not correct while brick_mux is enabled for single volume #1704888 : delete the snapshots and volume at the end of uss.t #1705865 : VM stuck in a shutdown because of a pending fuse request #1705884 : Image size as reported from the fuse mount is incorrect #1706603 : Glusterfsd crashing in ec-inode-write.c, in GF_ASSERT #1707081 : Self heal daemon not coming up after upgrade to glusterfs-6.0-2 (intermittently) on a brick mux setup #1707700 : maintain consistent values across for options when fetched at cluster level or volume level #1707728 : geo-rep: Sync hangs with tarssh as sync-engine #1707742 : tests/geo-rep: arequal checksum comparison always succeeds #1707746 : AFR-v2 does not log before attempting data self-heal #1708051 : Capture memory consumption for gluster process at the time of throwing no memory available message #1708156 : ec ignores lock contention notifications for partially acquired locks #1708163 : tests: fix bug-1319374.c compile warnings. #1708926 : Invalid memory access while executing cleanup_and_exit #1708929 : Add more test coverage for shd mux #1709248 : [geo-rep]: Non-root - Unable to set up mountbroker root directory and group #1709653 : geo-rep: With heavy rename workload geo-rep log if flooded #1710054 : Optimize the glustershd manager to send reconfigure #1710159 : glusterd: While upgrading (3-node cluster) 'gluster v status' times out on node to be upgraded #1711240 : [GNFS] gf_nfs_mt_inode_ctx serious memory leak #1711250 : bulkvoldict thread is not handling all volumes while brick multiplex is enabled #1711297 : Optimize glusterd code to copy dictionary in handshake code path #1711764 : Files inaccessible if one rebalance process is killed in a multinode volume #1711820 : Typo in cli return string. #1711827 : test case bug-1399598-uss-with-ssl.t is generating crash #1712322 : Brick logs inundated with [2019-04-27 22:14:53.378047] I [dict.c:541:dict_get] (-->/usr/lib64/glusterfs/6.0/xlator/features/worm.so(+0x7241) [0x7fe857bb3241] -->/usr/lib64/glusterfs/6.0/xlator/features/locks.so(+0x1c219) [0x7fe857dda219] [Invalid argumen #1712668 : Remove-brick shows warning cluster.force-migration enabled where as cluster.force-migration is disabled on the volume #1712741 : glusterd_svcs_stop should call individual wrapper function to stop rather than calling the glusterd_svc_stop #1713730 : Failure when glusterd is configured to bind specific IPv6 address. If bind-address is IPv6, *addr_len will be non-zero and it goes to ret = -1 branch, which will cause listen failure eventually #1714098 : Make debugging hung frames easier #1714415 : Script to make it easier to find hung frames #1714973 : upgrade after tier code removal results in peer rejection. #1715921 : uss.t tests times out with brick-mux regression #1716695 : Fix memory leaks that are present even after an xlator fini [client side xlator] #1716766 : [Thin-arbiter] TA process is not picking 24007 as port while starting up #1716812 : Failed to create volume which transport_type is \"tcp,rdma\" #1716830 : DHT: directory permissions are wiped out #1717757 : WORM: Segmentation Fault if bitrot stub do signature #1717782 : gluster v get all still showing storage.fips-mode-rchecksum off #1717819 : Changes to self-heal logic w.r.t. detecting metadata split-brains #1717953 : SELinux context labels are missing for newly added bricks using add-brick command #1718191 : Regression: Intermittent test failure for quick-read-with-upcall.t #1718273 : markdown formatting errors in files present under /doc directory of the project #1718316 : Ganesha-gfapi logs are flooded with error messages related to \"gf_uuid_is_null(gfid)) [Invalid argument]\" when lookups are running from multiple clients #1718338 : Upcall: Avoid sending upcalls for invalid Inode #1718848 : False positive logging of mount failure #1718998 : Fix test case \"tests/basic/afr/split-brain-favorite-child-policy.t\" failure #1720201 : Healing not proceeding during in-service upgrade on a disperse volume #1720290 : ctime changes: tar still complains file changed as we read it if uss is enabled #1720615 : [RHEL-8.1] yum update fails for rhel-8 glusterfs client packages 6.0-5.el8 #1720993 : tests/features/subdir-mount.t is failing for brick_mux regrssion #1721385 : glusterfs-libs: usage of inet_addr() may impact IPv6 #1721435 : DHT: Internal xattrs visible on the mount #1721441 : geo-rep: Fix permissions for GEOREP_DIR in non-root setup #1721601 : [SHD] : logs of one volume are going to log file of other volume #1722541 : stale shd process files leading to heal timing out and heal deamon not coming up for all volumes #1703322 : Need to document about fips-mode-rchecksum in gluster-7 release notes. #1722802 : Incorrect power of two calculation in mem_pool_get_fn #1723890 : Crash in glusterd when running test script bug-1699339.t #1728770 : Failures in remove-brick due to [Input/output error] errors #1736481 : capture stat failure error while setting the gfid #1739424 : Disperse volume : data corruption with ftruncate data in 4+2 config #1739426 : Open fd heal should filter O_APPEND/O_EXCL #1739427 : An Input/Output error happens on a disperse volume when doing unaligned writes to a sparse file #1741041 : atime/mtime is not restored after healing for entry self heals #1743200 : ./tests/bugs/glusterd/bug-1595320.t is failing #1744874 : interrupts leak memory #1745422 : ./tests/bugs/glusterd/bug-1595320.t is failing #1745914 : ESTALE change in fuse breaks get_real_filename implementation #1746142 : ctime: If atime is updated via utimensat syscall ctime is not getting updated #1746145 : CentOs 6 GlusterFS client creates files with time 01/01/1970 #1747301 : Setting cluster.heal-timeout requires volume restart #1747746 : The result (hostname) of getnameinfo for all bricks (ipv6 addresses) are the same, while they are not. #1748448 : syncop: Bail out if frame creation fails #1748774 : Incorrect power of two calculation in mem_pool_get_fn #1749155 : bug-1402841.t-mt-dir-scan-race.t fails spuriously #1749305 : Failures in remove-brick due to [Input/output error] errors #1749664 : The result (hostname) of getnameinfo for all bricks (ipv6 addresses) are the same, while they are not. #1751556 : syncop: Bail out if frame creation fails #1752245 : Crash in glusterd when running test script bug-1699339.t #1752429 : Ctime: Cannot see the \"trusted.glusterfs.mdata\" xattr for directory on a new brick after rebalance #1755212 : geo-rep: performance improvement while syncing heavy renames with existing destination #1755213 : geo-rep: non-root session going fault due improper sub-command #1755678 : Segmentation fault occurs while truncate file #1756002 : git clone fails on gluster volumes exported via nfs-ganesha","title":"Bugs addressed"},{"location":"release-notes/7.1/","text":"Release notes for Gluster 7.1 This is a bugfix release. The release notes for 7.0 contains a listing of all the new features that were added and bugs fixed in the GlusterFS 7 stable release. NOTE: Next minor release tentative date: Week of 20th Jan, 2020 Major changes, features and limitations addressed in this release None Major issues None Note GlusterFS-Samba Following parameters will be added to GlusterFS volume share section (if not present) in smb.conf when user.smb or user.cifs option is set on a volume: kernel share modes = no Following parameters will NOT be added to GlusterFS volume share section(if not present) in smb.conf when user.smb or user.cifs option is set on a volume: guest ok = yes Bugs addressed Bugs addressed since release-7.0 are listed below. #1760356 : packaging: remove leftover bd cruft in rpm .spec.in #1760699 : glustershd can not decide heald_sinks, and skip repair, so some entries lingering in volume heal info #1760791 : afr: support split-brain CLI for replica 3 #1761910 : Rebalance causing IO Error - File descriptor in bad state #1764003 : [Upgrade] Config files are not upgraded to new version #1764007 : geo-replication sessions going faulty #1764015 : geo-rep syncing significantly behind and also only one of the directories are synced with tracebacks seen #1764023 : geo-rep: Changelog archive file format is incorrect #1764026 : tests/geo-rep: Add test case to validate non-root geo-replication setup #1764028 : [geo-rep] sync_method showing rsync instead of tarssh post in-service upgrade #1764030 : [GSS] geo-rep entering into faulty state with OSError: [Errno 13] Permission denied #1765431 : test: fix non-root geo-rep test case #1766424 : cgroup control-cpu-load.sh script not working #1768742 : Memory leak in glusterfsd process #1768760 : tests/bugs/shard/unlinks-and-renames.t fails on RHEL8 #1769315 : Rebalance is causing glusterfs crash on client node #1769320 : Spurious failure tests/bugs/replicate/bug-1734370-entry-heal-restore-time.t #1771840 : [CENTOS 6] Geo-replication session not starting after creation #1775495 : [GNFS] showmout -a cause gnfs crash #1777769 : auth-allow of IPv4 address doesn't take netmask into consideration #1778175 : glusterfsd crashed with \"'MemoryError' Cannot access memory at address\" #1781483 : Remove guest access by default for GlusterFS volume SMB shares added by hook scripts #1781486 : gluster-smb:glusto-test access gluster by cifs test write report Device or resource busy #1782826 : event_slot_alloc not able to return index after reach slot_used count to 1024 #1783227 : GlusterFS brick process crash #1783858 : Heal Info is hung when I/O is in progress on a gluster block volume #1784790 : tests/00-geo-rep/00-georep-verify-non-root-setup.t fail on freshly installed builder #1785228 : Windows client fails to copy large file to GlusterFS volume share with fruit and streams_xattr VFS modules via Samba #1785493 : READDIRP incorrectly updates posix-acl inode ctx","title":7.1},{"location":"release-notes/7.1/#release-notes-for-gluster-71","text":"This is a bugfix release. The release notes for 7.0 contains a listing of all the new features that were added and bugs fixed in the GlusterFS 7 stable release. NOTE: Next minor release tentative date: Week of 20th Jan, 2020","title":"Release notes for Gluster 7.1"},{"location":"release-notes/7.1/#major-changes-features-and-limitations-addressed-in-this-release","text":"None","title":"Major changes, features and limitations addressed in this release"},{"location":"release-notes/7.1/#major-issues","text":"None","title":"Major issues"},{"location":"release-notes/7.1/#note","text":"","title":"Note"},{"location":"release-notes/7.1/#glusterfs-samba","text":"Following parameters will be added to GlusterFS volume share section (if not present) in smb.conf when user.smb or user.cifs option is set on a volume: kernel share modes = no Following parameters will NOT be added to GlusterFS volume share section(if not present) in smb.conf when user.smb or user.cifs option is set on a volume: guest ok = yes","title":"GlusterFS-Samba"},{"location":"release-notes/7.1/#bugs-addressed","text":"Bugs addressed since release-7.0 are listed below. #1760356 : packaging: remove leftover bd cruft in rpm .spec.in #1760699 : glustershd can not decide heald_sinks, and skip repair, so some entries lingering in volume heal info #1760791 : afr: support split-brain CLI for replica 3 #1761910 : Rebalance causing IO Error - File descriptor in bad state #1764003 : [Upgrade] Config files are not upgraded to new version #1764007 : geo-replication sessions going faulty #1764015 : geo-rep syncing significantly behind and also only one of the directories are synced with tracebacks seen #1764023 : geo-rep: Changelog archive file format is incorrect #1764026 : tests/geo-rep: Add test case to validate non-root geo-replication setup #1764028 : [geo-rep] sync_method showing rsync instead of tarssh post in-service upgrade #1764030 : [GSS] geo-rep entering into faulty state with OSError: [Errno 13] Permission denied #1765431 : test: fix non-root geo-rep test case #1766424 : cgroup control-cpu-load.sh script not working #1768742 : Memory leak in glusterfsd process #1768760 : tests/bugs/shard/unlinks-and-renames.t fails on RHEL8 #1769315 : Rebalance is causing glusterfs crash on client node #1769320 : Spurious failure tests/bugs/replicate/bug-1734370-entry-heal-restore-time.t #1771840 : [CENTOS 6] Geo-replication session not starting after creation #1775495 : [GNFS] showmout -a cause gnfs crash #1777769 : auth-allow of IPv4 address doesn't take netmask into consideration #1778175 : glusterfsd crashed with \"'MemoryError' Cannot access memory at address\" #1781483 : Remove guest access by default for GlusterFS volume SMB shares added by hook scripts #1781486 : gluster-smb:glusto-test access gluster by cifs test write report Device or resource busy #1782826 : event_slot_alloc not able to return index after reach slot_used count to 1024 #1783227 : GlusterFS brick process crash #1783858 : Heal Info is hung when I/O is in progress on a gluster block volume #1784790 : tests/00-geo-rep/00-georep-verify-non-root-setup.t fail on freshly installed builder #1785228 : Windows client fails to copy large file to GlusterFS volume share with fruit and streams_xattr VFS modules via Samba #1785493 : READDIRP incorrectly updates posix-acl inode ctx","title":"Bugs addressed"},{"location":"release-notes/7.2/","text":"Release notes for Gluster 7.2 This is a bugfix release. The release notes for 7.0 contains a listing of all the new features that were added and bugs fixed in the GlusterFS 7 stable release. NOTE: Next minor release tentative date: Week of 20th Feb, 2020 Major changes, features and limitations addressed in this release None Major issues None Bugs addressed Bugs addressed since release-7.1 are listed below. #1767264 : glusterfs client process coredump #1786753 : Functionality to enable log rotation for user serviceable snapshot's logs. #1788785 : Unable to set/modify optimistic-change-log for replicate volumes #1789336 : glusterfs process memory leak in ior test #1790423 : Glusterfind pre command fails #1790428 : glusterfind pre output file is empty #1790438 : S57glusterfind-delete-post.py not python3 ready (does not decode bytestring) #1790846 : Remove extra argument","title":7.2},{"location":"release-notes/7.2/#release-notes-for-gluster-72","text":"This is a bugfix release. The release notes for 7.0 contains a listing of all the new features that were added and bugs fixed in the GlusterFS 7 stable release. NOTE: Next minor release tentative date: Week of 20th Feb, 2020","title":"Release notes for Gluster 7.2"},{"location":"release-notes/7.2/#major-changes-features-and-limitations-addressed-in-this-release","text":"None","title":"Major changes, features and limitations addressed in this release"},{"location":"release-notes/7.2/#major-issues","text":"None","title":"Major issues"},{"location":"release-notes/7.2/#bugs-addressed","text":"Bugs addressed since release-7.1 are listed below. #1767264 : glusterfs client process coredump #1786753 : Functionality to enable log rotation for user serviceable snapshot's logs. #1788785 : Unable to set/modify optimistic-change-log for replicate volumes #1789336 : glusterfs process memory leak in ior test #1790423 : Glusterfind pre command fails #1790428 : glusterfind pre output file is empty #1790438 : S57glusterfind-delete-post.py not python3 ready (does not decode bytestring) #1790846 : Remove extra argument","title":"Bugs addressed"},{"location":"release-notes/7.3/","text":"Release notes for Gluster 7.3 This is a bugfix release. The release notes for 7.0 contains a listing of all the new features that were added and bugs fixed in the GlusterFS 7 stable release. NOTE: Next minor release tentative date: Week of 20th Mar, 2020 Major changes, features and limitations addressed in this release Features Make thin-arbiter name unique in 'pending-xattr' option. By making this unique, we can host single thin-arbiter node for multiple clusters. Major issues None Bugs addressed Bugs addressed since release-7.2 are listed below. #1768407 : glusterfsd memory leak observed after enable tls #1791154 : xlators/features/quota/src/quota.c:quota_log_usage #1793085 : gf_event doesn't work for glfsheal process #1793412 : config ssh-port can accept negative and outside allowed port range value #1793492 : cli: duplicate defns of cli_default_conn_timeout and cli_ten_minutes_timeout #1794019 : Mounts fails after reboot of 1/3 gluster nodes #1795540 : mem leak while using gluster tools #1802449 : spurious self-heald.t failure","title":7.3},{"location":"release-notes/7.3/#release-notes-for-gluster-73","text":"This is a bugfix release. The release notes for 7.0 contains a listing of all the new features that were added and bugs fixed in the GlusterFS 7 stable release. NOTE: Next minor release tentative date: Week of 20th Mar, 2020","title":"Release notes for Gluster 7.3"},{"location":"release-notes/7.3/#major-changes-features-and-limitations-addressed-in-this-release","text":"Features Make thin-arbiter name unique in 'pending-xattr' option. By making this unique, we can host single thin-arbiter node for multiple clusters.","title":"Major changes, features and limitations addressed in this release"},{"location":"release-notes/7.3/#major-issues","text":"None","title":"Major issues"},{"location":"release-notes/7.3/#bugs-addressed","text":"Bugs addressed since release-7.2 are listed below. #1768407 : glusterfsd memory leak observed after enable tls #1791154 : xlators/features/quota/src/quota.c:quota_log_usage #1793085 : gf_event doesn't work for glfsheal process #1793412 : config ssh-port can accept negative and outside allowed port range value #1793492 : cli: duplicate defns of cli_default_conn_timeout and cli_ten_minutes_timeout #1794019 : Mounts fails after reboot of 1/3 gluster nodes #1795540 : mem leak while using gluster tools #1802449 : spurious self-heald.t failure","title":"Bugs addressed"},{"location":"release-notes/7.4/","text":"Release notes for Gluster 7.4 This is a bugfix release. The release notes for 7.0 contains a listing of all the new features that were added and bugs fixed in the GlusterFS 7 stable release. NOTE: Next minor release tentative date: Week of 20th Apr, 2020 Major changes, features and limitations addressed in this release None Major issues None Bugs addressed Bugs addressed since release-7.3 are listed below. #1785323 : glusterfsd crashes after a few seconds #1804591 : Heal pending on volume, even after all the bricks are up #1805668 : Memory corruption when glfs_init() is called after glfs_fini() #1806843 : Disperse volume : Ganesha crash with IO in 4+2 config when one glusterfsd restarts every 600s #1807785 : seeing error message in glustershd.log on volume start(or may be as part of shd graph regeneration) inet_pton failed with return code 0 [Invalid argument] #1808964 : Set volume option when one of the nodes is powered off, After powering the node brick processes are offline #1809438 : [brickmux]: glustershd crashed when rebooting 1/3 nodes at regular intervals #1812849 : Setting volume option when one of the glusterds is stopped in the cluster, post glusterd restart seeing couldn't find vol info in glusterd logs and shd, brick process offline #1061 : [EC] shd crashed while heal failed due to out of memory error. #1030 : Memory corruption when sending events to an IPv6 host","title":7.4},{"location":"release-notes/7.4/#release-notes-for-gluster-74","text":"This is a bugfix release. The release notes for 7.0 contains a listing of all the new features that were added and bugs fixed in the GlusterFS 7 stable release. NOTE: Next minor release tentative date: Week of 20th Apr, 2020","title":"Release notes for Gluster 7.4"},{"location":"release-notes/7.4/#major-changes-features-and-limitations-addressed-in-this-release","text":"None","title":"Major changes, features and limitations addressed in this release"},{"location":"release-notes/7.4/#major-issues","text":"None","title":"Major issues"},{"location":"release-notes/7.4/#bugs-addressed","text":"Bugs addressed since release-7.3 are listed below. #1785323 : glusterfsd crashes after a few seconds #1804591 : Heal pending on volume, even after all the bricks are up #1805668 : Memory corruption when glfs_init() is called after glfs_fini() #1806843 : Disperse volume : Ganesha crash with IO in 4+2 config when one glusterfsd restarts every 600s #1807785 : seeing error message in glustershd.log on volume start(or may be as part of shd graph regeneration) inet_pton failed with return code 0 [Invalid argument] #1808964 : Set volume option when one of the nodes is powered off, After powering the node brick processes are offline #1809438 : [brickmux]: glustershd crashed when rebooting 1/3 nodes at regular intervals #1812849 : Setting volume option when one of the glusterds is stopped in the cluster, post glusterd restart seeing couldn't find vol info in glusterd logs and shd, brick process offline #1061 : [EC] shd crashed while heal failed due to out of memory error. #1030 : Memory corruption when sending events to an IPv6 host","title":"Bugs addressed"},{"location":"release-notes/7.5/","text":"Release notes for Gluster 7.5 This is a bugfix release. The release notes for 7.0 contains a listing of all the new features that were added and bugs fixed in the GlusterFS 7 stable release. NOTE: Next minor release tentative date: Week of 20th May, 2020 Major changes, features and limitations addressed in this release None Major issues None Bugs addressed Bugs addressed since release-7.4 are listed below. #832 Permission Denied in logs #884 [bug:1808688] Data corruption with asynchronous writes #1067 [bug:1661889] Metadata heal picks different brick each time as source if there are no pending xattrs. #1127 Mount crash during background shard cleanup #1134 snap_scheduler.py init failing with \"TypeError: Can't mix strings and bytes in path components\" #1152 Spurious failure of tests/bugs/protocol/bug-1433815-auth-allow.t #1168 glusterfsd crash due to health-check failed, going down ,system call errorno not return","title":7.5},{"location":"release-notes/7.5/#release-notes-for-gluster-75","text":"This is a bugfix release. The release notes for 7.0 contains a listing of all the new features that were added and bugs fixed in the GlusterFS 7 stable release. NOTE: Next minor release tentative date: Week of 20th May, 2020","title":"Release notes for Gluster 7.5"},{"location":"release-notes/7.5/#major-changes-features-and-limitations-addressed-in-this-release","text":"None","title":"Major changes, features and limitations addressed in this release"},{"location":"release-notes/7.5/#major-issues","text":"None","title":"Major issues"},{"location":"release-notes/7.5/#bugs-addressed","text":"Bugs addressed since release-7.4 are listed below. #832 Permission Denied in logs #884 [bug:1808688] Data corruption with asynchronous writes #1067 [bug:1661889] Metadata heal picks different brick each time as source if there are no pending xattrs. #1127 Mount crash during background shard cleanup #1134 snap_scheduler.py init failing with \"TypeError: Can't mix strings and bytes in path components\" #1152 Spurious failure of tests/bugs/protocol/bug-1433815-auth-allow.t #1168 glusterfsd crash due to health-check failed, going down ,system call errorno not return","title":"Bugs addressed"},{"location":"release-notes/7.6/","text":"Release notes for Gluster 7.6 This is a bugfix release. The release notes for 7.0 , 7.1 , 7.2 , 7.3 , 7.4 and 7.5 contains a listing of all the new features that were added and bugs fixed in the GlusterFS 7 stable release. NOTE: Next minor release tentative date: Week of 20th Jul, 2020 Major changes, features and limitations addressed in this release None Major issues None Bugs addressed Bugs addressed since release-7.5 are listed below. #1060 [bug:789278] Issues reported by Coverity static analysis tool #1140 getfattr returns ENOATTR for system.posix_acl_access on dispe... #1146 gfapi/Upcall: Potential deadlock in synctask threads processi... #1179 gnfs split brain when 1 server in 3x1 down (high load) #1000 [bug:1193929] GlusterFS can be improved","title":7.6},{"location":"release-notes/7.6/#release-notes-for-gluster-76","text":"This is a bugfix release. The release notes for 7.0 , 7.1 , 7.2 , 7.3 , 7.4 and 7.5 contains a listing of all the new features that were added and bugs fixed in the GlusterFS 7 stable release. NOTE: Next minor release tentative date: Week of 20th Jul, 2020","title":"Release notes for Gluster 7.6"},{"location":"release-notes/7.6/#major-changes-features-and-limitations-addressed-in-this-release","text":"None","title":"Major changes, features and limitations addressed in this release"},{"location":"release-notes/7.6/#major-issues","text":"None","title":"Major issues"},{"location":"release-notes/7.6/#bugs-addressed","text":"Bugs addressed since release-7.5 are listed below. #1060 [bug:789278] Issues reported by Coverity static analysis tool #1140 getfattr returns ENOATTR for system.posix_acl_access on dispe... #1146 gfapi/Upcall: Potential deadlock in synctask threads processi... #1179 gnfs split brain when 1 server in 3x1 down (high load) #1000 [bug:1193929] GlusterFS can be improved","title":"Bugs addressed"},{"location":"release-notes/7.7/","text":"Release notes for Gluster 7.7 This is a bugfix release. The release notes for 7.0 , 7.1 , 7.2 , 7.3 , 7.4 7.5 and 7.6 contains a listing of all the new features that were added and bugs fixed in the GlusterFS 7 stable release. NOTE: Next minor release tentative date: Week of 20th Sep, 2020 Major changes, features and limitations addressed in this release None Major issues None Bugs addressed Bugs addressed since release-7.6 are listed below. #1000 [bug:1193929] GlusterFS can be improved #1220 cluster/ec: return correct error code and log the message in ... #1223 Failure of tests/basic/gfapi/gfapi-copy-file-range.t #1225 fuse causes glusterd to dump core #1243 Modify and return iatt (especially size and block-count) in s... #1254 Prioritize ENOSPC over other lesser priority errors #1296 Implement seek in open-behind #1303 Failures in rebalance due to [Input/output error] #1348 Fuse mount crashes in shard translator when truncating a *real...","title":7.7},{"location":"release-notes/7.7/#release-notes-for-gluster-77","text":"This is a bugfix release. The release notes for 7.0 , 7.1 , 7.2 , 7.3 , 7.4 7.5 and 7.6 contains a listing of all the new features that were added and bugs fixed in the GlusterFS 7 stable release. NOTE: Next minor release tentative date: Week of 20th Sep, 2020","title":"Release notes for Gluster 7.7"},{"location":"release-notes/7.7/#major-changes-features-and-limitations-addressed-in-this-release","text":"None","title":"Major changes, features and limitations addressed in this release"},{"location":"release-notes/7.7/#major-issues","text":"None","title":"Major issues"},{"location":"release-notes/7.7/#bugs-addressed","text":"Bugs addressed since release-7.6 are listed below. #1000 [bug:1193929] GlusterFS can be improved #1220 cluster/ec: return correct error code and log the message in ... #1223 Failure of tests/basic/gfapi/gfapi-copy-file-range.t #1225 fuse causes glusterd to dump core #1243 Modify and return iatt (especially size and block-count) in s... #1254 Prioritize ENOSPC over other lesser priority errors #1296 Implement seek in open-behind #1303 Failures in rebalance due to [Input/output error] #1348 Fuse mount crashes in shard translator when truncating a *real...","title":"Bugs addressed"},{"location":"release-notes/7.8/","text":"Release notes for Gluster 7.8 This is a bugfix release. The release notes for 7.0 , 7.1 , 7.2 , 7.3 , 7.4 7.5 , 7.6 and 7.7 contain a listing of all the new features that were added and bugs fixed in the GlusterFS 7 stable release. NOTE: Next minor release tentative date: Week of 30th Nov, 2020. Next minor release would be the last release of release-7.x series. Highlights of Release This release contains majorly the bug fixes as described in the issues section. Builds are available at https://download.gluster.org/pub/gluster/glusterfs/7/7.8/ Issues addressed in this release Please find the list of issues added to this release below. #763 thin-arbiter: Testing report #1000 [bug:1193929] GlusterFS can be improved #1002 [bug:1679998] GlusterFS can be improved #1250 geo-rep: Fix corner case in rename on mkdir during hybrid crawl #1253 On Ovirt setup glusterfs performs poorly #1332 Unable to Upgrade to Gluster 7 from Earlier Version #1351 issue with gf_fill_iatt_for_dirent() #1354 High CPU utilization by self-heal on disperse volumes with no ... #1385 High CPU utilization by self-heal on disperse volumes when an ... #1407 glusterd keep crashing when upgrading from 6.5 to 7.7 #1438 syncdaemon/syncdutils.py: SyntaxWarning: \"is\" with a literal. ... #1440 glusterfs 7.7 fuse client memory leak #1472 Readdir-ahead leads to inconsistent ls results","title":7.8},{"location":"release-notes/7.8/#release-notes-for-gluster-78","text":"This is a bugfix release. The release notes for 7.0 , 7.1 , 7.2 , 7.3 , 7.4 7.5 , 7.6 and 7.7 contain a listing of all the new features that were added and bugs fixed in the GlusterFS 7 stable release. NOTE: Next minor release tentative date: Week of 30th Nov, 2020. Next minor release would be the last release of release-7.x series.","title":"Release notes for Gluster 7.8"},{"location":"release-notes/7.8/#highlights-of-release","text":"This release contains majorly the bug fixes as described in the issues section.","title":"Highlights of Release"},{"location":"release-notes/7.8/#builds-are-available-at","text":"https://download.gluster.org/pub/gluster/glusterfs/7/7.8/","title":"Builds are available at"},{"location":"release-notes/7.8/#issues-addressed-in-this-release","text":"Please find the list of issues added to this release below. #763 thin-arbiter: Testing report #1000 [bug:1193929] GlusterFS can be improved #1002 [bug:1679998] GlusterFS can be improved #1250 geo-rep: Fix corner case in rename on mkdir during hybrid crawl #1253 On Ovirt setup glusterfs performs poorly #1332 Unable to Upgrade to Gluster 7 from Earlier Version #1351 issue with gf_fill_iatt_for_dirent() #1354 High CPU utilization by self-heal on disperse volumes with no ... #1385 High CPU utilization by self-heal on disperse volumes when an ... #1407 glusterd keep crashing when upgrading from 6.5 to 7.7 #1438 syncdaemon/syncdutils.py: SyntaxWarning: \"is\" with a literal. ... #1440 glusterfs 7.7 fuse client memory leak #1472 Readdir-ahead leads to inconsistent ls results","title":"Issues addressed in this release"},{"location":"release-notes/7.9/","text":"Release notes for Gluster 7.9 This is a bugfix release. The release notes for 7.0 , 7.1 , 7.2 , 7.3 , 7.4 7.5 , 7.6 , 7.7 and 7.8 contain a listing of all the new features that were added and bugs fixed in the GlusterFS 7 stable release. NOTE: This release would be the last release of release-7.x series. Users are highly encouraged to upgrade to newer releases of GlusterFS. Highlights of Release This release contains majorly the bug fixes as described in the issues section. Builds are available at https://download.gluster.org/pub/gluster/glusterfs/7/7.9/ Issues addressed in this release Please find the list of issues added to this release below. #1852 glusterd: Can't run rebalance due to long unix socket #1836 posix: Update ret value in posix_get_gfid2path if GF_MALLOC fails #1738 [cli] Improper error message on command timeout #1699 One brick offline with signal received: 11 #1604 rfc.sh on release-7 needs to move to github flow #1499 why not use JumpConsistentHash to replace SuperFastHash to cho... #1221 features/bit-rot: invalid snprintf() buffer size #1060 [bug:789278] Issues reported by Coverity static analysis tool","title":7.9},{"location":"release-notes/7.9/#release-notes-for-gluster-79","text":"This is a bugfix release. The release notes for 7.0 , 7.1 , 7.2 , 7.3 , 7.4 7.5 , 7.6 , 7.7 and 7.8 contain a listing of all the new features that were added and bugs fixed in the GlusterFS 7 stable release. NOTE: This release would be the last release of release-7.x series. Users are highly encouraged to upgrade to newer releases of GlusterFS.","title":"Release notes for Gluster 7.9"},{"location":"release-notes/7.9/#highlights-of-release","text":"This release contains majorly the bug fixes as described in the issues section.","title":"Highlights of Release"},{"location":"release-notes/7.9/#builds-are-available-at","text":"https://download.gluster.org/pub/gluster/glusterfs/7/7.9/","title":"Builds are available at"},{"location":"release-notes/7.9/#issues-addressed-in-this-release","text":"Please find the list of issues added to this release below. #1852 glusterd: Can't run rebalance due to long unix socket #1836 posix: Update ret value in posix_get_gfid2path if GF_MALLOC fails #1738 [cli] Improper error message on command timeout #1699 One brick offline with signal received: 11 #1604 rfc.sh on release-7 needs to move to github flow #1499 why not use JumpConsistentHash to replace SuperFastHash to cho... #1221 features/bit-rot: invalid snprintf() buffer size #1060 [bug:789278] Issues reported by Coverity static analysis tool","title":"Issues addressed in this release"},{"location":"release-notes/8.0/","text":"Release notes for Gluster 8.0 Release date: 09-July-2020 This is a major release that includes a range of features, code improvements and stability fixes as noted below. A selection of the key features and changes are documented in this page. A full list of bugs that have been addressed is included further below. Announcements Major changes and features Major issues Bugs addressed in the release Announcements Releases that receive maintenance updates post release 8 are, 7 and 8 ( reference ) Release 8 will receive maintenance updates around the 10th of every month for the first 3 months post release (i.e Aug'20, Sep'20, Oct'20). Post the initial 3 months, it will receive maintenance updates every 2 months till EOL. Major changes and features Highlights Several stability fixes addressing coverity, clang-scan, address sanitizer and valgrind reported issues removal of unused and hence, deprecated code and features Performance Improvements CentOS 8 and RHEL 8 is supported Features Implemented seek file operation for open-behind Now storage.reserve option will take size of disk as input instead of percentage Added Functionality to enable log rotation for user serviceable snapshot's logs Mandatory locks enhancements in replicate subvolumes To validate other memory allocation implementations instead of libc's malloc added an option to build with tcmalloc library Integrated Thin-arbiter with GD1 Client Handling of Elastic Clusters The package glusterfs-libs is replaced by libgfchangelog0 , libgfrpc0 , libgfxdr0 , and libglusterfs0 ; and additional libraries in libgfapi0 , libglusterd0 Major issues None Bugs addressed Bugs addressed since release-7 are listed below. #789278 : Issues reported by Coverity static analysis tool #1158130 : Not possible to disable fopen-keeo-cache when mounting #1183054 : rpmlint throws couple of errors for RPM spec file #1193929 : GlusterFS can be improved #1387404 : geo-rep: gsync-sync-gfid binary installed in /usr/share/... #1410439 : glusterfind pre output file is empty #1423442 : group files to set volume options should have comments #1430623 : pthread mutexes and condition variables are not destroyed #1489610 : glusterfind saves var data under $prefix instead of localstatedir #1507896 : glfs_init returns incorrect errno on faliure #1514683 : Removal of bricks in volume isn't prevented if remaining brick doesn't contain all the files #1538900 : Found a missing unref in rpc_clnt_reconnect #1554286 : Xattr not updated if increasing the retention of a WORM/Retained file #1593542 : ctime: Upgrade/Enabling ctime feature wrongly updates older files with latest {a|m|c}time #1620580 : Deleted a volume and created a new volume with similar but not the same name. The kubernetes pod still keeps on running and doesn't crash. Still possible to write to gluster mount #1622665 : clang-scan report: glusterfs issues #1626543 : dht/tests: Create a .t to test all possible combinations for file rename #1635688 : Keep only the valid (maintained/supported) components in the build #1636297 : Make it easy to build / host a project which just builds glusterfs translator #1644322 : flooding log with \"glusterfs-fuse: read from /dev/fuse returned -1 (Operation not permitted)\" #1651445 : [RFE] storage.reserve option should take size of disk as input instead of percentage #1664335 : [geo-rep]: Transport endpoint not connected with arbiter volumes #1665358 : allow regression to not run tests with nfs, if nfs is disabled. #1668239 : [man page] Gluster(8) - Missing disperse-data parameter Gluster Console Manager man page #1668286 : READDIRP incorrectly updates posix-acl inode ctx #1676479 : read-ahead and io-cache degrading performance on sequential read #1688115 : Data heal not checking for locks on source & sink(s) before healing #1689097 : gfapi: provide an option for changing statedump path in glfs-api. #1690454 : mount-shared-storage.sh does not implement mount options #1693692 : Increase code coverage from regression tests #1694920 : Inconsistent locking in presence of disconnects #1697293 : DHT: print hash and layout values in hexadecimal format in the logs #1698042 : quick-read cache invalidation feature has the same key of md-cache #1707731 : [Upgrade] Config files are not upgraded to new version #1708603 : [geo-rep]: Note section in document is required for ignore_deletes true config option where it might delete a file #1708929 : Add more test coverage for shd mux #1716695 : Fix memory leaks that are present even after an xlator fini [client side xlator] #1716979 : Multiple disconnect events being propagated for the same child #1717754 : Enable features.locks-notify-contention by default #1717824 : Fencing: Added the tcmu-runner ALUA feature support but after one of node is rebooted the glfs_file_lock() get stucked #1717827 : tests/geo-rep: Add test case to validate non-root geo-replication setup #1719290 : Glusterfs mount helper script not working with IPv6 because of regular expression or man is wrong #1720463 : [Thin-arbiter] : Wait for connection with TA node before sending lookup/create of ta-replica id file #1720566 : Can't rebalance GlusterFS volume because unix socket's path name is too long #1721590 : tests/bugs/posix/bug-1040275-brick-uid-reset-on-volume-restart.t is failing #1721686 : Remove usage of obsolete function usleep() #1722507 : Incorrect reporting of type/gfid mismatch #1722541 : stale shd process files leading to heal timing out and heal deamon not coming up for all volumes #1722546 : do not assert in inode_unref if the inode table cleanup has started #1722598 : dump the min and max latency of each xlator in statedump #1722698 : DHT: severe memory leak in dht rename #1722740 : [GSS] geo-replication sessions going faulty #1722802 : Incorrect power of two calculation in mem_pool_get_fn #1722977 : ESTALE change in fuse breaks get_real_filename implementation #1723280 : windows cannot access mountpoint exportd from a disperse volume #1723402 : Brick multiplexing is not working. #1723455 : volume set group description missing space leading to words being merged in help output #1723658 : [In-service] Post upgrade glusterd is crashing with a backtrace on the upgraded node while issuing gluster volume status from non-upgraded nodes #1723761 : [Ganesha]: truncate operation not updating the ctime #1723890 : Crash in glusterd when running test script bug-1699339.t #1724024 : use more secure mode for mkdir operations #1724184 : Thin-arbiter: SHD takes lock and inspects the state on TA during every index crawl #1725034 : gluster volume help showing multiple commands for top instead of one. #1725211 : User serviceable snapshots (USS) are not accessible after changing transport.socket.bind-address of glusterd #1726205 : Windows client fails to copy large file to GlusterFS volume share with fruit and streams_xattr VFS modules via Samba #1726783 : snapd crashes sometimes #1726906 : get-state does not show correct brick status #1727068 : Deadlock when generating statedumps #1727081 : Disperse volume : data corruption with ftruncate data in 4+2 config #1727107 : geo-replication/setup.py missing license details in setup() #1727248 : [GNFS] showmout -a cause gnfs crash #1727256 : Directory pending heal in heal info output #1727329 : glustershd dumped core with seg fault at afr_has_quorum #1727852 : gluster-block: improvements to volume group profile options list #1728047 : interrupts leak memory #1728417 : Cleanup references to Hadoop in code base #1728554 : Spelling errors #1728683 : [geo-rep] gluster-mountbroker missing a brief description of what the argument does in # gluster-mountbroker (add|remove|setup) --help #1728766 : Volume start failed when shd is down in one of the node in cluster #1728770 : Failures in remove-brick due to [Input/output error] errors #1729085 : [EC] shd crashed while heal failed due to out of memory error. #1729107 : Memory leak in glusterfsd process #1729463 : gluster v geo-rep status command timing out #1729772 : Disperse volume : Ganesha crash with IO in 4+2 config when one glusterfsd restart every 600s #1729847 : Fix spurious failure of tests/bugs/replicate/bug-1717819-metadata-split-brain-detection.t #1730175 : Seeing failure due to \"getxattr err for dir [No data available]\" in rebalance #1730409 : core file generated - when EC volume stop and start is executed for 10 loops on a EC+Brickmux setup #1730715 : An Input/Output error happens on a disperse volume when doing unaligned writes to a sparse file #1730953 : mount generates errors after umount #1731920 : [geo-rep]: gluster command not found while setting up a non-root session #1732496 : [Coverity] RETURN_LOCAL in __nlc_inode_ctx_get() #1732717 : fuse: Limit the number of inode invalidation requests in the queue #1733042 : cluster.rc Create separate logdirs for each host instance #1733166 : potential deadlock while processing callbacks in gfapi #1733425 : Setting volume option when one of the glusterd is stopped in the cluster, post glusterd restart seeing couldn't find vol info in glusterd logs and shd, brick process offline #1733935 : Open fd heal should filter O_APPEND/O_EXCL #1734026 : Cannot see the \"trusted.glusterfs.mdata\" xattr for directory on a new brick after rebalance #1734252 : Heal not completing after geo-rep session is stopped on EC volumes. #1734299 : ctime: When healing ctime xattr for legacy files, if multiple clients access and modify the same file, the ctime might be updated incorrectly. #1734370 : atime/mtime is not restored after healing for entry self heals #1734738 : Unable to create geo-rep session on a non-root setup. #1736482 : capture stat failure error while setting the gfid #1737288 : nfs client gets bad ctime for copied file which is on glusterfs disperse volume with ctime on #1737291 : features/locks: avoid use after freed of frame for blocked lock #1737484 : geo-rep syncing significantly behind and also only one of the directories are synced with tracebacks seen #1737676 : Upgrading a Gluster node fails when user edited glusterd.vol file exists #1737778 : ocf resource agent for volumes don't work in non-standard environment #1738419 : read() returns more than file size when using direct I/O #1738763 : [EC] : fix coverity issue #1738786 : ctime: If atime is updated via utimensat syscall ctime is not getting updated #1739360 : [GNFS] gluster crash with nfs.nlm off #1740017 : tests/bugs/replicate/bug-880898.t created a core file. #1741734 : gluster-smb:glusto-test access gluster by cifs test write report Device or resource busy #1741779 : Fix spelling errors #1741890 : geo-rep: Changelog archive file format is incorrect #1743020 : glusterd start is failed and throwing an error Address already in use #1743069 : bug-1482023-snpashot-issue-with-other-processes-accessing-mounted-path.t fails in brick mux regression spuriously #1743094 : glusterfs build fails on centos7 #1743200 : ./tests/bugs/glusterd/bug-1595320.t is failing #1743573 : fuse client hung when issued a lookup \"ls\" on an ec volume #1743652 : CentOs 6 GlusterFS client creates files with time 01/01/1970 #1744519 : log aio_error return codes in posix_fs_health_check #1744548 : Setting cluster.heal-timeout requires volume restart #1745965 : glusterd fails to start due to SIGABRT dumping core #1745967 : File size was not truncated for all files when tried with rebalance in progress. #1746228 : systemctl start glusterd is getting timed out on the scaled setup with 2000 volumes #1746320 : SHORT-WRITE error leads to crash #1746810 : markdown files containing 404 links #1747746 : The result (hostname) of getnameinfo for all bricks (ipv6 addresses) are the same, while they are not. #1748448 : syncop: Bail out if frame creation fails #1748744 : bug-1402841.t-mt-dir-scan-race.t fails spuriously #1748836 : Application should know when update size/version went bad #1749322 : glustershd can not decide heald_sinks, and skip repair, so some entries lingering in volume heal info #1750387 : Deprecated log rotate command still present in \"# gluster v help\" #1750618 : Cleanup of executable in tests/bugs/gfapi/bug-1447266/bug-1447266.t not done #1751134 : Spurious failure tests/bugs/replicate/bug-1734370-entry-heal-restore-time.t #1751907 : bricks gone down unexpectedly #1752330 : seeing error message in glustershd.log on volume start(or may be as part of shd graph regeneration) inet_pton failed with return code 0 [Invalid argument] #1752331 : Test tests/basic/volume-scale-shd-mux.t is failing on upstream CI #1753569 : git clone fails on gluster volumes exported via nfs-ganesha #1753592 : Segmentation fault occurs while truncate file #1753843 : [Disperse volume]: Regression in IO performance seen in sequential read for large file #1753857 : geo-rep: performance improvement while syncing heavy renames with existing destination #1753859 : Typos in glusterd log messages #1753880 : Set the default lru-limit in fuse to a smaller number #1753928 : geo-rep: non-root session going fault due improper sub-command #1754448 : Re-alignment of Structure attributes #1754477 : Thin-arbiter: Raise error in CLI if replica-count is not 2 #1755344 : glustershd.log getting flooded with \"W [inode.c:1017:inode_find] (-->/usr/lib64/glusterfs/6.0/xlator/cluster/disperse.so(+0xe3f9) [0x7fd09b0543f9] -->/usr/lib64/glusterfs/6.0/xlator/cluster/disperse.so(+0xe19c) [0x7fd09b05419 TABLE NOT FOUND\" #1755900 : heketidbstorage bricks go down during PVC creation #1756211 : tests/bugs/shard/bug-1272986.t fails #1756900 : tests are failing in RHEL8 regression #1756938 : afr: support split-brain CLI for replica 3 #1757399 : Rebalance is causing glusterfs crash on client node #1758579 : Rebalance causing IO Error - File descriptor in bad state #1758878 : # gluster v info --xml is always returning 3 for all Nx3 volumes #1758984 : Enable direct-io options in group virt #1759002 : Spurious failure tests/bugs/replicate/bug-1744548-heal-timeout.t #1759081 : Spurious failure of /tests/bugs/replicate/bug-1134691-afr-lookup-metadata-heal.t #1760187 : Implement seek fop #1760189 : Use replica aware seek fop #1760467 : rebalance start is succeeding when quorum is not met #1761759 : Failure in ./tests/basic/posix/shared-statfs.t #1761769 : On some distros bug-1272986.t takes more than 2 minutes to run #1762220 : [geo-rep] sync_method showing rsync instead of tarssh post in-service upgrade #1762438 : DHT- gluster rebalance status shows wrong data size after rebalance is completed successfully #1763036 : glusterfsd crashed with \"'MemoryError' Cannot access memory at address\" #1763439 : [GSS] geo-rep entering into faulty state with OSError: [Errno 13] Permission denied #1764110 : tests/bugs/shard/unlinks-and-renames.t fails on RHEL8 #1764119 : gluster rebalance status doesn't show detailed information when a node is rebooted #1764129 : quota_fsck script KeyError: 'contri_size' #1764208 : cgroup control-cpu-load.sh script not working #1764418 : Add Mohit & Sanju as glusterd/cli maintainers #1765017 : gf_event doesn't work for glfsheal process #1765155 : replication shouldn't modify xattr-req coming from parent #1765186 : Problematic coding practices at logger #1765421 : DHT: Add comments to the code #1765426 : test: fix non-root geo-rep test case #1765542 : Add Sunny Kumar as co-maintainer of Geo-replication component #1768407 : glusterfsd memory leak observed after enable tls #1768896 : Long method in glusterfsd - set_fuse_mount_options(...) #1769712 : check if grapj is ready beforce process cli command #1769754 : dht_readdirp_cbk: Do not strip out entries with invalid stats #1771365 : libglusterfs/dict.c : memory leaks #1771577 : [RHEL 6] Geo-replication session not starting after creation #1771895 : geo-rep: Improve debugging in log_raise_exception #1772006 : NULL dict messages flooding fuse mount log #1773530 : ctime value is different from atime/mtime on a create of file #1773856 : Set volume option when one of the node is powered off, After powering the node brick processes are offline #1774011 : Heal Info is hung when I/O is in progress on a gluster block volume #1774866 : man page update needed for gluster volume top command #1775612 : Remove guest access by default for GlusterFS volume SMB shares added by hook scripts #1776264 : RFE: systemd should restart glusterd on crash #1776757 : DHT - Reduce methods scope #1776784 : glfsheal crash on unexpected volume name #1776801 : Bricks are not available when volume create fails #1776892 : [patch] .dirstamp should be in ignored #1778457 : Missing error logs(afr/self-heald ) #1779055 : glusterfs process memory leak in ior test #1779089 : glusterfsd do not release posix lock when multiple glusterfs client do flock -xo to the same file paralleled #1779742 : tests/00-geo-rep/00-georep-verify-non-root-setup.t fail on freshly installed builder #1779760 : Improve logging in EC, client and lock xlator #1780190 : glfsheal should be installed and invoked as architecture-dependent binary helper #1780260 : v7 fails to build on Debian 9 [patch?] #1781440 : event_slot_alloc not able to return index after reach slot_used count to 1024 #1782200 : glusterd restart failing to start. #1782495 : GlusterFS brick process crash #1784375 : 'gluster volume set disable.nfs' accidentally killed unexpected process, and forced a data brick offline. #1785143 : Multiple glusterfsd process spawn when glusterd restart during a volume start. #1785208 : glusterfs client process coredump #1785611 : glusterfsd cashes after a few seconds #1785998 : change the error message for heal statistics to reflect its supportability for disperse volume #1786276 : [geo-rep] Help for positional argument SLAVE in schedule_georep.py.in isn't clear. #1786459 : unable to enable brick-multiplex feature #1786478 : default option is disappeared in volume info after volume reset #1786679 : Duplicate entries in 'ls' output after a volume expansion #1786722 : Functionality to enable log rotation for user serviceable snapshot's logs. #1787122 : glusterd allowing to set server.statedump-path to file, non-existent file and non-existent paths #1787274 : heal not actually healing metadata of a regular file when only time stamps are changed(data heal not required) #1787554 : Unable to set/modify optimistic-change-log for replicate volumes #1789439 : Glusterfind pre command fails #1789478 : S57glusterfind-delete-post.py not python3 ready (does not decode bytestring) #1790748 : Remove extra argument #1790870 : Memory corruption when sending events to an IPv6 host #1791682 : fail to build on recent Fedora #1792276 : config ssh-port can accept negative and outside allowed port range value #1792707 : xlators/features/quota/src/quota.c:quota_log_usage #1793378 : dht_hash_compute() crashes when it receives a zero length name #1793852 : Mounts fails after reboot of 1/3 gluster nodes #1793995 : gluster crash when built without gNFS support #1797869 : bitrot: Number of signing process threads should be configurable. #1797882 : Segmentation fault occurs while truncate file #1797934 : Client should propagate ping event from brick #1800583 : Halo replication is not working #1800956 : Rebalance : Status lists failures on stopping rebalance while it is in progress #1801623 : spurious self-heald.t failure #1801624 : Heal pending on volume, even after all the bricks are up #1801684 : Memory corruption when glfs_init() is called after glfs_fini() #1804786 : mount.glusterfs strips off \"/\" from subdir-mounts #1808421 : WORM: If autocommit-period 0 file will be WORMed with 0 Byte during initial write #1808875 : [brickmux]: glustershd crashed when rebooting 1/3 nodes at regular intervals #1810042 : Changes to gluster peer probe in nightly build breaks ansible:gluster_volume call #1810842 : frequent heal observed when file opened during one brick is down #1810934 : Segfault in FUSE process, potential use after free #1811631 : brick crashed when creating and deleting volumes over time (with brick mux enabled only) #1812144 : Add a warning message during volume expansion or resize on volume with snapshots #1812353 : create-export-ganesha script: mention labelled nfs parameter #154 Optimized CHANGELOG #237 Validate other memory allocation implementations instead of l... #475 Reduce the number or threads used in the brick process #613 Mandatory locks enhancements in replicate subvolumes #657 Structured logging format support #663 Add Ganesha HA bits back to glusterfs code repo #687 Thin-arbiter integration with GD1 #699 executable program will crash if linked libgfapi together wit... #703 provide mechanism to test individual xlators #721 Introduce quorum-count option in disperse volumes as well #723 Provide scripts to reset xattrs of the entries which could be... #725 Disperse: A way to read from specific bricks #741 Client Handling of Elastic Clusters #745 storage.reserve enhancement for posix_write #748 Improve MAKE_HANDLE_GFID_PATH macro and posix_handle_gfid_path() #753 Remove fetching items in gf_cli_replace_brick(), gf_cli_reset... #755 [RFE] Geo-replication code improvements #761 Improve MAKE_HANDLE_PATH macro #763 thin-arbiter: Testing report #765 nfs.rpc-auth-allow gluster7 + gnfs #788 run-with-valgrind option causes gnfs and quota to fail to start #824 Migrate bugzilla workflow to github issues workflow #832 Permission Denied in logs #884 [bug:1808688] Data corruption with asynchronous writes (pleas... #891 [bug:1802451] Optimize posix code to improve file creation #977 [bug:1811631] brick crashed when creating and deleting volume... #999 [bug:1791285] Changing permissions on root directory(director... #1000 [bug:1193929] GlusterFS can be improved #1038 [bug:1787138] Crash on rpcsvc_drc_client_unref() - fails on G... #1042 [bug:1806499] afr-lock-heal-basic.t and /afr-lock-heal-advanc... #1044 [bug:1790730] Add a basic test file to glusterfind #1052 [bug:1693692] Increase code coverage from regression tests #1060 [bug:789278] Issues reported by Coverity static analysis tool #1067 [bug:1661889] Metadata heal picks different brick each time a... #1097 [bug:1635688] Keep only the valid (maintained/supported) comp... #1102 dht: gf_defrag_process_dir is called even if gf_defrag_fix_la... #1104 geo-replication: descriptive message when worker crashes due ... #1105 [bug:1794263] Multiple imports from the same library in the .... #1127 Mount crash during background shard cleanup #1134 snap_scheduler.py init failing with \"TypeError: Can't mix str... #1140 getfattr returns ENOATTR for system.posix_acl_access on dispe... #1141 Make SSL connection messages useful #1142 log the ENOENT error in posix_pstat #1144 [Disperse] Add test for reset-brick for disperse volume #1146 gfapi/Upcall: Potential deadlock in synctask threads processi... #1149 Add error logs to debug failures in ./tests/bugs/protocol/bug... #1150 Avoid dict_del logs in posix_is_layout_stale while key is NULL #1152 Spurious failure of tests/bugs/protocol/bug-1433815-auth-allow.t #1153 Spurious failure of ./tests/bugs/snapshot/bug-1111041.t #1154 failing test cases #1156 Spurious failure of tests/features/worm.t #1158 spurious failure of tests/bugs/glusterd/serialize-shd-manager... #1160 sys_stat should be used instead of stat #1161 tests: file offsets and sizes shouldn't be truncated to 32-bi... #1162 spurious failure of test case tests/bugs/glusterd/removing-mu... #1169 common-ha: cluster status shows \"FAILOVER\" even when all node... #1180 (glusterfs-8.0) - GlusterFS 8.0 tracker #1179 gnfs split brain when 1 server in 3x1 down (high load) #1220 cluster/ec: return correct error code and log the message in case of BADFD #1223 Failure of tests/basic/gfapi/gfapi-copy-file-range.t #1116 [bug:1790736] gluster volume list returning wrong volume list / volume list time out #990 [bug:1578405] EIO errors when updating and deleting entries co... #1126 packaging: overhaul glusterfs.spec(.in) to align with SUSE and... #1225 fuse causes glusterd to dump core #1243 Modify and return iatt (especially size and block-count) in sh... #1254 Prioritize ENOSPC over other lesser priority errors #1303 Failures in rebalance due to [Input/output error]","title":8.0},{"location":"release-notes/8.0/#release-notes-for-gluster-80","text":"Release date: 09-July-2020 This is a major release that includes a range of features, code improvements and stability fixes as noted below. A selection of the key features and changes are documented in this page. A full list of bugs that have been addressed is included further below. Announcements Major changes and features Major issues Bugs addressed in the release","title":"Release notes for Gluster 8.0"},{"location":"release-notes/8.0/#announcements","text":"Releases that receive maintenance updates post release 8 are, 7 and 8 ( reference ) Release 8 will receive maintenance updates around the 10th of every month for the first 3 months post release (i.e Aug'20, Sep'20, Oct'20). Post the initial 3 months, it will receive maintenance updates every 2 months till EOL.","title":"Announcements"},{"location":"release-notes/8.0/#major-changes-and-features","text":"","title":"Major changes and features"},{"location":"release-notes/8.0/#highlights","text":"Several stability fixes addressing coverity, clang-scan, address sanitizer and valgrind reported issues removal of unused and hence, deprecated code and features Performance Improvements CentOS 8 and RHEL 8 is supported","title":"Highlights"},{"location":"release-notes/8.0/#features","text":"Implemented seek file operation for open-behind Now storage.reserve option will take size of disk as input instead of percentage Added Functionality to enable log rotation for user serviceable snapshot's logs Mandatory locks enhancements in replicate subvolumes To validate other memory allocation implementations instead of libc's malloc added an option to build with tcmalloc library Integrated Thin-arbiter with GD1 Client Handling of Elastic Clusters The package glusterfs-libs is replaced by libgfchangelog0 , libgfrpc0 , libgfxdr0 , and libglusterfs0 ; and additional libraries in libgfapi0 , libglusterd0","title":"Features"},{"location":"release-notes/8.0/#major-issues","text":"None","title":"Major issues"},{"location":"release-notes/8.0/#bugs-addressed","text":"Bugs addressed since release-7 are listed below. #789278 : Issues reported by Coverity static analysis tool #1158130 : Not possible to disable fopen-keeo-cache when mounting #1183054 : rpmlint throws couple of errors for RPM spec file #1193929 : GlusterFS can be improved #1387404 : geo-rep: gsync-sync-gfid binary installed in /usr/share/... #1410439 : glusterfind pre output file is empty #1423442 : group files to set volume options should have comments #1430623 : pthread mutexes and condition variables are not destroyed #1489610 : glusterfind saves var data under $prefix instead of localstatedir #1507896 : glfs_init returns incorrect errno on faliure #1514683 : Removal of bricks in volume isn't prevented if remaining brick doesn't contain all the files #1538900 : Found a missing unref in rpc_clnt_reconnect #1554286 : Xattr not updated if increasing the retention of a WORM/Retained file #1593542 : ctime: Upgrade/Enabling ctime feature wrongly updates older files with latest {a|m|c}time #1620580 : Deleted a volume and created a new volume with similar but not the same name. The kubernetes pod still keeps on running and doesn't crash. Still possible to write to gluster mount #1622665 : clang-scan report: glusterfs issues #1626543 : dht/tests: Create a .t to test all possible combinations for file rename #1635688 : Keep only the valid (maintained/supported) components in the build #1636297 : Make it easy to build / host a project which just builds glusterfs translator #1644322 : flooding log with \"glusterfs-fuse: read from /dev/fuse returned -1 (Operation not permitted)\" #1651445 : [RFE] storage.reserve option should take size of disk as input instead of percentage #1664335 : [geo-rep]: Transport endpoint not connected with arbiter volumes #1665358 : allow regression to not run tests with nfs, if nfs is disabled. #1668239 : [man page] Gluster(8) - Missing disperse-data parameter Gluster Console Manager man page #1668286 : READDIRP incorrectly updates posix-acl inode ctx #1676479 : read-ahead and io-cache degrading performance on sequential read #1688115 : Data heal not checking for locks on source & sink(s) before healing #1689097 : gfapi: provide an option for changing statedump path in glfs-api. #1690454 : mount-shared-storage.sh does not implement mount options #1693692 : Increase code coverage from regression tests #1694920 : Inconsistent locking in presence of disconnects #1697293 : DHT: print hash and layout values in hexadecimal format in the logs #1698042 : quick-read cache invalidation feature has the same key of md-cache #1707731 : [Upgrade] Config files are not upgraded to new version #1708603 : [geo-rep]: Note section in document is required for ignore_deletes true config option where it might delete a file #1708929 : Add more test coverage for shd mux #1716695 : Fix memory leaks that are present even after an xlator fini [client side xlator] #1716979 : Multiple disconnect events being propagated for the same child #1717754 : Enable features.locks-notify-contention by default #1717824 : Fencing: Added the tcmu-runner ALUA feature support but after one of node is rebooted the glfs_file_lock() get stucked #1717827 : tests/geo-rep: Add test case to validate non-root geo-replication setup #1719290 : Glusterfs mount helper script not working with IPv6 because of regular expression or man is wrong #1720463 : [Thin-arbiter] : Wait for connection with TA node before sending lookup/create of ta-replica id file #1720566 : Can't rebalance GlusterFS volume because unix socket's path name is too long #1721590 : tests/bugs/posix/bug-1040275-brick-uid-reset-on-volume-restart.t is failing #1721686 : Remove usage of obsolete function usleep() #1722507 : Incorrect reporting of type/gfid mismatch #1722541 : stale shd process files leading to heal timing out and heal deamon not coming up for all volumes #1722546 : do not assert in inode_unref if the inode table cleanup has started #1722598 : dump the min and max latency of each xlator in statedump #1722698 : DHT: severe memory leak in dht rename #1722740 : [GSS] geo-replication sessions going faulty #1722802 : Incorrect power of two calculation in mem_pool_get_fn #1722977 : ESTALE change in fuse breaks get_real_filename implementation #1723280 : windows cannot access mountpoint exportd from a disperse volume #1723402 : Brick multiplexing is not working. #1723455 : volume set group description missing space leading to words being merged in help output #1723658 : [In-service] Post upgrade glusterd is crashing with a backtrace on the upgraded node while issuing gluster volume status from non-upgraded nodes #1723761 : [Ganesha]: truncate operation not updating the ctime #1723890 : Crash in glusterd when running test script bug-1699339.t #1724024 : use more secure mode for mkdir operations #1724184 : Thin-arbiter: SHD takes lock and inspects the state on TA during every index crawl #1725034 : gluster volume help showing multiple commands for top instead of one. #1725211 : User serviceable snapshots (USS) are not accessible after changing transport.socket.bind-address of glusterd #1726205 : Windows client fails to copy large file to GlusterFS volume share with fruit and streams_xattr VFS modules via Samba #1726783 : snapd crashes sometimes #1726906 : get-state does not show correct brick status #1727068 : Deadlock when generating statedumps #1727081 : Disperse volume : data corruption with ftruncate data in 4+2 config #1727107 : geo-replication/setup.py missing license details in setup() #1727248 : [GNFS] showmout -a cause gnfs crash #1727256 : Directory pending heal in heal info output #1727329 : glustershd dumped core with seg fault at afr_has_quorum #1727852 : gluster-block: improvements to volume group profile options list #1728047 : interrupts leak memory #1728417 : Cleanup references to Hadoop in code base #1728554 : Spelling errors #1728683 : [geo-rep] gluster-mountbroker missing a brief description of what the argument does in # gluster-mountbroker (add|remove|setup) --help #1728766 : Volume start failed when shd is down in one of the node in cluster #1728770 : Failures in remove-brick due to [Input/output error] errors #1729085 : [EC] shd crashed while heal failed due to out of memory error. #1729107 : Memory leak in glusterfsd process #1729463 : gluster v geo-rep status command timing out #1729772 : Disperse volume : Ganesha crash with IO in 4+2 config when one glusterfsd restart every 600s #1729847 : Fix spurious failure of tests/bugs/replicate/bug-1717819-metadata-split-brain-detection.t #1730175 : Seeing failure due to \"getxattr err for dir [No data available]\" in rebalance #1730409 : core file generated - when EC volume stop and start is executed for 10 loops on a EC+Brickmux setup #1730715 : An Input/Output error happens on a disperse volume when doing unaligned writes to a sparse file #1730953 : mount generates errors after umount #1731920 : [geo-rep]: gluster command not found while setting up a non-root session #1732496 : [Coverity] RETURN_LOCAL in __nlc_inode_ctx_get() #1732717 : fuse: Limit the number of inode invalidation requests in the queue #1733042 : cluster.rc Create separate logdirs for each host instance #1733166 : potential deadlock while processing callbacks in gfapi #1733425 : Setting volume option when one of the glusterd is stopped in the cluster, post glusterd restart seeing couldn't find vol info in glusterd logs and shd, brick process offline #1733935 : Open fd heal should filter O_APPEND/O_EXCL #1734026 : Cannot see the \"trusted.glusterfs.mdata\" xattr for directory on a new brick after rebalance #1734252 : Heal not completing after geo-rep session is stopped on EC volumes. #1734299 : ctime: When healing ctime xattr for legacy files, if multiple clients access and modify the same file, the ctime might be updated incorrectly. #1734370 : atime/mtime is not restored after healing for entry self heals #1734738 : Unable to create geo-rep session on a non-root setup. #1736482 : capture stat failure error while setting the gfid #1737288 : nfs client gets bad ctime for copied file which is on glusterfs disperse volume with ctime on #1737291 : features/locks: avoid use after freed of frame for blocked lock #1737484 : geo-rep syncing significantly behind and also only one of the directories are synced with tracebacks seen #1737676 : Upgrading a Gluster node fails when user edited glusterd.vol file exists #1737778 : ocf resource agent for volumes don't work in non-standard environment #1738419 : read() returns more than file size when using direct I/O #1738763 : [EC] : fix coverity issue #1738786 : ctime: If atime is updated via utimensat syscall ctime is not getting updated #1739360 : [GNFS] gluster crash with nfs.nlm off #1740017 : tests/bugs/replicate/bug-880898.t created a core file. #1741734 : gluster-smb:glusto-test access gluster by cifs test write report Device or resource busy #1741779 : Fix spelling errors #1741890 : geo-rep: Changelog archive file format is incorrect #1743020 : glusterd start is failed and throwing an error Address already in use #1743069 : bug-1482023-snpashot-issue-with-other-processes-accessing-mounted-path.t fails in brick mux regression spuriously #1743094 : glusterfs build fails on centos7 #1743200 : ./tests/bugs/glusterd/bug-1595320.t is failing #1743573 : fuse client hung when issued a lookup \"ls\" on an ec volume #1743652 : CentOs 6 GlusterFS client creates files with time 01/01/1970 #1744519 : log aio_error return codes in posix_fs_health_check #1744548 : Setting cluster.heal-timeout requires volume restart #1745965 : glusterd fails to start due to SIGABRT dumping core #1745967 : File size was not truncated for all files when tried with rebalance in progress. #1746228 : systemctl start glusterd is getting timed out on the scaled setup with 2000 volumes #1746320 : SHORT-WRITE error leads to crash #1746810 : markdown files containing 404 links #1747746 : The result (hostname) of getnameinfo for all bricks (ipv6 addresses) are the same, while they are not. #1748448 : syncop: Bail out if frame creation fails #1748744 : bug-1402841.t-mt-dir-scan-race.t fails spuriously #1748836 : Application should know when update size/version went bad #1749322 : glustershd can not decide heald_sinks, and skip repair, so some entries lingering in volume heal info #1750387 : Deprecated log rotate command still present in \"# gluster v help\" #1750618 : Cleanup of executable in tests/bugs/gfapi/bug-1447266/bug-1447266.t not done #1751134 : Spurious failure tests/bugs/replicate/bug-1734370-entry-heal-restore-time.t #1751907 : bricks gone down unexpectedly #1752330 : seeing error message in glustershd.log on volume start(or may be as part of shd graph regeneration) inet_pton failed with return code 0 [Invalid argument] #1752331 : Test tests/basic/volume-scale-shd-mux.t is failing on upstream CI #1753569 : git clone fails on gluster volumes exported via nfs-ganesha #1753592 : Segmentation fault occurs while truncate file #1753843 : [Disperse volume]: Regression in IO performance seen in sequential read for large file #1753857 : geo-rep: performance improvement while syncing heavy renames with existing destination #1753859 : Typos in glusterd log messages #1753880 : Set the default lru-limit in fuse to a smaller number #1753928 : geo-rep: non-root session going fault due improper sub-command #1754448 : Re-alignment of Structure attributes #1754477 : Thin-arbiter: Raise error in CLI if replica-count is not 2 #1755344 : glustershd.log getting flooded with \"W [inode.c:1017:inode_find] (-->/usr/lib64/glusterfs/6.0/xlator/cluster/disperse.so(+0xe3f9) [0x7fd09b0543f9] -->/usr/lib64/glusterfs/6.0/xlator/cluster/disperse.so(+0xe19c) [0x7fd09b05419 TABLE NOT FOUND\" #1755900 : heketidbstorage bricks go down during PVC creation #1756211 : tests/bugs/shard/bug-1272986.t fails #1756900 : tests are failing in RHEL8 regression #1756938 : afr: support split-brain CLI for replica 3 #1757399 : Rebalance is causing glusterfs crash on client node #1758579 : Rebalance causing IO Error - File descriptor in bad state #1758878 : # gluster v info --xml is always returning 3 for all Nx3 volumes #1758984 : Enable direct-io options in group virt #1759002 : Spurious failure tests/bugs/replicate/bug-1744548-heal-timeout.t #1759081 : Spurious failure of /tests/bugs/replicate/bug-1134691-afr-lookup-metadata-heal.t #1760187 : Implement seek fop #1760189 : Use replica aware seek fop #1760467 : rebalance start is succeeding when quorum is not met #1761759 : Failure in ./tests/basic/posix/shared-statfs.t #1761769 : On some distros bug-1272986.t takes more than 2 minutes to run #1762220 : [geo-rep] sync_method showing rsync instead of tarssh post in-service upgrade #1762438 : DHT- gluster rebalance status shows wrong data size after rebalance is completed successfully #1763036 : glusterfsd crashed with \"'MemoryError' Cannot access memory at address\" #1763439 : [GSS] geo-rep entering into faulty state with OSError: [Errno 13] Permission denied #1764110 : tests/bugs/shard/unlinks-and-renames.t fails on RHEL8 #1764119 : gluster rebalance status doesn't show detailed information when a node is rebooted #1764129 : quota_fsck script KeyError: 'contri_size' #1764208 : cgroup control-cpu-load.sh script not working #1764418 : Add Mohit & Sanju as glusterd/cli maintainers #1765017 : gf_event doesn't work for glfsheal process #1765155 : replication shouldn't modify xattr-req coming from parent #1765186 : Problematic coding practices at logger #1765421 : DHT: Add comments to the code #1765426 : test: fix non-root geo-rep test case #1765542 : Add Sunny Kumar as co-maintainer of Geo-replication component #1768407 : glusterfsd memory leak observed after enable tls #1768896 : Long method in glusterfsd - set_fuse_mount_options(...) #1769712 : check if grapj is ready beforce process cli command #1769754 : dht_readdirp_cbk: Do not strip out entries with invalid stats #1771365 : libglusterfs/dict.c : memory leaks #1771577 : [RHEL 6] Geo-replication session not starting after creation #1771895 : geo-rep: Improve debugging in log_raise_exception #1772006 : NULL dict messages flooding fuse mount log #1773530 : ctime value is different from atime/mtime on a create of file #1773856 : Set volume option when one of the node is powered off, After powering the node brick processes are offline #1774011 : Heal Info is hung when I/O is in progress on a gluster block volume #1774866 : man page update needed for gluster volume top command #1775612 : Remove guest access by default for GlusterFS volume SMB shares added by hook scripts #1776264 : RFE: systemd should restart glusterd on crash #1776757 : DHT - Reduce methods scope #1776784 : glfsheal crash on unexpected volume name #1776801 : Bricks are not available when volume create fails #1776892 : [patch] .dirstamp should be in ignored #1778457 : Missing error logs(afr/self-heald ) #1779055 : glusterfs process memory leak in ior test #1779089 : glusterfsd do not release posix lock when multiple glusterfs client do flock -xo to the same file paralleled #1779742 : tests/00-geo-rep/00-georep-verify-non-root-setup.t fail on freshly installed builder #1779760 : Improve logging in EC, client and lock xlator #1780190 : glfsheal should be installed and invoked as architecture-dependent binary helper #1780260 : v7 fails to build on Debian 9 [patch?] #1781440 : event_slot_alloc not able to return index after reach slot_used count to 1024 #1782200 : glusterd restart failing to start. #1782495 : GlusterFS brick process crash #1784375 : 'gluster volume set disable.nfs' accidentally killed unexpected process, and forced a data brick offline. #1785143 : Multiple glusterfsd process spawn when glusterd restart during a volume start. #1785208 : glusterfs client process coredump #1785611 : glusterfsd cashes after a few seconds #1785998 : change the error message for heal statistics to reflect its supportability for disperse volume #1786276 : [geo-rep] Help for positional argument SLAVE in schedule_georep.py.in isn't clear. #1786459 : unable to enable brick-multiplex feature #1786478 : default option is disappeared in volume info after volume reset #1786679 : Duplicate entries in 'ls' output after a volume expansion #1786722 : Functionality to enable log rotation for user serviceable snapshot's logs. #1787122 : glusterd allowing to set server.statedump-path to file, non-existent file and non-existent paths #1787274 : heal not actually healing metadata of a regular file when only time stamps are changed(data heal not required) #1787554 : Unable to set/modify optimistic-change-log for replicate volumes #1789439 : Glusterfind pre command fails #1789478 : S57glusterfind-delete-post.py not python3 ready (does not decode bytestring) #1790748 : Remove extra argument #1790870 : Memory corruption when sending events to an IPv6 host #1791682 : fail to build on recent Fedora #1792276 : config ssh-port can accept negative and outside allowed port range value #1792707 : xlators/features/quota/src/quota.c:quota_log_usage #1793378 : dht_hash_compute() crashes when it receives a zero length name #1793852 : Mounts fails after reboot of 1/3 gluster nodes #1793995 : gluster crash when built without gNFS support #1797869 : bitrot: Number of signing process threads should be configurable. #1797882 : Segmentation fault occurs while truncate file #1797934 : Client should propagate ping event from brick #1800583 : Halo replication is not working #1800956 : Rebalance : Status lists failures on stopping rebalance while it is in progress #1801623 : spurious self-heald.t failure #1801624 : Heal pending on volume, even after all the bricks are up #1801684 : Memory corruption when glfs_init() is called after glfs_fini() #1804786 : mount.glusterfs strips off \"/\" from subdir-mounts #1808421 : WORM: If autocommit-period 0 file will be WORMed with 0 Byte during initial write #1808875 : [brickmux]: glustershd crashed when rebooting 1/3 nodes at regular intervals #1810042 : Changes to gluster peer probe in nightly build breaks ansible:gluster_volume call #1810842 : frequent heal observed when file opened during one brick is down #1810934 : Segfault in FUSE process, potential use after free #1811631 : brick crashed when creating and deleting volumes over time (with brick mux enabled only) #1812144 : Add a warning message during volume expansion or resize on volume with snapshots #1812353 : create-export-ganesha script: mention labelled nfs parameter #154 Optimized CHANGELOG #237 Validate other memory allocation implementations instead of l... #475 Reduce the number or threads used in the brick process #613 Mandatory locks enhancements in replicate subvolumes #657 Structured logging format support #663 Add Ganesha HA bits back to glusterfs code repo #687 Thin-arbiter integration with GD1 #699 executable program will crash if linked libgfapi together wit... #703 provide mechanism to test individual xlators #721 Introduce quorum-count option in disperse volumes as well #723 Provide scripts to reset xattrs of the entries which could be... #725 Disperse: A way to read from specific bricks #741 Client Handling of Elastic Clusters #745 storage.reserve enhancement for posix_write #748 Improve MAKE_HANDLE_GFID_PATH macro and posix_handle_gfid_path() #753 Remove fetching items in gf_cli_replace_brick(), gf_cli_reset... #755 [RFE] Geo-replication code improvements #761 Improve MAKE_HANDLE_PATH macro #763 thin-arbiter: Testing report #765 nfs.rpc-auth-allow gluster7 + gnfs #788 run-with-valgrind option causes gnfs and quota to fail to start #824 Migrate bugzilla workflow to github issues workflow #832 Permission Denied in logs #884 [bug:1808688] Data corruption with asynchronous writes (pleas... #891 [bug:1802451] Optimize posix code to improve file creation #977 [bug:1811631] brick crashed when creating and deleting volume... #999 [bug:1791285] Changing permissions on root directory(director... #1000 [bug:1193929] GlusterFS can be improved #1038 [bug:1787138] Crash on rpcsvc_drc_client_unref() - fails on G... #1042 [bug:1806499] afr-lock-heal-basic.t and /afr-lock-heal-advanc... #1044 [bug:1790730] Add a basic test file to glusterfind #1052 [bug:1693692] Increase code coverage from regression tests #1060 [bug:789278] Issues reported by Coverity static analysis tool #1067 [bug:1661889] Metadata heal picks different brick each time a... #1097 [bug:1635688] Keep only the valid (maintained/supported) comp... #1102 dht: gf_defrag_process_dir is called even if gf_defrag_fix_la... #1104 geo-replication: descriptive message when worker crashes due ... #1105 [bug:1794263] Multiple imports from the same library in the .... #1127 Mount crash during background shard cleanup #1134 snap_scheduler.py init failing with \"TypeError: Can't mix str... #1140 getfattr returns ENOATTR for system.posix_acl_access on dispe... #1141 Make SSL connection messages useful #1142 log the ENOENT error in posix_pstat #1144 [Disperse] Add test for reset-brick for disperse volume #1146 gfapi/Upcall: Potential deadlock in synctask threads processi... #1149 Add error logs to debug failures in ./tests/bugs/protocol/bug... #1150 Avoid dict_del logs in posix_is_layout_stale while key is NULL #1152 Spurious failure of tests/bugs/protocol/bug-1433815-auth-allow.t #1153 Spurious failure of ./tests/bugs/snapshot/bug-1111041.t #1154 failing test cases #1156 Spurious failure of tests/features/worm.t #1158 spurious failure of tests/bugs/glusterd/serialize-shd-manager... #1160 sys_stat should be used instead of stat #1161 tests: file offsets and sizes shouldn't be truncated to 32-bi... #1162 spurious failure of test case tests/bugs/glusterd/removing-mu... #1169 common-ha: cluster status shows \"FAILOVER\" even when all node... #1180 (glusterfs-8.0) - GlusterFS 8.0 tracker #1179 gnfs split brain when 1 server in 3x1 down (high load) #1220 cluster/ec: return correct error code and log the message in case of BADFD #1223 Failure of tests/basic/gfapi/gfapi-copy-file-range.t #1116 [bug:1790736] gluster volume list returning wrong volume list / volume list time out #990 [bug:1578405] EIO errors when updating and deleting entries co... #1126 packaging: overhaul glusterfs.spec(.in) to align with SUSE and... #1225 fuse causes glusterd to dump core #1243 Modify and return iatt (especially size and block-count) in sh... #1254 Prioritize ENOSPC over other lesser priority errors #1303 Failures in rebalance due to [Input/output error]","title":"Bugs addressed"},{"location":"release-notes/8.1/","text":"Release notes for Gluster 8.1 Release date: 27-Aug-2020 This is a Improvements and bugfix release. The release notes for 8.0 contains a listing of all the new features that were added and bugs fixed in the GlusterFS 8 stable release. NOTE: Next minor release tentative date: Week of 20th Sep, 2020 Improvements and Highlights Below improvements have been added to this minor release. Performance improvement over the creation of large files - VM disks in oVirt by bringing down trivial lookups of non-existent shards. Issue ( #1425 ) Fsync in the replication module uses eager-lock functionality which improves the performance of VM workloads with an improvement of more than 50% in small-block of approximately 4kb with write heavy workloads. Issue ( #1253 ) Builds are available at https://download.gluster.org/pub/gluster/glusterfs/8/8.1/ Issues addressed in this release Please find the list of issues added to this release below. #763 thin-arbiter: Testing report #1217 Modify group \"virt\" to add rpc/network related changes #1250 geo-rep: Fix corner case in rename on mkdir during hybrid crawl #1281 Unlinking the file with open fd, returns ENOENT or stale file ... #1348 Fuse mount crashes in shard translator when truncating a *real... #1351 issue with gf_fill_iatt_for_dirent() #1352 api: libgfapi symbol versions break LTO in Fedora rawhide/f33 #1354 High CPU utilization by self-heal on disperse volumes with no ... #1385 High CPU utilization by self-heal on disperse volumes when an ... #1396 [bug-1851989] smallfile performance drops after commit the pat... #1407 glusterd keep crashing when upgrading from 6.5 to 7.7 #1418 GlusterFS 8.0: Intermittent error:1408F10B:SSL routines:SSL3_G... #1440 glusterfs 7.7 fuse client memory leak","title":8.1},{"location":"release-notes/8.1/#release-notes-for-gluster-81","text":"Release date: 27-Aug-2020 This is a Improvements and bugfix release. The release notes for 8.0 contains a listing of all the new features that were added and bugs fixed in the GlusterFS 8 stable release. NOTE: Next minor release tentative date: Week of 20th Sep, 2020","title":"Release notes for Gluster 8.1"},{"location":"release-notes/8.1/#improvements-and-highlights","text":"Below improvements have been added to this minor release. Performance improvement over the creation of large files - VM disks in oVirt by bringing down trivial lookups of non-existent shards. Issue ( #1425 ) Fsync in the replication module uses eager-lock functionality which improves the performance of VM workloads with an improvement of more than 50% in small-block of approximately 4kb with write heavy workloads. Issue ( #1253 )","title":"Improvements and Highlights"},{"location":"release-notes/8.1/#builds-are-available-at","text":"https://download.gluster.org/pub/gluster/glusterfs/8/8.1/","title":"Builds are available at"},{"location":"release-notes/8.1/#issues-addressed-in-this-release","text":"Please find the list of issues added to this release below. #763 thin-arbiter: Testing report #1217 Modify group \"virt\" to add rpc/network related changes #1250 geo-rep: Fix corner case in rename on mkdir during hybrid crawl #1281 Unlinking the file with open fd, returns ENOENT or stale file ... #1348 Fuse mount crashes in shard translator when truncating a *real... #1351 issue with gf_fill_iatt_for_dirent() #1352 api: libgfapi symbol versions break LTO in Fedora rawhide/f33 #1354 High CPU utilization by self-heal on disperse volumes with no ... #1385 High CPU utilization by self-heal on disperse volumes when an ... #1396 [bug-1851989] smallfile performance drops after commit the pat... #1407 glusterd keep crashing when upgrading from 6.5 to 7.7 #1418 GlusterFS 8.0: Intermittent error:1408F10B:SSL routines:SSL3_G... #1440 glusterfs 7.7 fuse client memory leak","title":"Issues addressed in this release"},{"location":"release-notes/8.2/","text":"Release notes for Gluster 8.2 Release date: 23-Sept-2020 This is a Improvements and bugfix release. The release notes for 8.0 , 8.1 contain a listing of all the new features that were added and bugs fixed in the GlusterFS 8 stable release. NOTE: Next minor release tentative date: Week of 20th Oct, 2020 Improvements and Highlights Below improvements have been added to this minor release. Glustereventsd will accept IPv6 packets too. Issue ( #1377 ) Builds are available at https://download.gluster.org/pub/gluster/glusterfs/8/8.2/ Issues addressed in this release Please find the list of issues added to this release below. #1000 [bug:1193929] GlusterFS can be improved #1060 [bug:789278] Issues reported by Coverity static analysis tool #1332 Unable to Upgrade to Gluster 7 from Earlier Version #1440 glusterfs 7.7 fuse client memory leak #1472 Readdir-ahead leads to inconsistent ls results","title":8.2},{"location":"release-notes/8.2/#release-notes-for-gluster-82","text":"Release date: 23-Sept-2020 This is a Improvements and bugfix release. The release notes for 8.0 , 8.1 contain a listing of all the new features that were added and bugs fixed in the GlusterFS 8 stable release. NOTE: Next minor release tentative date: Week of 20th Oct, 2020","title":"Release notes for Gluster 8.2"},{"location":"release-notes/8.2/#improvements-and-highlights","text":"Below improvements have been added to this minor release. Glustereventsd will accept IPv6 packets too. Issue ( #1377 )","title":"Improvements and Highlights"},{"location":"release-notes/8.2/#builds-are-available-at","text":"https://download.gluster.org/pub/gluster/glusterfs/8/8.2/","title":"Builds are available at"},{"location":"release-notes/8.2/#issues-addressed-in-this-release","text":"Please find the list of issues added to this release below. #1000 [bug:1193929] GlusterFS can be improved #1060 [bug:789278] Issues reported by Coverity static analysis tool #1332 Unable to Upgrade to Gluster 7 from Earlier Version #1440 glusterfs 7.7 fuse client memory leak #1472 Readdir-ahead leads to inconsistent ls results","title":"Issues addressed in this release"},{"location":"release-notes/8.3/","text":"Release notes for Gluster 8.3 Release date: 23-Dec-2020 This is a bugfix release. The release notes for 8.0 , 8.1 and 8.2 contain a listing of all the new features that were added and bugs fixed in the GlusterFS 8 stable release. NOTE: - Next minor release tentative date: Week of 20th Feb, 2021 - Users are highly encouraged to upgrade to newer releases of GlusterFS. Highlights of Release This release contains majorly the bug fixes as described in the issues section. Builds are available at https://download.gluster.org/pub/gluster/glusterfs/8/8.3/ Issues addressed in this release Please find the list of issues added to this release below. #1836 posix: Update ret value in posix_get_gfid2path if GF_MALLOC fails #1796 afr: call afr_is_lock_mode_mandatory only while xdata is valid #1778 volume set: failed: ganesha.enable is already 'off'. #1738 [cli] Improper error message on command timeout #1699 One brick offline with signal received: 11 #1663 test case ./tests/bugs/core/bug-1650403.t is getting timed out #1601 rfc.sh on release-8 needs to move to github flow #1499 why not use JumpConsistentHash to replace SuperFastHash to cho... #1438 syncdaemon/syncdutils.py: SyntaxWarning: \"is\" with a literal. ... #1221 features/bit-rot: invalid snprintf() buffer size #1060 [bug:789278] Issues reported by Coverity static analysis tool #1002 [bug:1679998] GlusterFS can be improved #1000 [bug:1193929] GlusterFS can be improved","title":8.3},{"location":"release-notes/8.3/#release-notes-for-gluster-83","text":"Release date: 23-Dec-2020 This is a bugfix release. The release notes for 8.0 , 8.1 and 8.2 contain a listing of all the new features that were added and bugs fixed in the GlusterFS 8 stable release. NOTE: - Next minor release tentative date: Week of 20th Feb, 2021 - Users are highly encouraged to upgrade to newer releases of GlusterFS.","title":"Release notes for Gluster 8.3"},{"location":"release-notes/8.3/#highlights-of-release","text":"This release contains majorly the bug fixes as described in the issues section.","title":"Highlights of Release"},{"location":"release-notes/8.3/#builds-are-available-at","text":"https://download.gluster.org/pub/gluster/glusterfs/8/8.3/","title":"Builds are available at"},{"location":"release-notes/8.3/#issues-addressed-in-this-release","text":"Please find the list of issues added to this release below. #1836 posix: Update ret value in posix_get_gfid2path if GF_MALLOC fails #1796 afr: call afr_is_lock_mode_mandatory only while xdata is valid #1778 volume set: failed: ganesha.enable is already 'off'. #1738 [cli] Improper error message on command timeout #1699 One brick offline with signal received: 11 #1663 test case ./tests/bugs/core/bug-1650403.t is getting timed out #1601 rfc.sh on release-8 needs to move to github flow #1499 why not use JumpConsistentHash to replace SuperFastHash to cho... #1438 syncdaemon/syncdutils.py: SyntaxWarning: \"is\" with a literal. ... #1221 features/bit-rot: invalid snprintf() buffer size #1060 [bug:789278] Issues reported by Coverity static analysis tool #1002 [bug:1679998] GlusterFS can be improved #1000 [bug:1193929] GlusterFS can be improved","title":"Issues addressed in this release"},{"location":"release-notes/8.4/","text":"Release notes for Gluster 8.4 Release date: 01-Mar-2021 This is a bugfix release. The release notes for 8.0 , 8.1 , 8.2 and 8.3 contain a listing of all the new features that were added and bugs fixed in the GlusterFS 8 stable release. NOTE: - Next minor release tentative date: Week of 20th Apr, 2021 - Users are highly encouraged to upgrade to newer releases of GlusterFS. Highlights of Release Healing data in 1MB chunks instead of 128KB for improving healing performance #2067 Builds are available at https://download.gluster.org/pub/gluster/glusterfs/8/8.4/ Issues addressed in this release Please find the list of issues added to this release below. #2154 \"Operation not supported\" doing a chmod on a symlink #2107 mount crashes when setfattr -n distribute.fix.layout -v \"yes\"... #1991 mdcache: bug causes getxattr() to report ENODATA when fetchin... #1925 dht_pt_getxattr does not seem to handle virtual xattrs. #1539 fuse mount crashes on graph-switch when reader-thread-count i... #1529 Fix regression in on demand migration feature #1406 shared storage volume fails to mount in ipv6 environment","title":8.4},{"location":"release-notes/8.4/#release-notes-for-gluster-84","text":"Release date: 01-Mar-2021 This is a bugfix release. The release notes for 8.0 , 8.1 , 8.2 and 8.3 contain a listing of all the new features that were added and bugs fixed in the GlusterFS 8 stable release. NOTE: - Next minor release tentative date: Week of 20th Apr, 2021 - Users are highly encouraged to upgrade to newer releases of GlusterFS.","title":"Release notes for Gluster 8.4"},{"location":"release-notes/8.4/#highlights-of-release","text":"Healing data in 1MB chunks instead of 128KB for improving healing performance #2067","title":"Highlights of Release"},{"location":"release-notes/8.4/#builds-are-available-at","text":"https://download.gluster.org/pub/gluster/glusterfs/8/8.4/","title":"Builds are available at"},{"location":"release-notes/8.4/#issues-addressed-in-this-release","text":"Please find the list of issues added to this release below. #2154 \"Operation not supported\" doing a chmod on a symlink #2107 mount crashes when setfattr -n distribute.fix.layout -v \"yes\"... #1991 mdcache: bug causes getxattr() to report ENODATA when fetchin... #1925 dht_pt_getxattr does not seem to handle virtual xattrs. #1539 fuse mount crashes on graph-switch when reader-thread-count i... #1529 Fix regression in on demand migration feature #1406 shared storage volume fails to mount in ipv6 environment","title":"Issues addressed in this release"},{"location":"release-notes/8.5/","text":"Release notes for Gluster 8.5 Release date: 17-May-2021 This is a bugfix release. The release notes for 8.0 , 8.1 , 8.2 , 8.3 and 8.4 contain a listing of all the new features that were added and bugs fixed in the GlusterFS 8 stable release. NOTE: - Next minor release tentative date: Week of 30th Jun, 2021 - Users are highly encouraged to upgrade to newer releases of GlusterFS. Important fixes in this release Slow write on ZFS bricks after healing millions of files due to adding arbiter brick #1764 4+1 arbiter setup is broken #2192 Builds are available at https://download.gluster.org/pub/gluster/glusterfs/8/8.5/ Issues addressed in this release #1214 Running tests/basic/afr/inodelk.t on my VM crashes in dht #1324 Inconsistent custom xattr on backend directories after bringing bac #1764 Slow write on ZFS bricks after healing millions of files due to add #2161 Crash caused by memory corruption #2192 4+1 arbiter setup is broken #2198 There are blocked inodelks for a long time #2210 glusterfsd memory leak observed when constantly running volume heal #2234 Segmentation fault in directory quota daemon for replicated volume #2253 Disable lookup-optimize by default in the virt group #2313 Long setting names mess up the columns and break parsing","title":8.5},{"location":"release-notes/8.5/#release-notes-for-gluster-85","text":"Release date: 17-May-2021 This is a bugfix release. The release notes for 8.0 , 8.1 , 8.2 , 8.3 and 8.4 contain a listing of all the new features that were added and bugs fixed in the GlusterFS 8 stable release. NOTE: - Next minor release tentative date: Week of 30th Jun, 2021 - Users are highly encouraged to upgrade to newer releases of GlusterFS.","title":"Release notes for Gluster 8.5"},{"location":"release-notes/8.5/#important-fixes-in-this-release","text":"Slow write on ZFS bricks after healing millions of files due to adding arbiter brick #1764 4+1 arbiter setup is broken #2192","title":"Important fixes in this release"},{"location":"release-notes/8.5/#builds-are-available-at","text":"https://download.gluster.org/pub/gluster/glusterfs/8/8.5/","title":"Builds are available at"},{"location":"release-notes/8.5/#issues-addressed-in-this-release","text":"#1214 Running tests/basic/afr/inodelk.t on my VM crashes in dht #1324 Inconsistent custom xattr on backend directories after bringing bac #1764 Slow write on ZFS bricks after healing millions of files due to add #2161 Crash caused by memory corruption #2192 4+1 arbiter setup is broken #2198 There are blocked inodelks for a long time #2210 glusterfsd memory leak observed when constantly running volume heal #2234 Segmentation fault in directory quota daemon for replicated volume #2253 Disable lookup-optimize by default in the virt group #2313 Long setting names mess up the columns and break parsing","title":"Issues addressed in this release"},{"location":"release-notes/8.6/","text":"Release notes for Gluster 8.6 Release date: 30-Aug-2021 This is a bugfix release. The release notes for 8.0 , 8.1 , 8.2 , 8.3 , 8.4 and 8.5 contain a listing of all the new features that were added and bugs fixed in the GlusterFS 8 stable release. NOTE: - This is the last release of Gluster 8 series - Users are highly encouraged to upgrade to newer releases of GlusterFS. Important fixes in this release Improvement in handling of gfid mismatches of geo-rep: Geo-replication gets delayed when there are many renames on primary volume(https://github.com/gluster/glusterfs/issues/2388) Resolve core dumps on Gluster 9 - 3 replicas: Initialize list head to prevent NULL de-reference(https://github.com/gluster/glusterfs/issues/2443) Builds are available at https://download.gluster.org/pub/gluster/glusterfs/8/8.6/ Issues addressed in this release #2388 Geo-replication gets delayed when there are many renames on primary volume #2689 glusterd: reset mgmt_v3_lock_timeout after it be used #1000 GlusterFS can be improved: fix getcwd usage warning #2394 Spurious failure in tests/basic/fencing/afr-lock-heal-basic.t #2691 georep-upgrade.t find failures #154 Optimized CHANGELOG: upgrade script for geo-rep #2443 Core dumps on Gluster 9 - 3 replicas: Initialize list head to prevent NULL de-reference #2404 Spurious failure of tests/bugs/ec/bug-1236065.t","title":8.6},{"location":"release-notes/8.6/#release-notes-for-gluster-86","text":"Release date: 30-Aug-2021 This is a bugfix release. The release notes for 8.0 , 8.1 , 8.2 , 8.3 , 8.4 and 8.5 contain a listing of all the new features that were added and bugs fixed in the GlusterFS 8 stable release. NOTE: - This is the last release of Gluster 8 series - Users are highly encouraged to upgrade to newer releases of GlusterFS.","title":"Release notes for Gluster 8.6"},{"location":"release-notes/8.6/#important-fixes-in-this-release","text":"Improvement in handling of gfid mismatches of geo-rep: Geo-replication gets delayed when there are many renames on primary volume(https://github.com/gluster/glusterfs/issues/2388) Resolve core dumps on Gluster 9 - 3 replicas: Initialize list head to prevent NULL de-reference(https://github.com/gluster/glusterfs/issues/2443)","title":"Important fixes in this release"},{"location":"release-notes/8.6/#builds-are-available-at","text":"https://download.gluster.org/pub/gluster/glusterfs/8/8.6/","title":"Builds are available at"},{"location":"release-notes/8.6/#issues-addressed-in-this-release","text":"#2388 Geo-replication gets delayed when there are many renames on primary volume #2689 glusterd: reset mgmt_v3_lock_timeout after it be used #1000 GlusterFS can be improved: fix getcwd usage warning #2394 Spurious failure in tests/basic/fencing/afr-lock-heal-basic.t #2691 georep-upgrade.t find failures #154 Optimized CHANGELOG: upgrade script for geo-rep #2443 Core dumps on Gluster 9 - 3 replicas: Initialize list head to prevent NULL de-reference #2404 Spurious failure of tests/bugs/ec/bug-1236065.t","title":"Issues addressed in this release"},{"location":"release-notes/9.0/","text":"Release notes for Gluster 9.0 Release date: 05-Feb-2021 This is a major release that includes a range of features, code improvements and stability fixes as noted below. A selection of the key features and changes are documented in this page. A full list of bugs that have been addressed is included further below. Announcements Major changes and features Major issues Bugs addressed in the release Announcements Releases that receive maintenance updates post release 9 is 8 ( reference ) Release 9 will receive maintenance updates around the 30th of every month for the first 3 months post release (i.e Mar'21, Apr'21, May'21). Post the initial 3 months, it will receive maintenance updates every 2 months till EOL. Major changes and features Highlights Added support for: io_uring in Gluster (io_uring support in kernel required along with the presence of liburing library and headers) support running with up to 5000 volumes (Testing done on: 5k volumes on 3 nodes, brick_mux was enabled with default configuration) Features Added io_uring support for Gluster #1398 Added Support for 5K volumes #1613 Enabled granular-entry-heal by default #1483 Optimizations for rename dir heal #1211 Added support for monitoring the epoll/rpc layer #1466 Brick mux: Added support to spawn a thread per process basis instead of spawning a per brick #1482 Improve rebalance of sparse files #1222 LTO/GCC10 - Gluster is now compiled with LTO enabled by default #1772 Major issues None Bugs addressed Bugs addressed since release-8 are listed below. #718 _store_global_opts(), _storeslaves() , _storeopts() should no... #280 Use internal error codes instead of UNIX errnos #1855 Makefile: failed to compile without git repository #1849 geo-rep: The newly setup geo-rep session goes faulty with syn... #1836 posix: Update ret value in posix_get_gfid2path if GF_MALLOC f... #1810 Implement option to generate core dump at will without killin... #1796 afr: call afr_is_lock_mode_mandatory only while xdata is valid #1794 posix: A brick process is getting crashed at the time of grap... #1782 Rebalance is reporting status twice upon stopping, resulting ... #1778 volume set: failed: ganesha.enable is already 'off'. #1775 core: lru_size showing -1 with zero inodes in the list in the... #1772 build: add LTO as a configure option #1743 Modify format to contain more information while raising glust... #1739 test case ./tests/basic/afr/entry-self-heal-anon-dir-off.t is... #1738 [cli] Improper error message on command timeout #1733 api: conscious language changes #1713 Conscious language changes in various xlators #1699 One brick offline with signal received: 11 #1692 Test tests/basic/0symbol-check.t should exclude more contrib/... #1663 test case ./tests/bugs/core/bug-1650403.t is getting timed out #1661 test case ./tests/bugs/bug-1064147.t is continuously failing #1659 wrong comparison in glusterd_brick_start() function #1654 Rebalance/migration per directory/file #1653 io-cache xlators lock/unlock are always accompanied by gf_msg... #1627 Stopping rebalance results in a failure #1613 glusterd[brick_mux]: Optimize friend handshake code to avoid ... #1594 ./tests/00-geo-rep/00-georep-verify-non-root-setup.t fails on... #1587 geo-rep: Enable rsync verbose logging to help debug rsync errors #1584 MAINTAINERS file needs to be revisited and updated #1582 ./rfc.sh doesn't pick upstream correctly #1577 cli-rpc: Call to global quota rpc init even though operation ... #1569 Introduce a compile time --enable-brickmux option to run bric... #1565 Implement pass-through option for write-behind #1550 MAINTAINERS list of DHT needs to be updated #154 Optimized CHANGELOG #1546 Wrong permissions syned to remote brick when using halo repli... #1545 fuse_invalidate_entry() - too many repetitive calls to uuid_u... #1544 file tree memory layout optimization #1543 trash: Create inode_table only while feature is enabled #1542 io-stats: Configure ios_sample_buf_size based on sample_inter... #1541 Geo-rep: some files(hardlinks) are missing in slave after mas... #1540 [RFE] Rebalance: suppurt migration to files with hardlinks (n... #1539 fuse mount crashes on graph-switch when reader-thread-count i... #1538 Need to configure optimum inode table hash_size for shd #1529 Fix regression in on demand migration feature #1526 Brick status is 'stopped' if socket file is absent but brick ... #1518 glusterfs: write operations fail when the size is equal or gr... #1516 Use of strchr glusterd_replace_slash_with_hyphen #1511 Crash due to memory allocation #1508 Add-brick with Increasing replica count fails with bad brick ... #1507 Time-to-completion mechansim in rebalance is broken #1506 tests/000-flaky/bugs_nfs_bug-1116503.t is crashed in in gf_me... #1499 why not use JumpConsistentHash to replace SuperFastHash to ch... #1497 Removing strlen and using the already existing len of data_t #1487 Quota accounting check script fails with UnicodeDecodeError #1483 Enable granular-entry-heal by default #1482 [Brick-mux] Attach several posix threads with glusterfs_ctx #1480 First letter in mount path of bricks are getting truncated fo... #1477 nfs server crashes in acl3svc_init #1476 Changes required at Snaphot as gluster-shared-storage mount p... #1475 gluster_shared_storage failed to automount on node reboot on ... #1472 Readdir-ahead leads to inconsistent ls results #1466 RPC handling latencies should be printed in statedump #1464 Avoid dict OR key (link-count) is NULL [Invalid argument] mes... #1459 gluster_shared_storage failed to automount on node reboot on ... #1453 Disperse shd heal activity should be observable #1442 Remove Glusterfs SELinux module from Distribution's selinux-p... #1440 glusterfs 7.7 fuse client memory leak #1438 syncdaemon/syncdutils.py: SyntaxWarning: \"is\" with a literal.... #1434 Inform failures while fop failed in disperse volume due to so... #1428 Redundant check in dict_get_with_refn() #1427 Bricks failed to restart after a power failure #1425 optimization over shard lookup in case of prealloc #1422 Rebalance - new volume option to turn on/off optimization in ... #1418 GlusterFS 8.0: Intermittent error:1408F10B:SSL routines:SSL3_... #1416 Dependencies of performance.parallel-readdir should be automa... #1410 01-georep-glusterd-tests.t times out on centos7 builders #1407 glusterd keep crashing when upgrading from 6.5 to 7.7 #1406 shared storage volume fails to mount in ipv6 environment #1404 Client side split-brain resolution using favourite-child-poli... #1403 Tests failure on C8: ./tests/features/ssl-ciphers.t #1401 quota_fsck.py throws TypeError #1400 Annotate synctasks with tsan API if --enable-tsan is requested #1399 Add xlator identifiers in statedumps for mem-pools #1398 io_uring support in glusterfs main branch #1397 glusterd_check_brick_order() is needlessly fetching volname, ... #1396 [bug-1851989] smallfile performance drops after commit the pa... #1395 optimize dict_serialized_length_lk function #1391 allow add-brick from nodes which are not part of auth.allow list #1385 High CPU utilization by self-heal on disperse volumes when an... #1383 Remove contrib/sunrpc/xdr_sizeof.c #1381 Optional FUSE notitications #1379 Fix NULL pointer #1378 Use better terminology and wording in the code #1377 Glustereventsd to accept not only IPv4 but IPv6 packets too. #1376 Runtime & Build Fixes for FreeBSD #1375 cluster: mount.glusterfs is stuck when trying to mount unknow... #1374 fuse interrupt issues identified in code review #1371 [RHEL 8.1] [Input/output error] observed in remove-brick oper... #1366 geo-replication session fails to start with IPV6 #1361 Screen .attribute directories on NetBSD #1359 Cleanup --disable-mempool #1357 options should display not only current values but also defau... #1356 cli: type mismatch global_quotad_rpc in cli-quotad-client.c #1355 Heal info desn't show split-brain info if halo is enabled #1354 High CPU utilization by self-heal on disperse volumes with no... #1353 errors seen with gluster v get all all #1352 api: libgfapi symbol versions break LTO in Fedora rawhide/f33 #1351 issue with gf_fill_iatt_for_dirent() #1350 Simplify directory scanning #1348 Fuse mount crashes in shard translator when truncating a *rea... #1347 NetBSD build fixes #1339 Rebalance status is not shown correctly after node reboot #1332 Unable to Upgrade to Gluster 7 from Earlier Version #1329 Move platform-dependent filesystem sync to a library function #1328 Linux kernel untar failed with errors immediate after add-brick #1327 Missing directory is not healed in dht #1324 Inconsistent custom xattr on backend directories after bringi... #1320 Unified support for building with sanitizers #1311 Data race when handling connection status #1310 tests/features/flock_interrupt.t leads to error logs #1306 add-brick command is failing #1303 Failures in rebalance due to [Input/output error] #1302 always print errno (and use English locale for strerror() out... #1291 Free volume info lock and mutex #1290 Test case brick-mux-validation-in-cluster.t is failing on RHEL-8 #1289 glustereventsd log file isn't reopened after rotation #1285 Use-after-destroy mutex error #1283 Undefined behavior in __builtin_ctz #1282 New file created with xattr \"trusted.glusterfs.dht\" #1281 Unlinking the file with open fd, returns ENOENT or stale file... #1279 Fix several signed integer overflows #1278 Fix memory leak in afr_priv_destroy() #1275 Make glusterfs compile on all recent and supported versions o... #1272 tests/bugs/glusterd/mgmt-handshake-and-volume-sync-post-glust... #1269 common-ha: ganesha-ha.sh bad test for {rhel,centos} for pcs o... #1263 Fix memory leak in glusterd_store_retrieve_bricks() #1260 Implement minimal proper synchronization for gf_attach #1259 Fix memory leak in gf_cli_gsync_status_output() #1258 dht: Add null check #1255 Improve snapshot clone error message #1254 Prioritize ENOSPC over other lesser priority errors #1253 On Ovirt setup glusterfs performs poorly #1250 geo-rep: Fix corner case in rename on mkdir during hybrid crawl #1249 Drop 'const' type qualifier on return type #1248 Fix thread naming and related convention #1245 Spurious failures in ./tests/basic/ec/ec-quorum-count.t #1243 Modify and return iatt (especially size and block-count) in s... #1242 Rebalance - Improve Crawl time in rebalance #1240 tests/basic/afr/gfid-mismatch-resolution-with-fav-child-polic... #1236 glusterfs-geo-replication requires policycoreutils-python-uti... #1234 Fix ./tests/basic/fencing/afr-lock-heal-basic.t failure #1230 core dumped executing tests/line-coverage/errorgen-coverage.t #1228 seek functionalty is broken #1226 Gluster webhook update throws error #1225 fuse causes glusterd to dump core #1223 Failure of tests/basic/gfapi/gfapi-copy-file-range.t #1222 [RFE] Improve rebalance of sparse files #1221 features/bit-rot: invalid snprintf() buffer size #1220 cluster/ec: return correct error code and log the message in ... #1218 dht: Do opendir selectively in gf_defrag_process_dir #1217 Modify group \"virt\" to add rpc/network related changes #1214 Running tests/basic/afr/inodelk.t on my VM crashes in dht #1211 AFR: Rename dir heal shouldn't delete the directory at oldloc... #1209 tests: georep-upgrade.t test failure #1208 warning: initializer overrides prior initialization of this s... #1207 warning: passing an object that undergoes default argument pr... #1204 GD_OP_VERSION needs to be updated #1202 Issues reported by Coverity static analysis tool #1200 Handle setxattr and rm race for directory in rebalance #1197 Geo-replication tests are spuriously failing in CI #1196 glusterfsd is having a leak while only mgmt SSL is enabled #1193 Scheduling of snapshot for a volume is failing to create snap... #1190 spurious failure of tests/basic/quick-read-with-upcall.t #1187 Failures in rebalance due to [No space left on device] error ... #1182 geo-rep requires relevant selinux permission for rsync #1179 gnfs split brain when 1 server in 3x1 down (high load) #1172 core, cli, quota: cleanup malloc debugging and stats #1169 common-ha: cluster status shows \"FAILOVER\" even when all node... #1164 migrate remove-brick operation to mgmt v3 frameowrk #1154 failing test cases #1135 Fix @sysconfdir@ expansion in extras/systemd/glusterd.service... #1126 packaging: overhaul glusterfs.spec(.in) to align with SUSE an... #1116 [bug:1790736] gluster volume list returning wrong volume list... #1101 [bug:1813029] volume brick fails to come online because other... #1097 [bug:1635688] Keep only the valid (maintained/supported) comp... #1096 [bug:1622665] clang-scan report: glusterfs issues #1075 [bug:1299203] resolve-gids is not needed for Linux kernels v3... #1072 [bug:1251614] gf_defrag_fix_layout recursively fails, distrac... #1060 [bug:789278] Issues reported by Coverity static analysis tool #1052 [bug:1693692] Increase code coverage from regression tests #1050 [bug:1787325] TLS/SSL access of GlusterFS mounts is slower th... #1047 [bug:1774379] check for same hostnames(bricks from same host/... #1043 [bug:1793490] snapshot clone volume is not exported via NFS-G... #1009 [bug:1756900] tests are failing in RHEL8 regression #1002 [bug:1679998] GlusterFS can be improved #1000 [bug:1193929] GlusterFS can be improved #990 [bug:1578405] EIO errors when updating and deleting entries c... #952 [bug:1589705] quick-read: separate performance.cache-size tun... #876 [bug:1797099] After upgrade from gluster 7.0 to 7.2 posix-acl... #874 [bug:1793390] Pre-validation failure does not provide any hin... #837 Indicate timezone offset in formatted timestamps #829 gfapi: Using ssl and glfs_set_volfile together does not work #827 undefined symbol: xlator_api #824 Migrate bugzilla workflow to github issues workflow #816 RFE: Data/MetaData separator Translator #790 infinite loop in common-utils.c - gf_rev_dns_lookup_cache() ? #763 thin-arbiter: Testing report","title":9.0},{"location":"release-notes/9.0/#release-notes-for-gluster-90","text":"Release date: 05-Feb-2021 This is a major release that includes a range of features, code improvements and stability fixes as noted below. A selection of the key features and changes are documented in this page. A full list of bugs that have been addressed is included further below. Announcements Major changes and features Major issues Bugs addressed in the release","title":"Release notes for Gluster 9.0"},{"location":"release-notes/9.0/#announcements","text":"Releases that receive maintenance updates post release 9 is 8 ( reference ) Release 9 will receive maintenance updates around the 30th of every month for the first 3 months post release (i.e Mar'21, Apr'21, May'21). Post the initial 3 months, it will receive maintenance updates every 2 months till EOL.","title":"Announcements"},{"location":"release-notes/9.0/#major-changes-and-features","text":"","title":"Major changes and features"},{"location":"release-notes/9.0/#highlights","text":"Added support for: io_uring in Gluster (io_uring support in kernel required along with the presence of liburing library and headers) support running with up to 5000 volumes (Testing done on: 5k volumes on 3 nodes, brick_mux was enabled with default configuration)","title":"Highlights"},{"location":"release-notes/9.0/#features","text":"Added io_uring support for Gluster #1398 Added Support for 5K volumes #1613 Enabled granular-entry-heal by default #1483 Optimizations for rename dir heal #1211 Added support for monitoring the epoll/rpc layer #1466 Brick mux: Added support to spawn a thread per process basis instead of spawning a per brick #1482 Improve rebalance of sparse files #1222 LTO/GCC10 - Gluster is now compiled with LTO enabled by default #1772","title":"Features"},{"location":"release-notes/9.0/#major-issues","text":"None","title":"Major issues"},{"location":"release-notes/9.0/#bugs-addressed","text":"Bugs addressed since release-8 are listed below. #718 _store_global_opts(), _storeslaves() , _storeopts() should no... #280 Use internal error codes instead of UNIX errnos #1855 Makefile: failed to compile without git repository #1849 geo-rep: The newly setup geo-rep session goes faulty with syn... #1836 posix: Update ret value in posix_get_gfid2path if GF_MALLOC f... #1810 Implement option to generate core dump at will without killin... #1796 afr: call afr_is_lock_mode_mandatory only while xdata is valid #1794 posix: A brick process is getting crashed at the time of grap... #1782 Rebalance is reporting status twice upon stopping, resulting ... #1778 volume set: failed: ganesha.enable is already 'off'. #1775 core: lru_size showing -1 with zero inodes in the list in the... #1772 build: add LTO as a configure option #1743 Modify format to contain more information while raising glust... #1739 test case ./tests/basic/afr/entry-self-heal-anon-dir-off.t is... #1738 [cli] Improper error message on command timeout #1733 api: conscious language changes #1713 Conscious language changes in various xlators #1699 One brick offline with signal received: 11 #1692 Test tests/basic/0symbol-check.t should exclude more contrib/... #1663 test case ./tests/bugs/core/bug-1650403.t is getting timed out #1661 test case ./tests/bugs/bug-1064147.t is continuously failing #1659 wrong comparison in glusterd_brick_start() function #1654 Rebalance/migration per directory/file #1653 io-cache xlators lock/unlock are always accompanied by gf_msg... #1627 Stopping rebalance results in a failure #1613 glusterd[brick_mux]: Optimize friend handshake code to avoid ... #1594 ./tests/00-geo-rep/00-georep-verify-non-root-setup.t fails on... #1587 geo-rep: Enable rsync verbose logging to help debug rsync errors #1584 MAINTAINERS file needs to be revisited and updated #1582 ./rfc.sh doesn't pick upstream correctly #1577 cli-rpc: Call to global quota rpc init even though operation ... #1569 Introduce a compile time --enable-brickmux option to run bric... #1565 Implement pass-through option for write-behind #1550 MAINTAINERS list of DHT needs to be updated #154 Optimized CHANGELOG #1546 Wrong permissions syned to remote brick when using halo repli... #1545 fuse_invalidate_entry() - too many repetitive calls to uuid_u... #1544 file tree memory layout optimization #1543 trash: Create inode_table only while feature is enabled #1542 io-stats: Configure ios_sample_buf_size based on sample_inter... #1541 Geo-rep: some files(hardlinks) are missing in slave after mas... #1540 [RFE] Rebalance: suppurt migration to files with hardlinks (n... #1539 fuse mount crashes on graph-switch when reader-thread-count i... #1538 Need to configure optimum inode table hash_size for shd #1529 Fix regression in on demand migration feature #1526 Brick status is 'stopped' if socket file is absent but brick ... #1518 glusterfs: write operations fail when the size is equal or gr... #1516 Use of strchr glusterd_replace_slash_with_hyphen #1511 Crash due to memory allocation #1508 Add-brick with Increasing replica count fails with bad brick ... #1507 Time-to-completion mechansim in rebalance is broken #1506 tests/000-flaky/bugs_nfs_bug-1116503.t is crashed in in gf_me... #1499 why not use JumpConsistentHash to replace SuperFastHash to ch... #1497 Removing strlen and using the already existing len of data_t #1487 Quota accounting check script fails with UnicodeDecodeError #1483 Enable granular-entry-heal by default #1482 [Brick-mux] Attach several posix threads with glusterfs_ctx #1480 First letter in mount path of bricks are getting truncated fo... #1477 nfs server crashes in acl3svc_init #1476 Changes required at Snaphot as gluster-shared-storage mount p... #1475 gluster_shared_storage failed to automount on node reboot on ... #1472 Readdir-ahead leads to inconsistent ls results #1466 RPC handling latencies should be printed in statedump #1464 Avoid dict OR key (link-count) is NULL [Invalid argument] mes... #1459 gluster_shared_storage failed to automount on node reboot on ... #1453 Disperse shd heal activity should be observable #1442 Remove Glusterfs SELinux module from Distribution's selinux-p... #1440 glusterfs 7.7 fuse client memory leak #1438 syncdaemon/syncdutils.py: SyntaxWarning: \"is\" with a literal.... #1434 Inform failures while fop failed in disperse volume due to so... #1428 Redundant check in dict_get_with_refn() #1427 Bricks failed to restart after a power failure #1425 optimization over shard lookup in case of prealloc #1422 Rebalance - new volume option to turn on/off optimization in ... #1418 GlusterFS 8.0: Intermittent error:1408F10B:SSL routines:SSL3_... #1416 Dependencies of performance.parallel-readdir should be automa... #1410 01-georep-glusterd-tests.t times out on centos7 builders #1407 glusterd keep crashing when upgrading from 6.5 to 7.7 #1406 shared storage volume fails to mount in ipv6 environment #1404 Client side split-brain resolution using favourite-child-poli... #1403 Tests failure on C8: ./tests/features/ssl-ciphers.t #1401 quota_fsck.py throws TypeError #1400 Annotate synctasks with tsan API if --enable-tsan is requested #1399 Add xlator identifiers in statedumps for mem-pools #1398 io_uring support in glusterfs main branch #1397 glusterd_check_brick_order() is needlessly fetching volname, ... #1396 [bug-1851989] smallfile performance drops after commit the pa... #1395 optimize dict_serialized_length_lk function #1391 allow add-brick from nodes which are not part of auth.allow list #1385 High CPU utilization by self-heal on disperse volumes when an... #1383 Remove contrib/sunrpc/xdr_sizeof.c #1381 Optional FUSE notitications #1379 Fix NULL pointer #1378 Use better terminology and wording in the code #1377 Glustereventsd to accept not only IPv4 but IPv6 packets too. #1376 Runtime & Build Fixes for FreeBSD #1375 cluster: mount.glusterfs is stuck when trying to mount unknow... #1374 fuse interrupt issues identified in code review #1371 [RHEL 8.1] [Input/output error] observed in remove-brick oper... #1366 geo-replication session fails to start with IPV6 #1361 Screen .attribute directories on NetBSD #1359 Cleanup --disable-mempool #1357 options should display not only current values but also defau... #1356 cli: type mismatch global_quotad_rpc in cli-quotad-client.c #1355 Heal info desn't show split-brain info if halo is enabled #1354 High CPU utilization by self-heal on disperse volumes with no... #1353 errors seen with gluster v get all all #1352 api: libgfapi symbol versions break LTO in Fedora rawhide/f33 #1351 issue with gf_fill_iatt_for_dirent() #1350 Simplify directory scanning #1348 Fuse mount crashes in shard translator when truncating a *rea... #1347 NetBSD build fixes #1339 Rebalance status is not shown correctly after node reboot #1332 Unable to Upgrade to Gluster 7 from Earlier Version #1329 Move platform-dependent filesystem sync to a library function #1328 Linux kernel untar failed with errors immediate after add-brick #1327 Missing directory is not healed in dht #1324 Inconsistent custom xattr on backend directories after bringi... #1320 Unified support for building with sanitizers #1311 Data race when handling connection status #1310 tests/features/flock_interrupt.t leads to error logs #1306 add-brick command is failing #1303 Failures in rebalance due to [Input/output error] #1302 always print errno (and use English locale for strerror() out... #1291 Free volume info lock and mutex #1290 Test case brick-mux-validation-in-cluster.t is failing on RHEL-8 #1289 glustereventsd log file isn't reopened after rotation #1285 Use-after-destroy mutex error #1283 Undefined behavior in __builtin_ctz #1282 New file created with xattr \"trusted.glusterfs.dht\" #1281 Unlinking the file with open fd, returns ENOENT or stale file... #1279 Fix several signed integer overflows #1278 Fix memory leak in afr_priv_destroy() #1275 Make glusterfs compile on all recent and supported versions o... #1272 tests/bugs/glusterd/mgmt-handshake-and-volume-sync-post-glust... #1269 common-ha: ganesha-ha.sh bad test for {rhel,centos} for pcs o... #1263 Fix memory leak in glusterd_store_retrieve_bricks() #1260 Implement minimal proper synchronization for gf_attach #1259 Fix memory leak in gf_cli_gsync_status_output() #1258 dht: Add null check #1255 Improve snapshot clone error message #1254 Prioritize ENOSPC over other lesser priority errors #1253 On Ovirt setup glusterfs performs poorly #1250 geo-rep: Fix corner case in rename on mkdir during hybrid crawl #1249 Drop 'const' type qualifier on return type #1248 Fix thread naming and related convention #1245 Spurious failures in ./tests/basic/ec/ec-quorum-count.t #1243 Modify and return iatt (especially size and block-count) in s... #1242 Rebalance - Improve Crawl time in rebalance #1240 tests/basic/afr/gfid-mismatch-resolution-with-fav-child-polic... #1236 glusterfs-geo-replication requires policycoreutils-python-uti... #1234 Fix ./tests/basic/fencing/afr-lock-heal-basic.t failure #1230 core dumped executing tests/line-coverage/errorgen-coverage.t #1228 seek functionalty is broken #1226 Gluster webhook update throws error #1225 fuse causes glusterd to dump core #1223 Failure of tests/basic/gfapi/gfapi-copy-file-range.t #1222 [RFE] Improve rebalance of sparse files #1221 features/bit-rot: invalid snprintf() buffer size #1220 cluster/ec: return correct error code and log the message in ... #1218 dht: Do opendir selectively in gf_defrag_process_dir #1217 Modify group \"virt\" to add rpc/network related changes #1214 Running tests/basic/afr/inodelk.t on my VM crashes in dht #1211 AFR: Rename dir heal shouldn't delete the directory at oldloc... #1209 tests: georep-upgrade.t test failure #1208 warning: initializer overrides prior initialization of this s... #1207 warning: passing an object that undergoes default argument pr... #1204 GD_OP_VERSION needs to be updated #1202 Issues reported by Coverity static analysis tool #1200 Handle setxattr and rm race for directory in rebalance #1197 Geo-replication tests are spuriously failing in CI #1196 glusterfsd is having a leak while only mgmt SSL is enabled #1193 Scheduling of snapshot for a volume is failing to create snap... #1190 spurious failure of tests/basic/quick-read-with-upcall.t #1187 Failures in rebalance due to [No space left on device] error ... #1182 geo-rep requires relevant selinux permission for rsync #1179 gnfs split brain when 1 server in 3x1 down (high load) #1172 core, cli, quota: cleanup malloc debugging and stats #1169 common-ha: cluster status shows \"FAILOVER\" even when all node... #1164 migrate remove-brick operation to mgmt v3 frameowrk #1154 failing test cases #1135 Fix @sysconfdir@ expansion in extras/systemd/glusterd.service... #1126 packaging: overhaul glusterfs.spec(.in) to align with SUSE an... #1116 [bug:1790736] gluster volume list returning wrong volume list... #1101 [bug:1813029] volume brick fails to come online because other... #1097 [bug:1635688] Keep only the valid (maintained/supported) comp... #1096 [bug:1622665] clang-scan report: glusterfs issues #1075 [bug:1299203] resolve-gids is not needed for Linux kernels v3... #1072 [bug:1251614] gf_defrag_fix_layout recursively fails, distrac... #1060 [bug:789278] Issues reported by Coverity static analysis tool #1052 [bug:1693692] Increase code coverage from regression tests #1050 [bug:1787325] TLS/SSL access of GlusterFS mounts is slower th... #1047 [bug:1774379] check for same hostnames(bricks from same host/... #1043 [bug:1793490] snapshot clone volume is not exported via NFS-G... #1009 [bug:1756900] tests are failing in RHEL8 regression #1002 [bug:1679998] GlusterFS can be improved #1000 [bug:1193929] GlusterFS can be improved #990 [bug:1578405] EIO errors when updating and deleting entries c... #952 [bug:1589705] quick-read: separate performance.cache-size tun... #876 [bug:1797099] After upgrade from gluster 7.0 to 7.2 posix-acl... #874 [bug:1793390] Pre-validation failure does not provide any hin... #837 Indicate timezone offset in formatted timestamps #829 gfapi: Using ssl and glfs_set_volfile together does not work #827 undefined symbol: xlator_api #824 Migrate bugzilla workflow to github issues workflow #816 RFE: Data/MetaData separator Translator #790 infinite loop in common-utils.c - gf_rev_dns_lookup_cache() ? #763 thin-arbiter: Testing report","title":"Bugs addressed"},{"location":"release-notes/9.1/","text":"Release notes for Gluster 9.1 Release date: 05-Apr-2021 This is a bugfix and improvement release. The release notes for 9.0 contain a listing of all the new features that were added and bugs fixed in the GlusterFS 9 stable release. NOTE: - Next minor release tentative date: Week of 30th Apr, 2021 - Users are highly encouraged to upgrade to newer releases of GlusterFS. Highlights of Release Provide autoconf option to enable/disable storage.linux-io_uring during compilation #2063 Healing data in 1MB chunks instead of 128KB for improving healing performance #2067 Builds are available at https://download.gluster.org/pub/gluster/glusterfs/9/9.1/ Issues addressed in this release Please find the list of issues added to this release below. #1406 shared storage volume fails to mount in ipv6 environment #1991 mdcache: bug causes getxattr() to report ENODATA when fetchin... #2063 Provide autoconf option to enable/disable storage.linux-io_ur... #2067 Change self-heal-window-size to 1MB by default #2107 mount crashes when setfattr -n distribute.fix.layout -v \"yes\"... #2154 \"Operation not supported\" doing a chmod on a symlink #2192 4+1 arbiter setup is broken #2198 There are blocked inodelks for a long time #2234 Segmentation fault in directory quota daemon for replicated v...","title":9.1},{"location":"release-notes/9.1/#release-notes-for-gluster-91","text":"Release date: 05-Apr-2021 This is a bugfix and improvement release. The release notes for 9.0 contain a listing of all the new features that were added and bugs fixed in the GlusterFS 9 stable release. NOTE: - Next minor release tentative date: Week of 30th Apr, 2021 - Users are highly encouraged to upgrade to newer releases of GlusterFS.","title":"Release notes for Gluster 9.1"},{"location":"release-notes/9.1/#highlights-of-release","text":"Provide autoconf option to enable/disable storage.linux-io_uring during compilation #2063 Healing data in 1MB chunks instead of 128KB for improving healing performance #2067","title":"Highlights of Release"},{"location":"release-notes/9.1/#builds-are-available-at","text":"https://download.gluster.org/pub/gluster/glusterfs/9/9.1/","title":"Builds are available at"},{"location":"release-notes/9.1/#issues-addressed-in-this-release","text":"Please find the list of issues added to this release below. #1406 shared storage volume fails to mount in ipv6 environment #1991 mdcache: bug causes getxattr() to report ENODATA when fetchin... #2063 Provide autoconf option to enable/disable storage.linux-io_ur... #2067 Change self-heal-window-size to 1MB by default #2107 mount crashes when setfattr -n distribute.fix.layout -v \"yes\"... #2154 \"Operation not supported\" doing a chmod on a symlink #2192 4+1 arbiter setup is broken #2198 There are blocked inodelks for a long time #2234 Segmentation fault in directory quota daemon for replicated v...","title":"Issues addressed in this release"},{"location":"release-notes/9.2/","text":"Release notes for Gluster 9.2 Release date: 17-May-2021 This is a bugfix and improvement release. The release notes for 9.0 , 9.1 contain a listing of all the new features that were added and bugs fixed in the GlusterFS 9 stable release. NOTE: - Next minor release tentative date: Week of 30th Jun, 2021 - Users are highly encouraged to upgrade to newer releases of GlusterFS. Important fixes in this release After upgrade on release 9.1 glusterd protocol is broken #2351 Disable lookup-optimize by default in the virt group #2253 Builds are available at https://download.gluster.org/pub/gluster/glusterfs/9/9.2/ Issues addressed in this release #1909 core: Avoid several dict OR key is NULL message in brick logs #2161 Crash caused by memory corruption #2232 \"Invalid argument\" when reading a directory with gfapi #2253 Disable lookup-optimize by default in the virt group #2313 Long setting names mess up the columns and break parsing #2337 memory leak observed in lock fop #2351 After upgrade on release 9.1 glusterd protocol is broken #2353 Permission issue after upgrading to Gluster v9.1","title":9.2},{"location":"release-notes/9.2/#release-notes-for-gluster-92","text":"Release date: 17-May-2021 This is a bugfix and improvement release. The release notes for 9.0 , 9.1 contain a listing of all the new features that were added and bugs fixed in the GlusterFS 9 stable release. NOTE: - Next minor release tentative date: Week of 30th Jun, 2021 - Users are highly encouraged to upgrade to newer releases of GlusterFS.","title":"Release notes for Gluster 9.2"},{"location":"release-notes/9.2/#important-fixes-in-this-release","text":"After upgrade on release 9.1 glusterd protocol is broken #2351 Disable lookup-optimize by default in the virt group #2253","title":"Important fixes in this release"},{"location":"release-notes/9.2/#builds-are-available-at","text":"https://download.gluster.org/pub/gluster/glusterfs/9/9.2/","title":"Builds are available at"},{"location":"release-notes/9.2/#issues-addressed-in-this-release","text":"#1909 core: Avoid several dict OR key is NULL message in brick logs #2161 Crash caused by memory corruption #2232 \"Invalid argument\" when reading a directory with gfapi #2253 Disable lookup-optimize by default in the virt group #2313 Long setting names mess up the columns and break parsing #2337 memory leak observed in lock fop #2351 After upgrade on release 9.1 glusterd protocol is broken #2353 Permission issue after upgrading to Gluster v9.1","title":"Issues addressed in this release"},{"location":"release-notes/9.3/","text":"Release notes for Gluster 9.3 Release date: 15-Jul-2021 This is a bugfix and improvement release. The release notes for 9.0 , 9.1 , 9.2 contain a listing of all the new features that were added and bugs fixed in the GlusterFS 9 stable release. NOTE: - Next minor release tentative date: Week of 30th Aug, 2021 - Users are highly encouraged to upgrade to newer releases of GlusterFS. Important fixes in this release Core dumps on Gluster 9 - 3 replicas #2443 geo-rep: Improve handling of gfid mismatches #2423 auth.allow list is corrupted after add-brick (buffer overflow?) #2524 Builds are available at https://download.gluster.org/pub/gluster/glusterfs/9/9.3/ Issues addressed in this release #705 gf_backtrace_save inefficiencies #1000 [bug:1193929] GlusterFS can be improved #1384 mount glusterfs volume, files larger than 64Mb only show 64Mb #2388 Geo-replication gets delayed when there are many renames on primary #2394 Spurious failure in tests/basic/fencing/afr-lock-heal-basic.t #2398 Bitrot and scrub process showed like unknown in the gluster volume #2421 rsync should not try to sync internal xattrs. #2440 Geo-replication not working on Ubuntu 21.04 #2443 Core dumps on Gluster 9 - 3 replicas #2470 sharding: [inode.c:1255:__inode_unlink] 0-inode: dentry not found #2524 auth.allow list is corrupted after add-brick (buffer overflow?)","title":9.3},{"location":"release-notes/9.3/#release-notes-for-gluster-93","text":"Release date: 15-Jul-2021 This is a bugfix and improvement release. The release notes for 9.0 , 9.1 , 9.2 contain a listing of all the new features that were added and bugs fixed in the GlusterFS 9 stable release. NOTE: - Next minor release tentative date: Week of 30th Aug, 2021 - Users are highly encouraged to upgrade to newer releases of GlusterFS.","title":"Release notes for Gluster 9.3"},{"location":"release-notes/9.3/#important-fixes-in-this-release","text":"Core dumps on Gluster 9 - 3 replicas #2443 geo-rep: Improve handling of gfid mismatches #2423 auth.allow list is corrupted after add-brick (buffer overflow?) #2524","title":"Important fixes in this release"},{"location":"release-notes/9.3/#builds-are-available-at","text":"https://download.gluster.org/pub/gluster/glusterfs/9/9.3/","title":"Builds are available at"},{"location":"release-notes/9.3/#issues-addressed-in-this-release","text":"#705 gf_backtrace_save inefficiencies #1000 [bug:1193929] GlusterFS can be improved #1384 mount glusterfs volume, files larger than 64Mb only show 64Mb #2388 Geo-replication gets delayed when there are many renames on primary #2394 Spurious failure in tests/basic/fencing/afr-lock-heal-basic.t #2398 Bitrot and scrub process showed like unknown in the gluster volume #2421 rsync should not try to sync internal xattrs. #2440 Geo-replication not working on Ubuntu 21.04 #2443 Core dumps on Gluster 9 - 3 replicas #2470 sharding: [inode.c:1255:__inode_unlink] 0-inode: dentry not found #2524 auth.allow list is corrupted after add-brick (buffer overflow?)","title":"Issues addressed in this release"},{"location":"release-notes/9.4/","text":"Release notes for Gluster 9.4 Release date: 14-Oct-2021 This is a bugfix and improvement release. The release notes for 9.0 , 9.1 , 9.2 , 9.3 contain a listing of all the new features that were added and bugs fixed in the GlusterFS 9 stable release. NOTE: - Next minor release tentative date: Week of 30th Dec, 2021 - Users are highly encouraged to upgrade to newer releases of GlusterFS. Important fixes in this release Fix changelog History Crawl resume failures after stop #2133 Fix Stack overflow when parallel-readdir is enabled #2169 Fix rebalance crashes in dht #2239 Builds are available at - https://download.gluster.org/pub/gluster/glusterfs/9/9.4/ Issues addressed in this release #2133 changelog History Crawl resume fails after stop #2169 Stack overflow when parallel-readdir is enabled #2239 rebalance crashes in dht on master #2625 auth.allow value is corrupted after add-brick operation #2649 glustershd failed in bind with error \"Address already in use\" #2659 tests/basic/afr/afr-anon-inode.t crashed #2754 It takes a long time to execute the \u201cgluster volume set volumename #2798 FUSE mount option for localtime-logging is not exposed #2690 glusterd: reset mgmt_v3_lock_timeout after it be used #2691 georep-upgrade.t find failures #1101 volume brick fails to come online because other process is using port 49152","title":9.4},{"location":"release-notes/9.4/#release-notes-for-gluster-94","text":"Release date: 14-Oct-2021 This is a bugfix and improvement release. The release notes for 9.0 , 9.1 , 9.2 , 9.3 contain a listing of all the new features that were added and bugs fixed in the GlusterFS 9 stable release. NOTE: - Next minor release tentative date: Week of 30th Dec, 2021 - Users are highly encouraged to upgrade to newer releases of GlusterFS.","title":"Release notes for Gluster 9.4"},{"location":"release-notes/9.4/#important-fixes-in-this-release","text":"Fix changelog History Crawl resume failures after stop #2133 Fix Stack overflow when parallel-readdir is enabled #2169 Fix rebalance crashes in dht #2239","title":"Important fixes in this release"},{"location":"release-notes/9.4/#builds-are-available-at-","text":"https://download.gluster.org/pub/gluster/glusterfs/9/9.4/","title":"Builds are available at -"},{"location":"release-notes/9.4/#issues-addressed-in-this-release","text":"#2133 changelog History Crawl resume fails after stop #2169 Stack overflow when parallel-readdir is enabled #2239 rebalance crashes in dht on master #2625 auth.allow value is corrupted after add-brick operation #2649 glustershd failed in bind with error \"Address already in use\" #2659 tests/basic/afr/afr-anon-inode.t crashed #2754 It takes a long time to execute the \u201cgluster volume set volumename #2798 FUSE mount option for localtime-logging is not exposed #2690 glusterd: reset mgmt_v3_lock_timeout after it be used #2691 georep-upgrade.t find failures #1101 volume brick fails to come online because other process is using port 49152","title":"Issues addressed in this release"},{"location":"release-notes/9.5/","text":"Release notes for Gluster 9.5 Release date: 1st-Feb-2021 This is a bugfix and improvement release. The release notes for 9.0 , 9.1 , 9.2 , 9.3 , 9.4 contain a listing of all the new features that were added and bugs fixed in the GlusterFS 9 stable release. NOTE: - Next minor release tentative date: Week of 20th Apr, 2022 - Users are highly encouraged to upgrade to newer releases of GlusterFS. Important fixes in this release Fix rebalance of sparse files (https://github.com/gluster/glusterfs/issues/2317) Fix anomalous brick offline scenario on non rebooted node by preventing bricks from connecting to a backup volfile (https://github.com/gluster/glusterfs/issues/2480) Builds are available at - https://download.gluster.org/pub/gluster/glusterfs/9/9.5/ Issues addressed in this release #2317 Fix rebalance of sparse files #2414 Prefer mallinfo2() to mallinfo() if available #2467 Handle failure in fuse to get gids gracefully #2480 Prevent bricks from connecting to a backup volfile and fix brick offline scenario on non rebooted node #2846 Avoid redundant logs in glusterd at info level #2903 Fix worker disconnect due to AttributeError in geo-replication #2939 Remove the deprecated commands from gluster man page","title":9.5},{"location":"release-notes/9.5/#release-notes-for-gluster-95","text":"Release date: 1st-Feb-2021 This is a bugfix and improvement release. The release notes for 9.0 , 9.1 , 9.2 , 9.3 , 9.4 contain a listing of all the new features that were added and bugs fixed in the GlusterFS 9 stable release. NOTE: - Next minor release tentative date: Week of 20th Apr, 2022 - Users are highly encouraged to upgrade to newer releases of GlusterFS.","title":"Release notes for Gluster 9.5"},{"location":"release-notes/9.5/#important-fixes-in-this-release","text":"Fix rebalance of sparse files (https://github.com/gluster/glusterfs/issues/2317) Fix anomalous brick offline scenario on non rebooted node by preventing bricks from connecting to a backup volfile (https://github.com/gluster/glusterfs/issues/2480)","title":"Important fixes in this release"},{"location":"release-notes/9.5/#builds-are-available-at-","text":"https://download.gluster.org/pub/gluster/glusterfs/9/9.5/","title":"Builds are available at -"},{"location":"release-notes/9.5/#issues-addressed-in-this-release","text":"#2317 Fix rebalance of sparse files #2414 Prefer mallinfo2() to mallinfo() if available #2467 Handle failure in fuse to get gids gracefully #2480 Prevent bricks from connecting to a backup volfile and fix brick offline scenario on non rebooted node #2846 Avoid redundant logs in glusterd at info level #2903 Fix worker disconnect due to AttributeError in geo-replication #2939 Remove the deprecated commands from gluster man page","title":"Issues addressed in this release"},{"location":"release-notes/geo-rep-in-3.7/","text":"Improved Node fail-over issues handling by using Gluster Meta Volume In replica pairs one Geo-rep worker should be active and all the other replica workers should be passive. When Active worker goes down, Passive worker will become active. In previous releases, this logic was based on node-uuid, but now it is based on Lock file in Meta Volume. Now it is possible to decide Active/Passive more accurately and multiple Active worker scenarios minimized. Geo-rep works without Meta Volume also, this feature is backward compatible. By default config option use_meta_volume is False. This feature can be turned on with geo-rep config use_meta_volume true. Without this feature Geo-rep works as it was working in previous releases. Issues if meta_volume is turned off: Multiple workers becoming active and participate in syncing. Duplicate efforts and all the issues related to concurrent execution exists. Failover only works at node level, if a brick process goes down but node is alive then fail-back will not happen and delay in syncing. Very difficult documented steps about placements of bricks in case of replica 3. For example, first brick in each replica should not be placed in same node. etc. Consuming Changelogs from previously failed node when it comes back, which may lead to issues like delayed syncing and data inconsistencies in case of Renames. Fixes : 1196632 , 1217939 Improved Historical Changelogs consumption Support for consuming Historical Changelogs introduced in previous releases, with this release this is more stable and improved. Use of Filesystem crawl is minimized and limited only during initial sync.In previous release, Node reboot or brick process going down was treated as Changelog Breakage and Geo-rep was fallback to XSync for that duration. With this release, Changelog session will be considered broken only if Changelog is turned off. All the other scenarios considered as safe. This feature is also required by glusterfind. Fixes : 1217944 Improved Status and Checkpoint Status got many improvements, Showing accurate details of Session info, User info, Slave node to which master node is connected, Last Synced Time etc. Initializing time is reduced, Status change will happen as soon as geo-rep workers ready.(In previous releases Initializing time was 60 sec) Fixes : 1212410 Worker Restart improvements Workers going down and coming back is very common in geo-rep for reasons like network failure, Slave node going down etc. When it comes up it has to reprocess the changelogs again because worker died before updating the last sync time. The batch size is now optimized such that the amount of reprocess is minimized. Fixes : 1210965 Improved RENAME handling When renamed filename hash falls to other brick, respective brick's changelog records RENAME, but rest of the fops like CREATE, DATA are recorded in first brick. Each Geo-rep worker per brick syncs data to Slave Volume independently, These things go out of order and Master and Slave Volume become inconsistent. With the help of DHT team, RENAMEs are recorded where CREATE and DATA are recorded. Fixes : 1141379 Syncing xattrs and acls Syncing both xattrs and acls to Slave cluster are now supported. These can be disabled setting config options sync-xattrs or sync-acls to false. Fixes : 1187021 , 1196690 Identifying Entry failures Logging improvements to identify exact reason for Entry failures, GFID conflicts, I/O errors etc. Safe errors are not logged in Mount logs in Slave, Safe errors are post processed and only genuine errors are logged in Master logs. Fixes : 1207115 , 1210562 Improved rm -rf issues handling Successive deletes and creates had issues, Handling these issues minimized. (Not completely fixed since it depends on Open issues of DHT) Fixes : 1211037 Non root Geo-replication simplified Manual editing of Glusterd vol file is simplified by introducing gluster system:: mountbroker command Fixes : 1136312 Logging Rsync performance on request basis Rsync performance can be evaluated by enabling a config option. After this Geo-rep starts recording rsync performance in log file, which can be post processed to get meaningful metrics. Fixes : 764827 Initial sync issues due to upper limit comparison during Filesystem Crawl Bug fix, Fixed wrong logic in Xsync Change detection. Upper limit was considered during xsync crawl. Geo-rep XSync was missing many files considering Changelog will take care. But Changelog will not have complete details of the files created before enabling Geo-replication. When rsync/tarssh fails, geo-rep is now capable of identifying safe errors and continue syncing by ignoring those issues. For example, rsync fails to sync a file which is deleted in master during sync. This can be ignored since the file is unlinked and no need to try syncing. Fixes : 1200733 Changelog failures and Brick failures handling When Brick process goes down, or any Changelog exception Geo-rep worker was failing back to XSync crawl. Which was bad since Xsync fails to identify Deletes and Renames. Now this is prevented, worker goes to Faulty and wait for that Brick process to comeback. Fixes : 1202649 Archive Changelogs in working directory after processing Archive Changelogs after processing not generate empty changelogs when no data is available. This is great improvement in terms of reducing the inode consumption in Brick. Fixes : 1169331 Virtual xattr to trigger sync Since we use Historical Changelogs when Geo-rep worker restarts. Only SETATTR will be recorded when we touch a file. In previous versions, Re triggering a file sync is stop geo-rep, touch files and start geo-replication. Now touch will not help since it records only SETATTR . Virtual Xattr is introduced to retrigger the sync. No Geo-rep restart required. Fixes : 1176934 SSH Keys overwrite issues during Geo-rep create Parallel creates or multiple Geo-rep session creation was overwriting the pem keys written by first one. This leads to connectivity issues when Geo-rep is started. Fixes : 1183229 Ownership sync improvements Geo-rep was failing to sync ownership information from master cluster to Slave cluster. Fixes : 1104954 Slave node failover handling improvements When slave node goes down, Master worker which is connected to that brick will go to faulty. Now it tries to connect to another slave node instead of waiting for that Slave node to come back. Fixes : 1151412 Support of ssh keys custom location If ssh authorized_keys are configured in non standard location instead of default $HOME/.ssh/authorized_keys . Geo-rep create was failing, now this is supported. Fixes : 1181117","title":"Geo rep in 3.7"},{"location":"release-notes/geo-rep-in-3.7/#improved-node-fail-over-issues-handling-by-using-gluster-meta-volume","text":"In replica pairs one Geo-rep worker should be active and all the other replica workers should be passive. When Active worker goes down, Passive worker will become active. In previous releases, this logic was based on node-uuid, but now it is based on Lock file in Meta Volume. Now it is possible to decide Active/Passive more accurately and multiple Active worker scenarios minimized. Geo-rep works without Meta Volume also, this feature is backward compatible. By default config option use_meta_volume is False. This feature can be turned on with geo-rep config use_meta_volume true. Without this feature Geo-rep works as it was working in previous releases. Issues if meta_volume is turned off: Multiple workers becoming active and participate in syncing. Duplicate efforts and all the issues related to concurrent execution exists. Failover only works at node level, if a brick process goes down but node is alive then fail-back will not happen and delay in syncing. Very difficult documented steps about placements of bricks in case of replica 3. For example, first brick in each replica should not be placed in same node. etc. Consuming Changelogs from previously failed node when it comes back, which may lead to issues like delayed syncing and data inconsistencies in case of Renames. Fixes : 1196632 , 1217939","title":"Improved Node fail-over issues handling by using Gluster Meta Volume"},{"location":"release-notes/geo-rep-in-3.7/#improved-historical-changelogs-consumption","text":"Support for consuming Historical Changelogs introduced in previous releases, with this release this is more stable and improved. Use of Filesystem crawl is minimized and limited only during initial sync.In previous release, Node reboot or brick process going down was treated as Changelog Breakage and Geo-rep was fallback to XSync for that duration. With this release, Changelog session will be considered broken only if Changelog is turned off. All the other scenarios considered as safe. This feature is also required by glusterfind. Fixes : 1217944","title":"Improved Historical Changelogs consumption"},{"location":"release-notes/geo-rep-in-3.7/#improved-status-and-checkpoint","text":"Status got many improvements, Showing accurate details of Session info, User info, Slave node to which master node is connected, Last Synced Time etc. Initializing time is reduced, Status change will happen as soon as geo-rep workers ready.(In previous releases Initializing time was 60 sec) Fixes : 1212410","title":"Improved Status and Checkpoint"},{"location":"release-notes/geo-rep-in-3.7/#worker-restart-improvements","text":"Workers going down and coming back is very common in geo-rep for reasons like network failure, Slave node going down etc. When it comes up it has to reprocess the changelogs again because worker died before updating the last sync time. The batch size is now optimized such that the amount of reprocess is minimized. Fixes : 1210965","title":"Worker Restart improvements"},{"location":"release-notes/geo-rep-in-3.7/#improved-rename-handling","text":"When renamed filename hash falls to other brick, respective brick's changelog records RENAME, but rest of the fops like CREATE, DATA are recorded in first brick. Each Geo-rep worker per brick syncs data to Slave Volume independently, These things go out of order and Master and Slave Volume become inconsistent. With the help of DHT team, RENAMEs are recorded where CREATE and DATA are recorded. Fixes : 1141379","title":"Improved RENAME handling"},{"location":"release-notes/geo-rep-in-3.7/#syncing-xattrs-and-acls","text":"Syncing both xattrs and acls to Slave cluster are now supported. These can be disabled setting config options sync-xattrs or sync-acls to false. Fixes : 1187021 , 1196690","title":"Syncing xattrs and acls"},{"location":"release-notes/geo-rep-in-3.7/#identifying-entry-failures","text":"Logging improvements to identify exact reason for Entry failures, GFID conflicts, I/O errors etc. Safe errors are not logged in Mount logs in Slave, Safe errors are post processed and only genuine errors are logged in Master logs. Fixes : 1207115 , 1210562","title":"Identifying Entry failures"},{"location":"release-notes/geo-rep-in-3.7/#improved-rm-rf-issues-handling","text":"Successive deletes and creates had issues, Handling these issues minimized. (Not completely fixed since it depends on Open issues of DHT) Fixes : 1211037","title":"Improved rm -rf issues handling"},{"location":"release-notes/geo-rep-in-3.7/#non-root-geo-replication-simplified","text":"Manual editing of Glusterd vol file is simplified by introducing gluster system:: mountbroker command Fixes : 1136312","title":"Non root Geo-replication simplified"},{"location":"release-notes/geo-rep-in-3.7/#logging-rsync-performance-on-request-basis","text":"Rsync performance can be evaluated by enabling a config option. After this Geo-rep starts recording rsync performance in log file, which can be post processed to get meaningful metrics. Fixes : 764827","title":"Logging Rsync performance on request basis"},{"location":"release-notes/geo-rep-in-3.7/#initial-sync-issues-due-to-upper-limit-comparison-during-filesystem-crawl","text":"Bug fix, Fixed wrong logic in Xsync Change detection. Upper limit was considered during xsync crawl. Geo-rep XSync was missing many files considering Changelog will take care. But Changelog will not have complete details of the files created before enabling Geo-replication. When rsync/tarssh fails, geo-rep is now capable of identifying safe errors and continue syncing by ignoring those issues. For example, rsync fails to sync a file which is deleted in master during sync. This can be ignored since the file is unlinked and no need to try syncing. Fixes : 1200733","title":"Initial sync issues due to upper limit comparison during Filesystem Crawl"},{"location":"release-notes/geo-rep-in-3.7/#changelog-failures-and-brick-failures-handling","text":"When Brick process goes down, or any Changelog exception Geo-rep worker was failing back to XSync crawl. Which was bad since Xsync fails to identify Deletes and Renames. Now this is prevented, worker goes to Faulty and wait for that Brick process to comeback. Fixes : 1202649","title":"Changelog failures and Brick failures handling"},{"location":"release-notes/geo-rep-in-3.7/#archive-changelogs-in-working-directory-after-processing","text":"Archive Changelogs after processing not generate empty changelogs when no data is available. This is great improvement in terms of reducing the inode consumption in Brick. Fixes : 1169331","title":"Archive Changelogs in working directory after processing"},{"location":"release-notes/geo-rep-in-3.7/#virtual-xattr-to-trigger-sync","text":"Since we use Historical Changelogs when Geo-rep worker restarts. Only SETATTR will be recorded when we touch a file. In previous versions, Re triggering a file sync is stop geo-rep, touch files and start geo-replication. Now touch will not help since it records only SETATTR . Virtual Xattr is introduced to retrigger the sync. No Geo-rep restart required. Fixes : 1176934","title":"Virtual xattr to trigger sync"},{"location":"release-notes/geo-rep-in-3.7/#ssh-keys-overwrite-issues-during-geo-rep-create","text":"Parallel creates or multiple Geo-rep session creation was overwriting the pem keys written by first one. This leads to connectivity issues when Geo-rep is started. Fixes : 1183229","title":"SSH Keys overwrite issues during Geo-rep create"},{"location":"release-notes/geo-rep-in-3.7/#ownership-sync-improvements","text":"Geo-rep was failing to sync ownership information from master cluster to Slave cluster. Fixes : 1104954","title":"Ownership sync improvements"},{"location":"release-notes/geo-rep-in-3.7/#slave-node-failover-handling-improvements","text":"When slave node goes down, Master worker which is connected to that brick will go to faulty. Now it tries to connect to another slave node instead of waiting for that Slave node to come back. Fixes : 1151412","title":"Slave node failover handling improvements"},{"location":"release-notes/geo-rep-in-3.7/#support-of-ssh-keys-custom-location","text":"If ssh authorized_keys are configured in non standard location instead of default $HOME/.ssh/authorized_keys . Geo-rep create was failing, now this is supported. Fixes : 1181117","title":"Support of ssh keys custom location"},{"location":"release-notes/glusterfs-selinux2.0.1/","text":"Release notes for glusterfs-selinux 2.0.1 This is a bugfix and improvement release. Important fixes in this release #rhbz1955415 glusterfs-selinux package should own the files created by it #20 Fixing verification failure for ghost #rhbz1779052 Adds rule to allow glusterd to access RDMA socket Issues addressed in this release #rhbz1955415 glusterfs-selinux package should own the files created by it #22 Fixed mixed use of tabs and spaces (rpmlint warning) #20 Fixing verification failure for ghost file #rhbz1779052 Adds rule to allow glusterd to access RDMA socket #15 Modifying the path provided for glustereventsd .py","title":"Release notes for glusterfs-selinux 2.0.1"},{"location":"release-notes/glusterfs-selinux2.0.1/#release-notes-for-glusterfs-selinux-201","text":"This is a bugfix and improvement release.","title":"Release notes for glusterfs-selinux 2.0.1"},{"location":"release-notes/glusterfs-selinux2.0.1/#important-fixes-in-this-release","text":"#rhbz1955415 glusterfs-selinux package should own the files created by it #20 Fixing verification failure for ghost #rhbz1779052 Adds rule to allow glusterd to access RDMA socket","title":"Important fixes in this release"},{"location":"release-notes/glusterfs-selinux2.0.1/#issues-addressed-in-this-release","text":"#rhbz1955415 glusterfs-selinux package should own the files created by it #22 Fixed mixed use of tabs and spaces (rpmlint warning) #20 Fixing verification failure for ghost file #rhbz1779052 Adds rule to allow glusterd to access RDMA socket #15 Modifying the path provided for glustereventsd .py","title":"Issues addressed in this release"}]}